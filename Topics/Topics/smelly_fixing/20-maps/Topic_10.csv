Topic_no,Keywords,Contrib,System,Text
10,"file, option, fbshipit_source, user, time, create, comment_unuse, operation, implementation, revision, methods_lambda, handle_fixe, igorsugak_differential, fbcode_case, clang_tidy, summary_use, tool_failed, parameters_function, read, function",0.212,conscrypt,"Use duck typing for checkServerTrusted Instead of casting to the platform TrustManagerImpl to call checkServerTrusted(X509Certificate[], String, String) use reflection to instead lookup the method on the X509TrustManager and use it if present otherwise fall back to checkServerTrusted(X509Certificate[], String). Change-Id: Ia9893cc4ac0e9f844246624723ae9cb33101e85b/Merge AOSP version of Conscrypt into mnc-ub-dev This adds support for the latest BoringSSL revision and fixes quite a few bugs. Change-Id: I6eafb04d7c15d3b365191d1fe2fe107308cea894/Move BlockGuard and CloseGuard to Platform This was causing issues on Gingerbread devices since CloseGuard was not in that release yet. Move them out to Platform so we can filter on release when we decide whether to instantiate or not. (cherry picked from commit 126ec77aacd58c8b5d62d433af65386fa3dd3fc1) Bug: 24607028 Change-Id: Iba0bbb0b878076319ace40f848aa5e307e2c3ad8/Move BlockGuard and CloseGuard to Platform This was causing issues on Gingerbread devices since CloseGuard was not in that release yet. Move them out to Platform so we can filter on release when we decide whether to instantiate or not. Bug: 24607028 Change-Id: Iba0bbb0b878076319ace40f848aa5e307e2c3ad8/"
,,0.0848,conscrypt,Merge AOSP version of Conscrypt into mnc-ub-dev This adds support for the latest BoringSSL revision and fixes quite a few bugs. Change-Id: I6eafb04d7c15d3b365191d1fe2fe107308cea894/
,,0.2168,conscrypt,"Use duck typing for checkServerTrusted Instead of casting to the platform TrustManagerImpl to call checkServerTrusted(X509Certificate[], String, String) use reflection to instead lookup the method on the X509TrustManager and use it if present otherwise fall back to checkServerTrusted(X509Certificate[], String). Change-Id: Ia9893cc4ac0e9f844246624723ae9cb33101e85b/Merge AOSP version of Conscrypt into mnc-ub-dev This adds support for the latest BoringSSL revision and fixes quite a few bugs. Change-Id: I6eafb04d7c15d3b365191d1fe2fe107308cea894/Move BlockGuard and CloseGuard to Platform This was causing issues on Gingerbread devices since CloseGuard was not in that release yet. Move them out to Platform so we can filter on release when we decide whether to instantiate or not. (cherry picked from commit 126ec77aacd58c8b5d62d433af65386fa3dd3fc1) Bug: 24607028 Change-Id: Iba0bbb0b878076319ace40f848aa5e307e2c3ad8/Move BlockGuard and CloseGuard to Platform This was causing issues on Gingerbread devices since CloseGuard was not in that release yet. Move them out to Platform so we can filter on release when we decide whether to instantiate or not. Bug: 24607028 Change-Id: Iba0bbb0b878076319ace40f848aa5e307e2c3ad8/"
,,0.3425,conscrypt,"Unbreak Conscrypt when its built against OpenSSL. (cherry picked from commit 179e1d5f63fdc69f35e755d3fe34f9e93168a71f) Change-Id: I28c6f123fa3bbe084bdee4a4615a67ca7319234e/Make EVP_DigestSignFinal throw when signature too big. This fixes NativeCrypto.EVP_DigestSignFinal to unconditionally throw a RuntimeException when the generated signature is larger than expected. Change-Id: I56d77adbb0cbc004d941a2cfcb30482450a1ddbf/Unbreak Conscrypt when its built against OpenSSL. Change-Id: I28c6f123fa3bbe084bdee4a4615a67ca7319234e/Revert ""external/conscrypt: drop BORINGSSL_201510 ifdefs."" This breaks the unbundled build with OpenSSL. This reverts commit f1754f30b818d8e67800322f24e8f6c623890861. (cherry picked from commit da909c2a58445e7a9b623196ad5f7bc88ad7c0e8) Change-Id: I2f3098414004aeb3f019c28bf0a0085a9d7eb3cc/Clear BoringSSL error queue in NativeCrypto.EVP_DigestVerifyFinal. am: 881c0953d7 * commit 881c0953d7166a5020ba2ec2b6f8c39371a6ded6: Clear BoringSSL error queue in NativeCrypto.EVP_DigestVerifyFinal./Revert ""external/conscrypt: drop BORINGSSL_201510 ifdefs."" This breaks the unbundled build with OpenSSL. This reverts commit f1754f30b818d8e67800322f24e8f6c623890861. Change-Id: I2f3098414004aeb3f019c28bf0a0085a9d7eb3cc/Clear BoringSSL error queue in NativeCrypto.EVP_DigestVerifyFinal. This fixes a bug introduced in b4345a619c1f34e2390210d11476a8619cebd695 where NativeCrypto.EVP_DigestVerifyFinal left the BAD_SIGNATURE error in the BoringSSL error queue when a signature did not verify. Some of the following NativeCrypto operations would then fail because they assumed that it was their BoringSSL calls that generated the BAD_SIGNATURE error. The fix is to unconditionally clear the BoringSSL error queue at the end of NativeCrypto.EVP_DigestVerifyFinal, same as its predecessor NativeCrypto.EVP_VerifyFinal did. Change-Id: I0d092b1b39afa3c6d19a785cbf7dd311ffcd4c04/Merge ""Speed up digesting by avoiding unnecessary operations."" am: 22324dd963 * commit 22324dd9635b9a7fa0b0e524a9313bba524db3ad: Speed up digesting by avoiding unnecessary operations./Merge ""Speed up digesting by avoiding unnecessary operations.""/Speed up digesting by avoiding unnecessary operations. Conscrypts MessageDigest implementations at the end of computing a digest create and initialize a new EVP_MD_CTX and then also intialize the digest struct there. This is done because the MessageDigest instance could be reused for a new digesting session. This change implements three optimizations: 1. MessageDigestSpi now reuses its EVP_MD_CTX instead of creating a new one for each new digesting session. 2. MessageDigestSpi now defers the initialization of the digest struct in EVP_MD_CTX till the first invocation of engineUpdate/engineDigest. 3. MessagDigestSpi (and SignatureSpi) no longer invoke EVP_MD_CTX_init after EVP_MD_CTX_create because EVP_MD_CTX_create initializes the EVP_MD_CTX it creates. libcores MessageDigestBenchmark on Nexus 5 shows: * 10-15% faster performance for a single digest of 8192 bytes. * 15-20% faster performance for reusing a MessageDigest instance to compute a digest of 8192 bytes ten times. Change-Id: I8a0697310ef7efcd4db6870e54eb46102fd4a941/Use poll() instead of select() Since select() has a limit of FD_SETSIZE (typically 1024 on Linux systems) for the highest fd number it will accept, switch to poll() instead to avoid these problems when the app Conscrypt is running under is opening a large amount of file descriptors. Bug: 25390062 Change-Id: Id54a9cc9379e8db8facd2136e84e9e6fd1f5b0f9/Zero-copy HMAC and signing/verification for direct ByteBuffer. Prior to this change, Conscrypts Mac and Signature implementations copied the contents of direct ByteBuffer inputs. This change implements an optimization which avoids the allocation and copying of contents of direct ByteBuffer inputs. Bug: 24674857 Change-Id: I1436839182483fd42318d4b0af4d633283e3453d/Zero-copy digesting for direct ByteBuffer input. Prior to this change, Conscrypts MessageDigest.update(ByteBuffer) invoked for a direct ByteBuffer resulted in the creation of a new byte[] of size ByteBuffer.remaining() and the copying of the ByteBuffers contents into that array. This change implements an optimization which avoids the allocation and copying, by making BoringSSL EVP_DigestUpdate read directly from the memory region represented by the direct ByteBuffer. Change-Id: I112d318128402d1d78e226df9dfe54af55955953/Merge AOSP version of Conscrypt into mnc-ub-dev This adds support for the latest BoringSSL revision and fixes quite a few bugs. Change-Id: I6eafb04d7c15d3b365191d1fe2fe107308cea894/Prepare for BoringSSL update. This change tweaks things as needed so that the code will compile against both the BoringSSL thats currently in Android and a version from upstream. The BORINGSSL_201509 define is temporary to allow the switch to happen without breaking the build and a followup change will remove it. (cherry picked from commit f417aca8ffd57b3817bb24d5a58e01873eecfbde) Change-Id: Ie60d8fc4d88154feaca8ab5ea85645b78a85640f/am d593e6ca: am 79309ee8: am 8acb72c0: Fix typo in previous checkin * commit d593e6ca603fa3a073e9613367dd5ab90d5c5876: Fix typo in previous checkin/am 79309ee8: am 8acb72c0: Fix typo in previous checkin * commit 79309ee8b8d071713d7464f513898a33221b6b31: Fix typo in previous checkin/am 8acb72c0: Fix typo in previous checkin * commit 8acb72c0bdfdf3a19245f85cfb61d122c4d418b3: Fix typo in previous checkin/Fix typo in previous checkin There are still a few builds that use OpenSSL, but its only tested on BoringSSL by default. A character transposition caused this to fail on OpenSSL only. Change-Id: I1b07211a4dbe7dc2e134ce8ddba4b1cfdf627b71/Prepare for BoringSSL update. This change tweaks things as needed so that the code will compile against both the BoringSSL thats currently in Android and a version from upstream. The BORINGSSL_201509 define is temporary to allow the switch to happen without breaking the build and a followup change will remove it. Change-Id: Ie60d8fc4d88154feaca8ab5ea85645b78a85640f/external/conscrypt: allow server-initiated renegotiations. BoringSSL disables server-initiated renegotiations by default. However, its unclear what the impact of this will be. On the other hand, rejecting renegotiations certainly makes things simplier. (cherry picked from commit ed628f94df430278a203da28055b309346b0bce2) Bug: 23189319 Change-Id: I0cd3f04838c0afea665a88d4f0cd0a16c1e811de/"
,,0.0809,conscrypt,Merge AOSP version of Conscrypt into mnc-ub-dev This adds support for the latest BoringSSL revision and fixes quite a few bugs. Change-Id: I6eafb04d7c15d3b365191d1fe2fe107308cea894/
,,0.3595,conscrypt,"Merge ""Speed up digesting by avoiding unnecessary operations."" am: 22324dd963 * commit 22324dd9635b9a7fa0b0e524a9313bba524db3ad: Speed up digesting by avoiding unnecessary operations./Merge ""Speed up digesting by avoiding unnecessary operations.""/Speed up digesting by avoiding unnecessary operations. Conscrypts MessageDigest implementations at the end of computing a digest create and initialize a new EVP_MD_CTX and then also intialize the digest struct there. This is done because the MessageDigest instance could be reused for a new digesting session. This change implements three optimizations: 1. MessageDigestSpi now reuses its EVP_MD_CTX instead of creating a new one for each new digesting session. 2. MessageDigestSpi now defers the initialization of the digest struct in EVP_MD_CTX till the first invocation of engineUpdate/engineDigest. 3. MessagDigestSpi (and SignatureSpi) no longer invoke EVP_MD_CTX_init after EVP_MD_CTX_create because EVP_MD_CTX_create initializes the EVP_MD_CTX it creates. libcores MessageDigestBenchmark on Nexus 5 shows: * 10-15% faster performance for a single digest of 8192 bytes. * 15-20% faster performance for reusing a MessageDigest instance to compute a digest of 8192 bytes ten times. Change-Id: I8a0697310ef7efcd4db6870e54eb46102fd4a941/Zero-copy HMAC and signing/verification for direct ByteBuffer. Prior to this change, Conscrypts Mac and Signature implementations copied the contents of direct ByteBuffer inputs. This change implements an optimization which avoids the allocation and copying of contents of direct ByteBuffer inputs. Bug: 24674857 Change-Id: I1436839182483fd42318d4b0af4d633283e3453d/"
,,0.4002,conscrypt,"Merge ""Speed up digesting by avoiding unnecessary operations."" am: 22324dd963 * commit 22324dd9635b9a7fa0b0e524a9313bba524db3ad: Speed up digesting by avoiding unnecessary operations./Merge ""Speed up digesting by avoiding unnecessary operations.""/Speed up digesting by avoiding unnecessary operations. Conscrypts MessageDigest implementations at the end of computing a digest create and initialize a new EVP_MD_CTX and then also intialize the digest struct there. This is done because the MessageDigest instance could be reused for a new digesting session. This change implements three optimizations: 1. MessageDigestSpi now reuses its EVP_MD_CTX instead of creating a new one for each new digesting session. 2. MessageDigestSpi now defers the initialization of the digest struct in EVP_MD_CTX till the first invocation of engineUpdate/engineDigest. 3. MessagDigestSpi (and SignatureSpi) no longer invoke EVP_MD_CTX_init after EVP_MD_CTX_create because EVP_MD_CTX_create initializes the EVP_MD_CTX it creates. libcores MessageDigestBenchmark on Nexus 5 shows: * 10-15% faster performance for a single digest of 8192 bytes. * 15-20% faster performance for reusing a MessageDigest instance to compute a digest of 8192 bytes ten times. Change-Id: I8a0697310ef7efcd4db6870e54eb46102fd4a941/Zero-copy HMAC and signing/verification for direct ByteBuffer. Prior to this change, Conscrypts Mac and Signature implementations copied the contents of direct ByteBuffer inputs. This change implements an optimization which avoids the allocation and copying of contents of direct ByteBuffer inputs. Bug: 24674857 Change-Id: I1436839182483fd42318d4b0af4d633283e3453d/Zero-copy digesting for direct ByteBuffer input. Prior to this change, Conscrypts MessageDigest.update(ByteBuffer) invoked for a direct ByteBuffer resulted in the creation of a new byte[] of size ByteBuffer.remaining() and the copying of the ByteBuffers contents into that array. This change implements an optimization which avoids the allocation and copying, by making BoringSSL EVP_DigestUpdate read directly from the memory region represented by the direct ByteBuffer. Change-Id: I112d318128402d1d78e226df9dfe54af55955953/Merge AOSP version of Conscrypt into mnc-ub-dev This adds support for the latest BoringSSL revision and fixes quite a few bugs. Change-Id: I6eafb04d7c15d3b365191d1fe2fe107308cea894/external/conscrypt: allow server-initiated renegotiations. BoringSSL disables server-initiated renegotiations by default. However, its unclear what the impact of this will be. On the other hand, rejecting renegotiations certainly makes things simplier. (cherry picked from commit ed628f94df430278a203da28055b309346b0bce2) Bug: 23189319 Change-Id: I0cd3f04838c0afea665a88d4f0cd0a16c1e811de/"
,,0.0828,conscrypt,Merge AOSP version of Conscrypt into mnc-ub-dev This adds support for the latest BoringSSL revision and fixes quite a few bugs. Change-Id: I6eafb04d7c15d3b365191d1fe2fe107308cea894/
,,0.0848,conscrypt,Merge AOSP version of Conscrypt into mnc-ub-dev This adds support for the latest BoringSSL revision and fixes quite a few bugs. Change-Id: I6eafb04d7c15d3b365191d1fe2fe107308cea894/
,,0.2658,conscrypt,"Merge AOSP version of Conscrypt into mnc-ub-dev This adds support for the latest BoringSSL revision and fixes quite a few bugs. Change-Id: I6eafb04d7c15d3b365191d1fe2fe107308cea894/Move BlockGuard and CloseGuard to Platform This was causing issues on Gingerbread devices since CloseGuard was not in that release yet. Move them out to Platform so we can filter on release when we decide whether to instantiate or not. (cherry picked from commit 126ec77aacd58c8b5d62d433af65386fa3dd3fc1) Bug: 24607028 Change-Id: Iba0bbb0b878076319ace40f848aa5e307e2c3ad8/Move BlockGuard and CloseGuard to Platform This was causing issues on Gingerbread devices since CloseGuard was not in that release yet. Move them out to Platform so we can filter on release when we decide whether to instantiate or not. Bug: 24607028 Change-Id: Iba0bbb0b878076319ace40f848aa5e307e2c3ad8/external/conscrypt: allow server-initiated renegotiations. BoringSSL disables server-initiated renegotiations by default. However, its unclear what the impact of this will be. On the other hand, rejecting renegotiations certainly makes things simplier. (cherry picked from commit ed628f94df430278a203da28055b309346b0bce2) Bug: 23189319 Change-Id: I0cd3f04838c0afea665a88d4f0cd0a16c1e811de/"
,,0.0926,Frostwire,[common] username fix for YT/[common] Fix in YT extractor/[common] Fixed YT dash extraction/
,,0.066,Frostwire,[android] removed strange/unused DnD debug tracker/
,,0.066,Frostwire,[android] removed strange/unused DnD debug tracker/
,,0.066,Frostwire,[android] removed strange/unused DnD debug tracker/
,,0.0556,jna,Fix callbacks when DEP is enabled git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0709,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1068,realm-java,remove breaking changes in SyncSession.ErrorHandler (#4408) * remove breaking changes in SyncSession.ErrorHandler * update CHANGELOG * update JNI file * update CMakeLists.txt * address review comments/
,,0.2945,realm-java,"Add SyncUser.allSessions() (#5047) * Add SyncUser.allSessions. * Update CHANGELOG.md * Update SyncUser.allSessions test. * PR feedback * Fix allSessions to exclude sessions in error state. * PR feedback * Update CHANGELOG.md/Work around jmethod bug with Android cherry-pick to enabled debug in menifest which caused a crash described in Removed API to create JavaMethod from jobject/class name string. There seems to be a bug in Android JVM when getting the method by: jclass cls env->GetObjectClass(obj); jmethodID method env->GetMethodID(cls, ""xxx"", ""xxx"") The methodID retrieved by the above way triggers a strange bug in JVM, it reports the method cannot be found on a non-relevant object. Like: "" cant call boolean io.realm.DefaultCompactOnLaunchCallback.shouldCompact(long, long) on instance of io.realm.RealmTests$6 "" Where RealmTest$6 has nothing to do with that JNI call. It is not because of the local ref of cls has been deleted Even if it is deleted, the methodID should still be valid according to the doc: The class references, field IDs, and method IDs are guaranteed valid until the class is unloaded. Classes are only unloaded if all classes associated with a ClassLoader can be garbage collected, which is rare but will not be impossible in Android. The class should never be unload in this case For safety reasons, the JavaMethod should only be created from a JavaClass right now to avoid future surprise. Close (#5000) * fixes admin users not connection correctly to ROS (#4760)/"
,,0.2808,realm-java,"Work around jmethod bug with Android cherry-pick to enabled debug in menifest which caused a crash described in Removed API to create JavaMethod from jobject/class name string. There seems to be a bug in Android JVM when getting the method by: jclass cls env->GetObjectClass(obj); jmethodID method env->GetMethodID(cls, ""xxx"", ""xxx"") The methodID retrieved by the above way triggers a strange bug in JVM, it reports the method cannot be found on a non-relevant object. Like: "" cant call boolean io.realm.DefaultCompactOnLaunchCallback.shouldCompact(long, long) on instance of io.realm.RealmTests$6 "" Where RealmTest$6 has nothing to do with that JNI call. It is not because of the local ref of cls has been deleted Even if it is deleted, the methodID should still be valid according to the doc: The class references, field IDs, and method IDs are guaranteed valid until the class is unloaded. Classes are only unloaded if all classes associated with a ClassLoader can be garbage collected, which is rare but will not be impossible in Android. The class should never be unload in this case For safety reasons, the JavaMethod should only be created from a JavaClass right now to avoid future surprise. Close admin users not connection correctly to ROS (#4760)/fix crash when authentication error happens (#4726) (#4732)/"
,,0.2218,realm-java,"Work around jmethod bug with Android cherry-pick to enabled debug in menifest which caused a crash described in Removed API to create JavaMethod from jobject/class name string. There seems to be a bug in Android JVM when getting the method by: jclass cls env->GetObjectClass(obj); jmethodID method env->GetMethodID(cls, ""xxx"", ""xxx"") The methodID retrieved by the above way triggers a strange bug in JVM, it reports the method cannot be found on a non-relevant object. Like: "" cant call boolean io.realm.DefaultCompactOnLaunchCallback.shouldCompact(long, long) on instance of io.realm.RealmTests$6 "" Where RealmTest$6 has nothing to do with that JNI call. It is not because of the local ref of cls has been deleted Even if it is deleted, the methodID should still be valid according to the doc: The class references, field IDs, and method IDs are guaranteed valid until the class is unloaded. Classes are only unloaded if all classes associated with a ClassLoader can be garbage collected, which is rare but will not be impossible in Android. The class should never be unload in this case For safety reasons, the JavaMethod should only be created from a JavaClass right now to avoid future surprise. Close sync 2.0.0-rc12 (#4928) There are some breaking changes from sync/core in this release: Object ID column name has been renamed from __OID to OID Instead of hardcoding it in java, we read it from sync if it is built with sync. Otherwise hardcode it in JNI. Since core started using cmake, the openssl objects are not a part of core release anymore. Instead, it will be released to s3 independently. Update ROS to 2.0.0-rc2-285/Merge pull request from realm/merge-ea02c9-to-master Fix merge from ea02c9 to master/"
,,0.2917,realm-java,"Work around jmethod bug with Android cherry-pick to enabled debug in menifest which caused a crash described in Removed API to create JavaMethod from jobject/class name string. There seems to be a bug in Android JVM when getting the method by: jclass cls env->GetObjectClass(obj); jmethodID method env->GetMethodID(cls, ""xxx"", ""xxx"") The methodID retrieved by the above way triggers a strange bug in JVM, it reports the method cannot be found on a non-relevant object. Like: "" cant call boolean io.realm.DefaultCompactOnLaunchCallback.shouldCompact(long, long) on instance of io.realm.RealmTests$6 "" Where RealmTest$6 has nothing to do with that JNI call. It is not because of the local ref of cls has been deleted Even if it is deleted, the methodID should still be valid according to the doc: The class references, field IDs, and method IDs are guaranteed valid until the class is unloaded. Classes are only unloaded if all classes associated with a ClassLoader can be garbage collected, which is rare but will not be impossible in Android. The class should never be unload in this case For safety reasons, the JavaMethod should only be created from a JavaClass right now to avoid future surprise. Close"
,,0.0673,realm-java,Fix SyncSession tests (#5853)/
,,0.063,realm-java,Fix SyncSession tests (#5853)/
,,0.0652,realm-java,Fix SyncSession tests (#5853)/
,,0.3076,rocksdb,"[RocksDB] [MergeOperator] The new Merge Interface Uses merge sequences. Summary: Here are the major changes to the Merge Interface. It has been expanded to handle cases where the MergeOperator is not associative. It does so by stacking up merge operations while scanning through the key history (i.e.: during Get() or Compaction), until a valid Put/Delete/end-of-history is encountered; it then applies all of the merge operations in the correct sequence starting with the base/sentinel value. I have also introduced an ""AssociativeMerge"" function which allows the user to take advantage of associative merge operations (such as in the case of counters). The implementation will always attempt to merge the operations/operands themselves together when they are encountered, and will resort to the ""stacking"" method if and only if the ""associative-merge"" fails. This implementation is conjectured to allow MergeOperator to handle the general case, while still providing the user with the ability to take advantage of certain efficiencies in their own merge-operator / data-structure. NOTE: This is a preliminary diff. This must still go through a lot of review, revision, and testing. Feedback welcome Test Plan: is a preliminary diff. I have only just begun testing/debugging it. will be testing this with the existing MergeOperator use-cases and unit-tests (counters, string-append, and redis-lists) will be ""desk-checking"" and walking through the code with the help gdb. will find a way of stress-testing the new interface / implementation using db_bench, db_test, merge_test, and/or db_stress. will ensure that my tests cover all cases: Get-Memtable, Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0, Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found, end-of-history, end-of-file, etc. lot of feedback from the reviewers. Reviewers: haobo, dhruba, zshao, emayanke Reviewed By: haobo CC: leveldb Differential Revision:"
,,0.3196,rocksdb,"[RocksDB] [MergeOperator] The new Merge Interface Uses merge sequences. Summary: Here are the major changes to the Merge Interface. It has been expanded to handle cases where the MergeOperator is not associative. It does so by stacking up merge operations while scanning through the key history (i.e.: during Get() or Compaction), until a valid Put/Delete/end-of-history is encountered; it then applies all of the merge operations in the correct sequence starting with the base/sentinel value. I have also introduced an ""AssociativeMerge"" function which allows the user to take advantage of associative merge operations (such as in the case of counters). The implementation will always attempt to merge the operations/operands themselves together when they are encountered, and will resort to the ""stacking"" method if and only if the ""associative-merge"" fails. This implementation is conjectured to allow MergeOperator to handle the general case, while still providing the user with the ability to take advantage of certain efficiencies in their own merge-operator / data-structure. NOTE: This is a preliminary diff. This must still go through a lot of review, revision, and testing. Feedback welcome Test Plan: is a preliminary diff. I have only just begun testing/debugging it. will be testing this with the existing MergeOperator use-cases and unit-tests (counters, string-append, and redis-lists) will be ""desk-checking"" and walking through the code with the help gdb. will find a way of stress-testing the new interface / implementation using db_bench, db_test, merge_test, and/or db_stress. will ensure that my tests cover all cases: Get-Memtable, Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0, Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found, end-of-history, end-of-file, etc. lot of feedback from the reviewers. Reviewers: haobo, dhruba, zshao, emayanke Reviewed By: haobo CC: leveldb Differential Revision:"
,,0.2627,rocksdb,"Prefix filters for scans (v4) Summary: Similar to v2 (db and table code understands prefixes), but use ReadOptions as in v3. Also, make the CreateFilter code faster and cleaner. Test Plan: make db_test; export LEVELDB_TESTS=PrefixScan; ./db_test Reviewers: dhruba Reviewed By: dhruba CC: haobo, emayanke Differential Revision: [MergeOperator] The new Merge Interface Uses merge sequences. Summary: Here are the major changes to the Merge Interface. It has been expanded to handle cases where the MergeOperator is not associative. It does so by stacking up merge operations while scanning through the key history (i.e.: during Get() or Compaction), until a valid Put/Delete/end-of-history is encountered; it then applies all of the merge operations in the correct sequence starting with the base/sentinel value. I have also introduced an ""AssociativeMerge"" function which allows the user to take advantage of associative merge operations (such as in the case of counters). The implementation will always attempt to merge the operations/operands themselves together when they are encountered, and will resort to the ""stacking"" method if and only if the ""associative-merge"" fails. This implementation is conjectured to allow MergeOperator to handle the general case, while still providing the user with the ability to take advantage of certain efficiencies in their own merge-operator / data-structure. NOTE: This is a preliminary diff. This must still go through a lot of review, revision, and testing. Feedback welcome Test Plan: is a preliminary diff. I have only just begun testing/debugging it. will be testing this with the existing MergeOperator use-cases and unit-tests (counters, string-append, and redis-lists) will be ""desk-checking"" and walking through the code with the help gdb. will find a way of stress-testing the new interface / implementation using db_bench, db_test, merge_test, and/or db_stress. will ensure that my tests cover all cases: Get-Memtable, Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0, Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found, end-of-history, end-of-file, etc. lot of feedback from the reviewers. Reviewers: haobo, dhruba, zshao, emayanke Reviewed By: haobo CC: leveldb Differential Revision: KeyMayExist to return the proper value if it can be found in memory and also check block_cache Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldnt just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test Test Plan: make all check;db_stress for 1 hour Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision:"
,,0.1087,rocksdb,"PlainTable to encode to avoid to rewrite prefix when it is the same as the previous key Summary: Add a encoding feature of PlainTable to encode PlainTables keys to save some bytes for the same prefixes. The data format is documented in table/plain_table_factory.h Test Plan: Add unit test coverage in plain_table_db_test Reviewers: yhchiang, igor, dhruba, ljin, haobo Reviewed By: haobo Subscribers: nkg-, leveldb Differential Revision:"
,,0.0577,rocksdb,Fix formatting/
,,0.2025,rocksdb,"plain table reader: non-mmap mode to keep two recent buffers Summary: In plain table readers non-mmap mode, we only keep the most recent read buffer. However, for binary search, it is likely we come back to a location to read. To avoid one pread in such a case, we keep two read buffers. It should cover most of the cases. Test Plan: 1. run tests 2. check the optimization works through strace when running ./table_reader_bench Reviewers: anthony, rven, kradhakrishnan, igor, yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision: table reader: avoid re-read the same position for index and data in non-mmap mode Summary: In non-mmap mode, plain table reader can issue two pread() for index checking and reading the actual data, although its for the same location. By reusing the key decoder, we reuse the buffer used for the two to avoid it. Test Plan: Run unit tests. Run table_reader_bench and see from strace the repeat read cases to disappear. Reviewers: anthony, yhchiang, rven, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision:"
,,0.1396,rocksdb,"plain table reader: non-mmap mode to keep two recent buffers Summary: In plain table readers non-mmap mode, we only keep the most recent read buffer. However, for binary search, it is likely we come back to a location to read. To avoid one pread in such a case, we keep two read buffers. It should cover most of the cases. Test Plan: 1. run tests 2. check the optimization works through strace when running ./table_reader_bench Reviewers: anthony, rven, kradhakrishnan, igor, yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision:"
,,0.3274,rocksdb,"Simplify thread-local static initialization Summary: The call stack used to look like this during static initialization: 0x00000000008032d1 in rocksdb::ThreadLocalPtr::StaticMeta::StaticMeta() (this=0x7ffff683b300) at util/thread_local.cc:172 0x00000000008030a7 in rocksdb::ThreadLocalPtr::Instance() () at util/thread_local.cc:135 0x000000000080310f in rocksdb::ThreadLocalPtr::StaticMeta::Mutex() () at util/thread_local.cc:141 0x0000000000803103 in rocksdb::ThreadLocalPtr::StaticMeta::InitSingletons() () at util/thread_local.cc:139 0x000000000080305d in rocksdb::ThreadLocalPtr::InitSingletons() () at util/thread_local.cc:106 It involves outer/inner classes and the call stacks goes outer->inner->outer->inner, which is too difficult to understand. We can avoid a level of back-and-forth by skipping StaticMeta::InitSingletons(), which doesnt initialize anything beyond what ThreadLocalPtr::Instance() already initializes. Now the call stack looks like this during static initialization: 0x00000000008032c5 in rocksdb::ThreadLocalPtr::StaticMeta::StaticMeta() (this=0x7ffff683b300) at util/thread_local.cc:170 0x00000000008030a7 in rocksdb::ThreadLocalPtr::Instance() () at util/thread_local.cc:135 0x000000000080305d in rocksdb::ThreadLocalPtr::InitSingletons() () at util/thread_local.cc:106 Test Plan: unit tests verify StaticMeta::mutex_ is still initialized in DefaultEnv() (StaticMeta::mutex_ is the only variable intended to be initialized via StaticMeta::InitSingletons() which I removed) 0x00000000005cee17 in rocksdb::port::Mutex::Mutex(bool) (this=0x7ffff69500b0, adaptive=false) at port/port_posix.cc:52 0x0000000000769cf8 in rocksdb::ThreadLocalPtr::StaticMeta::StaticMeta() (this=0x7ffff6950000) at util/thread_local.cc:168 0x0000000000769a53 in rocksdb::ThreadLocalPtr::Instance() () at util/thread_local.cc:133 0x0000000000769a09 in rocksdb::ThreadLocalPtr::InitSingletons() () at util/thread_local.cc:105 0x0000000000647d98 in rocksdb::Env::Default() () at util/env_posix.cc:845 Reviewers: lightmark, yhchiang, sdong Reviewed By: sdong Subscribers: arahut, IslamAbdelRahman, yiwu, andrewkr, dhruba, leveldb Differential Revision:"
,,0.0915,rocksdb,"Fix shared lock upgrades Summary: Upgrading a shared lock was silently succeeding because the actual locking code was skipped. This is because if the keys are tracked, it is assumed that they are already locked and do not require locking. Fix this by recording in tracked keys whether the key was locked exclusively or not. Note that lock downgrades are impossible, which is the behaviour we expect. This fixes facebook/mysql-5.6#587. Closes Differential Revision: D4861489 Pulled By: IslamAbdelRahman fbshipit-source-id: 58c7ebe7af098bf01b9774b666d3e9867747d8fd/"
,,0.4277,rocksdb,"Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Allow ignoring unknown options when loading options from a file Summary: Added a flag, `ignore_unknown_options`, to skip unknown options when loading an options file (using `LoadLatestOptions`/`LoadOptionsFromFile`) or while verifying options (using `CheckOptionsCompatibility`). This will help in downgrading the db to an older version. Also added `--ignore_unknown_options` flag to ldb **Example Use case:** In MyRocks, if copying from newer version to older version, it is often impossible to start because of new RocksDB options that dont exist in older version, even though data format is compatible. MyRocks uses these load and verify functions in [ha_rocksdb.cc::check_rocksdb_options_compatibility]( **Test Plan:** Updated the unit tests. `make check` ldb: $ ./ldb put a1 b1 OK Now edit /tmp/test_db/<OPTIONS-file> and add an unknown option. Try loading the options now, and it fails: $ ./ldb get a1 Failed: Invalid argument: Unrecognized option DBOptions:: abcd Passes with the new flag $ ./ldb get a1 b1 Closes Differential Revision: D5212091 Pulled By: sagar0 fbshipit-source-id: 2ec17636feb47dc0351b53a77e5f15ef7cbf2ca7/"
,,0.4446,rocksdb,"Dump Blob DB options to info log Summary: * Dump blob db options to info log * Remove BlobDBOptionsImpl to disallow dynamic cast *BlobDBOptions into *BlobDBOptionsImpl. Move options there to be constants or into BlobDBOptions. The dynamic cast is broken after * Change some of the default options * Remove blob_db_options.min_blob_size, which is unimplemented. Will implement it soon. Closes Differential Revision: D5529912 Pulled By: yiwu-arbug fbshipit-source-id: dcd58ca981db5bcc7f123b65a0d6f6ae0dc703c7/Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/fix db_bench argument type Summary: it should be a bool Closes Differential Revision: D5506148 Pulled By: ajkr fbshipit-source-id: f142f0f3aa8b678c68adef12e5ac6e1e163306f3/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Enable write rate limit for updaterandom benchmark Summary: We have FLAGS_benchmark_write_rate_limit to limit write rate in db_bench, but it was not in use for updaterandom benchmark. Closes Differential Revision: D5420328 Pulled By: yiwu-arbug fbshipit-source-id: 5fa48c2b88f2f2dc83d615cb9c40c472bc916835/Fixes db_bench with blob db Summary: * Create info log before db open to make blob db able to log to LOG file. * Properly destroy blob db. Closes Differential Revision: D5400034 Pulled By: yiwu-arbug fbshipit-source-id: a49cfaf4b5c67d42d4cbb872bd5a9441828c17ce/db_bench_tool: fix buffer size Summary: Found by gcc warning: x86_64-pc-linux-gnu-g++ x86_64-pc-linux-gnu-g++ (GCC) 7.1.1 20170710 tools/db_bench_tool.cc: In member function void rocksdb::Benchmark::RandomWithVerify(rocksdb::ThreadState*): tools/db_bench_tool.cc:4430:8: error: %lu directive output may be truncated writing between 1 and 19 bytes into a region of size between 0 and 66 [-Werror=format-truncation=] void RandomWithVerify(ThreadState* thread) { ^~~~~~~~~~~~~~~~ tools/db_bench_tool.cc:4430:8: note: directive argument in the range [0, 9223372036854775807] tools/db_bench_tool.cc:4492:13: note: snprintf output between 37 and 128 bytes into a destination of size 100 snprintf(msg, sizeof(msg), ~~~~~~~~^~~~~~~~~~~~~~~~~~ ""( get:%"" PRIu64 "" put:%"" PRIu64 "" del:%"" PRIu64 "" total:%"" \ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PRIu64 "" found:%"" PRIu64 "")"", ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ gets_done, puts_done, deletes_done, readwrites_, found); ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ cc1plus: all warnings being treated as errors Makefile:1707: recipe for target tools/db_bench_tool.o failed Closes Differential Revision: D5398703 Pulled By: siying fbshipit-source-id: 6ffa552bbd8b59cfc2c36289f86ff9b9acca8ca6/Add max_background_jobs to db_bench Summary: As titled. Also fixed an off-by-one error causing us to add one less range deletion than the user specified. Closes Differential Revision: D5383451 Pulled By: ajkr fbshipit-source-id: cbd5890c33f09bbb5c0c1f4bb952a1add32336e0/FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/using ThreadLocalPtr to hide ROCKSDB_SUPPORT_THREAD_LOCAL from publicÖ Summary: Ö headers should not reference RocksDB-specific macros (like ROCKSDB_SUPPORT_THREAD_LOCAL in this case) to public headers, `iostats_context.h` and `perf_context.h`. We shouldnt do that because users have to provide these compiler flags when building their binary with RocksDB. We should hide the thread local global variable inside our implementation and just expose a function api to retrieve these variables. It may break some users for now but good for long term. make check Closes Differential Revision: D5177896 Pulled By: lightmark fbshipit-source-id: 6fcdfac57f2e2dcfe60992b7385c5403f6dcb390/fix regression test Summary: fix regression test by not reporting stats when building db Closes Differential Revision: D5159909 Pulled By: lightmark fbshipit-source-id: c3f4b9deb9c6799ff84207fd341c529144f8158d/change regression rebuild to one level Summary: abandon fillseqdeterministic test locally Closes Differential Revision: D5151867 Pulled By: lightmark fbshipit-source-id: 4c8a24cc937212ffb5ceb9bfaf7288eb8726d0c1/Fix db_bench build break with blob db Summary: Lite build does not recognize FLAGS_use_blob_db. Fixing it. Closes Reviewed By: anirbanr-fb Differential Revision: D5130773 Pulled By: yiwu-arbug fbshipit-source-id: 43131d9d0be5811f2129af562be72cca26369cb3/"
,,0.4363,rocksdb,"Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Allow ignoring unknown options when loading options from a file Summary: Added a flag, `ignore_unknown_options`, to skip unknown options when loading an options file (using `LoadLatestOptions`/`LoadOptionsFromFile`) or while verifying options (using `CheckOptionsCompatibility`). This will help in downgrading the db to an older version. Also added `--ignore_unknown_options` flag to ldb **Example Use case:** In MyRocks, if copying from newer version to older version, it is often impossible to start because of new RocksDB options that dont exist in older version, even though data format is compatible. MyRocks uses these load and verify functions in [ha_rocksdb.cc::check_rocksdb_options_compatibility]( **Test Plan:** Updated the unit tests. `make check` ldb: $ ./ldb put a1 b1 OK Now edit /tmp/test_db/<OPTIONS-file> and add an unknown option. Try loading the options now, and it fails: $ ./ldb get a1 Failed: Invalid argument: Unrecognized option DBOptions:: abcd Passes with the new flag $ ./ldb get a1 b1 Closes Differential Revision: D5212091 Pulled By: sagar0 fbshipit-source-id: 2ec17636feb47dc0351b53a77e5f15ef7cbf2ca7/"
,,0.6079,rocksdb,"Support prefetch last 512KB with direct I/O in block based file reader Summary: Right now, if direct I/O is enabled, prefetching the last 512KB cannot be applied, except compaction inputs or readahead is enabled for iterators. This can create a lot of I/O for HDD cases. To solve the problem, the 512KB is prefetched in block based table if direct I/O is enabled. The prefetched buffer is passed in totegher with random access file reader, so that we try to read from the buffer before reading from the file. This can be extended in the future to support flexible user iterator readahead too. Closes Differential Revision: D5593091 Pulled By: siying fbshipit-source-id: ee36ff6d8af11c312a2622272b21957a7b5c81e7/add VerifyChecksum() to db.h Summary: We need a tool to check any sst file corruption in the db. It will check all the sst files in current version and read all the blocks (data, meta, index) with checksum verification. If any verification fails, the function will return non-OK status. Closes Differential Revision: D5324269 Pulled By: lightmark fbshipit-source-id: 6f8a272008b722402a772acfc804524c9d1a483b/Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Improve Status message for block checksum mismatches Summary: Weve got some DBs where iterators return Status with message ""Corruption: block checksum mismatch"" all the time. Thats not very informative. It would be much easier to investigate if the error message contained the file name then we would know e.g. how old the corrupted file is, which would be very useful for finding the root cause. This PR adds file name, offset and other stuff to some block corruption-related status messages. It doesnt improve all the error messages, just a few that were easy to improve. Im mostly interested in ""block checksum mismatch"" and ""Bad table magic number"" since theyre the only corruption errors that Ive ever seen in the wild. Closes Differential Revision: D5345702 Pulled By: al13n321 fbshipit-source-id: fc8023d43f1935ad927cef1b9c55481ab3cb1339/"
,,0.4189,rocksdb,"Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2122,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2067,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2195,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2122,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.4075,rocksdb,"Windows.h macro call fix Summary: moved the max call for numeric limits into paranthesis so that max wont be called as macro when including Closes Differential Revision: D5600773 Pulled By: yiwu-arbug fbshipit-source-id: fd28b6f7c10ddce21bad4030f2db06f965bb08da/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Fix GetCurrentTime() initialization for valgrind Summary: Valgrind had false positive complaints about the initialization pattern for `GetCurrentTime()`s argument in We can instead have the client initialize the time variable before calling `GetCurrentTime()`, and have `GetCurrentTime()` promise to only overwrite it in success case. Closes Differential Revision: D5358689 Pulled By: ajkr fbshipit-source-id: 857b189f24c19196f6bb299216f3e23e7bc4be42/Encryption at rest support Summary: This PR adds support for encrypting data stored by RocksDB when written to disk. It adds an `EncryptedEnv` override of the `Env` class with matching overrides for sequential&random access files. The encryption itself is done through a configurable `EncryptionProvider`. This class creates is asked to create `BlockAccessCipherStream` for a file. This is where the actual encryption/decryption is being done. Currently there is a Counter mode implementation of `BlockAccessCipherStream` with a `ROT13` block cipher (NOTE the `ROT13` is for demo purposes only). The Counter operation mode uses an initial counter & random initialization vector (IV). Both are created randomly for each file and stored in a 4K (default size) block that is prefixed to that file. The `EncryptedEnv` implementation is such that clients of the `Env` class do not see this prefix (nor data, nor in filesize). The largest part of the prefix block is also encrypted, and there is room left for implementation specific settings/values/keys in there. To test the encryption, the `DBTestBase` class has been extended to consider a new environment variable called `ENCRYPTED_ENV`. If set, the test will setup a encrypted instance of the `Env` class to use for all tests. Typically you would run it like this: ``` ENCRYPTED_ENV=1 make check_some ``` There is also an added test that checks that some data inserted into the database is or is not ""visible"" on disk. With `ENCRYPTED_ENV` active it must not find plain text strings, with `ENCRYPTED_ENV` unset, it must find the plain text strings. Closes Differential Revision: D5322178 Pulled By: sdwilsh fbshipit-source-id: 253b0a9c2c498cc98f580df7f2623cbf7678a27f/Implement ReopenWritibaleFile on Windows and other fixes Summary: Make default impl return NoSupported so the db_blob tests exist in a meaningful manner. Replace std::thread to port::Thread Closes Differential Revision: D5275563 Pulled By: yiwu-arbug fbshipit-source-id: cedf1a18a2c05e20d768c1308b3f3224dbd70ab6/Allow SstFileWriter to use the rate limiter Summary: The default IO priority of WritableFiles is IO_TOTAL, meaning that they will bypass the rate limiter if its passed in the options. This change allows to pass an io priority in construction, so that by setting IO_LOW or IO_HIGH the rate limit will be honored. It also fixes a minor bug: SstFileWriters copy and move constructor are not disabled and incorrect, as any copy/move will result in a double free. Switching to unique_ptr makes the object correctly movable and non-copyable as expected. Also fix minor style inconsistencies. Closes Differential Revision: D5113260 Pulled By: sagar0 fbshipit-source-id: e084236e7ff0b50a56cbeceaa9fedd5e210bf9f8/Fixed some spelling mistakes Summary: Closes Differential Revision: D5079601 Pulled By: sagar0 fbshipit-source-id: ae5696fd735718f544435c64c3179c49b8c04349/"
,,0.2134,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/remove unnecessary fadvise Summary: We has to remove this line because previously it is only called when use_os_buffer false. But now we have direct io to replace it. Closes Differential Revision: D5412824 Pulled By: yiwu-arbug fbshipit-source-id: 81f3f0cdf94566bfc09ef2ff123e40cddbe36b36/Improve the error message for I/O related errors. Summary: Force people to write something other than file name while returning status for IOError. Closes Differential Revision: D5321309 Pulled By: siying fbshipit-source-id: 38bcf6c19e80831cd3e300a047e975cbb131d822/Fix crash in PosixWritableFile::Close() when fstat() fails Summary: We had a crash in this code: `fstat()` failed; `file_stats` contained garbage, in particular `file_stats.st_blksize 6`; the expression `file_stats.st_blocks / (file_stats.st_blksize / 512)` divided by zero. Closes Differential Revision: D5216110 Pulled By: al13n321 fbshipit-source-id: 6d8fc5e7c4f98c1139e68c7829ebdbac68b0fce0/Fix clang errors by asserting the precondition Summary: USE_CLANG=1 make analyze The two errors would disappear after the assertion. Closes Differential Revision: D5193526 Pulled By: maysamyabandeh fbshipit-source-id: 16a21f18f68023f862764dd3ab9e00ca60b0eefa/"
,,0.2122,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.4018,rocksdb,"Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Fix GetCurrentTime() initialization for valgrind Summary: Valgrind had false positive complaints about the initialization pattern for `GetCurrentTime()`s argument in We can instead have the client initialize the time variable before calling `GetCurrentTime()`, and have `GetCurrentTime()` promise to only overwrite it in success case. Closes Differential Revision: D5358689 Pulled By: ajkr fbshipit-source-id: 857b189f24c19196f6bb299216f3e23e7bc4be42/GNU C library for struct tm has 2 additional fields. Summary: initialize 2 additional fields tm_gmtoff and tm_zone, otherwise under strict warnings for initialization, we get errors in myrocks. Closes Differential Revision: D5229013 Pulled By: yiwu-arbug fbshipit-source-id: 9fc1615a1919656f36064791706ed41e10e9db84/Fix mock_env.cc uninitialized variable Summary: Mingw is complaining about uninitialized variable in mock_env.cc. e.g. The fix is to initialize the variable. Closes Differential Revision: D5211306 Pulled By: yiwu-arbug fbshipit-source-id: ee02bf0327dcea8590a2aa087f0176fecaf8621c/fix travis error with init time in mockenv Summary: /home/travis/build/facebook/rocksdb/env/mock_env.cc: In member function ëvirtual void rocksdb::{anonymous}::TestMemLogger::Logv(const char*, va_list)í: /home/travis/build/facebook/rocksdb/env/mock_env.cc:391:53: error: ët.tm::tm_yearí may be used uninitialized in this function [-Werror=maybe-uninitialized] static_cast<int>(now_tv.tv_usec)); Closes Differential Revision: D5193597 Pulled By: maysamyabandeh fbshipit-source-id: 8801a3ef27f33eb419d534f7de747702cdf504a0/"
,,0.396,rocksdb,"Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Downgrade option sanitiy check level for prefix_extractor Summary: With c7004840d2f4ad5fc1bdce042902b822492f3a0e, its safe to open a DB with different prefix extractor. So its safe to skip prefix extractor check. Closes Differential Revision: D5294700 Pulled By: siying fbshipit-source-id: eeb500da795eecb29b8c9c56a14cfd4afda12ecc/"
,,0.214,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.3804,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/CodeMod: Prefer ADD_FAILURE() over EXPECT_TRUE(false), et cetera Summary: CodeMod: Prefer `ADD_FAILURE()` over `EXPECT_TRUE(false)`, et cetera. The tautologically-conditioned and tautologically-contradicted boolean expectations/assertions have better alternatives: unconditional passes and failures. Reviewed By: Orvid Differential Revision: D5432398 Tags: codemod, codemod-opensource fbshipit-source-id: d16b447e8696a6feaa94b41199f5052226ef6914/Fix GetCurrentTime() initialization for valgrind Summary: Valgrind had false positive complaints about the initialization pattern for `GetCurrentTime()`s argument in We can instead have the client initialize the time variable before calling `GetCurrentTime()`, and have `GetCurrentTime()` promise to only overwrite it in success case. Closes Differential Revision: D5358689 Pulled By: ajkr fbshipit-source-id: 857b189f24c19196f6bb299216f3e23e7bc4be42/"
,,0.3112,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/CodeMod: Prefer ADD_FAILURE() over EXPECT_TRUE(false), et cetera Summary: CodeMod: Prefer `ADD_FAILURE()` over `EXPECT_TRUE(false)`, et cetera. The tautologically-conditioned and tautologically-contradicted boolean expectations/assertions have better alternatives: unconditional passes and failures. Reviewed By: Orvid Differential Revision: D5432398 Tags: codemod, codemod-opensource fbshipit-source-id: d16b447e8696a6feaa94b41199f5052226ef6914/"
,,0.2122,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2087,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/fixed typo Summary: fixed typo Closes Differential Revision: D5183630 Pulled By: ajkr fbshipit-source-id: 133cfd0445959e70aa2cd1a12151bf3c0c5c3ac5/"
,,0.4064,rocksdb,"Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2067,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2177,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2104,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2122,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.3866,rocksdb,"Improve Status message for block checksum mismatches Summary: Weve got some DBs where iterators return Status with message ""Corruption: block checksum mismatch"" all the time. Thats not very informative. It would be much easier to investigate if the error message contained the file name then we would know e.g. how old the corrupted file is, which would be very useful for finding the root cause. This PR adds file name, offset and other stuff to some block corruption-related status messages. It doesnt improve all the error messages, just a few that were easy to improve. Im mostly interested in ""block checksum mismatch"" and ""Bad table magic number"" since theyre the only corruption errors that Ive ever seen in the wild. Closes Differential Revision: D5345702 Pulled By: al13n321 fbshipit-source-id: fc8023d43f1935ad927cef1b9c55481ab3cb1339/"
,,0.4673,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Gcc 7 ignored quantifiers Summary: The casting seemed to cause a problem. I think this might increase it to unsigned long. Closes Differential Revision: D5406842 Pulled By: siying fbshipit-source-id: 736adef31448229a58a1a48bdbe77792f36736e8/Fix clang error in PartitionedFilterBlockBuilder Summary: Closes Differential Revision: D5371271 Pulled By: maysamyabandeh fbshipit-source-id: f1355ac658a79c9982a24986f0925c9e24fc39d5/FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/Unit Tests for sync, range sync and file close failures Summary: Closes Differential Revision: D5255320 Pulled By: siying fbshipit-source-id: 0080830fa8eb5da6de25e17ba68aee91018c7913/"
,,0.442,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Improve Status message for block checksum mismatches Summary: Weve got some DBs where iterators return Status with message ""Corruption: block checksum mismatch"" all the time. Thats not very informative. It would be much easier to investigate if the error message contained the file name then we would know e.g. how old the corrupted file is, which would be very useful for finding the root cause. This PR adds file name, offset and other stuff to some block corruption-related status messages. It doesnt improve all the error messages, just a few that were easy to improve. Im mostly interested in ""block checksum mismatch"" and ""Bad table magic number"" since theyre the only corruption errors that Ive ever seen in the wild. Closes Differential Revision: D5345702 Pulled By: al13n321 fbshipit-source-id: fc8023d43f1935ad927cef1b9c55481ab3cb1339/Fixed some spelling mistakes Summary: Closes Differential Revision: D5079601 Pulled By: sagar0 fbshipit-source-id: ae5696fd735718f544435c64c3179c49b8c04349/"
,,0.214,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.3153,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/CodeMod: Prefer ADD_FAILURE() over EXPECT_TRUE(false), et cetera Summary: CodeMod: Prefer `ADD_FAILURE()` over `EXPECT_TRUE(false)`, et cetera. The tautologically-conditioned and tautologically-contradicted boolean expectations/assertions have better alternatives: unconditional passes and failures. Reviewed By: Orvid Differential Revision: D5432398 Tags: codemod, codemod-opensource fbshipit-source-id: d16b447e8696a6feaa94b41199f5052226ef6914/"
,,0.3712,rocksdb,"Support prefetch last 512KB with direct I/O in block based file reader Summary: Right now, if direct I/O is enabled, prefetching the last 512KB cannot be applied, except compaction inputs or readahead is enabled for iterators. This can create a lot of I/O for HDD cases. To solve the problem, the 512KB is prefetched in block based table if direct I/O is enabled. The prefetched buffer is passed in totegher with random access file reader, so that we try to read from the buffer before reading from the file. This can be extended in the future to support flexible user iterator readahead too. Closes Differential Revision: D5593091 Pulled By: siying fbshipit-source-id: ee36ff6d8af11c312a2622272b21957a7b5c81e7/add VerifyChecksum() to db.h Summary: We need a tool to check any sst file corruption in the db. It will check all the sst files in current version and read all the blocks (data, meta, index) with checksum verification. If any verification fails, the function will return non-OK status. Closes Differential Revision: D5324269 Pulled By: lightmark fbshipit-source-id: 6f8a272008b722402a772acfc804524c9d1a483b/fix asan/valgrind for TableCache cleanup Summary: Breaking commit: d12691b86fb788f0ee7180db626c4ea2445fa976 In the above commit, I moved the `TableCache` cleanup logic from `Version` destructor into `PurgeObsoleteFiles`. I missed cleaning up `TableCache` entries for the current `Version` during DB destruction. This PR adds that logic to `VersionSet` destructor. One unfortunate side effect is now were potentially deleting `TableReader`s after `column_family_set_.reset()`, which means we cant call `BlockBasedTableReader::Close` a second time as the block cache might already be destroyed. Closes Differential Revision: D5515108 Pulled By: ajkr fbshipit-source-id: 2cb820e19aa813e0d258d17f76b2d7b6b7ee0b18/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/fixed typo Summary: fixed typo Closes Differential Revision: D5079631 Pulled By: sagar0 fbshipit-source-id: e4c8d1d89b244ee69e9dea1dd013227cc5241026/"
,,0.214,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.5775,rocksdb,"Support prefetch last 512KB with direct I/O in block based file reader Summary: Right now, if direct I/O is enabled, prefetching the last 512KB cannot be applied, except compaction inputs or readahead is enabled for iterators. This can create a lot of I/O for HDD cases. To solve the problem, the 512KB is prefetched in block based table if direct I/O is enabled. The prefetched buffer is passed in totegher with random access file reader, so that we try to read from the buffer before reading from the file. This can be extended in the future to support flexible user iterator readahead too. Closes Differential Revision: D5593091 Pulled By: siying fbshipit-source-id: ee36ff6d8af11c312a2622272b21957a7b5c81e7/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Fix interaction between CompactionFilter::Decision::kRemoveAndSkipUntÖ Summary: Fixes the following scenario: 1. Set prefix extractor. Enable bloom filters, with `whole_key_filtering false`. Use compaction filter that sometimes returns `kRemoveAndSkipUntil`. 2. Do a compaction. 3. Compaction creates an iterator with `total_order_seek false`, calls `SeekToFirst()` on it, then repeatedly calls `Next()`. 4. At some point compaction filter returns `kRemoveAndSkipUntil`. 5. Compaction calls `Seek(skip_until)` on the iterator. The key that it seeks to happens to have prefix that doesnt match the bloom filter. Since `total_order_seek false`, iterator becomes invalid, and compaction thinks that it has reached the end. The rest of the compaction input is silently discarded. The fix is to make compaction iterator use `total_order_seek true`. The implementation for PlainTable is quite awkward. Ive made `kRemoveAndSkipUntil` officially incompatible with PlainTable. If you try to use them together, compaction will fail, and DB will enter read-only mode (`bg_error_`). Thats not a very graceful way to communicate a misconfiguration, but the alternatives dont seem worth the implementation time and complexity. To be able to check in advance that `kRemoveAndSkipUntil` is not going to be used with PlainTable, wed need to extend the interface of either `CompactionFilter` or `InternalIterator`. It seems unlikely that anyone will ever want to use `kRemoveAndSkipUntil` with PlainTable: PlainTable probably has very few users, and `kRemoveAndSkipUntil` has only one user so far: us (logdevice). Closes Differential Revision: D5110388 Pulled By: lightmark fbshipit-source-id: ec29101a99d9dcd97db33923b87f72bce56cc17a/"
,,0.494,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Fix interaction between CompactionFilter::Decision::kRemoveAndSkipUntÖ Summary: Fixes the following scenario: 1. Set prefix extractor. Enable bloom filters, with `whole_key_filtering false`. Use compaction filter that sometimes returns `kRemoveAndSkipUntil`. 2. Do a compaction. 3. Compaction creates an iterator with `total_order_seek false`, calls `SeekToFirst()` on it, then repeatedly calls `Next()`. 4. At some point compaction filter returns `kRemoveAndSkipUntil`. 5. Compaction calls `Seek(skip_until)` on the iterator. The key that it seeks to happens to have prefix that doesnt match the bloom filter. Since `total_order_seek false`, iterator becomes invalid, and compaction thinks that it has reached the end. The rest of the compaction input is silently discarded. The fix is to make compaction iterator use `total_order_seek true`. The implementation for PlainTable is quite awkward. Ive made `kRemoveAndSkipUntil` officially incompatible with PlainTable. If you try to use them together, compaction will fail, and DB will enter read-only mode (`bg_error_`). Thats not a very graceful way to communicate a misconfiguration, but the alternatives dont seem worth the implementation time and complexity. To be able to check in advance that `kRemoveAndSkipUntil` is not going to be used with PlainTable, wed need to extend the interface of either `CompactionFilter` or `InternalIterator`. It seems unlikely that anyone will ever want to use `kRemoveAndSkipUntil` with PlainTable: PlainTable probably has very few users, and `kRemoveAndSkipUntil` has only one user so far: us (logdevice). Closes Differential Revision: D5110388 Pulled By: lightmark fbshipit-source-id: ec29101a99d9dcd97db33923b87f72bce56cc17a/"
,,0.2177,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.214,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.4314,rocksdb,"Support prefetch last 512KB with direct I/O in block based file reader Summary: Right now, if direct I/O is enabled, prefetching the last 512KB cannot be applied, except compaction inputs or readahead is enabled for iterators. This can create a lot of I/O for HDD cases. To solve the problem, the 512KB is prefetched in block based table if direct I/O is enabled. The prefetched buffer is passed in totegher with random access file reader, so that we try to read from the buffer before reading from the file. This can be extended in the future to support flexible user iterator readahead too. Closes Differential Revision: D5593091 Pulled By: siying fbshipit-source-id: ee36ff6d8af11c312a2622272b21957a7b5c81e7/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.1872,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Gcc 7 ignored quantifiers Summary: The casting seemed to cause a problem. I think this might increase it to unsigned long. Closes Differential Revision: D5406842 Pulled By: siying fbshipit-source-id: 736adef31448229a58a1a48bdbe77792f36736e8/fix asan and valgrind leak report in test Summary: Closes Differential Revision: D5371433 Pulled By: maysamyabandeh fbshipit-source-id: 90d3e8bb1a8576f48b1ddf1bdbba5512b5986ba0/Fix clang error in PartitionedFilterBlockBuilder Summary: Closes Differential Revision: D5371271 Pulled By: maysamyabandeh fbshipit-source-id: f1355ac658a79c9982a24986f0925c9e24fc39d5/"
,,0.4151,rocksdb,"Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2651,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/using ThreadLocalPtr to hide ROCKSDB_SUPPORT_THREAD_LOCAL from publicÖ Summary: Ö headers should not reference RocksDB-specific macros (like ROCKSDB_SUPPORT_THREAD_LOCAL in this case) to public headers, `iostats_context.h` and `perf_context.h`. We shouldnt do that because users have to provide these compiler flags when building their binary with RocksDB. We should hide the thread local global variable inside our implementation and just expose a function api to retrieve these variables. It may break some users for now but good for long term. make check Closes Differential Revision: D5177896 Pulled By: lightmark fbshipit-source-id: 6fcdfac57f2e2dcfe60992b7385c5403f6dcb390/"
,,0.2104,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.3121,rocksdb,"Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/"
,,0.3572,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
,,0.214,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2382,rocksdb,"add VerifyChecksum() to db.h Summary: We need a tool to check any sst file corruption in the db. It will check all the sst files in current version and read all the blocks (data, meta, index) with checksum verification. If any verification fails, the function will return non-OK status. Closes Differential Revision: D5324269 Pulled By: lightmark fbshipit-source-id: 6f8a272008b722402a772acfc804524c9d1a483b/Try to repair db with wal_dir option, avoid leak some WAL files Summary: We should search wal_dir in Repairer::FindFiles function, and avoid use LogFileNmae(dbname, number) to get WAL files name, which will get a wrong WAL filename. as following: ``` [WARN] [/home/liuchang/Workspace/rocksdb/db/repair.cc:310] Log ignoring conversion error: IO error: While opening a file for sequentially reading: /tmp/rocksdbtest-1000/repair_test/000003.log: No such file or directory ``` I have added a new test case to repair_test.cc, which try to repair db with all WAL options. Signed-off-by: Chang Liu Closes Differential Revision: D5575888 Pulled By: ajkr fbshipit-source-id: 5b93e9f85cddc01663ccecd87631fa723ac466a3/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Fix Data Race Between CreateColumnFamily() and GetAggregatedIntProperty() Summary: CreateColumnFamily() releases DB mutex after adding column family to the set and install super version (to write option file), so if users call GetAggregatedIntProperty() in the middle, then super version will be null and the process will crash. Fix it by skipping those column families without super version installed. Maybe we should also fix the problem of releasing the lock when reading option file, but it is more risky. so Im doing a quick and safer fix and we can investigate it later. Closes Differential Revision: D5298053 Pulled By: siying fbshipit-source-id: 4b3c8f91c60400b163fcc6cda8a0c77723be0ef6/fixed typo Summary: fixed typo Closes Differential Revision: D5183630 Pulled By: ajkr fbshipit-source-id: 133cfd0445959e70aa2cd1a12151bf3c0c5c3ac5/db: avoid `#include`ing malloc and jemalloc simultaneously Summary: This fixes a compilation failure on Linux when the system libc is not glibc. jemallocs configure script incorrectly assumes that glibc is always used on Linux systems, producing glibc-style signatures; when the system libc is e.g. musl, the following error is observed: ``` [ 0%] Building CXX object CMakeFiles/rocksdb.dir/db/db_impl.cc.o In file included from /go/src/github.com/cockroachdb/cockroach/c-deps/rocksdb.src/table/block.h:19:0, from /go/src/github.com/cockroachdb/cockroach/c-deps/rocksdb.src/db/db_impl.cc:77: /x-tools/x86_64-unknown-linux-musl/x86_64-unknown-linux-musl/sysroot/usr/include/malloc.h:19:8: error: declaration of size_t malloc_usable_size(void*) has a different exception specifier size_t malloc_usable_size(void *); ^~~~~~~~~~~~~~~~~~ In file included from /go/src/github.com/cockroachdb/cockroach/c-deps/rocksdb.src/db/db_impl.cc:20:0: /go/native/x86_64-unknown-linux-musl/jemalloc/include/jemalloc/jemalloc.h:78:33: note: from previous declaration size_t malloc_usable_size(void*) throw () define je_malloc_usable_size malloc_usable_size ^ /go/native/x86_64-unknown-linux-musl/jemalloc/include/jemalloc/jemalloc.h:239:41: note: in expansion of macro je_malloc_usable_size JEMALLOC_EXPORT size_t JEMALLOC_NOTHROW je_malloc_usable_size( ^~~~~~~~~~~~~~~~~~~~~ CMakeFiles/rocksdb.dir/build.make:350: recipe for target CMakeFiles/rocksdb.dir/db/db_impl.cc.o failed ``` This works around the issue by rearranging the sources such that jemallocs headers are never in the same scope as the systems malloc header. The jemalloc issue has been reported as well, see: cc tschottdorf Closes Differential Revision: D5163048 Pulled By: siying fbshipit-source-id: c553125458892def175c1be5682b0330d80b2a0d/New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
,,0.5579,rocksdb,"Support prefetch last 512KB with direct I/O in block based file reader Summary: Right now, if direct I/O is enabled, prefetching the last 512KB cannot be applied, except compaction inputs or readahead is enabled for iterators. This can create a lot of I/O for HDD cases. To solve the problem, the 512KB is prefetched in block based table if direct I/O is enabled. The prefetched buffer is passed in totegher with random access file reader, so that we try to read from the buffer before reading from the file. This can be extended in the future to support flexible user iterator readahead too. Closes Differential Revision: D5593091 Pulled By: siying fbshipit-source-id: ee36ff6d8af11c312a2622272b21957a7b5c81e7/Fix LITE unit tests Summary: Closes Differential Revision: D5505778 Pulled By: siying fbshipit-source-id: 7e935603ede3d958ea087ed6b8cfc4121e8797bc/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Encryption at rest support Summary: This PR adds support for encrypting data stored by RocksDB when written to disk. It adds an `EncryptedEnv` override of the `Env` class with matching overrides for sequential&random access files. The encryption itself is done through a configurable `EncryptionProvider`. This class creates is asked to create `BlockAccessCipherStream` for a file. This is where the actual encryption/decryption is being done. Currently there is a Counter mode implementation of `BlockAccessCipherStream` with a `ROT13` block cipher (NOTE the `ROT13` is for demo purposes only). The Counter operation mode uses an initial counter & random initialization vector (IV). Both are created randomly for each file and stored in a 4K (default size) block that is prefixed to that file. The `EncryptedEnv` implementation is such that clients of the `Env` class do not see this prefix (nor data, nor in filesize). The largest part of the prefix block is also encrypted, and there is room left for implementation specific settings/values/keys in there. To test the encryption, the `DBTestBase` class has been extended to consider a new environment variable called `ENCRYPTED_ENV`. If set, the test will setup a encrypted instance of the `Env` class to use for all tests. Typically you would run it like this: ``` ENCRYPTED_ENV=1 make check_some ``` There is also an added test that checks that some data inserted into the database is or is not ""visible"" on disk. With `ENCRYPTED_ENV` active it must not find plain text strings, with `ENCRYPTED_ENV` unset, it must find the plain text strings. Closes Differential Revision: D5322178 Pulled By: sdwilsh fbshipit-source-id: 253b0a9c2c498cc98f580df7f2623cbf7678a27f/"
,,0.3436,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
,,0.4363,rocksdb,"Fix LITE unit tests Summary: Closes Differential Revision: D5505778 Pulled By: siying fbshipit-source-id: 7e935603ede3d958ea087ed6b8cfc4121e8797bc/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Encryption at rest support Summary: This PR adds support for encrypting data stored by RocksDB when written to disk. It adds an `EncryptedEnv` override of the `Env` class with matching overrides for sequential&random access files. The encryption itself is done through a configurable `EncryptionProvider`. This class creates is asked to create `BlockAccessCipherStream` for a file. This is where the actual encryption/decryption is being done. Currently there is a Counter mode implementation of `BlockAccessCipherStream` with a `ROT13` block cipher (NOTE the `ROT13` is for demo purposes only). The Counter operation mode uses an initial counter & random initialization vector (IV). Both are created randomly for each file and stored in a 4K (default size) block that is prefixed to that file. The `EncryptedEnv` implementation is such that clients of the `Env` class do not see this prefix (nor data, nor in filesize). The largest part of the prefix block is also encrypted, and there is room left for implementation specific settings/values/keys in there. To test the encryption, the `DBTestBase` class has been extended to consider a new environment variable called `ENCRYPTED_ENV`. If set, the test will setup a encrypted instance of the `Env` class to use for all tests. Typically you would run it like this: ``` ENCRYPTED_ENV=1 make check_some ``` There is also an added test that checks that some data inserted into the database is or is not ""visible"" on disk. With `ENCRYPTED_ENV` active it must not find plain text strings, with `ENCRYPTED_ENV` unset, it must find the plain text strings. Closes Differential Revision: D5322178 Pulled By: sdwilsh fbshipit-source-id: 253b0a9c2c498cc98f580df7f2623cbf7678a27f/Fixing blob db sequence number handling Summary: Blob db rely on base db returning sequence number through write batch after DB::Write(). However after recent changes to the write path, DB::Writ()e no longer return sequence number in some cases. Fixing it by have WriteBatchInternal::InsertInto() always encode sequence number into write batch. Stacking on Closes Differential Revision: D5148358 Pulled By: yiwu-arbug fbshipit-source-id: 8bda0aa07b9334ed03ed381548b39d167dc20c33/"
,,0.4638,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/Fix bug that flush doesnt respond to fsync result Summary: With a regression bug was introduced two years ago, by , we fail to check return status of fsync call. This can cause we miss the information from the file system and can potentially cause corrupted data which we could have been detected. Closes Reviewed By: ajkr Differential Revision: D5321949 Pulled By: siying fbshipit-source-id: c68117914bb40700198fc37d0e4c63163a8a1031/"
,,0.3275,rocksdb,"Fix caching of compaction pickers next index Summary: The previous implementation of caching `file_size` index made no sense. It only remembered the original span of locked files starting from beginning of `file_size`. We should remember the index after all compactions that have been considered but rejected. This will reduce the work we do while holding the db mutex. Closes Differential Revision: D5468152 Pulled By: ajkr fbshipit-source-id: ab92a4bffe76f9f174d861bb5812b974d1013400/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/overlapping endpoint fixes in level compaction picker Summary: This diff addresses two problems. Both problems cause us to miss scheduling desirable compactions. One side effect is compaction picking can spam logs, as theres no delay after failed attempts to pick compactions. 1. If a compaction pulled in a locked input-level file due to user-key overlap, we would not consider picking another file from the same input level. 2. If a compaction pulled in a locked output-level file due to user-key overlap, we would not consider picking any other compaction on any level. The code changes are dependent, which is why I solved both problems in a single diff. Moved input-level `ExpandInputsToCleanCut` into the loop inside `PickFileToCompact`. This gives two benefits: (1) if it fails, we will try the next-largest file on the same input level; (2) we get the fully-expanded input-level key-range with which we can check for pending compactions in output level. Added another call to `ExpandInputsToCleanCut` inside `PickFileToCompact`s to check for compaction conflicts in output level. Deleted call to `IsRangeInCompaction` in `PickFileToCompact`, as `ExpandInputsToCleanCut` also correctly handles the case where original output-level files (i.e., ones not pulled in due to user-key overlap) are pending compaction. Closes Differential Revision: D5454643 Pulled By: ajkr fbshipit-source-id: ea3fb5477d83e97148951af3fd4558d2039e9872/delete ExpandInputsToCleanCut failure log Summary: I decided not even to keep it as an INFO-level log as it is too normal for compactions to be skipped due to locked input files. Removing logging here makes us consistent with how we treat locked files that werent pulled in due to overlap. We may want some error handling on line 422, which should never happen when called by `LevelCompactionBuilder::PickCompaction`, as `SetupInitialFiles` skips compactions where overlap causes the output level to pull in locked files. Closes Differential Revision: D5458502 Pulled By: ajkr fbshipit-source-id: c2e5f867c0a77c1812ce4242ab3e085b3eee0bae/FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
,,0.2047,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Fix column family seconds_up accounting Summary: `cf_stats_snapshot_.seconds_up` appears to be never updated, unlike `db_stats_snapshot_.seconds_up`, which is updated here: This leads to wrong information in the log, for example: ``` ** Compaction Stats [default] ** .... Uptime(secs): 85591.2 total, 85591.2 interval ``` Even though DBs interval is correctly logged as 60 seconds: ``` ** DB Stats ** Uptime(secs): 85591.2 total, 637.8 interval ``` Closes Differential Revision: D5114131 Pulled By: sagar0 fbshipit-source-id: 85243a38213236ccbb601a7f7aaa8865eaa8083c/Fix rocksdb.estimate-num-keys DB property underflow Summary: rocksdb.estimate-num-keys is compute from `estimate_num_keys 2 * estimate_num_deletes`. If `2 * estimate_num_deletes > estimate_num_keys` it will underflow. Fixing it. Closes Differential Revision: D5109272 Pulled By: yiwu-arbug fbshipit-source-id: e1bfb91346a59b7282a282b615002507e9d7c246/"
,,0.2104,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.2177,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.4803,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Fix interaction between CompactionFilter::Decision::kRemoveAndSkipUntÖ Summary: Fixes the following scenario: 1. Set prefix extractor. Enable bloom filters, with `whole_key_filtering false`. Use compaction filter that sometimes returns `kRemoveAndSkipUntil`. 2. Do a compaction. 3. Compaction creates an iterator with `total_order_seek false`, calls `SeekToFirst()` on it, then repeatedly calls `Next()`. 4. At some point compaction filter returns `kRemoveAndSkipUntil`. 5. Compaction calls `Seek(skip_until)` on the iterator. The key that it seeks to happens to have prefix that doesnt match the bloom filter. Since `total_order_seek false`, iterator becomes invalid, and compaction thinks that it has reached the end. The rest of the compaction input is silently discarded. The fix is to make compaction iterator use `total_order_seek true`. The implementation for PlainTable is quite awkward. Ive made `kRemoveAndSkipUntil` officially incompatible with PlainTable. If you try to use them together, compaction will fail, and DB will enter read-only mode (`bg_error_`). Thats not a very graceful way to communicate a misconfiguration, but the alternatives dont seem worth the implementation time and complexity. To be able to check in advance that `kRemoveAndSkipUntil` is not going to be used with PlainTable, wed need to extend the interface of either `CompactionFilter` or `InternalIterator`. It seems unlikely that anyone will ever want to use `kRemoveAndSkipUntil` with PlainTable: PlainTable probably has very few users, and `kRemoveAndSkipUntil` has only one user so far: us (logdevice). Closes Differential Revision: D5110388 Pulled By: lightmark fbshipit-source-id: ec29101a99d9dcd97db33923b87f72bce56cc17a/"
,,0.214,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.3099,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/using ThreadLocalPtr to hide ROCKSDB_SUPPORT_THREAD_LOCAL from publicÖ Summary: Ö headers should not reference RocksDB-specific macros (like ROCKSDB_SUPPORT_THREAD_LOCAL in this case) to public headers, `iostats_context.h` and `perf_context.h`. We shouldnt do that because users have to provide these compiler flags when building their binary with RocksDB. We should hide the thread local global variable inside our implementation and just expose a function api to retrieve these variables. It may break some users for now but good for long term. make check Closes Differential Revision: D5177896 Pulled By: lightmark fbshipit-source-id: 6fcdfac57f2e2dcfe60992b7385c5403f6dcb390/Fix rocksdb.estimate-num-keys DB property underflow Summary: rocksdb.estimate-num-keys is compute from `estimate_num_keys 2 * estimate_num_deletes`. If `2 * estimate_num_deletes > estimate_num_keys` it will underflow. Fixing it. Closes Differential Revision: D5109272 Pulled By: yiwu-arbug fbshipit-source-id: e1bfb91346a59b7282a282b615002507e9d7c246/"
,,0.511,rocksdb,"add VerifyChecksum() to db.h Summary: We need a tool to check any sst file corruption in the db. It will check all the sst files in current version and read all the blocks (data, meta, index) with checksum verification. If any verification fails, the function will return non-OK status. Closes Differential Revision: D5324269 Pulled By: lightmark fbshipit-source-id: 6f8a272008b722402a772acfc804524c9d1a483b/Fix FIFO Compaction with TTL tests Summary: FIFOCompactionWithTTLTest was flaky when run in parallel earlier, and hence it was disabled. Fixed it now. Also, faking sleep now instead of really sleeping to make tests more realistic by using TTLs like 1 hour and 1 day. Closes Differential Revision: D5506038 Pulled By: sagar0 fbshipit-source-id: deb429a527f045e3e2c5138b547c3e8ac8586aa2/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/"
,,0.4426,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Encryption at rest support Summary: This PR adds support for encrypting data stored by RocksDB when written to disk. It adds an `EncryptedEnv` override of the `Env` class with matching overrides for sequential&random access files. The encryption itself is done through a configurable `EncryptionProvider`. This class creates is asked to create `BlockAccessCipherStream` for a file. This is where the actual encryption/decryption is being done. Currently there is a Counter mode implementation of `BlockAccessCipherStream` with a `ROT13` block cipher (NOTE the `ROT13` is for demo purposes only). The Counter operation mode uses an initial counter & random initialization vector (IV). Both are created randomly for each file and stored in a 4K (default size) block that is prefixed to that file. The `EncryptedEnv` implementation is such that clients of the `Env` class do not see this prefix (nor data, nor in filesize). The largest part of the prefix block is also encrypted, and there is room left for implementation specific settings/values/keys in there. To test the encryption, the `DBTestBase` class has been extended to consider a new environment variable called `ENCRYPTED_ENV`. If set, the test will setup a encrypted instance of the `Env` class to use for all tests. Typically you would run it like this: ``` ENCRYPTED_ENV=1 make check_some ``` There is also an added test that checks that some data inserted into the database is or is not ""visible"" on disk. With `ENCRYPTED_ENV` active it must not find plain text strings, with `ENCRYPTED_ENV` unset, it must find the plain text strings. Closes Differential Revision: D5322178 Pulled By: sdwilsh fbshipit-source-id: 253b0a9c2c498cc98f580df7f2623cbf7678a27f/Introduce OnBackgroundError callback Summary: Some users want to prevent rocksdb from entering read-only mode in certain error cases. This diff gives them a callback, `OnBackgroundError`, that they can use to achieve it. call `OnBackgroundError` every time we consider setting `bg_error_`. Use its result to assign `bg_error_` but not to change the functions return status. classified calls using `BackgroundErrorReason` to give the callback some info about where the error happened renamed `ParanoidCheck` to something more specific so we can provide a clear `BackgroundErrorReason` unit tests for the most common cases: flush or compaction errors Closes Differential Revision: D5300190 Pulled By: ajkr fbshipit-source-id: a0ea4564249719b83428e3f4c6ca2c49e366e9b3/"
,,0.5196,rocksdb,"fix asan/valgrind for TableCache cleanup Summary: Breaking commit: d12691b86fb788f0ee7180db626c4ea2445fa976 In the above commit, I moved the `TableCache` cleanup logic from `Version` destructor into `PurgeObsoleteFiles`. I missed cleaning up `TableCache` entries for the current `Version` during DB destruction. This PR adds that logic to `VersionSet` destructor. One unfortunate side effect is now were potentially deleting `TableReader`s after `column_family_set_.reset()`, which means we cant call `BlockBasedTableReader::Close` a second time as the block cache might already be destroyed. Closes Differential Revision: D5515108 Pulled By: ajkr fbshipit-source-id: 2cb820e19aa813e0d258d17f76b2d7b6b7ee0b18/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Improve Status message for block checksum mismatches Summary: Weve got some DBs where iterators return Status with message ""Corruption: block checksum mismatch"" all the time. Thats not very informative. It would be much easier to investigate if the error message contained the file name then we would know e.g. how old the corrupted file is, which would be very useful for finding the root cause. This PR adds file name, offset and other stuff to some block corruption-related status messages. It doesnt improve all the error messages, just a few that were easy to improve. Im mostly interested in ""block checksum mismatch"" and ""Bad table magic number"" since theyre the only corruption errors that Ive ever seen in the wild. Closes Differential Revision: D5345702 Pulled By: al13n321 fbshipit-source-id: fc8023d43f1935ad927cef1b9c55481ab3cb1339/FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/Fix Data Race Between CreateColumnFamily() and GetAggregatedIntProperty() Summary: CreateColumnFamily() releases DB mutex after adding column family to the set and install super version (to write option file), so if users call GetAggregatedIntProperty() in the middle, then super version will be null and the process will crash. Fix it by skipping those column families without super version installed. Maybe we should also fix the problem of releasing the lock when reading option file, but it is more risky. so Im doing a quick and safer fix and we can investigate it later. Closes Differential Revision: D5298053 Pulled By: siying fbshipit-source-id: 4b3c8f91c60400b163fcc6cda8a0c77723be0ef6/Fix Clang release build broken by 5582123dee8426a5191dfd5e846cea8c676c793c Summary: 5582123dee8426a5191dfd5e846cea8c676c793c broken CLANG release build because of an unexpected change. Fix it. Closes Differential Revision: D5236297 Pulled By: siying fbshipit-source-id: 1b410adf13ded149c53e8235e9ea9f3130fb5403/Fix interaction between CompactionFilter::Decision::kRemoveAndSkipUntÖ Summary: Fixes the following scenario: 1. Set prefix extractor. Enable bloom filters, with `whole_key_filtering false`. Use compaction filter that sometimes returns `kRemoveAndSkipUntil`. 2. Do a compaction. 3. Compaction creates an iterator with `total_order_seek false`, calls `SeekToFirst()` on it, then repeatedly calls `Next()`. 4. At some point compaction filter returns `kRemoveAndSkipUntil`. 5. Compaction calls `Seek(skip_until)` on the iterator. The key that it seeks to happens to have prefix that doesnt match the bloom filter. Since `total_order_seek false`, iterator becomes invalid, and compaction thinks that it has reached the end. The rest of the compaction input is silently discarded. The fix is to make compaction iterator use `total_order_seek true`. The implementation for PlainTable is quite awkward. Ive made `kRemoveAndSkipUntil` officially incompatible with PlainTable. If you try to use them together, compaction will fail, and DB will enter read-only mode (`bg_error_`). Thats not a very graceful way to communicate a misconfiguration, but the alternatives dont seem worth the implementation time and complexity. To be able to check in advance that `kRemoveAndSkipUntil` is not going to be used with PlainTable, wed need to extend the interface of either `CompactionFilter` or `InternalIterator`. It seems unlikely that anyone will ever want to use `kRemoveAndSkipUntil` with PlainTable: PlainTable probably has very few users, and `kRemoveAndSkipUntil` has only one user so far: us (logdevice). Closes Differential Revision: D5110388 Pulled By: lightmark fbshipit-source-id: ec29101a99d9dcd97db33923b87f72bce56cc17a/Fix TSAN: avoid arena mode with range deletions Summary: The range deletion meta-block iterators werent getting cleaned up properly since they dont support arena allocation. I didnt implement arena support since, in the general case, each iterator is used only once and separately from all other iterators, so there should be no benefit to data locality. Anyways, this diff fixes up by treating range deletion iterators as non-arena-allocated. Closes Differential Revision: D5171119 Pulled By: ajkr fbshipit-source-id: bef6f5c4c5905a124f4993945aed4bd86e2807d8/"
,,0.214,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.3193,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/CodeMod: Prefer ADD_FAILURE() over EXPECT_TRUE(false), et cetera Summary: CodeMod: Prefer `ADD_FAILURE()` over `EXPECT_TRUE(false)`, et cetera. The tautologically-conditioned and tautologically-contradicted boolean expectations/assertions have better alternatives: unconditional passes and failures. Reviewed By: Orvid Differential Revision: D5432398 Tags: codemod, codemod-opensource fbshipit-source-id: d16b447e8696a6feaa94b41199f5052226ef6914/"
,,0.2195,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/"
,,0.3092,rocksdb,"Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/"
,,0.0932,rocksdb,"Provide an option so that SST ingestion wont fall back to copy after hard linking fails (#5333) Summary: RocksDB always tries to perform a hard link operation on the external SST file to ingest. This operation can fail if the external SST resides on a different device/FS, or the underlying FS does not support hard link. Currently RocksDB assumes that if the link fails, the user is willing to perform file copy, which is not true according to the post. This commit provides an option named failed_move_fall_back_to_copy for users to choose which behavior they want. Pull Request resolved: Differential Revision: D15457597 Pulled By: HaoyuHuang fbshipit-source-id: f3626e13f845db4f7ed970a53ec8a2b1f0d62214/"
