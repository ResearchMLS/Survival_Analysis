Topic_no,Keywords,Contrib,System,Text
7,"key, add, filter, implementation, bloom_filter, datum, size, run, unit, memory, pull_request, query, count, issue, access, spelling_error, summary, bit, change, index",0.2005,conscrypt,Add TLS 1.3 benchmarks (#543) Adds a BenchmarkProtocol parameter to ClientSocketBenchmark and EngineHandshakeBenchmark to determine which protocol to use. Switched EngineHandshakeBenchmark off TestUtils.doEngineHandshake and onto its own implementation. This lets us simulate network transit time as well as eliminating all the test assertions and generally operating closer to the typical usage of an SSLEngine. Also adds the ability to specify JMH params on the command line./
,,0.2101,conscrypt,Add TLS 1.3 benchmarks (#543) Adds a BenchmarkProtocol parameter to ClientSocketBenchmark and EngineHandshakeBenchmark to determine which protocol to use. Switched EngineHandshakeBenchmark off TestUtils.doEngineHandshake and onto its own implementation. This lets us simulate network transit time as well as eliminating all the test assertions and generally operating closer to the typical usage of an SSLEngine. Also adds the ability to specify JMH params on the command line./
,,0.2005,conscrypt,Add TLS 1.3 benchmarks (#543) Adds a BenchmarkProtocol parameter to ClientSocketBenchmark and EngineHandshakeBenchmark to determine which protocol to use. Switched EngineHandshakeBenchmark off TestUtils.doEngineHandshake and onto its own implementation. This lets us simulate network transit time as well as eliminating all the test assertions and generally operating closer to the typical usage of an SSLEngine. Also adds the ability to specify JMH params on the command line./
,,0.2118,conscrypt,Add TLS 1.3 benchmarks (#543) Adds a BenchmarkProtocol parameter to ClientSocketBenchmark and EngineHandshakeBenchmark to determine which protocol to use. Switched EngineHandshakeBenchmark off TestUtils.doEngineHandshake and onto its own implementation. This lets us simulate network transit time as well as eliminating all the test assertions and generally operating closer to the typical usage of an SSLEngine. Also adds the ability to specify JMH params on the command line./
,,0.223,conscrypt,Add TLS 1.3 benchmarks (#543) Adds a BenchmarkProtocol parameter to ClientSocketBenchmark and EngineHandshakeBenchmark to determine which protocol to use. Switched EngineHandshakeBenchmark off TestUtils.doEngineHandshake and onto its own implementation. This lets us simulate network transit time as well as eliminating all the test assertions and generally operating closer to the typical usage of an SSLEngine. Also adds the ability to specify JMH params on the command line./
,,0.2166,conscrypt,Add TLS 1.3 benchmarks (#543) Adds a BenchmarkProtocol parameter to ClientSocketBenchmark and EngineHandshakeBenchmark to determine which protocol to use. Switched EngineHandshakeBenchmark off TestUtils.doEngineHandshake and onto its own implementation. This lets us simulate network transit time as well as eliminating all the test assertions and generally operating closer to the typical usage of an SSLEngine. Also adds the ability to specify JMH params on the command line./
,,0.0661,frostwire,[android] commented suggestion for future debugging on restart waitForDebugger() call/
,,0.0908,frostwire,"[desktop] Fixes false positive VPN detection issue After VPN technology updates, its no longer reliable to query for network interfaces that contain the string tun in them. Nowadays such interfaces are added and active even if theyre not part of the ip route table. Were using a similar technique for posix to detect if the tunnel interface is the parting ip on the route table, every 20 seconds/"
,,0.0778,frostwire,[android] fix issue with documents query filter/
,,0.0778,frostwire,[android] fix issue with documents query filter/
,,0.0648,frostwire,[desktop] ExpressVPN detection fixed. build 253. changelog/
,,0.071,frostwire,[desktop] fix UI presentation of invalid values in search filter/
,,0.0653,frostwire,"[android] TransferManager.onPreferenceChanged() refactor Easier to debug than with previous double lambda, now we know if BTEngine.getInstance() is being held, where its actually happening, also better readability/"
,,0.066,frostwire,[common] avoid excessive invocation of isValidHtml/
,,0.0731,frostwire,[common] avoid excessive invocation of isValidHtml/[common/desktop] Yify search fixes/
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1017,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1017,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.0945,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/[desktop] Java 9 reflection deprecation warning fixed/"
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.1038,frostwire,"[desktop] deprecation fixes, unboxing, labmda refactors/"
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0828,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0891,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0828,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.0849,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.087,jna,Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
,,0.387,jna,"Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.3779,jna,"fix merge conflicts for StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.3404,jna,"fix mappings to properly work on 32-bit windows (fixes memory faults)/Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.0664,jna,fix line terminators/
,,0.0929,jna,Merge pull request from matthiasblaesing/fix_wince_arm Fix wince-arm detection (affects only unittests)/Fix wince-arm detection (affects only unittests)/Merge pull request from matthiasblaesing/sspi2 Enhance SSPI function converage and bugfix SecBufferDesc/
,,0.0664,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.3461,jna,"fix merge conflicts for javadoc errors/Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0954,jna,"Make Netapi32Utils.getUserInfo(String,String) work. Netapi32Utils.getUserInfo was ignoring the parameter for the domain name, which caused it to fail when the ""domain"" name was not actually a domain controller. On fixing that, I got an error about the SID being in an invalid format. It seems like JNAs ""PSID"" is actually just the SID, so to get PSID you have to use ""PSID.ByReference"". This is bound to confuse someone else in the future so it might be worth renaming PSID to SID?/"
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.3596,jna,"Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.34600000000000003,jna,"Merge pull request from java-native-access/w32api-cleanup Avoid overwriting test results from different platforms/Avoid overwriting test results from different platforms/Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.3658,jna,"fix javadoc errors/Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.3631,jna,"fix javadoc errors/Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.0686,jna,fix line terminators/
,,0.3917,jna,"Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0891,jna,fix line terminators/Fixed PR number in CHANGES.md/Merge pull request from headcrashing/CLSIDFromProgID Clsid from prog/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.3658,jna,"Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/fix
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.3427,jna,"Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.0838,jna,fix line terminators/Merge pull request from headcrashing/WindowMessages RegisterWindowMessage/
,,0.3908,jna,"Added Win32 mappings. * Advapi32 * Added LOGON_WITH_PROFILE constant * Added LOGON_NETCREDENTIALS_ONLY constant * Added CreateProcessWithLogonW(String, String, String, int, String, String, int, Pointer, String, STARTUPINFO, PROCESS_INFORMATION) * No Unit Test I would have to make it create a user to be 100% able to run a process as another user and that would be a security issue I figure. * Crypt32 * Added CertAddEncodedCertificateToSystemStore(String, Pointer, DWORD) * No Unit Test I doubt anyone would want the security risk of a unit test installing a root certificate. * GDI32 * Added SRCCOPY constant * Added BitBlt(HDC, int, int, int, int, HDC, int, int, int) * No direct unit test the test for GDI32Util.getScreenshot() seemed to cover it just fine. * Added GDI32Util.getScreenshot(HWND) * Added unit test as GDI32UtilTest.testGetScreenshot() * Shell32 * Added SHERB_NOCONFIRMATION constant * Added SHERB_NOPROGRESSUI constant * Added SHERB_NOSOUND constant * Added SEE_MASK_NOCLOSEPROCESS constant * Added SHEmptyRecycleBin(HANDLE, String, int) * No unit test no idea how to tell if the recycle bin is empty afterwards * Added ShellExecuteEx(SHELLEXECUTEINFO) * No unit test there are a bunch of cases where the hProcess member in SHELLEXECUTEINFO isnt set not sure how to control for that * ShellAPI * Added SHELLEXECUTEINFO structure * User32 * Added GetDesktopWindow() * Added test as User32Test.testGetDesktopWindow() * WinGDI * Added HGDI_ERROR * Removed superfluous ""public"" and ""public final"" from WinGDI/"
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0664,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.063,jna,"Fix REFIID signatures in tests, fix testFindName/"
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0686,jna,fix line terminators/
,,0.0588,jna,"Fix REFIID signatures in tests, fix testFindName/"
,,0.0673,jna,"Fix REFIID signatures in tests, fix testFindName/"
,,0.0652,jna,Fix PdhTest + update PdhEnumObjectItems/
,,0.0652,jna,Fix PdhTest + update PdhEnumObjectItems/
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.078,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./
,,0.1451,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.0762,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./
,,0.0726,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1534,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1584,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.0726,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./
,,0.0744,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1551,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1434,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.0797,OpenDDS,Wed Nov 3 14:23:58 UTC 2010 Don Hudson Added dump() for transport configurations. Changed configuration of UDP transport to fail if the local address isnt set. Removed Messenger default UDP test./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1896,OpenDDS,Tue Nov 2 19:09:03 UTC 2010 Don Hudson Made minor tweaks to formatting of error messages for consistency and readability that were missed in the previous commit./Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1188,OpenDDS,"Wed Nov 3 15:29:11 UTC 2010 Don Hudson Removed DataReaderImpl::operator<<(ostream& str, WriterInfo::WriterState value) since it didnt function as intended on some platforms because WriterState is an enum nad replaced it with get_state_str(). Added type ENTITYKIND_OPENDDS_NIL_WRITER for use in infrastructure testing where there is no writer. Changed FooTest4 to use ENTITYKIND_OPENDDS_NIL_WRITER since the test has no writer by design. This eliminates multiple warnings ""failed to find publication data for ..."" that are misleading for this test./Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./"
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.066,OpenDDS,Thu Nov 4 19:52:05 UTC 2010 Johnny Willemsen * dds/DCPS/DataSampleHeader.h: * dds/DCPS/DataSampleList.h: * dds/DCPS/DataWriterImpl.h: * dds/DCPS/WriteDataContainer.h: Fixed some typos in comments/
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.1364,OpenDDS,Tue Nov 2 18:29:27 UTC 2010 Don Hudson Made minor tweaks to formatting of error and debug messages for consistency and readability. Corrected a few incorrect method names in error and debug messages. Corrected a few spelling errors./
,,0.0642,OpenDDS,"completed basic fragmentation (no NACK_FRAG handling), fixed a bug in reassembly/"
,,0.0661,OpenDDS,"completed basic fragmentation (no NACK_FRAG handling), fixed a bug in reassembly/"
,,0.0758,OpenDDS,Same ostrstream fix make only for lynxos/Fix build errors on LynxOS/
,,0.098,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1081,OpenDDS,Fix wide-character builds./More fixes for localhost./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.0968,OpenDDS,Backing out fix for gcc 4.1./Dynamic cast workaround for GCC 4.1 (for unit test)./Merge pull request from objectcomputing/writer_reader_impl_templates Fix build error for safety profile./Fix build errors for safety profile./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.098,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.134,OpenDDS,"Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/TS API Update: Get_Connection_Parameters() Treat name and ID as both optional. If not provided, fill in, if both provided, validate, if neither provided, error. Connection_Id is only out param if specified as 0 otherwise will assume connection id has been specified. Updated Messenger tests get connection params validation section with a couple additional scenarios to test new functionality./Merge pull request from mitza-oci/master Test fixed data in FACE Messenger./Test fixed data in FACE Messenger./"
,,0.1207,OpenDDS,fix typo/Fixed bug in ACE logging macro usage./Only print out fixed field if its not the right value./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/Merge pull request from mitza-oci/master Test fixed data in FACE Messenger./Test fixed data in FACE Messenger./
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1193,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix build error for safety profile./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.0905,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix build error for safety profile./Fix build errors./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.098,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.0965,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/Fix compiler warning/Fix warning/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.098,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.096,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.098,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.096,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Fixed fuzz errors/Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.098,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1216,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Replace standard containers with OPENDDS variants to avoid calls to global new./Merge pull request from objectcomputing/writer_reader_impl_templates Fix errors in no BITS builds and java builds./Fix built-in-topics builds./Fixes for no builtin topics builds./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.098,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1176,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix errors in no BITS builds and java builds./Fix built-in-topics builds./Fixes for no builtin topics builds./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.098,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1196,OpenDDS,Export allocator to avoid problems with multiple compilation units./Merge pull request from objectcomputing/writer_reader_impl_templates Replace standard containers with OPENDDS variants to avoid calls to global new./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.098,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Fix no BIT builds./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1082,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix errors in no BITS builds and java builds./Fix built-in-topics builds./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1025,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix errors in no BITS builds and java builds./Fix built-in-topics builds./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1063,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix errors in no BITS builds and java builds./Fix built-in-topics builds./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1082,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix errors in no BITS builds and java builds./Fix built-in-topics builds./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1063,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix errors in no BITS builds and java builds./Fix built-in-topics builds./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.0967,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/Fix missing includes and build errors./
,,0.1215,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix errors in no BITS builds and java builds./Fixes for no builtin topics builds./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.098,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.0929,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix errors in no BITS builds and java builds./Fix built-in-topics builds./Fixes for no builtin topics builds./Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/Provide some context for Redmine Issue# 1446 error message logging/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1078,OpenDDS,"Add Dcps_debug_level guards around logging. Add typedefs for Entitiess maps/Wrap debug statement in Dcps_debug_level check/fix trailing whitespace/Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/TS API Update: Get_Connection_Parameters() Treat name and ID as both optional. If not provided, fill in, if both provided, validate, if neither provided, error. Connection_Id is only out param if specified as 0 otherwise will assume connection id has been specified. Updated Messenger tests get connection params validation section with a couple additional scenarios to test new functionality./"
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.1,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/
,,0.0717,OpenDDS,Merge pull request from objectcomputing/writer_reader_impl_templates Fix errors in no BITS builds and java builds./Fixes for no builtin topics builds./
,,0.0987,OpenDDS,Merge remote branch upstream/master into face-conf-test-suite Conflicts: dds/CORBA/tao/BasicSequences.cpp dds/CORBA/tao/ORB_Misc.cpp dds/CORBA/tao/OctetSeqC.cpp dds/CORBA/tao/SystemException.cpp/Fix missing includes and build errors./
,,0.0873,OpenDDS,Merge pull request from mitza-oci/master FACE bug fixes/FACE messenger test: fixed invalid logging statement (missing param)/
,,0.0648,OpenDDS,Fix TransportHeader size in Shmem for TransportHeader source_ change from ACE_INT32 to ACE_INT64/
,,0.0657,OpenDDS,"Introduce a new Loaner interface which is now used as part of the zero copy sequences so that they dont pull in the full reader implementation, see issue * dds/DCPS/Loaner.h: Added. * dds/DCPS/DataReaderImpl.h: * dds/DCPS/MultiTopicDataReaderBase.h: * dds/DCPS/WriterInfo.h: * dds/DCPS/ZeroCopySeq_T.h: * dds/DCPS/ZeroCopySeq_T.inl:/"
,,0.0676,OpenDDS,"Introduce a new Loaner interface which is now used as part of the zero copy sequences so that they dont pull in the full reader implementation, see issue * dds/DCPS/Loaner.h: Added. * dds/DCPS/DataReaderImpl.h: * dds/DCPS/MultiTopicDataReaderBase.h: * dds/DCPS/WriterInfo.h: * dds/DCPS/ZeroCopySeq_T.h: * dds/DCPS/ZeroCopySeq_T.inl:/"
,,0.0534,OpenDDS,Coverity Scan Defect: Addressing Unchecked dynamic_cast issues./
,,0.0577,OpenDDS,Merge branch master into jwi-endhistorysampleleak/Fix issue where setting DCPSBitTransportPort without DCPSBitTransportIPAddress has no effect/
,,0.0577,OpenDDS,Fix unique_ptr on C++03 mode/
,,0.0628,OpenDDS,"Merge pull request from jwillemsen/jwi-dp-handleexceptionresult Instead of returning an error we have to set the shutdown_result_, thÖ/"
,,0.0669,OpenDDS,crypto plugin: submsg encryption (bug fixes)/
,,0.0556,OpenDDS,Merge pull request from huangminghuang/deadlock-fix Deadlock fix/Fix deadlock issue/
,,0.0806,OpenDDS,"Dissector: Removed RCH, Fixed Warnings Using Reference Count Handles was not working out as well as I liked, so I have removed them, but left the double tree pass initialization so that something similar (along the lines of the Proxy pattern) might be able to be done in the future maybe. Also got rid of proto_tree_add_*_format() warnings by inserting ""%s"" in the parameters./"
,,0.0837,OpenDDS,"Dissector: Removed RCH, Fixed Warnings Using Reference Count Handles was not working out as well as I liked, so I have removed them, but left the double tree pass initialization so that something similar (along the lines of the Proxy pattern) might be able to be done in the future maybe. Also got rid of proto_tree_add_*_format() warnings by inserting ""%s"" in the parameters./"
,,0.1031,OpenDDS,"Incremental addition of ifdefs in dds/DCPS/RTPS/*; build fails though/fixes for secure endpoint discovery and added secure discovery scenario test/secure endpoint discovery fixes / cleanup/fixing more error messages for test scripts/adding support for 8.8.7.1 (unauth) and fixing half_sec test issues (also unauth)/failed remote topic permissions check should be a warning (not an error)/Initial implementation of 8.8.5 and rest of 8.8.6, changes to tests & script & dds log messages to allow for failure test/"
,,0.1019,OpenDDS,Merge pull request from mitza-oci/master Modeling tests: fixed typo/Modeling tests: fixed typo/fixup Problem: Data(Reader|Writer)Impl dont check that topic is enabled/
,,0.1216,OpenDDS,fixup Problem: Data(Reader|Writer)Impl dont check that topic is enabled/
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.1075,OpenDDS,"removing mutex locking from static discovery tests data reader listener, fixing issue where heartbeat recipient guids for static discoverys interesting readers are lost when reliable writer exists and produces its own heartbeat/fixing bug setting prev_dst in send_bundled_responses, switching static discovery acknacks to use send_bundled_replies, adding mutex protection to static discovery test listener/"
,,0.5194,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5214,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.2879,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Fix errors revealed by tests/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./"
,,0.5905,OpenDDS,"Merge pull request from jrw972/coverity Coverity issues/Coverity 1484373/Coverity 1484374/Coverity 1484375/Coverity 1484376/delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/resolving review comments participant flag type renaming, re-adding check for is_expectant_opendds to EndpointManager::match(), const bool issue/removing poorly-scaling vendor-specific extension for datareader announcement by adding new vendor-specific extension to participant announcement, indicating that incoming datareader annoucements are not expected to contain associated datawriters. This should allow us to remove datareader announcement of associated writers without breaking backwards compatibility with previous versions of OpenDDS/Fix errors revealed by tests/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Merge pull request from jrw972/master Make inconsistent topic tests fail predictably/Problem: Creating inconsistent topic in same participant succeeds This is only an issue in RTPS. Solution: Check for inconsistent topic when asserting topic with RTPS./Problem: Create topic fails for multiple participants in same process When using RTPS, multiple participants in different processes can create inconsistent topics. However, when the participants are in the same process, an inconsistent topic cannot be created. Solution: Force all knowledge of topics to the endpoint manager of discovery./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5296,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5224,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.3653,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Merge pull request from jrw972/master Make inconsistent topic tests fail predictably/Problem: Create topic fails for multiple participants in same process When using RTPS, multiple participants in different processes can create inconsistent topics. However, when the participants are in the same process, an inconsistent topic cannot be created. Solution: Force all knowledge of topics to the endpoint manager of discovery./"
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.3369,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.4294,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.3298,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.3369,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.3312,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.3312,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.3369,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.4268,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4268,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.3312,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.3241,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.3269,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4268,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5334,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4268,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4294,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4559,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./removing mutex locking from static discovery tests data reader listener, fixing issue where heartbeat recipient guids for static discoverys interesting readers are lost when reliable writer exists and produces its own heartbeat/fixing iterator comparison bug (wrong map variable) and preserving heartbeat counts when matched reliable readers disappear for a writer/fixing bug setting prev_dst in send_bundled_responses, switching static discovery acknacks to use send_bundled_replies, adding mutex protection to static discovery test listener/skipping send_bundled_responses for empty response vectors, removing testing assertion, setting default max_bundle_size to be big (udp_max_message_size rtps header size), making max_bundle_size configurable (per rtps_udp instance)/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.3969,OpenDDS,"Merge pull request from jrw972/mixed-rtps Serialization error when mixing secure and plain RTPS submessages/Problem: Send error when mixing secure and plain submessages Solution: Allow plain submessages indicatated by null cryptohandle./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4109,OpenDDS,"ICE and ACE_RECVPKTINFO Fix for compiling with security and earlier versions of ACE/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4148,OpenDDS,"ICE: avoid error from setsockopt on IPv6/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4065,OpenDDS,"skipping send_bundled_responses for empty response vectors, removing testing assertion, setting default max_bundle_size to be big (udp_max_message_size rtps header size), making max_bundle_size configurable (per rtps_udp instance)/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4268,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.428,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./fix security test issues (double association / volatile gap without info_dst / secure discovery), fix rtps transport test to associate correctly/Incorporated fixes from PR review The beacon message was changed to be a valid RTPS Pad submessage, and the RelayHandler now tests for the value of the first byte as well as the length./removing mutex locking from static discovery tests data reader listener, fixing issue where heartbeat recipient guids for static discoverys interesting readers are lost when reliable writer exists and produces its own heartbeat/fixing iterator comparison bug (wrong map variable) and preserving heartbeat counts when matched reliable readers disappear for a writer/fixing bug setting prev_dst in send_bundled_responses, switching static discovery acknacks to use send_bundled_replies, adding mutex protection to static discovery test listener/skipping send_bundled_responses for empty response vectors, removing testing assertion, setting default max_bundle_size to be big (udp_max_message_size rtps header size), making max_bundle_size configurable (per rtps_udp instance)/cleaning up some warnings / errors for older compilers/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./Merge pull request from mitza-oci/pr1128-fix Fixed an issue in with fragmentation/Fixed an issue in with fragmentation/"
,,0.3988,OpenDDS,"skipping send_bundled_responses for empty response vectors, removing testing assertion, setting default max_bundle_size to be big (udp_max_message_size rtps header size), making max_bundle_size configurable (per rtps_udp instance)/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.3312,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.3554,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./Merge pull request from simpsont-oci/tcp_transport_local_address_resolution_failure adding configure_i failure for fqdn resolution failure for TcpTransport/adding configure_i failure for fqdn resolution failure for TcpTransport/"
,,0.5344,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5344,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.3355,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.3184,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.4294,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4229,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.2701,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Merge pull request from jwillemsen/jwi-missingdebuglevelcheck Check the debug level before logging the error/Check the debug level before logging the error * dds/DCPS/DomainParticipantImpl.cpp: * dds/DCPS/TopicImpl.cpp:/Fix errors revealed by tests/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./"
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4268,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4268,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.41700000000000004,OpenDDS,"Merge pull request from jrw972/coverity Coverity issues/Coverity 1481723/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.3056,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./"
,,0.3071,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./"
,,0.2995,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./"
,,0.4268,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.0609,OpenDDS,Merge pull request from c4m3lc4s3/master Fixed listener invocation for Presentation QoS policy coherent_access/
,,0.3298,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./"
,,0.4819,OpenDDS,"Misc Style Fixes/fixup Problem: Data(Reader|Writer)Impl dont check that topic is enabled/Data*Impls: Misc Fixes/Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Merge pull request from iguessthislldo/igtd/presqos Fixes for Bugs Introduced in 1094/Fix Group Presentation by Instance QoS/Merge pull request from c4m3lc4s3/master Fixed listener invocation for Presentation QoS policy coherent_access/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Call Listeners on Topic Coherent Set Completion Fix For: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5043,OpenDDS,"resolving review comments participant flag type renaming, re-adding check for is_expectant_opendds to EndpointManager::match(), const bool issue/removing poorly-scaling vendor-specific extension for datareader announcement by adding new vendor-specific extension to participant announcement, indicating that incoming datareader annoucements are not expected to contain associated datawriters. This should allow us to remove datareader announcement of associated writers without breaking backwards compatibility with previous versions of OpenDDS/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5854,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/resolving review comments participant flag type renaming, re-adding check for is_expectant_opendds to EndpointManager::match(), const bool issue/removing poorly-scaling vendor-specific extension for datareader announcement by adding new vendor-specific extension to participant announcement, indicating that incoming datareader annoucements are not expected to contain associated datawriters. This should allow us to remove datareader announcement of associated writers without breaking backwards compatibility with previous versions of OpenDDS/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Merge pull request from jrw972/master Make inconsistent topic tests fail predictably/Problem: Create topic fails for multiple participants in same process When using RTPS, multiple participants in different processes can create inconsistent topics. However, when the participants are in the same process, an inconsistent topic cannot be created. Solution: Force all knowledge of topics to the endpoint manager of discovery./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5886,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./removing poorly-scaling vendor-specific extension for datareader announcement by adding new vendor-specific extension to participant announcement, indicating that incoming datareader annoucements are not expected to contain associated datawriters. This should allow us to remove datareader announcement of associated writers without breaking backwards compatibility with previous versions of OpenDDS/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5797,OpenDDS,"fix security test issues (double association / volatile gap without info_dst / secure discovery), fix rtps transport test to associate correctly/Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/removing poorly-scaling vendor-specific extension for datareader announcement by adding new vendor-specific extension to participant announcement, indicating that incoming datareader annoucements are not expected to contain associated datawriters. This should allow us to remove datareader announcement of associated writers without breaking backwards compatibility with previous versions of OpenDDS/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Merge pull request from jrw972/master Make inconsistent topic tests fail predictably/Problem: Create topic fails for multiple participants in same process When using RTPS, multiple participants in different processes can create inconsistent topics. However, when the participants are in the same process, an inconsistent topic cannot be created. Solution: Force all knowledge of topics to the endpoint manager of discovery./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4281,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5759,OpenDDS,"InfoRepo does not start correctly on Windows InfoRepo has a static initializer which results in the construction of a reactor. This must be preceded by a call to ACE::init(). To fix this, we delay the creation of the reactor to the open method of the ReactorTask./delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./resolving review comments participant flag type renaming, re-adding check for is_expectant_opendds to EndpointManager::match(), const bool issue/removing poorly-scaling vendor-specific extension for datareader announcement by adding new vendor-specific extension to participant announcement, indicating that incoming datareader annoucements are not expected to contain associated datawriters. This should allow us to remove datareader announcement of associated writers without breaking backwards compatibility with previous versions of OpenDDS/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4148,OpenDDS,"Merge pull request from jrw972/coverity Coverity issues/fixup Coverity 1481717 and 1481688/Coverity 1481717 and 1481688/Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.3468,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Merge pull request from jwillemsen/jwi-missingdebuglevelcheck Check the debug level before logging the error/Check the debug level before logging the error * dds/DCPS/DomainParticipantImpl.cpp: * dds/DCPS/TopicImpl.cpp:/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Merge pull request from jrw972/master Make inconsistent topic tests fail predictably/Problem: Create topic fails for multiple participants in same process When using RTPS, multiple participants in different processes can create inconsistent topics. However, when the participants are in the same process, an inconsistent topic cannot be created. Solution: Force all knowledge of topics to the endpoint manager of discovery./Merge pull request from objectcomputing/opendds-issue-1100 Issue 1100: Domain Participant interaction with Monitoring library/"
,,0.292,OpenDDS,"delete_contained_entities hangs when Service Participant thread is interrupted (#1206) An interrupt delievered to the Service Participant thread causes the run_reactor_event_loop function to return with an ""interrupted system call"" error. An attempt to shutdown will notify the now stopped reactor. Since the reactor isnt running, DomainParticipantImpl::handle_exception is never called. Since handle_exception is never called, the condition variable is not released and shutdown hangs. The solution is to block signals in the Service Participant thread (and other threads that are running the reactor event loop). This was already being done in the transport reactor task so it was promoted and similar classes were consolidated./Merge pull request from simpsont-oci/security_registry_instance_fix Fix security registry instance creation during shutdown issues/avoid accidental creation of ServiceRegistryInstance during shutdown (do we need this for TransportRegistry too?/"
,,0.2995,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./"
,,0.3762,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Merge pull request from jrw972/master Make inconsistent topic tests fail predictably/Problem: Create topic fails for multiple participants in same process When using RTPS, multiple participants in different processes can create inconsistent topics. However, when the participants are in the same process, an inconsistent topic cannot be created. Solution: Force all knowledge of topics to the endpoint manager of discovery./"
,,0.3665,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Merge pull request from jrw972/master Make inconsistent topic tests fail predictably/Problem: Create topic fails for multiple participants in same process When using RTPS, multiple participants in different processes can create inconsistent topics. However, when the participants are in the same process, an inconsistent topic cannot be created. Solution: Force all knowledge of topics to the endpoint manager of discovery./"
,,0.5224,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5255,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4764,OpenDDS,"resolving review comments participant flag type renaming, re-adding check for is_expectant_opendds to EndpointManager::match(), const bool issue/Merge pull request from jrw972/master Make inconsistent topic tests fail predictably/Problem: Create topic fails for multiple participants in same process When using RTPS, multiple participants in different processes can create inconsistent topics. However, when the participants are in the same process, an inconsistent topic cannot be created. Solution: Force all knowledge of topics to the endpoint manager of discovery./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5245,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4666,OpenDDS,"resolving review comments participant flag type renaming, re-adding check for is_expectant_opendds to EndpointManager::match(), const bool issue/Merge pull request from jrw972/master Make inconsistent topic tests fail predictably/Problem: Create topic fails for multiple participants in same process When using RTPS, multiple participants in different processes can create inconsistent topics. However, when the participants are in the same process, an inconsistent topic cannot be created. Solution: Force all knowledge of topics to the endpoint manager of discovery./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5255,OpenDDS,"Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.0718,OpenDDS,Merge pull request from c4m3lc4s3/master Fixed listener invocation for Presentation QoS policy coherent_access/Call Listeners on Topic Coherent Set Completion Fix For:
,,0.4294,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.5075,OpenDDS,"fixup Problem: Data(Reader|Writer)Impl dont check that topic is enabled/Merge pull request from jwillemsen/jwi-logretcode Log the return code when register instance fails/Merge pull request from jrw972/inconsistent-topic Fix inconsistent topic test/Fixed comment/Log the return code in case of an error * dds/DCPS/DataWriterImpl.cpp:/Problem: Inconsistent topic only discovered with local reader/writer A participant can only get inconconsistent topic when it has a local reader or writer on that topic. While usual, a participant should be able to create a topic and then get inconsistent topic on it without creating readers and writers. This also lead to a race condition in earlier versions of the InconsistentTopic test which made it fail sporadically. Solution: Track local and remote topics and link the local topic to its impl through callbacks so that inconsistent topic can be reported without a reader or writer./Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.4255,OpenDDS,"Problem: Participants cannot communicate directly when behing NATs Participants that are behind NATs cannot communicate directly. The RtpsRelay solves the problem of enabling communication. However, all of the data transferred between the participants must pass through the relay. This solution provides an optimization where participants can communicate directly after bootstrapping discovery with the RtpsRelay. Solution: Implement ICE for RTPS. ICE involves exchanging candidate information and then performing connectivity checks to determine a pair of candidates that can be used to exchange data. Candidate information is exchange via SPDP; presumably by using the RtpsRelay. ICE is then attempted for SEDP. Similarly, SEDP is used to exchange candidate information for the data transport. ICE is then attempted for the data transports./"
,,0.114,OpenDDS,Merge pull request from simpsont-oci/fix_rtps_shutdown_segv_under_load Fix segv caused by race condition in discovery transport associations / disassociations and user topic writes./
,,0.1102,OpenDDS,Merge pull request from simpsont-oci/fix_rtps_shutdown_segv_under_load Fix segv caused by race condition in discovery transport associations / disassociations and user topic writes./
,,0.1178,OpenDDS,Merge pull request from simpsont-oci/fix_rtps_shutdown_segv_under_load Fix segv caused by race condition in discovery transport associations / disassociations and user topic writes./
,,0.1543,OpenDDS,Merge pull request from simpsont-oci/fix_rtps_shutdown_segv_under_load Fix segv caused by race condition in discovery transport associations / disassociations and user topic writes./fix segv caused by race conditions in discovery transport associations / disassociations/
,,0.1462,OpenDDS,Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./Merge pull request from simpsont-oci/fix_rtps_shutdown_segv_under_load Fix segv caused by race condition in discovery transport associations / disassociations and user topic writes./fix segv caused by race conditions in discovery transport associations / disassociations/
,,0.0704,pljava,"Fixed glitch causing bytea[] to fail. Added ""char""[]/"
,,0.0706,pljava,"Bit of vacuuming after release 1.5.0. Old Deployer is obsolete, as are install/uninstall.sql and old fixes for GCJ. In 1.5.0 theyve seen their last release. Its all in the history if ever needed./"
,,0.0764,realm-java,Merge pull request from realm/merge-43e2e6-to-master Fix merge from 43e2e6 to master/fix Javadoc documentation about encryption key length (64 bit 64 bytes) (#3390) fixes
,,0.0642,rocksdb,Fixing Makefile issue reported in Issue 15 (misspelled flag) git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/
,,0.2907,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.0657,rocksdb,use mmap on 64-bit machines to speed-up reads; small build fixes/
,,0.2893,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.2988,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.2961,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.2961,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.3015,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.3052,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./use mmap on 64-bit machines to speed-up reads; small build fixes/"
,,0.3015,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./use mmap on 64-bit machines to speed-up reads; small build fixes/"
,,0.3083,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.3002,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./use mmap on 64-bit machines to speed-up reads; small build fixes/"
,,0.294,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./use mmap on 64-bit machines to speed-up reads; small build fixes/"
,,0.299,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./use mmap on 64-bit machines to speed-up reads; small build fixes/"
,,0.309,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./use mmap on 64-bit machines to speed-up reads; small build fixes/"
,,0.3069,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.2549,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./fix LOCK file deletion to prevent crash on windows/fixed issues 66 (leaking files on disk error) and 68 (no sync of CURRENT file)/"
,,0.292,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.292,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.3001,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.3028,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.3096,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.2931,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./avoid very large compactions; fix build on Linux/"
,,0.3015,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.3028,rocksdb,"Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
,,0.1694,rocksdb,"[RocksDB] Reduce memory footprint of the blockbased table hash index. Summary: Currently, the in-memory hash index of blockbased table uses a precise hash map to track the prefix to block range mapping. In some use cases, especially when prefix itself is big, the memory overhead becomes a problem. This diff introduces a fixed hash bucket array that does not store the prefix and allows prefix collision, which is similar to the plaintable hash index, in order to reduce the memory consumption. Just a quick draft, still testing and refining. Test Plan: unit test and shadow testing Reviewers: dhruba, kailiu, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision:"
,,0.1572,rocksdb,"[RocksDB] Reduce memory footprint of the blockbased table hash index. Summary: Currently, the in-memory hash index of blockbased table uses a precise hash map to track the prefix to block range mapping. In some use cases, especially when prefix itself is big, the memory overhead becomes a problem. This diff introduces a fixed hash bucket array that does not store the prefix and allows prefix collision, which is similar to the plaintable hash index, in order to reduce the memory consumption. Just a quick draft, still testing and refining. Test Plan: unit test and shadow testing Reviewers: dhruba, kailiu, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision:"
,,0.106,rocksdb,"Adding NUMA support to db_bench tests Summary: Changes: Adding numa_aware flag to db_bench.cc Using numa.h library to bind memory and cpu of threads to a fixed NUMA node Result: There seems to be no significant change in the micros/op time with numa_aware enabled. I also tried this with other implementations, including a combination of pthread_setaffinity_np, sched_setaffinity and set_mempolicy methods. Itd be great if someone could point out where Im going wrong and if we can achieve a better micors/op. Test Plan: Ran db_bench tests using following command: ./db_bench The tests were run in private devserver with 24 cores and the db was prepopulated using filluniquerandom test. The tests resulted in 0.145 us/op with numa_aware=False and 0.161 us/op with numa_aware=True. Reviewers: sdong, yhchiang, ljin, igor Reviewed By: ljin, igor Subscribers: igor, leveldb Differential Revision: an assertion in RandomGenerator::Generate() in db_bench. Summary: RandomGenerator::Generate() currently has an assertion len data_.size(). However, it is actually fine to have len data_.size(). This diff change the assertion to len data_.size(). Test Plan: make db_bench ./db_bench Reviewers: haobo, sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision:"
,,0.0677,rocksdb,"Merge pull request from warrenfalk/capi_full_bloom Fix for Support creation of ""full"" format bloom filter from C API/"
,,0.3394,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.34600000000000003,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.3339,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.335,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.2399,rocksdb,"Fix an ASAN error in transaction_test.cc Summary: One test in transaction_test.cc forgets to call SyncPoint::DisableProcessing(). As a result, a program might to access the SyncPoint singleton after it already goes out of scope. This patch fix this error by calling SyncPoint::DisableProcessing(). Test Plan: transaction_test Reviewers: sdong, IslamAbdelRahman, kradhakrishnan, anthony Reviewed By: anthony Subscribers: dhruba, leveldb Differential Revision: SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision: public api to schedule flush/compaction, code to prevent race with db::open Summary: Fixes T8781168. Added a new function EnableAutoCompactions in db.h to be publicly avialable. This allows compaction to be re-enabled after disabling it via SetOptions Refactored code to set the dbptr earlier on in TransactionDB::Open and DB::Open Temporarily disable auto_compaction in TransactionDB::Open until dbptr is set to prevent race condition. Test Plan: Ran make all check verified fix on myrocks side: was able to reproduce the seg fault with ../tools/mysqltest.sh rocksdb.drop_table method was to manually sleep the thread after DB::Open but before TransactionDB ptr was assigned in transaction_db_impl.cc: DB::Open(db_options, dbname, column_families_copy, handles, &db); clock_t goal (60000 * 10) + clock(); while (goal > clock()); ...dbptr(aka rdb) gets assigned below verified my changes fixed the issue. Also added unit test ToggleAutoCompaction in transaction_test.cc Reviewers: hermanlee4, anthony Reviewed By: anthony Subscribers: alex, dhruba Differential Revision:"
,,0.3328,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.3207,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.3251,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.3394,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.3262,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.335,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.239,rocksdb,"[directory includes cleanup] Remove util->db dependency for ThreadStatusUtil Summary: We can avoid the dependency by forward-declaring ColumnFamilyData and then treating it as a black box. That means callers of ThreadStatusUtil need to explicitly provide more options, even if they can be derived from the ColumnFamilyData, since ThreadStatusUtil doesnt include the definition. This is part of a series of diffs to eliminate circular dependencies between directories (e.g., db/* files depending on util/* files and vice-versa). Test Plan: $ ./db_test $ make commit-prereq Reviewers: sdong, yhchiang, IslamAbdelRahman Subscribers: dhruba, leveldb Differential Revision: intermittent hang in ColumnFamilyTest.FlushAndDropRaceCondition Summary: ColumnFamilyTest.FlushAndDropRaceCondition sometimes hangs because the sync point, ""FlushJob::InstallResults"", sleeps holding the DB mutex. Fixing it by releasing the mutex before sleeping. Test Plan: seq 1000 |parallel t=/dev/shm/rdb-{}; rm $t; mkdir $t && export TEST_TMPDIR=$t; ./column_family_test > $t/log-{} Reviewers: IslamAbdelRahman, anthony, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.306,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision: marking snapshots for write-conflict checking Take 2 Summary: D51183 was reverted due to breaking the LITE build. This diff is the same as D51183 but with a fix for the LITE BUILD(D51693) Test Plan: run all unit tests Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.2062,rocksdb,"Skip filters for last L0 file if hit-optimized Summary: Following up on D53493, we can still enable the filter-skipping optimization for last file in L0. Its correct to assume the key will be present in the last L0 file when were hit-optimized and L0 is deepest. The FilePicker encapsulates the state for traversing each levels files, so I needed to make it expose whether the returned file is last in its level. Test Plan: verified below test fails before this patch and passes afterwards. The change to how the test memtable is populated is needed so file 1 has keys (0, 30, 60), file 2 has keys (10, 40, 70), etc. $ ./db_universal_compaction_test Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: D7809 Summary: Revert the functionaility of D7809 (but Im keeping the logging and test code). We decided it was dangerous to ignore sync failures based on attempting to read the data written. The read does not tell us whether the data was synced. Test Plan: There was no test for the particular functionaility that was reverted. Keeping the test code from D7809 that tests whether we set the DB to be readonly when paranoid checks are enabled. Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision: SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision: pull request from SherlockNoMad/CounterFix Fix EstimateNumKeys Counter Inaccurate Issue/"
,,0.3262,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.2863,rocksdb,"compaction assertion triggering test fix for sequence zeroing assertion trip/Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision: marking snapshots for write-conflict checking Take 2 Summary: D51183 was reverted due to breaking the LITE build. This diff is the same as D51183 but with a fix for the LITE BUILD(D51693) Test Plan: run all unit tests Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.3116,rocksdb,"Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision: marking snapshots for write-conflict checking Take 2 Summary: D51183 was reverted due to breaking the LITE build. This diff is the same as D51183 but with a fix for the LITE BUILD(D51693) Test Plan: run all unit tests Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.20199999999999999,rocksdb,"Fix Statistics TickersNameMap miss match with Tickers enum Summary: TickersNameMap is not consistent with Tickers enum. this cause us to report wrong statistics and sometimes to access TickersNameMap outside its boundary causing crashes (in Fb303 statistics) Test Plan: added new unit test Reviewers: sdong, kradhakrishnan Reviewed By: kradhakrishnan Subscribers: andrewkr, dhruba Differential Revision: Statistics to track Compression/Decompression (#1197) * Added new statistics and refactored to allow ioptions to be passed around as required to access environment and statistics pointers (and, as a convenient side effect, info_log pointer). * Prevent incrementing compression counter when compression is turned off in options. * Prevent incrementing compression counter when compression is turned off in options. * Added two more supported compression types to test code in db_test.cc * Prevent incrementing compression counter when compression is turned off in options. * Added new StatsLevel that excludes compression timing. * Fixed casting error in coding.h * Fixed CompressionStatsTest for new StatsLevel. * Removed unused variable that was breaking the Linux build/fix simple typos (#1183)/Add statistics field to show total size of index and filter blocks in block cache Summary: With `table_options.cache_index_and_filter_blocks true`, index and filter blocks are stored in block cache. Then people are curious how much of the block cache total size is used by indexes and bloom filters. It will be nice we have a way to report that. It can help people tune performance and plan for optimized hardware setting. We add several enum values for db Statistics. BLOCK_CACHE_INDEX/FILTER_BYTES_INSERT BLOCK_CACHE_INDEX/FILTER_BYTES_ERASE current INDEX/FILTER total block size in bytes. Test Plan: write a test case called `DBBlockCacheTest.IndexAndFilterBlocksStats`. The result is: ``` ~/local/rocksdb] make db_block_cache_test && ./db_block_cache_test Makefile:101: Warning: Compiling in debug mode. Dont use the resulting binary in production GEN util/build_version.cc make: `db_block_cache_test is up to date. Note: Google Test filter DBBlockCacheTest.IndexAndFilterBlocksStats [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from DBBlockCacheTest [ RUN ] DBBlockCacheTest.IndexAndFilterBlocksStats [ OK ] DBBlockCacheTest.IndexAndFilterBlocksStats (689 ms) [----------] 1 test from DBBlockCacheTest (689 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (689 ms total) [ PASSED ] 1 test. ``` Reviewers: IslamAbdelRahman, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.3031,rocksdb,"BlockBasedTable::FullFilterKeyMayMatch() Should skip prefix bloom if full key bloom exists Summary: Currently, if users define both of full key bloom and prefix bloom in SST files. During Get(), if full key bloom shows the key may exist, we still go ahead and check prefix bloom. This is wasteful. If bloom filter for full keys exists, we should always ignore prefix bloom in Get(). Test Plan: Run existing tests Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision: statistics field to show total size of index and filter blocks in block cache Summary: With `table_options.cache_index_and_filter_blocks true`, index and filter blocks are stored in block cache. Then people are curious how much of the block cache total size is used by indexes and bloom filters. It will be nice we have a way to report that. It can help people tune performance and plan for optimized hardware setting. We add several enum values for db Statistics. BLOCK_CACHE_INDEX/FILTER_BYTES_INSERT BLOCK_CACHE_INDEX/FILTER_BYTES_ERASE current INDEX/FILTER total block size in bytes. Test Plan: write a test case called `DBBlockCacheTest.IndexAndFilterBlocksStats`. The result is: ``` ~/local/rocksdb] make db_block_cache_test && ./db_block_cache_test Makefile:101: Warning: Compiling in debug mode. Dont use the resulting binary in production GEN util/build_version.cc make: `db_block_cache_test is up to date. Note: Google Test filter DBBlockCacheTest.IndexAndFilterBlocksStats [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from DBBlockCacheTest [ RUN ] DBBlockCacheTest.IndexAndFilterBlocksStats [ OK ] DBBlockCacheTest.IndexAndFilterBlocksStats (689 ms) [----------] 1 test from DBBlockCacheTest (689 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (689 ms total) [ PASSED ] 1 test. ``` Reviewers: IslamAbdelRahman, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.2538,rocksdb,"Add statistics field to show total size of index and filter blocks in block cache Summary: With `table_options.cache_index_and_filter_blocks true`, index and filter blocks are stored in block cache. Then people are curious how much of the block cache total size is used by indexes and bloom filters. It will be nice we have a way to report that. It can help people tune performance and plan for optimized hardware setting. We add several enum values for db Statistics. BLOCK_CACHE_INDEX/FILTER_BYTES_INSERT BLOCK_CACHE_INDEX/FILTER_BYTES_ERASE current INDEX/FILTER total block size in bytes. Test Plan: write a test case called `DBBlockCacheTest.IndexAndFilterBlocksStats`. The result is: ``` ~/local/rocksdb] make db_block_cache_test && ./db_block_cache_test Makefile:101: Warning: Compiling in debug mode. Dont use the resulting binary in production GEN util/build_version.cc make: `db_block_cache_test is up to date. Note: Google Test filter DBBlockCacheTest.IndexAndFilterBlocksStats [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from DBBlockCacheTest [ RUN ] DBBlockCacheTest.IndexAndFilterBlocksStats [ OK ] DBBlockCacheTest.IndexAndFilterBlocksStats (689 ms) [----------] 1 test from DBBlockCacheTest (689 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (689 ms total) [ PASSED ] 1 test. ``` Reviewers: IslamAbdelRahman, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.3031,rocksdb,"BlockBasedTable::FullFilterKeyMayMatch() Should skip prefix bloom if full key bloom exists Summary: Currently, if users define both of full key bloom and prefix bloom in SST files. During Get(), if full key bloom shows the key may exist, we still go ahead and check prefix bloom. This is wasteful. If bloom filter for full keys exists, we should always ignore prefix bloom in Get(). Test Plan: Run existing tests Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision: statistics field to show total size of index and filter blocks in block cache Summary: With `table_options.cache_index_and_filter_blocks true`, index and filter blocks are stored in block cache. Then people are curious how much of the block cache total size is used by indexes and bloom filters. It will be nice we have a way to report that. It can help people tune performance and plan for optimized hardware setting. We add several enum values for db Statistics. BLOCK_CACHE_INDEX/FILTER_BYTES_INSERT BLOCK_CACHE_INDEX/FILTER_BYTES_ERASE current INDEX/FILTER total block size in bytes. Test Plan: write a test case called `DBBlockCacheTest.IndexAndFilterBlocksStats`. The result is: ``` ~/local/rocksdb] make db_block_cache_test && ./db_block_cache_test Makefile:101: Warning: Compiling in debug mode. Dont use the resulting binary in production GEN util/build_version.cc make: `db_block_cache_test is up to date. Note: Google Test filter DBBlockCacheTest.IndexAndFilterBlocksStats [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from DBBlockCacheTest [ RUN ] DBBlockCacheTest.IndexAndFilterBlocksStats [ OK ] DBBlockCacheTest.IndexAndFilterBlocksStats (689 ms) [----------] 1 test from DBBlockCacheTest (689 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (689 ms total) [ PASSED ] 1 test. ``` Reviewers: IslamAbdelRahman, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.2874,rocksdb,"BlockBasedTable::FullFilterKeyMayMatch() Should skip prefix bloom if full key bloom exists Summary: Currently, if users define both of full key bloom and prefix bloom in SST files. During Get(), if full key bloom shows the key may exist, we still go ahead and check prefix bloom. This is wasteful. If bloom filter for full keys exists, we should always ignore prefix bloom in Get(). Test Plan: Run existing tests Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision: statistics field to show total size of index and filter blocks in block cache Summary: With `table_options.cache_index_and_filter_blocks true`, index and filter blocks are stored in block cache. Then people are curious how much of the block cache total size is used by indexes and bloom filters. It will be nice we have a way to report that. It can help people tune performance and plan for optimized hardware setting. We add several enum values for db Statistics. BLOCK_CACHE_INDEX/FILTER_BYTES_INSERT BLOCK_CACHE_INDEX/FILTER_BYTES_ERASE current INDEX/FILTER total block size in bytes. Test Plan: write a test case called `DBBlockCacheTest.IndexAndFilterBlocksStats`. The result is: ``` ~/local/rocksdb] make db_block_cache_test && ./db_block_cache_test Makefile:101: Warning: Compiling in debug mode. Dont use the resulting binary in production GEN util/build_version.cc make: `db_block_cache_test is up to date. Note: Google Test filter DBBlockCacheTest.IndexAndFilterBlocksStats [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from DBBlockCacheTest [ RUN ] DBBlockCacheTest.IndexAndFilterBlocksStats [ OK ] DBBlockCacheTest.IndexAndFilterBlocksStats (689 ms) [----------] 1 test from DBBlockCacheTest (689 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (689 ms total) [ PASSED ] 1 test. ``` Reviewers: IslamAbdelRahman, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.2491,rocksdb,"Add statistics field to show total size of index and filter blocks in block cache Summary: With `table_options.cache_index_and_filter_blocks true`, index and filter blocks are stored in block cache. Then people are curious how much of the block cache total size is used by indexes and bloom filters. It will be nice we have a way to report that. It can help people tune performance and plan for optimized hardware setting. We add several enum values for db Statistics. BLOCK_CACHE_INDEX/FILTER_BYTES_INSERT BLOCK_CACHE_INDEX/FILTER_BYTES_ERASE current INDEX/FILTER total block size in bytes. Test Plan: write a test case called `DBBlockCacheTest.IndexAndFilterBlocksStats`. The result is: ``` ~/local/rocksdb] make db_block_cache_test && ./db_block_cache_test Makefile:101: Warning: Compiling in debug mode. Dont use the resulting binary in production GEN util/build_version.cc make: `db_block_cache_test is up to date. Note: Google Test filter DBBlockCacheTest.IndexAndFilterBlocksStats [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from DBBlockCacheTest [ RUN ] DBBlockCacheTest.IndexAndFilterBlocksStats [ OK ] DBBlockCacheTest.IndexAndFilterBlocksStats (689 ms) [----------] 1 test from DBBlockCacheTest (689 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (689 ms total) [ PASSED ] 1 test. ``` Reviewers: IslamAbdelRahman, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.2967,rocksdb,"BlockBasedTable::FullFilterKeyMayMatch() Should skip prefix bloom if full key bloom exists Summary: Currently, if users define both of full key bloom and prefix bloom in SST files. During Get(), if full key bloom shows the key may exist, we still go ahead and check prefix bloom. This is wasteful. If bloom filter for full keys exists, we should always ignore prefix bloom in Get(). Test Plan: Run existing tests Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision: statistics field to show total size of index and filter blocks in block cache Summary: With `table_options.cache_index_and_filter_blocks true`, index and filter blocks are stored in block cache. Then people are curious how much of the block cache total size is used by indexes and bloom filters. It will be nice we have a way to report that. It can help people tune performance and plan for optimized hardware setting. We add several enum values for db Statistics. BLOCK_CACHE_INDEX/FILTER_BYTES_INSERT BLOCK_CACHE_INDEX/FILTER_BYTES_ERASE current INDEX/FILTER total block size in bytes. Test Plan: write a test case called `DBBlockCacheTest.IndexAndFilterBlocksStats`. The result is: ``` ~/local/rocksdb] make db_block_cache_test && ./db_block_cache_test Makefile:101: Warning: Compiling in debug mode. Dont use the resulting binary in production GEN util/build_version.cc make: `db_block_cache_test is up to date. Note: Google Test filter DBBlockCacheTest.IndexAndFilterBlocksStats [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from DBBlockCacheTest [ RUN ] DBBlockCacheTest.IndexAndFilterBlocksStats [ OK ] DBBlockCacheTest.IndexAndFilterBlocksStats (689 ms) [----------] 1 test from DBBlockCacheTest (689 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (689 ms total) [ PASSED ] 1 test. ``` Reviewers: IslamAbdelRahman, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.2945,rocksdb,"BlockBasedTable::FullFilterKeyMayMatch() Should skip prefix bloom if full key bloom exists Summary: Currently, if users define both of full key bloom and prefix bloom in SST files. During Get(), if full key bloom shows the key may exist, we still go ahead and check prefix bloom. This is wasteful. If bloom filter for full keys exists, we should always ignore prefix bloom in Get(). Test Plan: Run existing tests Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision: statistics field to show total size of index and filter blocks in block cache Summary: With `table_options.cache_index_and_filter_blocks true`, index and filter blocks are stored in block cache. Then people are curious how much of the block cache total size is used by indexes and bloom filters. It will be nice we have a way to report that. It can help people tune performance and plan for optimized hardware setting. We add several enum values for db Statistics. BLOCK_CACHE_INDEX/FILTER_BYTES_INSERT BLOCK_CACHE_INDEX/FILTER_BYTES_ERASE current INDEX/FILTER total block size in bytes. Test Plan: write a test case called `DBBlockCacheTest.IndexAndFilterBlocksStats`. The result is: ``` ~/local/rocksdb] make db_block_cache_test && ./db_block_cache_test Makefile:101: Warning: Compiling in debug mode. Dont use the resulting binary in production GEN util/build_version.cc make: `db_block_cache_test is up to date. Note: Google Test filter DBBlockCacheTest.IndexAndFilterBlocksStats [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from DBBlockCacheTest [ RUN ] DBBlockCacheTest.IndexAndFilterBlocksStats [ OK ] DBBlockCacheTest.IndexAndFilterBlocksStats (689 ms) [----------] 1 test from DBBlockCacheTest (689 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (689 ms total) [ PASSED ] 1 test. ``` Reviewers: IslamAbdelRahman, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.1989,rocksdb,"BlockBasedTable::FullFilterKeyMayMatch() Should skip prefix bloom if full key bloom exists Summary: Currently, if users define both of full key bloom and prefix bloom in SST files. During Get(), if full key bloom shows the key may exist, we still go ahead and check prefix bloom. This is wasteful. If bloom filter for full keys exists, we should always ignore prefix bloom in Get(). Test Plan: Run existing tests Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision:"
,,0.214,rocksdb,"fix previous typo Summary: old typos with FILTER/INDEX_CACHE Test Plan: still pass this unit test Reviewers: andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, leveldb Differential Revision: statistics field to show total size of index and filter blocks in block cache Summary: With `table_options.cache_index_and_filter_blocks true`, index and filter blocks are stored in block cache. Then people are curious how much of the block cache total size is used by indexes and bloom filters. It will be nice we have a way to report that. It can help people tune performance and plan for optimized hardware setting. We add several enum values for db Statistics. BLOCK_CACHE_INDEX/FILTER_BYTES_INSERT BLOCK_CACHE_INDEX/FILTER_BYTES_ERASE current INDEX/FILTER total block size in bytes. Test Plan: write a test case called `DBBlockCacheTest.IndexAndFilterBlocksStats`. The result is: ``` ~/local/rocksdb] make db_block_cache_test && ./db_block_cache_test Makefile:101: Warning: Compiling in debug mode. Dont use the resulting binary in production GEN util/build_version.cc make: `db_block_cache_test is up to date. Note: Google Test filter DBBlockCacheTest.IndexAndFilterBlocksStats [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from DBBlockCacheTest [ RUN ] DBBlockCacheTest.IndexAndFilterBlocksStats [ OK ] DBBlockCacheTest.IndexAndFilterBlocksStats (689 ms) [----------] 1 test from DBBlockCacheTest (689 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (689 ms total) [ PASSED ] 1 test. ``` Reviewers: IslamAbdelRahman, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.1856,rocksdb,"store prefix_extractor_name in table Summary: Make sure prefix extractor name is stored in SST files and if DB is opened with a prefix extractor of a different name, prefix bloom is skipped when read the file. Also add unit tests for that. Test Plan: before change: ``` Note: Google Test filter BlockBasedTableTest.SkipPrefixBloomFilter [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from BlockBasedTableTest [ RUN ] BlockBasedTableTest.SkipPrefixBloomFilter table/table_test.cc:1421: Failure Value of: db_iter->Valid() Actual: false Expected: true [ FAILED ] BlockBasedTableTest.SkipPrefixBloomFilter (1 ms) [----------] 1 test from BlockBasedTableTest (1 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (1 ms total) [ PASSED ] 0 tests. [ FAILED ] 1 test, listed below: [ FAILED ] BlockBasedTableTest.SkipPrefixBloomFilter 1 FAILED TEST ``` after: ``` Note: Google Test filter BlockBasedTableTest.SkipPrefixBloomFilter [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from BlockBasedTableTest [ RUN ] BlockBasedTableTest.SkipPrefixBloomFilter [ OK ] BlockBasedTableTest.SkipPrefixBloomFilter (0 ms) [----------] 1 test from BlockBasedTableTest (0 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (0 ms total) [ PASSED ] 1 test. ``` Reviewers: sdong, andrewkr, yiwu, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.1477,rocksdb,"Regression test for empty dedicated range deletion file Summary: Issue: Fix: The bug happened when all of these conditions were satisfied: A subcompaction generates no keys `RangeDelAggregator::ShouldAddTombstones()` returns true because theres at least one non-obsoleted range deletion in its map None of the non-obsolete tombstones overlap with the subcompaction key-range Under those conditions, we were creating a dedicated file for range deletions which was left empty, thus causing an error in VersionEdit. I verified this test case fails before the fix and passes after. Closes Differential Revision: D5352568 Pulled By: ajkr fbshipit-source-id: f619cae39984ce9bb9b7a4e7a9ac0f2bb2ce43e9/Encryption at rest support Summary: This PR adds support for encrypting data stored by RocksDB when written to disk. It adds an `EncryptedEnv` override of the `Env` class with matching overrides for sequential&random access files. The encryption itself is done through a configurable `EncryptionProvider`. This class creates is asked to create `BlockAccessCipherStream` for a file. This is where the actual encryption/decryption is being done. Currently there is a Counter mode implementation of `BlockAccessCipherStream` with a `ROT13` block cipher (NOTE the `ROT13` is for demo purposes only). The Counter operation mode uses an initial counter & random initialization vector (IV). Both are created randomly for each file and stored in a 4K (default size) block that is prefixed to that file. The `EncryptedEnv` implementation is such that clients of the `Env` class do not see this prefix (nor data, nor in filesize). The largest part of the prefix block is also encrypted, and there is room left for implementation specific settings/values/keys in there. To test the encryption, the `DBTestBase` class has been extended to consider a new environment variable called `ENCRYPTED_ENV`. If set, the test will setup a encrypted instance of the `Env` class to use for all tests. Typically you would run it like this: ``` ENCRYPTED_ENV=1 make check_some ``` There is also an added test that checks that some data inserted into the database is or is not ""visible"" on disk. With `ENCRYPTED_ENV` active it must not find plain text strings, with `ENCRYPTED_ENV` unset, it must find the plain text strings. Closes Differential Revision: D5322178 Pulled By: sdwilsh fbshipit-source-id: 253b0a9c2c498cc98f580df7f2623cbf7678a27f/Disable DBRangeDelTest::TailingIteratorRangeTombstoneUnsupported for ubsan Summary: UBSAN crashes when it run the test. Disabling it for UBSAN. Closes Differential Revision: D5210897 Pulled By: yiwu-arbug fbshipit-source-id: 2f5a876807c98d8db79ab9581965f7e6b29d4163/"
,,0.2113,rocksdb,"Encryption at rest support Summary: This PR adds support for encrypting data stored by RocksDB when written to disk. It adds an `EncryptedEnv` override of the `Env` class with matching overrides for sequential&random access files. The encryption itself is done through a configurable `EncryptionProvider`. This class creates is asked to create `BlockAccessCipherStream` for a file. This is where the actual encryption/decryption is being done. Currently there is a Counter mode implementation of `BlockAccessCipherStream` with a `ROT13` block cipher (NOTE the `ROT13` is for demo purposes only). The Counter operation mode uses an initial counter & random initialization vector (IV). Both are created randomly for each file and stored in a 4K (default size) block that is prefixed to that file. The `EncryptedEnv` implementation is such that clients of the `Env` class do not see this prefix (nor data, nor in filesize). The largest part of the prefix block is also encrypted, and there is room left for implementation specific settings/values/keys in there. To test the encryption, the `DBTestBase` class has been extended to consider a new environment variable called `ENCRYPTED_ENV`. If set, the test will setup a encrypted instance of the `Env` class to use for all tests. Typically you would run it like this: ``` ENCRYPTED_ENV=1 make check_some ``` There is also an added test that checks that some data inserted into the database is or is not ""visible"" on disk. With `ENCRYPTED_ENV` active it must not find plain text strings, with `ENCRYPTED_ENV` unset, it must find the plain text strings. Closes Differential Revision: D5322178 Pulled By: sdwilsh fbshipit-source-id: 253b0a9c2c498cc98f580df7f2623cbf7678a27f/"
,,0.1538,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Encryption at rest support Summary: This PR adds support for encrypting data stored by RocksDB when written to disk. It adds an `EncryptedEnv` override of the `Env` class with matching overrides for sequential&random access files. The encryption itself is done through a configurable `EncryptionProvider`. This class creates is asked to create `BlockAccessCipherStream` for a file. This is where the actual encryption/decryption is being done. Currently there is a Counter mode implementation of `BlockAccessCipherStream` with a `ROT13` block cipher (NOTE the `ROT13` is for demo purposes only). The Counter operation mode uses an initial counter & random initialization vector (IV). Both are created randomly for each file and stored in a 4K (default size) block that is prefixed to that file. The `EncryptedEnv` implementation is such that clients of the `Env` class do not see this prefix (nor data, nor in filesize). The largest part of the prefix block is also encrypted, and there is room left for implementation specific settings/values/keys in there. To test the encryption, the `DBTestBase` class has been extended to consider a new environment variable called `ENCRYPTED_ENV`. If set, the test will setup a encrypted instance of the `Env` class to use for all tests. Typically you would run it like this: ``` ENCRYPTED_ENV=1 make check_some ``` There is also an added test that checks that some data inserted into the database is or is not ""visible"" on disk. With `ENCRYPTED_ENV` active it must not find plain text strings, with `ENCRYPTED_ENV` unset, it must find the plain text strings. Closes Differential Revision: D5322178 Pulled By: sdwilsh fbshipit-source-id: 253b0a9c2c498cc98f580df7f2623cbf7678a27f/Unit Tests for sync, range sync and file close failures Summary: Closes Differential Revision: D5255320 Pulled By: siying fbshipit-source-id: 0080830fa8eb5da6de25e17ba68aee91018c7913/Fixing blob db sequence number handling Summary: Blob db rely on base db returning sequence number through write batch after DB::Write(). However after recent changes to the write path, DB::Writ()e no longer return sequence number in some cases. Fixing it by have WriteBatchInternal::InsertInto() always encode sequence number into write batch. Stacking on Closes Differential Revision: D5148358 Pulled By: yiwu-arbug fbshipit-source-id: 8bda0aa07b9334ed03ed381548b39d167dc20c33/Avoid unsupported attributes when not building with UBSAN Summary: yiwu-arbug see individual commits. Closes Differential Revision: D5141520 Pulled By: yiwu-arbug fbshipit-source-id: 7987c92ab4461eef36afce5a133d3a0ee0c96300/"
,,0.1011,rocksdb,Fix calculating filter partition target size Summary: block_size_deviation is in percentage while the partition size is in bytes. The current code fails to take that into account resulting into very large target size for filter partitions. Closes Differential Revision: D6376069 Pulled By: maysamyabandeh fbshipit-source-id: 276546fc68f50e0da32c462abb46f6cf676db9b2/
,,0.0842,rocksdb,"Update 64-bit shift in compression.h Summary: This was failing the build on windows with zstd, warning treated as an error, 32-bit shift implicitly converted to 64-bit. Closes Differential Revision: D7307883 Pulled By: gfosco fbshipit-source-id: 68110e9b5b1b59b668dec6cf86b67556402574e7/"
,,0.1681,rocksdb,"Improve FullFilterBitsReader::HashMayMatchs doc (#4202) Summary: HashMayMatch is related to AddKey() instead of CreateFilter(). Also applies some minor Fixes Pull Request resolved: Differential Revision: D9180945 Pulled By: maysamyabandeh fbshipit-source-id: 6f07b81c5bb9bda5c0273475b486ba8a030471e6/Avoid integer division in filter probing (#4071) Summary: The cache line size was computed dynamically based on the length of the filter bits, and the number of cache-lines encoded in the footer. This calculation had to be dynamic in case users migrate their data between platforms with different cache line sizes. The downside, though, was bloom filter probing became expensive as it did integer mod and division. However, since we know all possible cache line sizes are powers of two, we should be able to use bit shift to find the cache line, and bitwise-and to find the bit within the cache line. To do this, we compute the log-base-two of cache line size in the constructor, and use that in bitwise operations to replace division/mod. Pull Request resolved: Differential Revision: D8684067 Pulled By: ajkr fbshipit-source-id: 50298872fba5acd01e8269cd7abcc51a095e0f61/"
,,0.3155,rocksdb,"Add path to WritableFileWriter. (#4039) Summary: We want to sample the file I/O issued by RocksDB and report the function calls. This requires us to include the file paths otherwise its hard to tell what has been going on. Pull Request resolved: Differential Revision: D8670178 Pulled By: riversand963 fbshipit-source-id: 97ee806d1c583a2983e28e213ee764dc6ac28f7a/RocksDB Trace Analyzer (#4091) Summary: A framework of trace analyzing for RocksDB After collecting the trace by using the tool of [PR User can use the Trace Analyzer to interpret, analyze, and characterize the collected workload. **Input:** 1. trace file 2. Whole keys space file **Statistics:** 1. Access count of each operation (Get, Put, Delete, SingleDelete, DeleteRange, Merge) in each column family. 2. Key hotness (access count) of each one 3. Key space separation based on given prefix 4. Key size distribution 5. Value size distribution if appliable 6. Top K accessed keys 7. QPS statistics including the average QPS and peak QPS 8. Top K accessed prefix 9. The query correlation analyzing, output the number of X after Y and the corresponding average time intervals **Output:** 1. key access heat map (either in the accessed key space or whole key space) 2. trace sequence file (interpret the raw trace file to line base text file for future use) 3. Time serial (The key space ID and its access time) 4. Key access count distritbution 5. Key size distribution 6. Value size distribution (in each intervals) 7. whole key space separation by the prefix 8. Accessed key space separation by the prefix 9. QPS of each operation and each column family 10. Top K QPS and their accessed prefix range **Test:** 1. Added the unit test of analyzing Get, Put, Delete, SingleDelete, DeleteRange, Merge 2. Generated the trace and analyze the trace **Implemented but not tested (due to the limitation of trace_replay):** 1. Analyzing Iterator, supporting Seek() and SeekForPrev() analyzing 2. Analyzing the number of Key found by Get **Future Work:** 1. Support execution time analyzing of each requests 2. Support cache hit situation and block read situation of Get Pull Request resolved: Differential Revision: D9256157 Pulled By: zhichao-cao fbshipit-source-id: f0ceacb7eedbc43a3eee6e85b76087d7832a8fe6/"
,,0.2898,rocksdb,"Replace string with const string& in FileOperationInfo (#4491) Summary: Using const string& can avoid one extra string copy. This PR addresses a recent comment made by siying on Pull Request resolved: Differential Revision: D10381211 Pulled By: riversand963 fbshipit-source-id: 27fc2d65d84bc7cd07833c77cdc47f06dcfaeb31/Add path to WritableFileWriter. (#4039) Summary: We want to sample the file I/O issued by RocksDB and report the function calls. This requires us to include the file paths otherwise its hard to tell what has been going on. Pull Request resolved: Differential Revision: D8670178 Pulled By: riversand963 fbshipit-source-id: 97ee806d1c583a2983e28e213ee764dc6ac28f7a/RocksDB Trace Analyzer (#4091) Summary: A framework of trace analyzing for RocksDB After collecting the trace by using the tool of [PR User can use the Trace Analyzer to interpret, analyze, and characterize the collected workload. **Input:** 1. trace file 2. Whole keys space file **Statistics:** 1. Access count of each operation (Get, Put, Delete, SingleDelete, DeleteRange, Merge) in each column family. 2. Key hotness (access count) of each one 3. Key space separation based on given prefix 4. Key size distribution 5. Value size distribution if appliable 6. Top K accessed keys 7. QPS statistics including the average QPS and peak QPS 8. Top K accessed prefix 9. The query correlation analyzing, output the number of X after Y and the corresponding average time intervals **Output:** 1. key access heat map (either in the accessed key space or whole key space) 2. trace sequence file (interpret the raw trace file to line base text file for future use) 3. Time serial (The key space ID and its access time) 4. Key access count distritbution 5. Key size distribution 6. Value size distribution (in each intervals) 7. whole key space separation by the prefix 8. Accessed key space separation by the prefix 9. QPS of each operation and each column family 10. Top K QPS and their accessed prefix range **Test:** 1. Added the unit test of analyzing Get, Put, Delete, SingleDelete, DeleteRange, Merge 2. Generated the trace and analyze the trace **Implemented but not tested (due to the limitation of trace_replay):** 1. Analyzing Iterator, supporting Seek() and SeekForPrev() analyzing 2. Analyzing the number of Key found by Get **Future Work:** 1. Support execution time analyzing of each requests 2. Support cache hit situation and block read situation of Get Pull Request resolved: Differential Revision: D9256157 Pulled By: zhichao-cao fbshipit-source-id: f0ceacb7eedbc43a3eee6e85b76087d7832a8fe6/"
,,0.3198,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/RocksDB Trace Analyzer (#4091) Summary: A framework of trace analyzing for RocksDB After collecting the trace by using the tool of [PR User can use the Trace Analyzer to interpret, analyze, and characterize the collected workload. **Input:** 1. trace file 2. Whole keys space file **Statistics:** 1. Access count of each operation (Get, Put, Delete, SingleDelete, DeleteRange, Merge) in each column family. 2. Key hotness (access count) of each one 3. Key space separation based on given prefix 4. Key size distribution 5. Value size distribution if appliable 6. Top K accessed keys 7. QPS statistics including the average QPS and peak QPS 8. Top K accessed prefix 9. The query correlation analyzing, output the number of X after Y and the corresponding average time intervals **Output:** 1. key access heat map (either in the accessed key space or whole key space) 2. trace sequence file (interpret the raw trace file to line base text file for future use) 3. Time serial (The key space ID and its access time) 4. Key access count distritbution 5. Key size distribution 6. Value size distribution (in each intervals) 7. whole key space separation by the prefix 8. Accessed key space separation by the prefix 9. QPS of each operation and each column family 10. Top K QPS and their accessed prefix range **Test:** 1. Added the unit test of analyzing Get, Put, Delete, SingleDelete, DeleteRange, Merge 2. Generated the trace and analyze the trace **Implemented but not tested (due to the limitation of trace_replay):** 1. Analyzing Iterator, supporting Seek() and SeekForPrev() analyzing 2. Analyzing the number of Key found by Get **Future Work:** 1. Support execution time analyzing of each requests 2. Support cache hit situation and block read situation of Get Pull Request resolved: Differential Revision: D9256157 Pulled By: zhichao-cao fbshipit-source-id: f0ceacb7eedbc43a3eee6e85b76087d7832a8fe6/"
,,0.2962,rocksdb,"Avoid double-compacting data in bottom level in manual compactions (#5138) Summary: Depending on the config, manual compaction (leveled compaction style) does following compactions: L0->L1 L1->L2 ... Ln-1 Ln Ln Ln The final Ln Ln compaction is partly unnecessary as it recompacts all the files that were just generated by the Ln-1 Ln. We should avoid recompacting such files. This rule should be applied to Lmax only. Resolves issue Pull Request resolved: Differential Revision: D14940106 Pulled By: miasantreble fbshipit-source-id: 8d3cf5507a17e76f3333cfd4bac5256d005636e5/#5145 , rename port/dirent.h to port/port_dirent.h to avoid compile err when use port dir as header dir output (#5152) Summary: mv port/dirent.h to port/port_dirent.h to avoid compile err when use port dir as header dir output Pull Request resolved: Differential Revision: D14779409 Pulled By: siying fbshipit-source-id: d4162c47c979c6e8cc6a9e601802864ab3768ecb/Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.1265,rocksdb,"Add the option to trace_analyzer (#5067) Summary: In the current trace_analyzer implementation, once the trace file has corrupted content, which can be caused by unexpected tracing operations or other reasons, trace_analyzer will print the error and stop analyzing. By adding the option, user can try to process the corrupted trace file and get the analyzing results of the trace records from the beginning to the the first corrupted point in the trace file. Analyzing might fail even this option is enabled. Pull Request resolved: Differential Revision: D14433037 Pulled By: zhichao-cao fbshipit-source-id: d095233ba371726869af0def0cdee23b69896831/"
,,0.3791,rocksdb,"Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.3791,rocksdb,"Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.3686,rocksdb,"Fix some variable naming in db/transaction_log_impl.* (#5112) Summary: We follow Google C++ Style which indicates variable names should be all underscore: Fix some variable names under db/transaction_log_impl.* Pull Request resolved: Differential Revision: D14631157 Pulled By: siying fbshipit-source-id: 9525c9b0976b843bca377b03897700d87cc60af8/Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.3935,rocksdb,"Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.3276,rocksdb,"Smooth the deletion of WAL files (#5116) Summary: WAL files are currently not subject to deletion rate limiting by DeleteScheduler. If the size of the WAL files is significant, this can cause a high delete rate on SSDs that may affect other operations. To fix it, force WAL file deletions to go through the SstFileManager. Original PR for this is Pull Request resolved: Differential Revision: D14669437 Pulled By: anand1976 fbshipit-source-id: c5f62d0640cebaa1574de841a1d01e4ce2faadf0/Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.3925,rocksdb,"Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.3791,rocksdb,"Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.3956,rocksdb,"Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.3771,rocksdb,"Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.3873,rocksdb,"Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.3884,rocksdb,"Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.2569,rocksdb,"Fix many bugs in log statement arguments (#5089) Summary: Annotate all of the logging functions to inform the compiler that these use printf-style formatting arguments. This allows the compiler to emit warnings if the format arguments are incorrect. This also fixes many problems reported now that format string checking is enabled. Many of these are simply mix-ups in the argument type (e.g, int vs uint64_t), but in several cases the wrong number of arguments were being passed in which can cause the code to crash. The primary motivation for this was to fix the log message in `DBImpl::SwitchMemtable()` which caused a segfault due to an extra %s format parameter with no argument supplied. Pull Request resolved: Differential Revision: D14574795 Pulled By: simpkins fbshipit-source-id: 0921b03f0743652bf4ae21e414ff54b3bb65422a/Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/Header logger should call LogHeader() (#4980) Summary: The info log header feature never worked well, because log level Header was not translated to Logger::LogHeader() call. Fix it. Pull Request resolved: Differential Revision: D14087283 Pulled By: siying fbshipit-source-id: 7e7d03ce35fa8d13d4ee549f46f7326f7bc0006d/"
,,0.3894,rocksdb,"Support for single-primary, multi-secondary instances (#4899) Summary: This PR allows RocksDB to run in single-primary, multi-secondary process mode. The writer is a regular RocksDB (e.g. an `DBImpl`) instance playing the role of a primary. Multiple `DBImplSecondary` processes (secondaries) share the same set of SST files, MANIFEST, WAL files with the primary. Secondaries tail the MANIFEST of the primary and apply updates to their own in-memory state of the file system, e.g. `VersionStorageInfo`. This PR has several components: 1. (Originally in Add a `PathNotFound` subcode to `IOError` to denote the failure when a secondary tries to open a file which has been deleted by the primary. 2. (Similar to Add `FragmentBufferedReader` to handle partially-read, trailing record at the end of a log from where future read can continue. 3. (Originally in and Add implementation of the secondary, i.e. `DBImplSecondary`. 3.1 Tail the primarys MANIFEST during recovery. 3.2 Tail the primarys MANIFEST during normal processing by calling `ReadAndApply`. 3.3 Tailing WAL will be in a future PR. 4. Add an example in examples/multi_processes_example.cc to demonstrate the usage of secondary RocksDB instance in a multi-process setting. Instructions to run the example can be found at the beginning of the source code. Pull Request resolved: Differential Revision: D14510945 Pulled By: riversand963 fbshipit-source-id: 4ac1c5693e6012ad23f7b4b42d3c374fecbe8886/"
,,0.0899,rocksdb,"Added trace replay fast forward function (#5273) Summary: In the current db_bench trace replay, the replay process strictly follows the timestamp to issue the queries. In some cases, user does not care about the time. Therefore, fast forward is needed for users to speed up the replay process. Pull Request resolved: Differential Revision: D15389232 Pulled By: zhichao-cao fbshipit-source-id: 735d629b9d2a167b05af3e4fa0ddf9d5d0be1806/"
,,0.0899,rocksdb,"Added trace replay fast forward function (#5273) Summary: In the current db_bench trace replay, the replay process strictly follows the timestamp to issue the queries. In some cases, user does not care about the time. Therefore, fast forward is needed for users to speed up the replay process. Pull Request resolved: Differential Revision: D15389232 Pulled By: zhichao-cao fbshipit-source-id: 735d629b9d2a167b05af3e4fa0ddf9d5d0be1806/"
,,0.223,rocksdb,Java: Make the generics of the Options interfaces more strict (#5461) Summary: Make the generics of the Options interfaces more strict so they are usable in a Kotlin Multiplatform expect/actual typealias implementation without causing a Violation of Finite Bound Restriction. This fix would enable the creation of a generic Kotlin multiplatform library by just typealiasing the JVM implementation to the current Java implementation. Pull Request resolved: Differential Revision: D15903288 Pulled By: sagar0 fbshipit-source-id: 75e83fdf5d2fcede40744a17e767563d6a4b0696/
,,0.2276,rocksdb,Java: Make the generics of the Options interfaces more strict (#5461) Summary: Make the generics of the Options interfaces more strict so they are usable in a Kotlin Multiplatform expect/actual typealias implementation without causing a Violation of Finite Bound Restriction. This fix would enable the creation of a generic Kotlin multiplatform library by just typealiasing the JVM implementation to the current Java implementation. Pull Request resolved: Differential Revision: D15903288 Pulled By: sagar0 fbshipit-source-id: 75e83fdf5d2fcede40744a17e767563d6a4b0696/
,,0.2245,rocksdb,Java: Make the generics of the Options interfaces more strict (#5461) Summary: Make the generics of the Options interfaces more strict so they are usable in a Kotlin Multiplatform expect/actual typealias implementation without causing a Violation of Finite Bound Restriction. This fix would enable the creation of a generic Kotlin multiplatform library by just typealiasing the JVM implementation to the current Java implementation. Pull Request resolved: Differential Revision: D15903288 Pulled By: sagar0 fbshipit-source-id: 75e83fdf5d2fcede40744a17e767563d6a4b0696/
,,0.2353,rocksdb,Java: Make the generics of the Options interfaces more strict (#5461) Summary: Make the generics of the Options interfaces more strict so they are usable in a Kotlin Multiplatform expect/actual typealias implementation without causing a Violation of Finite Bound Restriction. This fix would enable the creation of a generic Kotlin multiplatform library by just typealiasing the JVM implementation to the current Java implementation. Pull Request resolved: Differential Revision: D15903288 Pulled By: sagar0 fbshipit-source-id: 75e83fdf5d2fcede40744a17e767563d6a4b0696/
,,0.2307,rocksdb,Java: Make the generics of the Options interfaces more strict (#5461) Summary: Make the generics of the Options interfaces more strict so they are usable in a Kotlin Multiplatform expect/actual typealias implementation without causing a Violation of Finite Bound Restriction. This fix would enable the creation of a generic Kotlin multiplatform library by just typealiasing the JVM implementation to the current Java implementation. Pull Request resolved: Differential Revision: D15903288 Pulled By: sagar0 fbshipit-source-id: 75e83fdf5d2fcede40744a17e767563d6a4b0696/
,,0.2153,rocksdb,Java: Make the generics of the Options interfaces more strict (#5461) Summary: Make the generics of the Options interfaces more strict so they are usable in a Kotlin Multiplatform expect/actual typealias implementation without causing a Violation of Finite Bound Restriction. This fix would enable the creation of a generic Kotlin multiplatform library by just typealiasing the JVM implementation to the current Java implementation. Pull Request resolved: Differential Revision: D15903288 Pulled By: sagar0 fbshipit-source-id: 75e83fdf5d2fcede40744a17e767563d6a4b0696/
,,0.5054,rocksdb,"Refactor / clean up / optimize FullFilterBitsReader (#5941) Summary: FullFilterBitsReader, after creating in BloomFilterPolicy, was responsible for decoding metadata bits. This meant that FullFilterBitsReader::MayMatch had some metadata checks in order to implement ""always true"" or ""always false"" functionality in the case of inconsistent or trivial metadata. This made for ugly mixing-of-concerns code and probably had some runtime cost. It also didnt really support plugging in alternative filter implementations with extensions to the existing metadata schema. BloomFilterPolicy::GetFilterBitsReader is now (exclusively) responsible for decoding filter metadata bits and constructing appropriate instances deriving from FilterBitsReader. ""Always false"" and ""always true"" derived classes allow FullFilterBitsReader not to be concerned with handling of trivial or inconsistent metadata. This also makes for easy expansion to alternative filter implementations in new, alternative derived classes. This change makes calls to FilterBitsReader::MayMatch *necessarily* virtual because theres now more than one built-in implementation. Compared with the previous implementations extra if checks in MayMatch, theres no consistent performance difference, measured by (an older revision of) filter_bench (differences here seem to be within noise): Inside queries... Dry run (407) ns/op: 35.9996 + Dry run (407) ns/op: 35.2034 Single filter ns/op: 47.5483 + Single filter ns/op: 47.4034 Batched, prepared ns/op: 43.1559 + Batched, prepared ns/op: 42.2923 ... Random filter ns/op: 150.697 + Random filter ns/op: 149.403 Outside queries... Dry run (980) ns/op: 34.6114 + Dry run (980) ns/op: 34.0405 Single filter ns/op: 56.8326 + Single filter ns/op: 55.8414 Batched, prepared ns/op: 48.2346 + Batched, prepared ns/op: 47.5667 Random filter ns/op: 155.377 + Random filter ns/op: 153.942 Average FP rate %: 1.1386 Also, the FullFilterBitsReader ctor was responsible for a surprising amount of CPU in production, due in part to inefficient determination of the CACHE_LINE_SIZE used to construct the filter being read. The overwhelming common case (same as my CACHE_LINE_SIZE) is now substantially optimized, as shown with filter_bench with (old option see below) (repeatable result): Inside queries... Dry run (453) ns/op: 118.799 + Dry run (453) ns/op: 105.869 Single filter ns/op: 82.5831 + Single filter ns/op: 74.2509 ... Random filter ns/op: 224.936 + Random filter ns/op: 194.833 Outside queries... Dry run (aa1) ns/op: 118.503 + Dry run (aa1) ns/op: 104.925 Single filter ns/op: 90.3023 + Single filter ns/op: 83.425 ... Random filter ns/op: 220.455 + Random filter ns/op: 175.7 Average FP rate %: 1.13886 However PR#5936 has/will reclaim most of this cost. After that PR, the optimization of this code path is likely negligible, but nonetheless its clear we arent making performance any worse. Also fixed inadequate check of consistency between filter data size and num_lines. (Unit test updated.) Pull Request resolved: Test Plan: previously added unit tests FullBloomTest.CorruptFilters and FullBloomTest.RawSchema Differential Revision: D18018353 Pulled By: pdillinger fbshipit-source-id: 8e04c2b4a7d93223f49a237fd52ef2483929ed9c/Refactor/consolidate legacy Bloom implementation details (#5784) Summary: Refactoring to consolidate implementation details of legacy Bloom filters. This helps to organize and document some related, obscure code. Also added make/cpp var TEST_CACHE_LINE_SIZE so that its easy to compile and run unit tests for non-native cache line size. (Fixed a related test failure in db_properties_test.) Pull Request resolved: Test Plan: make check, including Recently added Bloom schema unit tests (in ./plain_table_db_test && ./bloom_test), and including with TEST_CACHE_LINE_SIZE=128U and TEST_CACHE_LINE_SIZE=256U. Tested the schema tests with temporary fault injection into new implementations. Some performance testing with modified unit tests suggest a small to moderate improvement in speed. Differential Revision: D17381384 Pulled By: pdillinger fbshipit-source-id: ee42586da996798910fc45ac0b6289147f16d8df/Revert changes from PR#5784 accidentally in PR#5780 (#5810) Summary: This will allow us to fix history by having the code changes for PR#5784 properly attributed to it. Pull Request resolved: Differential Revision: D17400231 Pulled By: pdillinger fbshipit-source-id: 2da8b1cdf2533cfedb35b5526eadefb38c291f09/"
,,0.1105,rocksdb,"Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/arm64 crc prefetch optimise (#5773) Summary: prefetch data for following block?avoid cache miss when doing crc caculate I do performance test at kunpeng-920 server(arm-v8, ./db_bench before optimise : 587313.500 micros/op 1 ops/sec; 811.9 MB/s (500000000 per op) after optimise : 289248.500 micros/op 3 ops/sec; 1648.5 MB/s (500000000 per op) Pull Request resolved: Differential Revision: D17347339 fbshipit-source-id: bfcd74f0f0eb4b322b959be68019ddcaae1e3341/"
,,0.4826,rocksdb,"Clean up some filter tests and comments (#5960) Summary: Some filtering tests were unfriendly to new implementations of FilterBitsBuilder because of dynamic_cast to FullFilterBitsBuilder. Most of those have now been cleaned up, worked around, or at least changed from crash on dynamic_cast failure to individual test failure. Also put some clarifying comments on filter-related APIs. Pull Request resolved: Test Plan: make check Differential Revision: D18121223 Pulled By: pdillinger fbshipit-source-id: e83827d9d5d96315d96f8e25a99cd70f497d802c/Refactor / clean up / optimize FullFilterBitsReader (#5941) Summary: FullFilterBitsReader, after creating in BloomFilterPolicy, was responsible for decoding metadata bits. This meant that FullFilterBitsReader::MayMatch had some metadata checks in order to implement ""always true"" or ""always false"" functionality in the case of inconsistent or trivial metadata. This made for ugly mixing-of-concerns code and probably had some runtime cost. It also didnt really support plugging in alternative filter implementations with extensions to the existing metadata schema. BloomFilterPolicy::GetFilterBitsReader is now (exclusively) responsible for decoding filter metadata bits and constructing appropriate instances deriving from FilterBitsReader. ""Always false"" and ""always true"" derived classes allow FullFilterBitsReader not to be concerned with handling of trivial or inconsistent metadata. This also makes for easy expansion to alternative filter implementations in new, alternative derived classes. This change makes calls to FilterBitsReader::MayMatch *necessarily* virtual because theres now more than one built-in implementation. Compared with the previous implementations extra if checks in MayMatch, theres no consistent performance difference, measured by (an older revision of) filter_bench (differences here seem to be within noise): Inside queries... Dry run (407) ns/op: 35.9996 + Dry run (407) ns/op: 35.2034 Single filter ns/op: 47.5483 + Single filter ns/op: 47.4034 Batched, prepared ns/op: 43.1559 + Batched, prepared ns/op: 42.2923 ... Random filter ns/op: 150.697 + Random filter ns/op: 149.403 Outside queries... Dry run (980) ns/op: 34.6114 + Dry run (980) ns/op: 34.0405 Single filter ns/op: 56.8326 + Single filter ns/op: 55.8414 Batched, prepared ns/op: 48.2346 + Batched, prepared ns/op: 47.5667 Random filter ns/op: 155.377 + Random filter ns/op: 153.942 Average FP rate %: 1.1386 Also, the FullFilterBitsReader ctor was responsible for a surprising amount of CPU in production, due in part to inefficient determination of the CACHE_LINE_SIZE used to construct the filter being read. The overwhelming common case (same as my CACHE_LINE_SIZE) is now substantially optimized, as shown with filter_bench with (old option see below) (repeatable result): Inside queries... Dry run (453) ns/op: 118.799 + Dry run (453) ns/op: 105.869 Single filter ns/op: 82.5831 + Single filter ns/op: 74.2509 ... Random filter ns/op: 224.936 + Random filter ns/op: 194.833 Outside queries... Dry run (aa1) ns/op: 118.503 + Dry run (aa1) ns/op: 104.925 Single filter ns/op: 90.3023 + Single filter ns/op: 83.425 ... Random filter ns/op: 220.455 + Random filter ns/op: 175.7 Average FP rate %: 1.13886 However PR#5936 has/will reclaim most of this cost. After that PR, the optimization of this code path is likely negligible, but nonetheless its clear we arent making performance any worse. Also fixed inadequate check of consistency between filter data size and num_lines. (Unit test updated.) Pull Request resolved: Test Plan: previously added unit tests FullBloomTest.CorruptFilters and FullBloomTest.RawSchema Differential Revision: D18018353 Pulled By: pdillinger fbshipit-source-id: 8e04c2b4a7d93223f49a237fd52ef2483929ed9c/bloom_test.cc: include (#5920) Summary: Fix build failure on some platforms, reported in issue Pull Request resolved: Test Plan: make bloom_test && ./bloom_test Differential Revision: D17918328 Pulled By: pdillinger fbshipit-source-id: b822004d4442de0171db2aeff433677783f7b94e/Fix type in shift operation in bloom_test (#5882) Summary: Broken type for shift in PR#5834. Fixing code means fixing expected values in test. Pull Request resolved: Test Plan: thisisthetest Differential Revision: D17746136 Pulled By: pdillinger fbshipit-source-id: d3c456ed30b433d55fcab6fc7d836940fe3b46b8/Refactor/consolidate legacy Bloom implementation details (#5784) Summary: Refactoring to consolidate implementation details of legacy Bloom filters. This helps to organize and document some related, obscure code. Also added make/cpp var TEST_CACHE_LINE_SIZE so that its easy to compile and run unit tests for non-native cache line size. (Fixed a related test failure in db_properties_test.) Pull Request resolved: Test Plan: make check, including Recently added Bloom schema unit tests (in ./plain_table_db_test && ./bloom_test), and including with TEST_CACHE_LINE_SIZE=128U and TEST_CACHE_LINE_SIZE=256U. Tested the schema tests with temporary fault injection into new implementations. Some performance testing with modified unit tests suggest a small to moderate improvement in speed. Differential Revision: D17381384 Pulled By: pdillinger fbshipit-source-id: ee42586da996798910fc45ac0b6289147f16d8df/Revert changes from PR#5784 accidentally in PR#5780 (#5810) Summary: This will allow us to fix history by having the code changes for PR#5784 properly attributed to it. Pull Request resolved: Differential Revision: D17400231 Pulled By: pdillinger fbshipit-source-id: 2da8b1cdf2533cfedb35b5526eadefb38c291f09/Add regression test for serialized Bloom filters (#5778) Summary: Check that we dont accidentally change the on-disk format of existing Bloom filter implementations, including for various CACHE_LINE_SIZE (by changing temporarily). Pull Request resolved: Test Plan: thisisthetest Differential Revision: D17269630 Pulled By: pdillinger fbshipit-source-id: c77017662f010a77603b7d475892b1f0d5563d8b/bloom test check fail on arm (#5745) Summary: FullFilterBitsBuilder::CalculateSpace use CACHE_LINE_SIZE which is but when it run bloom_test.FullVaryingLengths it failed on ARM64 server, the assert can be fixed by change 128->CACHE_LINE_SIZE*2 as merged ASSERT_LE(FilterSize(), (size_t)((length * 10 / 8) + CACHE_LINE_SIZE * 2 + 5)) length; run bloom_test before fix: /root/rocksdb-master/util/bloom_test.cc:281: Failure Expected: (FilterSize()) ((size_t)((length * 10 / 8) + 128 + 5)), actual: 389 vs 383 200 [ FAILED ] FullBloomTest.FullVaryingLengths (32 ms) [----------] 4 tests from FullBloomTest (32 ms total) [----------] Global test environment tear-down [==========] 7 tests from 2 test cases ran. (116 ms total) [ PASSED ] 6 tests. [ FAILED ] 1 test, listed below: [ FAILED ] FullBloomTest.FullVaryingLengths after fix: Filters: 37 good, 0 mediocre [ OK ] FullBloomTest.FullVaryingLengths (90 ms) [----------] 4 tests from FullBloomTest (90 ms total) [----------] Global test environment tear-down [==========] 7 tests from 2 test cases ran. (174 ms total) [ PASSED ] 7 tests. Pull Request resolved: Differential Revision: D17076047 fbshipit-source-id: e7beb5d55d4855fceb2b84bc8119a6b0759de635/"
,,0.1324,rocksdb,"Fix some implicit conversions in filter_bench (#5894) Summary: Fixed some spots where converting size_t or uint_fast32_t to uint32_t. Wrapped mt19937 in a new Random32 class to avoid future such traps. NB: I tried using Random32::Uniform (std::uniform_int_distribution) in filter_bench instead of fastrange, but that more than doubled the dry run time So I added fastrange as Random32::Uniformish. ;) Pull Request resolved: Test Plan: USE_CLANG=1 build, and manual re-run filter_bench Differential Revision: D17825131 Pulled By: pdillinger fbshipit-source-id: 68feee333b5f8193c084ded760e3d6679b405ecd/"
,,0.1119,rocksdb,"arm64 crc prefetch optimise (#5773) Summary: prefetch data for following block?avoid cache miss when doing crc caculate I do performance test at kunpeng-920 server(arm-v8, ./db_bench before optimise : 587313.500 micros/op 1 ops/sec; 811.9 MB/s (500000000 per op) after optimise : 289248.500 micros/op 3 ops/sec; 1648.5 MB/s (500000000 per op) Pull Request resolved: Differential Revision: D17347339 fbshipit-source-id: bfcd74f0f0eb4b322b959be68019ddcaae1e3341/"
,,0.1396,rocksdb,"Clean up some filter tests and comments (#5960) Summary: Some filtering tests were unfriendly to new implementations of FilterBitsBuilder because of dynamic_cast to FullFilterBitsBuilder. Most of those have now been cleaned up, worked around, or at least changed from crash on dynamic_cast failure to individual test failure. Also put some clarifying comments on filter-related APIs. Pull Request resolved: Test Plan: make check Differential Revision: D18121223 Pulled By: pdillinger fbshipit-source-id: e83827d9d5d96315d96f8e25a99cd70f497d802c/"
,,0.1864,rocksdb,"Fix memory leak on error opening PlainTable (#5951) Summary: Several error paths in opening of a plain table would leak memory. PR opened the leak to one more error path, which happens to have been (mistakenly) exercised by CuckooTableDBTest.AdaptiveTable. That test has been fixed, and the exercising of plain table error cases (more than before) has been added as BadOptions1 and BadOptions2 to PlainTableDBTest. This effectively moved the memory leak to plain_table_db_test. Also here is a cheap fix for the memory leak, without (yet?) changing the signature of ReadTableProperties. This fixes ASAN on unit tests. Pull Request resolved: Test Plan: make COMPILE_WITH_ASAN=1 check Differential Revision: D18051940 Pulled By: pdillinger fbshipit-source-id: e2952930c09a2b46c4f1ff09818c5090426929de/Fix PlainTableReader not to crash sst_dump (#5940) Summary: Plain table SSTs could crash sst_dump because of a bug in PlainTableReader that can leave table_properties_ as null. Even if it was intended not to keep the table properties in some cases, they were leaked on the offending code path. Steps to reproduce: $ db_bench $ sst_dump from [] to [] Process /dev/shm/dbbench/000014.sst Sst file format: plain table Raw user collected properties Segmentation fault (core dumped) Also added missing unit testing of plain table full_scan_mode, and an assertion in NewIterator to check for regression. Pull Request resolved: Test Plan: new unit test, manual, make check Differential Revision: D18018145 Pulled By: pdillinger fbshipit-source-id: 4310c755e824c4cd6f3f86a3abc20dfa417c5e07/Refactor/consolidate legacy Bloom implementation details (#5784) Summary: Refactoring to consolidate implementation details of legacy Bloom filters. This helps to organize and document some related, obscure code. Also added make/cpp var TEST_CACHE_LINE_SIZE so that its easy to compile and run unit tests for non-native cache line size. (Fixed a related test failure in db_properties_test.) Pull Request resolved: Test Plan: make check, including Recently added Bloom schema unit tests (in ./plain_table_db_test && ./bloom_test), and including with TEST_CACHE_LINE_SIZE=128U and TEST_CACHE_LINE_SIZE=256U. Tested the schema tests with temporary fault injection into new implementations. Some performance testing with modified unit tests suggest a small to moderate improvement in speed. Differential Revision: D17381384 Pulled By: pdillinger fbshipit-source-id: ee42586da996798910fc45ac0b6289147f16d8df/Revert changes from PR#5784 accidentally in PR#5780 (#5810) Summary: This will allow us to fix history by having the code changes for PR#5784 properly attributed to it. Pull Request resolved: Differential Revision: D17400231 Pulled By: pdillinger fbshipit-source-id: 2da8b1cdf2533cfedb35b5526eadefb38c291f09/"
,,0.2832,rocksdb,"Refactor/consolidate legacy Bloom implementation details (#5784) Summary: Refactoring to consolidate implementation details of legacy Bloom filters. This helps to organize and document some related, obscure code. Also added make/cpp var TEST_CACHE_LINE_SIZE so that its easy to compile and run unit tests for non-native cache line size. (Fixed a related test failure in db_properties_test.) Pull Request resolved: Test Plan: make check, including Recently added Bloom schema unit tests (in ./plain_table_db_test && ./bloom_test), and including with TEST_CACHE_LINE_SIZE=128U and TEST_CACHE_LINE_SIZE=256U. Tested the schema tests with temporary fault injection into new implementations. Some performance testing with modified unit tests suggest a small to moderate improvement in speed. Differential Revision: D17381384 Pulled By: pdillinger fbshipit-source-id: ee42586da996798910fc45ac0b6289147f16d8df/Revert changes from PR#5784 accidentally in PR#5780 (#5810) Summary: This will allow us to fix history by having the code changes for PR#5784 properly attributed to it. Pull Request resolved: Differential Revision: D17400231 Pulled By: pdillinger fbshipit-source-id: 2da8b1cdf2533cfedb35b5526eadefb38c291f09/"
,,0.2641,rocksdb,"Revert ""Merging iterator to avoid child iterator reseek for some cases (#5286)"" (#5871) Summary: This reverts commit 9fad3e21eb90d215b6719097baba417bc1eeca3c. Iterator verification in stress tests sometimes fail for assertion table/block_based/block_based_table_reader.cc:2973: void rocksdb::BlockBasedTableIterator<TBlockIter, TValue>::FindBlockForward() [with TBlockIter rocksdb::DataBlockIter; TValue rocksdb::Slice]: Assertion `next_block_is_out_of_bound || user_comparator_.Compare(*read_options_.iterate_upper_bound, index_iter_->user_key()) 0 failed. It is likely to be linked to together with as the former PR makes some child iterators seek being avoided, so that upper bound condition fails to be updated there. Strictly speaking, the former PR was merged before the latter one, but the latter one feels a more important improvement so I choose to revert the former one for now. Pull Request resolved: Differential Revision: D17689196 fbshipit-source-id: 4ded5be68f67bee2782d31a29cb72ea68f59dd8c/Refactor MultiGet names in BlockBasedTable (#5726) Summary: To improve code readability, since RetrieveBlock already calls MaybeReadBlockAndLoadToCache, we avoid name similarity of the functions that call RetrieveBlock with MaybeReadBlockAndLoadToCache. The patch thus renames MaybeLoadBlocksToCache to RetrieveMultipleBlock and deletes GetDataBlockFromCache, which contains only two lines. Pull Request resolved: Differential Revision: D16962535 Pulled By: maysamyabandeh fbshipit-source-id: 99e8946808ce4eb7857592b9003812e3004f92d6/Fix regression affecting partitioned indexes/filters when cache_index_and_filter_blocks is false (#5705) Summary: PR (and subsequent related patches) unintentionally changed the semantics of cache_index_and_filter_blocks: historically, this option only affected the main index/filter block; with the changes, it affects index/filter partitions as well. This can cause performance issues when cache_index_and_filter_blocks is false since in this case, partitions are neither cached nor preloaded (i.e. they are loaded on demand upon each access). The patch reverts to the earlier behavior, that is, partitions are cached similarly to data blocks regardless of the value of the above option. Pull Request resolved: Test Plan: make check ./db_bench ./db_bench Relevant statistics from the readrandom benchmark with the old code: rocksdb.block.cache.index.miss COUNT : 0 rocksdb.block.cache.index.hit COUNT : 0 rocksdb.block.cache.index.add COUNT : 0 rocksdb.block.cache.index.bytes.insert COUNT : 0 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 0 rocksdb.block.cache.filter.hit COUNT : 0 rocksdb.block.cache.filter.add COUNT : 0 rocksdb.block.cache.filter.bytes.insert COUNT : 0 rocksdb.block.cache.filter.bytes.evict COUNT : 0 With the new code: rocksdb.block.cache.index.miss COUNT : 2500 rocksdb.block.cache.index.hit COUNT : 42696 rocksdb.block.cache.index.add COUNT : 2500 rocksdb.block.cache.index.bytes.insert COUNT : 4050048 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 2500 rocksdb.block.cache.filter.hit COUNT : 4550493 rocksdb.block.cache.filter.add COUNT : 2500 rocksdb.block.cache.filter.bytes.insert COUNT : 10331040 rocksdb.block.cache.filter.bytes.evict COUNT : 0 Differential Revision: D16817382 Pulled By: ltamasi fbshipit-source-id: 28a516b0da1f041a03313e0b70b28cf5cf205d00/"
,,0.6321,rocksdb,"Store the filter bits reader alongside the filter block contents (#5936) Summary: Amongst other things, PR refactored the filter block readers so that only the filter block contents are stored in the block cache (as opposed to the earlier design where the cache stored the filter block reader itself, leading to potentially dangling pointers and concurrency bugs). However, this change introduced a performance hit since with the new code, the metadata fields are re-parsed upon every access. This patch reunites the block contents with the filter bits reader to eliminate this overhead; since this is still a self-contained pure data object, it is safe to store it in the cache. (Note: this is similar to how the zstd digest is handled.) Pull Request resolved: Test Plan: make asan_check filter_bench results for the old code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.7153 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.4258 Single filter ns/op: 42.5974 Random filter ns/op: 217.861 Outside queries... Dry run (25d) ns/op: 32.4217 Single filter ns/op: 50.9855 Random filter ns/op: 219.167 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5172 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 32.3556 Single filter ns/op: 83.2239 Random filter ns/op: 370.676 Outside queries... Dry run (25d) ns/op: 32.2265 Single filter ns/op: 93.5651 Random filter ns/op: 408.393 Average FP rate %: 1.13993 Done. (For more info, run with or ``` With the new code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 25.4285 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 31.0594 Single filter ns/op: 43.8974 Random filter ns/op: 226.075 Outside queries... Dry run (25d) ns/op: 31.0295 Single filter ns/op: 50.3824 Random filter ns/op: 226.805 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5308 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.2968 Single filter ns/op: 58.6163 Random filter ns/op: 291.434 Outside queries... Dry run (25d) ns/op: 32.1839 Single filter ns/op: 66.9039 Random filter ns/op: 292.828 Average FP rate %: 1.13993 Done. (For more info, run with or ``` Differential Revision: D17991712 Pulled By: ltamasi fbshipit-source-id: 7ea205550217bfaaa1d5158ebd658e5832e60f29/Fix a bug in format_version 3 + partition filters + prefix search (#5835) Summary: Partitioned filters make use of a top-level index to find the partition in which the filter resides. The top-level index has a key per partition. The key is guaranteed to be larger or equal than any key in that partition. When used with format_version 3, which excludes the sequence number form index keys, the separator key in the index could be equal to the prefix of the keys in the next partition. In this way, when searching for the key, the top-level index will lead us to the previous partition, which has no key with that prefix. The prefix bloom test thus returns false, although the prefix exists in the bloom of the next partition. The patch fixes that by a hack: It always adds the prefix of the first key of the next partition to the bloom of the current partition. In this way, in the corner cases that the index will lead us to the previous partition, we still can find the bloom filter there. Pull Request resolved: Differential Revision: D17513585 Pulled By: maysamyabandeh fbshipit-source-id: e2d1ff26c759e6e03875c4d57f4228316ecf50e9/Fix MultiGet() bug when whole_key_filtering is disabled (#5665) Summary: The batched MultiGet() implementation was not correctly handling bloom filter lookups when whole_key_filtering is disabled. It was incorrectly skipping keys not in the prefix_extractor domain, and not calling transform for keys in domain. This PR fixes both problems by moving the domain check and transformation to the FilterBlockReader. Tests: Unit test (confirmed failed before the fix) make check Pull Request resolved: Differential Revision: D16902380 Pulled By: anand1976 fbshipit-source-id: a6be81ad68a6e37134a65246aec7a2c590eccf00/"
,,0.623,rocksdb,"Store the filter bits reader alongside the filter block contents (#5936) Summary: Amongst other things, PR refactored the filter block readers so that only the filter block contents are stored in the block cache (as opposed to the earlier design where the cache stored the filter block reader itself, leading to potentially dangling pointers and concurrency bugs). However, this change introduced a performance hit since with the new code, the metadata fields are re-parsed upon every access. This patch reunites the block contents with the filter bits reader to eliminate this overhead; since this is still a self-contained pure data object, it is safe to store it in the cache. (Note: this is similar to how the zstd digest is handled.) Pull Request resolved: Test Plan: make asan_check filter_bench results for the old code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.7153 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.4258 Single filter ns/op: 42.5974 Random filter ns/op: 217.861 Outside queries... Dry run (25d) ns/op: 32.4217 Single filter ns/op: 50.9855 Random filter ns/op: 219.167 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5172 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 32.3556 Single filter ns/op: 83.2239 Random filter ns/op: 370.676 Outside queries... Dry run (25d) ns/op: 32.2265 Single filter ns/op: 93.5651 Random filter ns/op: 408.393 Average FP rate %: 1.13993 Done. (For more info, run with or ``` With the new code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 25.4285 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 31.0594 Single filter ns/op: 43.8974 Random filter ns/op: 226.075 Outside queries... Dry run (25d) ns/op: 31.0295 Single filter ns/op: 50.3824 Random filter ns/op: 226.805 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5308 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.2968 Single filter ns/op: 58.6163 Random filter ns/op: 291.434 Outside queries... Dry run (25d) ns/op: 32.1839 Single filter ns/op: 66.9039 Random filter ns/op: 292.828 Average FP rate %: 1.13993 Done. (For more info, run with or ``` Differential Revision: D17991712 Pulled By: ltamasi fbshipit-source-id: 7ea205550217bfaaa1d5158ebd658e5832e60f29/Fix SeekForPrev bug with Partitioned Filters and Prefix (#5907) Summary: Partition Filters make use of a top-level index to find the partition that might have the bloom hash of the key. The index is with internal key format (before format version 3). Each partition contains the i) blooms of the keys in that range ii) bloom of prefixes of keys in that range, iii) the bloom of the prefix of the last key in the previous partition. When ::SeekForPrev(key), we first perform a prefix bloom test on the SST file. The partition however is identified using the full internal key, rather than the prefix key. The reason is to be compatible with the internal key format of the top-level index. This creates a corner case. Example: SST k, Partition N: P1K1, P1K2 SST k, top-level index: P1K2 SST k+1, Partition 1: P2K1, P3K1 SST k+1 top-level index: P3K1 When SeekForPrev(P1K3), it should point us to P1K2. However SST k top-level index would reject P1K3 since it is out of range. One possible fix would be to search with the prefix P1 (instead of full internal key P1K3) however the details of properly comparing prefix with full internal key might get complicated. The fix we apply in this PR is to look into the last partition anyway even if the key is out of range. Pull Request resolved: Differential Revision: D17889918 Pulled By: maysamyabandeh fbshipit-source-id: 169fd7b3c71dbc08808eae5a8340611ebe5bdc1e/Fix compilation error (#5872) Summary: Without this fix, compiler complains. ``` $ROCKSDB_NO_FBCODE=1 USE_CLANG=1 make ldb table/block_based/full_filter_block.cc: In constructor ërocksdb::FullFilterBlockBuilder::FullFilterBlockBuilder(const rocksdb::SliceTransform*, bool, rocksdb::FilterBitsBuilder*)í: table/block_based/full_filter_block.cc:20:43: error: declaration of ëprefix_extractorí shadows a member of this [-Werror=shadow] FilterBitsBuilder* filter_bits_builder) ``` Pull Request resolved: Test Plan: ``` $ROCKSDB_NO_FBCODE=1 make all ``` Differential Revision: D17690058 Pulled By: riversand963 fbshipit-source-id: 19e3d9bd86e1123847095240e73d30da5d66240e/Fix a bug in format_version 3 + partition filters + prefix search (#5835) Summary: Partitioned filters make use of a top-level index to find the partition in which the filter resides. The top-level index has a key per partition. The key is guaranteed to be larger or equal than any key in that partition. When used with format_version 3, which excludes the sequence number form index keys, the separator key in the index could be equal to the prefix of the keys in the next partition. In this way, when searching for the key, the top-level index will lead us to the previous partition, which has no key with that prefix. The prefix bloom test thus returns false, although the prefix exists in the bloom of the next partition. The patch fixes that by a hack: It always adds the prefix of the first key of the next partition to the bloom of the current partition. In this way, in the corner cases that the index will lead us to the previous partition, we still can find the bloom filter there. Pull Request resolved: Differential Revision: D17513585 Pulled By: maysamyabandeh fbshipit-source-id: e2d1ff26c759e6e03875c4d57f4228316ecf50e9/Fix regression affecting partitioned indexes/filters when cache_index_and_filter_blocks is false (#5705) Summary: PR (and subsequent related patches) unintentionally changed the semantics of cache_index_and_filter_blocks: historically, this option only affected the main index/filter block; with the changes, it affects index/filter partitions as well. This can cause performance issues when cache_index_and_filter_blocks is false since in this case, partitions are neither cached nor preloaded (i.e. they are loaded on demand upon each access). The patch reverts to the earlier behavior, that is, partitions are cached similarly to data blocks regardless of the value of the above option. Pull Request resolved: Test Plan: make check ./db_bench ./db_bench Relevant statistics from the readrandom benchmark with the old code: rocksdb.block.cache.index.miss COUNT : 0 rocksdb.block.cache.index.hit COUNT : 0 rocksdb.block.cache.index.add COUNT : 0 rocksdb.block.cache.index.bytes.insert COUNT : 0 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 0 rocksdb.block.cache.filter.hit COUNT : 0 rocksdb.block.cache.filter.add COUNT : 0 rocksdb.block.cache.filter.bytes.insert COUNT : 0 rocksdb.block.cache.filter.bytes.evict COUNT : 0 With the new code: rocksdb.block.cache.index.miss COUNT : 2500 rocksdb.block.cache.index.hit COUNT : 42696 rocksdb.block.cache.index.add COUNT : 2500 rocksdb.block.cache.index.bytes.insert COUNT : 4050048 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 2500 rocksdb.block.cache.filter.hit COUNT : 4550493 rocksdb.block.cache.filter.add COUNT : 2500 rocksdb.block.cache.filter.bytes.insert COUNT : 10331040 rocksdb.block.cache.filter.bytes.evict COUNT : 0 Differential Revision: D16817382 Pulled By: ltamasi fbshipit-source-id: 28a516b0da1f041a03313e0b70b28cf5cf205d00/"
,,0.5993,rocksdb,"Clean up some filter tests and comments (#5960) Summary: Some filtering tests were unfriendly to new implementations of FilterBitsBuilder because of dynamic_cast to FullFilterBitsBuilder. Most of those have now been cleaned up, worked around, or at least changed from crash on dynamic_cast failure to individual test failure. Also put some clarifying comments on filter-related APIs. Pull Request resolved: Test Plan: make check Differential Revision: D18121223 Pulled By: pdillinger fbshipit-source-id: e83827d9d5d96315d96f8e25a99cd70f497d802c/Store the filter bits reader alongside the filter block contents (#5936) Summary: Amongst other things, PR refactored the filter block readers so that only the filter block contents are stored in the block cache (as opposed to the earlier design where the cache stored the filter block reader itself, leading to potentially dangling pointers and concurrency bugs). However, this change introduced a performance hit since with the new code, the metadata fields are re-parsed upon every access. This patch reunites the block contents with the filter bits reader to eliminate this overhead; since this is still a self-contained pure data object, it is safe to store it in the cache. (Note: this is similar to how the zstd digest is handled.) Pull Request resolved: Test Plan: make asan_check filter_bench results for the old code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.7153 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.4258 Single filter ns/op: 42.5974 Random filter ns/op: 217.861 Outside queries... Dry run (25d) ns/op: 32.4217 Single filter ns/op: 50.9855 Random filter ns/op: 219.167 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5172 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 32.3556 Single filter ns/op: 83.2239 Random filter ns/op: 370.676 Outside queries... Dry run (25d) ns/op: 32.2265 Single filter ns/op: 93.5651 Random filter ns/op: 408.393 Average FP rate %: 1.13993 Done. (For more info, run with or ``` With the new code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 25.4285 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 31.0594 Single filter ns/op: 43.8974 Random filter ns/op: 226.075 Outside queries... Dry run (25d) ns/op: 31.0295 Single filter ns/op: 50.3824 Random filter ns/op: 226.805 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5308 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.2968 Single filter ns/op: 58.6163 Random filter ns/op: 291.434 Outside queries... Dry run (25d) ns/op: 32.1839 Single filter ns/op: 66.9039 Random filter ns/op: 292.828 Average FP rate %: 1.13993 Done. (For more info, run with or ``` Differential Revision: D17991712 Pulled By: ltamasi fbshipit-source-id: 7ea205550217bfaaa1d5158ebd658e5832e60f29/"
,,0.3843,rocksdb,"Fix regression affecting partitioned indexes/filters when cache_index_and_filter_blocks is false (#5705) Summary: PR (and subsequent related patches) unintentionally changed the semantics of cache_index_and_filter_blocks: historically, this option only affected the main index/filter block; with the changes, it affects index/filter partitions as well. This can cause performance issues when cache_index_and_filter_blocks is false since in this case, partitions are neither cached nor preloaded (i.e. they are loaded on demand upon each access). The patch reverts to the earlier behavior, that is, partitions are cached similarly to data blocks regardless of the value of the above option. Pull Request resolved: Test Plan: make check ./db_bench ./db_bench Relevant statistics from the readrandom benchmark with the old code: rocksdb.block.cache.index.miss COUNT : 0 rocksdb.block.cache.index.hit COUNT : 0 rocksdb.block.cache.index.add COUNT : 0 rocksdb.block.cache.index.bytes.insert COUNT : 0 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 0 rocksdb.block.cache.filter.hit COUNT : 0 rocksdb.block.cache.filter.add COUNT : 0 rocksdb.block.cache.filter.bytes.insert COUNT : 0 rocksdb.block.cache.filter.bytes.evict COUNT : 0 With the new code: rocksdb.block.cache.index.miss COUNT : 2500 rocksdb.block.cache.index.hit COUNT : 42696 rocksdb.block.cache.index.add COUNT : 2500 rocksdb.block.cache.index.bytes.insert COUNT : 4050048 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 2500 rocksdb.block.cache.filter.hit COUNT : 4550493 rocksdb.block.cache.filter.add COUNT : 2500 rocksdb.block.cache.filter.bytes.insert COUNT : 10331040 rocksdb.block.cache.filter.bytes.evict COUNT : 0 Differential Revision: D16817382 Pulled By: ltamasi fbshipit-source-id: 28a516b0da1f041a03313e0b70b28cf5cf205d00/"
,,0.6467,rocksdb,"Store the filter bits reader alongside the filter block contents (#5936) Summary: Amongst other things, PR refactored the filter block readers so that only the filter block contents are stored in the block cache (as opposed to the earlier design where the cache stored the filter block reader itself, leading to potentially dangling pointers and concurrency bugs). However, this change introduced a performance hit since with the new code, the metadata fields are re-parsed upon every access. This patch reunites the block contents with the filter bits reader to eliminate this overhead; since this is still a self-contained pure data object, it is safe to store it in the cache. (Note: this is similar to how the zstd digest is handled.) Pull Request resolved: Test Plan: make asan_check filter_bench results for the old code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.7153 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.4258 Single filter ns/op: 42.5974 Random filter ns/op: 217.861 Outside queries... Dry run (25d) ns/op: 32.4217 Single filter ns/op: 50.9855 Random filter ns/op: 219.167 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5172 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 32.3556 Single filter ns/op: 83.2239 Random filter ns/op: 370.676 Outside queries... Dry run (25d) ns/op: 32.2265 Single filter ns/op: 93.5651 Random filter ns/op: 408.393 Average FP rate %: 1.13993 Done. (For more info, run with or ``` With the new code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 25.4285 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 31.0594 Single filter ns/op: 43.8974 Random filter ns/op: 226.075 Outside queries... Dry run (25d) ns/op: 31.0295 Single filter ns/op: 50.3824 Random filter ns/op: 226.805 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5308 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.2968 Single filter ns/op: 58.6163 Random filter ns/op: 291.434 Outside queries... Dry run (25d) ns/op: 32.1839 Single filter ns/op: 66.9039 Random filter ns/op: 292.828 Average FP rate %: 1.13993 Done. (For more info, run with or ``` Differential Revision: D17991712 Pulled By: ltamasi fbshipit-source-id: 7ea205550217bfaaa1d5158ebd658e5832e60f29/Fix compilation error (#5872) Summary: Without this fix, compiler complains. ``` $ROCKSDB_NO_FBCODE=1 USE_CLANG=1 make ldb table/block_based/full_filter_block.cc: In constructor ërocksdb::FullFilterBlockBuilder::FullFilterBlockBuilder(const rocksdb::SliceTransform*, bool, rocksdb::FilterBitsBuilder*)í: table/block_based/full_filter_block.cc:20:43: error: declaration of ëprefix_extractorí shadows a member of this [-Werror=shadow] FilterBitsBuilder* filter_bits_builder) ``` Pull Request resolved: Test Plan: ``` $ROCKSDB_NO_FBCODE=1 make all ``` Differential Revision: D17690058 Pulled By: riversand963 fbshipit-source-id: 19e3d9bd86e1123847095240e73d30da5d66240e/Fix a bug in format_version 3 + partition filters + prefix search (#5835) Summary: Partitioned filters make use of a top-level index to find the partition in which the filter resides. The top-level index has a key per partition. The key is guaranteed to be larger or equal than any key in that partition. When used with format_version 3, which excludes the sequence number form index keys, the separator key in the index could be equal to the prefix of the keys in the next partition. In this way, when searching for the key, the top-level index will lead us to the previous partition, which has no key with that prefix. The prefix bloom test thus returns false, although the prefix exists in the bloom of the next partition. The patch fixes that by a hack: It always adds the prefix of the first key of the next partition to the bloom of the current partition. In this way, in the corner cases that the index will lead us to the previous partition, we still can find the bloom filter there. Pull Request resolved: Differential Revision: D17513585 Pulled By: maysamyabandeh fbshipit-source-id: e2d1ff26c759e6e03875c4d57f4228316ecf50e9/Fix MultiGet() bug when whole_key_filtering is disabled (#5665) Summary: The batched MultiGet() implementation was not correctly handling bloom filter lookups when whole_key_filtering is disabled. It was incorrectly skipping keys not in the prefix_extractor domain, and not calling transform for keys in domain. This PR fixes both problems by moving the domain check and transformation to the FilterBlockReader. Tests: Unit test (confirmed failed before the fix) make check Pull Request resolved: Differential Revision: D16902380 Pulled By: anand1976 fbshipit-source-id: a6be81ad68a6e37134a65246aec7a2c590eccf00/Fix regression affecting partitioned indexes/filters when cache_index_and_filter_blocks is false (#5705) Summary: PR (and subsequent related patches) unintentionally changed the semantics of cache_index_and_filter_blocks: historically, this option only affected the main index/filter block; with the changes, it affects index/filter partitions as well. This can cause performance issues when cache_index_and_filter_blocks is false since in this case, partitions are neither cached nor preloaded (i.e. they are loaded on demand upon each access). The patch reverts to the earlier behavior, that is, partitions are cached similarly to data blocks regardless of the value of the above option. Pull Request resolved: Test Plan: make check ./db_bench ./db_bench Relevant statistics from the readrandom benchmark with the old code: rocksdb.block.cache.index.miss COUNT : 0 rocksdb.block.cache.index.hit COUNT : 0 rocksdb.block.cache.index.add COUNT : 0 rocksdb.block.cache.index.bytes.insert COUNT : 0 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 0 rocksdb.block.cache.filter.hit COUNT : 0 rocksdb.block.cache.filter.add COUNT : 0 rocksdb.block.cache.filter.bytes.insert COUNT : 0 rocksdb.block.cache.filter.bytes.evict COUNT : 0 With the new code: rocksdb.block.cache.index.miss COUNT : 2500 rocksdb.block.cache.index.hit COUNT : 42696 rocksdb.block.cache.index.add COUNT : 2500 rocksdb.block.cache.index.bytes.insert COUNT : 4050048 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 2500 rocksdb.block.cache.filter.hit COUNT : 4550493 rocksdb.block.cache.filter.add COUNT : 2500 rocksdb.block.cache.filter.bytes.insert COUNT : 10331040 rocksdb.block.cache.filter.bytes.evict COUNT : 0 Differential Revision: D16817382 Pulled By: ltamasi fbshipit-source-id: 28a516b0da1f041a03313e0b70b28cf5cf205d00/"
,,0.4379,rocksdb,"Fix MultiGet crash when no_block_cache is set (#5991) Summary: This PR fixes In ```BlockBasedTable::RetrieveMultipleBlocks()```, we were calling ```MaybeReadBlocksAndLoadToCache()```, which is a no-op if neither uncompressed nor compressed block cache are configured. Pull Request resolved: Test Plan: 1. Add unit tests that fail with the old code and pass with the new 2. make check and asan_check Cc spetrunia Differential Revision: D18272744 Pulled By: anand1976 fbshipit-source-id: e62fa6090d1a6adf84fcd51dfd6859b03c6aebfe/Fix VerifyChecksum readahead with mmap mode (#5945) Summary: A recent change introduced readahead inside VerifyChecksum(). However it is not compatible with mmap mode and generated wrong checksum verification failure. Fix it by not enabling readahead in mmap mode. Pull Request resolved: Test Plan: Add a unit test that used to fail. Differential Revision: D18021443 fbshipit-source-id: 6f2eb600f81b26edb02222563a4006869d576bff/Store the filter bits reader alongside the filter block contents (#5936) Summary: Amongst other things, PR refactored the filter block readers so that only the filter block contents are stored in the block cache (as opposed to the earlier design where the cache stored the filter block reader itself, leading to potentially dangling pointers and concurrency bugs). However, this change introduced a performance hit since with the new code, the metadata fields are re-parsed upon every access. This patch reunites the block contents with the filter bits reader to eliminate this overhead; since this is still a self-contained pure data object, it is safe to store it in the cache. (Note: this is similar to how the zstd digest is handled.) Pull Request resolved: Test Plan: make asan_check filter_bench results for the old code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.7153 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.4258 Single filter ns/op: 42.5974 Random filter ns/op: 217.861 Outside queries... Dry run (25d) ns/op: 32.4217 Single filter ns/op: 50.9855 Random filter ns/op: 219.167 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5172 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 32.3556 Single filter ns/op: 83.2239 Random filter ns/op: 370.676 Outside queries... Dry run (25d) ns/op: 32.2265 Single filter ns/op: 93.5651 Random filter ns/op: 408.393 Average FP rate %: 1.13993 Done. (For more info, run with or ``` With the new code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 25.4285 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 31.0594 Single filter ns/op: 43.8974 Random filter ns/op: 226.075 Outside queries... Dry run (25d) ns/op: 31.0295 Single filter ns/op: 50.3824 Random filter ns/op: 226.805 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5308 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.2968 Single filter ns/op: 58.6163 Random filter ns/op: 291.434 Outside queries... Dry run (25d) ns/op: 32.1839 Single filter ns/op: 66.9039 Random filter ns/op: 292.828 Average FP rate %: 1.13993 Done. (For more info, run with or ``` Differential Revision: D17991712 Pulled By: ltamasi fbshipit-source-id: 7ea205550217bfaaa1d5158ebd658e5832e60f29/Fix block cache ID uniqueness for Windows builds (#5844) Summary: Since we do not evict a files blocks from block cache before that file is deleted, we require a files cache ID prefix is both unique and non-reusable. However, the Windows functionality we were relying on only guaranteed uniqueness. That meant a newly created file could be assigned the same cache ID prefix as a deleted file. If the newly created file had block offsets matching the deleted file, full cache keys could be exactly the same, resulting in obsolete data blocks returned from cache when trying to read from the new file. We noticed this when running on FAT32 where compaction was writing out of order keys due to reading obsolete blocks from its input files. The functionality is documented as behaving the same on NTFS, although I wasnt able to repro it there. Pull Request resolved: Test Plan: we had a reliable repro of out-of-order keys on FAT32 that was fixed by this change Differential Revision: D17752442 fbshipit-source-id: 95d983f9196cf415f269e19293b97341edbf7e00/Fix data block upper bound checking for iterator reseek case (#5883) Summary: When an iterator reseek happens with the user specifying a new iterate_upper_bound in ReadOptions, and the new seek position is at the end of the same data block, the Seek() ends up using a stale value of data_block_within_upper_bound_ and may return incorrect results. Pull Request resolved: Test Plan: Added a new test case DBIteratorTest.IterReseekNewUpperBound. Verified that it failed due to the assertion failure without the fix, and passes with the fix. Differential Revision: D17752740 Pulled By: anand1976 fbshipit-source-id: f9b635ff5d6aeb0e1bef102cf8b2f900efd378e3/Refactor MultiGet names in BlockBasedTable (#5726) Summary: To improve code readability, since RetrieveBlock already calls MaybeReadBlockAndLoadToCache, we avoid name similarity of the functions that call RetrieveBlock with MaybeReadBlockAndLoadToCache. The patch thus renames MaybeLoadBlocksToCache to RetrieveMultipleBlock and deletes GetDataBlockFromCache, which contains only two lines. Pull Request resolved: Differential Revision: D16962535 Pulled By: maysamyabandeh fbshipit-source-id: 99e8946808ce4eb7857592b9003812e3004f92d6/Fix MultiGet() bug when whole_key_filtering is disabled (#5665) Summary: The batched MultiGet() implementation was not correctly handling bloom filter lookups when whole_key_filtering is disabled. It was incorrectly skipping keys not in the prefix_extractor domain, and not calling transform for keys in domain. This PR fixes both problems by moving the domain check and transformation to the FilterBlockReader. Tests: Unit test (confirmed failed before the fix) make check Pull Request resolved: Differential Revision: D16902380 Pulled By: anand1976 fbshipit-source-id: a6be81ad68a6e37134a65246aec7a2c590eccf00/add missing check for hash index when calling BlockBasedTableIterator (#5712) Summary: Previous PR added support for making prefix_extractor dynamically mutable. However, there was a missing check for hash index when creating new BlockBasedTableIterator. While the check may be redundant because no other types of IndexReader makes uses of the flag, it is less error-prone to add the missing check so that future index reader implementation will not worry about violating the contract. Pull Request resolved: Differential Revision: D16842052 Pulled By: miasantreble fbshipit-source-id: aef11c0ff7a690ed248f5b8fe23481cac486b381/Fix regression affecting partitioned indexes/filters when cache_index_and_filter_blocks is false (#5705) Summary: PR (and subsequent related patches) unintentionally changed the semantics of cache_index_and_filter_blocks: historically, this option only affected the main index/filter block; with the changes, it affects index/filter partitions as well. This can cause performance issues when cache_index_and_filter_blocks is false since in this case, partitions are neither cached nor preloaded (i.e. they are loaded on demand upon each access). The patch reverts to the earlier behavior, that is, partitions are cached similarly to data blocks regardless of the value of the above option. Pull Request resolved: Test Plan: make check ./db_bench ./db_bench Relevant statistics from the readrandom benchmark with the old code: rocksdb.block.cache.index.miss COUNT : 0 rocksdb.block.cache.index.hit COUNT : 0 rocksdb.block.cache.index.add COUNT : 0 rocksdb.block.cache.index.bytes.insert COUNT : 0 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 0 rocksdb.block.cache.filter.hit COUNT : 0 rocksdb.block.cache.filter.add COUNT : 0 rocksdb.block.cache.filter.bytes.insert COUNT : 0 rocksdb.block.cache.filter.bytes.evict COUNT : 0 With the new code: rocksdb.block.cache.index.miss COUNT : 2500 rocksdb.block.cache.index.hit COUNT : 42696 rocksdb.block.cache.index.add COUNT : 2500 rocksdb.block.cache.index.bytes.insert COUNT : 4050048 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 2500 rocksdb.block.cache.filter.hit COUNT : 4550493 rocksdb.block.cache.filter.add COUNT : 2500 rocksdb.block.cache.filter.bytes.insert COUNT : 10331040 rocksdb.block.cache.filter.bytes.evict COUNT : 0 Differential Revision: D16817382 Pulled By: ltamasi fbshipit-source-id: 28a516b0da1f041a03313e0b70b28cf5cf205d00/"
,,0.6514,rocksdb,"Store the filter bits reader alongside the filter block contents (#5936) Summary: Amongst other things, PR refactored the filter block readers so that only the filter block contents are stored in the block cache (as opposed to the earlier design where the cache stored the filter block reader itself, leading to potentially dangling pointers and concurrency bugs). However, this change introduced a performance hit since with the new code, the metadata fields are re-parsed upon every access. This patch reunites the block contents with the filter bits reader to eliminate this overhead; since this is still a self-contained pure data object, it is safe to store it in the cache. (Note: this is similar to how the zstd digest is handled.) Pull Request resolved: Test Plan: make asan_check filter_bench results for the old code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.7153 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.4258 Single filter ns/op: 42.5974 Random filter ns/op: 217.861 Outside queries... Dry run (25d) ns/op: 32.4217 Single filter ns/op: 50.9855 Random filter ns/op: 219.167 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5172 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 32.3556 Single filter ns/op: 83.2239 Random filter ns/op: 370.676 Outside queries... Dry run (25d) ns/op: 32.2265 Single filter ns/op: 93.5651 Random filter ns/op: 408.393 Average FP rate %: 1.13993 Done. (For more info, run with or ``` With the new code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 25.4285 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 31.0594 Single filter ns/op: 43.8974 Random filter ns/op: 226.075 Outside queries... Dry run (25d) ns/op: 31.0295 Single filter ns/op: 50.3824 Random filter ns/op: 226.805 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5308 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.2968 Single filter ns/op: 58.6163 Random filter ns/op: 291.434 Outside queries... Dry run (25d) ns/op: 32.1839 Single filter ns/op: 66.9039 Random filter ns/op: 292.828 Average FP rate %: 1.13993 Done. (For more info, run with or ``` Differential Revision: D17991712 Pulled By: ltamasi fbshipit-source-id: 7ea205550217bfaaa1d5158ebd658e5832e60f29/Fix a bug in format_version 3 + partition filters + prefix search (#5835) Summary: Partitioned filters make use of a top-level index to find the partition in which the filter resides. The top-level index has a key per partition. The key is guaranteed to be larger or equal than any key in that partition. When used with format_version 3, which excludes the sequence number form index keys, the separator key in the index could be equal to the prefix of the keys in the next partition. In this way, when searching for the key, the top-level index will lead us to the previous partition, which has no key with that prefix. The prefix bloom test thus returns false, although the prefix exists in the bloom of the next partition. The patch fixes that by a hack: It always adds the prefix of the first key of the next partition to the bloom of the current partition. In this way, in the corner cases that the index will lead us to the previous partition, we still can find the bloom filter there. Pull Request resolved: Differential Revision: D17513585 Pulled By: maysamyabandeh fbshipit-source-id: e2d1ff26c759e6e03875c4d57f4228316ecf50e9/"
,,0.3843,rocksdb,"Fix regression affecting partitioned indexes/filters when cache_index_and_filter_blocks is false (#5705) Summary: PR (and subsequent related patches) unintentionally changed the semantics of cache_index_and_filter_blocks: historically, this option only affected the main index/filter block; with the changes, it affects index/filter partitions as well. This can cause performance issues when cache_index_and_filter_blocks is false since in this case, partitions are neither cached nor preloaded (i.e. they are loaded on demand upon each access). The patch reverts to the earlier behavior, that is, partitions are cached similarly to data blocks regardless of the value of the above option. Pull Request resolved: Test Plan: make check ./db_bench ./db_bench Relevant statistics from the readrandom benchmark with the old code: rocksdb.block.cache.index.miss COUNT : 0 rocksdb.block.cache.index.hit COUNT : 0 rocksdb.block.cache.index.add COUNT : 0 rocksdb.block.cache.index.bytes.insert COUNT : 0 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 0 rocksdb.block.cache.filter.hit COUNT : 0 rocksdb.block.cache.filter.add COUNT : 0 rocksdb.block.cache.filter.bytes.insert COUNT : 0 rocksdb.block.cache.filter.bytes.evict COUNT : 0 With the new code: rocksdb.block.cache.index.miss COUNT : 2500 rocksdb.block.cache.index.hit COUNT : 42696 rocksdb.block.cache.index.add COUNT : 2500 rocksdb.block.cache.index.bytes.insert COUNT : 4050048 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 2500 rocksdb.block.cache.filter.hit COUNT : 4550493 rocksdb.block.cache.filter.add COUNT : 2500 rocksdb.block.cache.filter.bytes.insert COUNT : 10331040 rocksdb.block.cache.filter.bytes.evict COUNT : 0 Differential Revision: D16817382 Pulled By: ltamasi fbshipit-source-id: 28a516b0da1f041a03313e0b70b28cf5cf205d00/"
,,0.6583,rocksdb,"Store the filter bits reader alongside the filter block contents (#5936) Summary: Amongst other things, PR refactored the filter block readers so that only the filter block contents are stored in the block cache (as opposed to the earlier design where the cache stored the filter block reader itself, leading to potentially dangling pointers and concurrency bugs). However, this change introduced a performance hit since with the new code, the metadata fields are re-parsed upon every access. This patch reunites the block contents with the filter bits reader to eliminate this overhead; since this is still a self-contained pure data object, it is safe to store it in the cache. (Note: this is similar to how the zstd digest is handled.) Pull Request resolved: Test Plan: make asan_check filter_bench results for the old code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.7153 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.4258 Single filter ns/op: 42.5974 Random filter ns/op: 217.861 Outside queries... Dry run (25d) ns/op: 32.4217 Single filter ns/op: 50.9855 Random filter ns/op: 219.167 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5172 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 32.3556 Single filter ns/op: 83.2239 Random filter ns/op: 370.676 Outside queries... Dry run (25d) ns/op: 32.2265 Single filter ns/op: 93.5651 Random filter ns/op: 408.393 Average FP rate %: 1.13993 Done. (For more info, run with or ``` With the new code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 25.4285 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 31.0594 Single filter ns/op: 43.8974 Random filter ns/op: 226.075 Outside queries... Dry run (25d) ns/op: 31.0295 Single filter ns/op: 50.3824 Random filter ns/op: 226.805 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5308 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.2968 Single filter ns/op: 58.6163 Random filter ns/op: 291.434 Outside queries... Dry run (25d) ns/op: 32.1839 Single filter ns/op: 66.9039 Random filter ns/op: 292.828 Average FP rate %: 1.13993 Done. (For more info, run with or ``` Differential Revision: D17991712 Pulled By: ltamasi fbshipit-source-id: 7ea205550217bfaaa1d5158ebd658e5832e60f29/Fix regression affecting partitioned indexes/filters when cache_index_and_filter_blocks is false (#5705) Summary: PR (and subsequent related patches) unintentionally changed the semantics of cache_index_and_filter_blocks: historically, this option only affected the main index/filter block; with the changes, it affects index/filter partitions as well. This can cause performance issues when cache_index_and_filter_blocks is false since in this case, partitions are neither cached nor preloaded (i.e. they are loaded on demand upon each access). The patch reverts to the earlier behavior, that is, partitions are cached similarly to data blocks regardless of the value of the above option. Pull Request resolved: Test Plan: make check ./db_bench ./db_bench Relevant statistics from the readrandom benchmark with the old code: rocksdb.block.cache.index.miss COUNT : 0 rocksdb.block.cache.index.hit COUNT : 0 rocksdb.block.cache.index.add COUNT : 0 rocksdb.block.cache.index.bytes.insert COUNT : 0 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 0 rocksdb.block.cache.filter.hit COUNT : 0 rocksdb.block.cache.filter.add COUNT : 0 rocksdb.block.cache.filter.bytes.insert COUNT : 0 rocksdb.block.cache.filter.bytes.evict COUNT : 0 With the new code: rocksdb.block.cache.index.miss COUNT : 2500 rocksdb.block.cache.index.hit COUNT : 42696 rocksdb.block.cache.index.add COUNT : 2500 rocksdb.block.cache.index.bytes.insert COUNT : 4050048 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 2500 rocksdb.block.cache.filter.hit COUNT : 4550493 rocksdb.block.cache.filter.add COUNT : 2500 rocksdb.block.cache.filter.bytes.insert COUNT : 10331040 rocksdb.block.cache.filter.bytes.evict COUNT : 0 Differential Revision: D16817382 Pulled By: ltamasi fbshipit-source-id: 28a516b0da1f041a03313e0b70b28cf5cf205d00/"
,,0.3945,rocksdb,"Fix regression affecting partitioned indexes/filters when cache_index_and_filter_blocks is false (#5705) Summary: PR (and subsequent related patches) unintentionally changed the semantics of cache_index_and_filter_blocks: historically, this option only affected the main index/filter block; with the changes, it affects index/filter partitions as well. This can cause performance issues when cache_index_and_filter_blocks is false since in this case, partitions are neither cached nor preloaded (i.e. they are loaded on demand upon each access). The patch reverts to the earlier behavior, that is, partitions are cached similarly to data blocks regardless of the value of the above option. Pull Request resolved: Test Plan: make check ./db_bench ./db_bench Relevant statistics from the readrandom benchmark with the old code: rocksdb.block.cache.index.miss COUNT : 0 rocksdb.block.cache.index.hit COUNT : 0 rocksdb.block.cache.index.add COUNT : 0 rocksdb.block.cache.index.bytes.insert COUNT : 0 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 0 rocksdb.block.cache.filter.hit COUNT : 0 rocksdb.block.cache.filter.add COUNT : 0 rocksdb.block.cache.filter.bytes.insert COUNT : 0 rocksdb.block.cache.filter.bytes.evict COUNT : 0 With the new code: rocksdb.block.cache.index.miss COUNT : 2500 rocksdb.block.cache.index.hit COUNT : 42696 rocksdb.block.cache.index.add COUNT : 2500 rocksdb.block.cache.index.bytes.insert COUNT : 4050048 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 2500 rocksdb.block.cache.filter.hit COUNT : 4550493 rocksdb.block.cache.filter.add COUNT : 2500 rocksdb.block.cache.filter.bytes.insert COUNT : 10331040 rocksdb.block.cache.filter.bytes.evict COUNT : 0 Differential Revision: D16817382 Pulled By: ltamasi fbshipit-source-id: 28a516b0da1f041a03313e0b70b28cf5cf205d00/"
,,0.1797,rocksdb,"Fix MultiGet() bug when whole_key_filtering is disabled (#5665) Summary: The batched MultiGet() implementation was not correctly handling bloom filter lookups when whole_key_filtering is disabled. It was incorrectly skipping keys not in the prefix_extractor domain, and not calling transform for keys in domain. This PR fixes both problems by moving the domain check and transformation to the FilterBlockReader. Tests: Unit test (confirmed failed before the fix) make check Pull Request resolved: Differential Revision: D16902380 Pulled By: anand1976 fbshipit-source-id: a6be81ad68a6e37134a65246aec7a2c590eccf00/"
,,0.6054,rocksdb,"Clean up some filter tests and comments (#5960) Summary: Some filtering tests were unfriendly to new implementations of FilterBitsBuilder because of dynamic_cast to FullFilterBitsBuilder. Most of those have now been cleaned up, worked around, or at least changed from crash on dynamic_cast failure to individual test failure. Also put some clarifying comments on filter-related APIs. Pull Request resolved: Test Plan: make check Differential Revision: D18121223 Pulled By: pdillinger fbshipit-source-id: e83827d9d5d96315d96f8e25a99cd70f497d802c/Store the filter bits reader alongside the filter block contents (#5936) Summary: Amongst other things, PR refactored the filter block readers so that only the filter block contents are stored in the block cache (as opposed to the earlier design where the cache stored the filter block reader itself, leading to potentially dangling pointers and concurrency bugs). However, this change introduced a performance hit since with the new code, the metadata fields are re-parsed upon every access. This patch reunites the block contents with the filter bits reader to eliminate this overhead; since this is still a self-contained pure data object, it is safe to store it in the cache. (Note: this is similar to how the zstd digest is handled.) Pull Request resolved: Test Plan: make asan_check filter_bench results for the old code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.7153 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.4258 Single filter ns/op: 42.5974 Random filter ns/op: 217.861 Outside queries... Dry run (25d) ns/op: 32.4217 Single filter ns/op: 50.9855 Random filter ns/op: 219.167 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5172 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 32.3556 Single filter ns/op: 83.2239 Random filter ns/op: 370.676 Outside queries... Dry run (25d) ns/op: 32.2265 Single filter ns/op: 93.5651 Random filter ns/op: 408.393 Average FP rate %: 1.13993 Done. (For more info, run with or ``` With the new code: ``` $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 25.4285 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 31.0594 Single filter ns/op: 43.8974 Random filter ns/op: 226.075 Outside queries... Dry run (25d) ns/op: 31.0295 Single filter ns/op: 50.3824 Random filter ns/op: 226.805 Average FP rate %: 1.13993 Done. (For more info, run with or $ ./filter_bench WARNING: Assertions are enabled; benchmarks unnecessarily slow Building... Build avg ns/key: 26.5308 Number of filters: 16669 Total memory (MB): 200.009 Bits/key actual: 10.0647 Inside queries... Dry run (46b) ns/op: 33.2968 Single filter ns/op: 58.6163 Random filter ns/op: 291.434 Outside queries... Dry run (25d) ns/op: 32.1839 Single filter ns/op: 66.9039 Random filter ns/op: 292.828 Average FP rate %: 1.13993 Done. (For more info, run with or ``` Differential Revision: D17991712 Pulled By: ltamasi fbshipit-source-id: 7ea205550217bfaaa1d5158ebd658e5832e60f29/Fix SeekForPrev bug with Partitioned Filters and Prefix (#5907) Summary: Partition Filters make use of a top-level index to find the partition that might have the bloom hash of the key. The index is with internal key format (before format version 3). Each partition contains the i) blooms of the keys in that range ii) bloom of prefixes of keys in that range, iii) the bloom of the prefix of the last key in the previous partition. When ::SeekForPrev(key), we first perform a prefix bloom test on the SST file. The partition however is identified using the full internal key, rather than the prefix key. The reason is to be compatible with the internal key format of the top-level index. This creates a corner case. Example: SST k, Partition N: P1K1, P1K2 SST k, top-level index: P1K2 SST k+1, Partition 1: P2K1, P3K1 SST k+1 top-level index: P3K1 When SeekForPrev(P1K3), it should point us to P1K2. However SST k top-level index would reject P1K3 since it is out of range. One possible fix would be to search with the prefix P1 (instead of full internal key P1K3) however the details of properly comparing prefix with full internal key might get complicated. The fix we apply in this PR is to look into the last partition anyway even if the key is out of range. Pull Request resolved: Differential Revision: D17889918 Pulled By: maysamyabandeh fbshipit-source-id: 169fd7b3c71dbc08808eae5a8340611ebe5bdc1e/Fix a bug in format_version 3 + partition filters + prefix search (#5835) Summary: Partitioned filters make use of a top-level index to find the partition in which the filter resides. The top-level index has a key per partition. The key is guaranteed to be larger or equal than any key in that partition. When used with format_version 3, which excludes the sequence number form index keys, the separator key in the index could be equal to the prefix of the keys in the next partition. In this way, when searching for the key, the top-level index will lead us to the previous partition, which has no key with that prefix. The prefix bloom test thus returns false, although the prefix exists in the bloom of the next partition. The patch fixes that by a hack: It always adds the prefix of the first key of the next partition to the bloom of the current partition. In this way, in the corner cases that the index will lead us to the previous partition, we still can find the bloom filter there. Pull Request resolved: Differential Revision: D17513585 Pulled By: maysamyabandeh fbshipit-source-id: e2d1ff26c759e6e03875c4d57f4228316ecf50e9/"
,,0.4007,rocksdb,"Fix regression affecting partitioned indexes/filters when cache_index_and_filter_blocks is false (#5705) Summary: PR (and subsequent related patches) unintentionally changed the semantics of cache_index_and_filter_blocks: historically, this option only affected the main index/filter block; with the changes, it affects index/filter partitions as well. This can cause performance issues when cache_index_and_filter_blocks is false since in this case, partitions are neither cached nor preloaded (i.e. they are loaded on demand upon each access). The patch reverts to the earlier behavior, that is, partitions are cached similarly to data blocks regardless of the value of the above option. Pull Request resolved: Test Plan: make check ./db_bench ./db_bench Relevant statistics from the readrandom benchmark with the old code: rocksdb.block.cache.index.miss COUNT : 0 rocksdb.block.cache.index.hit COUNT : 0 rocksdb.block.cache.index.add COUNT : 0 rocksdb.block.cache.index.bytes.insert COUNT : 0 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 0 rocksdb.block.cache.filter.hit COUNT : 0 rocksdb.block.cache.filter.add COUNT : 0 rocksdb.block.cache.filter.bytes.insert COUNT : 0 rocksdb.block.cache.filter.bytes.evict COUNT : 0 With the new code: rocksdb.block.cache.index.miss COUNT : 2500 rocksdb.block.cache.index.hit COUNT : 42696 rocksdb.block.cache.index.add COUNT : 2500 rocksdb.block.cache.index.bytes.insert COUNT : 4050048 rocksdb.block.cache.index.bytes.evict COUNT : 0 rocksdb.block.cache.filter.miss COUNT : 2500 rocksdb.block.cache.filter.hit COUNT : 4550493 rocksdb.block.cache.filter.add COUNT : 2500 rocksdb.block.cache.filter.bytes.insert COUNT : 10331040 rocksdb.block.cache.filter.bytes.evict COUNT : 0 Differential Revision: D16817382 Pulled By: ltamasi fbshipit-source-id: 28a516b0da1f041a03313e0b70b28cf5cf205d00/"
,,0.4939,rocksdb,"Fix TSAN failures in DistributedMutex tests (#5684) Summary: TSAN was not able to correctly instrument atomic bts and btr instructions, so when TSAN is enabled implement those with std::atomic::fetch_or and std::atomic::fetch_and. Also disable tests that fail on TSAN with false negatives (we know these are false negatives because this other verifiably correct program fails with the same TSAN error ``` make clean TEST_TMPDIR=/dev/shm/rocksdb OPT=-g COMPILE_WITH_TSAN=1 make J=1 folly_synchronization_distributed_mutex_test ``` This is the code that fails with the same false-negative with TSAN ``` namespace { class ExceptionWithConstructionTrack : public std::exception { public: explicit ExceptionWithConstructionTrack(int id) : id_{folly::to<std::string>(id)}, constructionTrack_{id} {} const char* what() const noexcept override { return id_.c_str(); } private: std::string id_; TestConstruction constructionTrack_; }; template Storage, typename Atomic> void transferCurrentException(Storage& storage, Atomic& produced) { assert(std::current_exception()); new (&storage) std::exception_ptr(std::current_exception()); produced->store(true, std::memory_order_release); } void concurrentExceptionPropagationStress( int numThreads, std::chrono::milliseconds milliseconds) { auto&& stop std::atomic<bool>{false}; auto&& exceptions std::vector<std::aligned_storage<48, 8>::type>{}; auto&& produced std::vector<std::unique_ptr<std::atomic<bool>>>{}; auto&& consumed std::vector<std::unique_ptr<std::atomic<bool>>>{}; auto&& consumers std::vector<std::thread>{}; for (auto i 0; i numThreads; ++i) { produced.emplace_back(new std::atomic<bool>{false}); consumed.emplace_back(new std::atomic<bool>{false}); exceptions.push_back({}); } auto producer std::thread{[&]() { auto counter std::vector<int>(numThreads, 0); for (auto i 0; true; i ((i + 1) % numThreads)) { try { throw ExceptionWithConstructionTrack{counter.at(i)++}; } catch (...) { transferCurrentException(exceptions.at(i), produced.at(i)); } while (consumed.at(i)->load(std::memory_order_acquire)) { if (stop.load(std::memory_order_acquire)) { return; } } consumed.at(i)->store(false, std::memory_order_release); } }}; for (auto i 0; i numThreads; ++i) { consumers.emplace_back([&, i]() { auto counter 0; while (true) { while (produced.at(i)->load(std::memory_order_acquire)) { if (stop.load(std::memory_order_acquire)) { return; } } produced.at(i)->store(false, std::memory_order_release); try { auto storage &exceptions.at(i); auto exc folly::launder( reinterpret_cast<std::exception_ptr*>(storage)); auto copy std::move(*exc); exc->std::exception_ptr::~exception_ptr(); std::rethrow_exception(std::move(copy)); } catch (std::exception& exc) { auto value std::stoi(exc.what()); EXPECT_EQ(value, counter++); } consumed.at(i)->store(true, std::memory_order_release); } }); } std::this_thread::sleep_for(milliseconds); stop.store(true); producer.join(); for (auto& thread : consumers) { thread.join(); } } } // namespace ``` Pull Request resolved: Differential Revision: D16746077 Pulled By: miasantreble fbshipit-source-id: 8af88dcf9161c05daec1a76290f577918638f79d/"
,,0.4828,rocksdb,"Fix TSAN failures in DistributedMutex tests (#5684) Summary: TSAN was not able to correctly instrument atomic bts and btr instructions, so when TSAN is enabled implement those with std::atomic::fetch_or and std::atomic::fetch_and. Also disable tests that fail on TSAN with false negatives (we know these are false negatives because this other verifiably correct program fails with the same TSAN error ``` make clean TEST_TMPDIR=/dev/shm/rocksdb OPT=-g COMPILE_WITH_TSAN=1 make J=1 folly_synchronization_distributed_mutex_test ``` This is the code that fails with the same false-negative with TSAN ``` namespace { class ExceptionWithConstructionTrack : public std::exception { public: explicit ExceptionWithConstructionTrack(int id) : id_{folly::to<std::string>(id)}, constructionTrack_{id} {} const char* what() const noexcept override { return id_.c_str(); } private: std::string id_; TestConstruction constructionTrack_; }; template Storage, typename Atomic> void transferCurrentException(Storage& storage, Atomic& produced) { assert(std::current_exception()); new (&storage) std::exception_ptr(std::current_exception()); produced->store(true, std::memory_order_release); } void concurrentExceptionPropagationStress( int numThreads, std::chrono::milliseconds milliseconds) { auto&& stop std::atomic<bool>{false}; auto&& exceptions std::vector<std::aligned_storage<48, 8>::type>{}; auto&& produced std::vector<std::unique_ptr<std::atomic<bool>>>{}; auto&& consumed std::vector<std::unique_ptr<std::atomic<bool>>>{}; auto&& consumers std::vector<std::thread>{}; for (auto i 0; i numThreads; ++i) { produced.emplace_back(new std::atomic<bool>{false}); consumed.emplace_back(new std::atomic<bool>{false}); exceptions.push_back({}); } auto producer std::thread{[&]() { auto counter std::vector<int>(numThreads, 0); for (auto i 0; true; i ((i + 1) % numThreads)) { try { throw ExceptionWithConstructionTrack{counter.at(i)++}; } catch (...) { transferCurrentException(exceptions.at(i), produced.at(i)); } while (consumed.at(i)->load(std::memory_order_acquire)) { if (stop.load(std::memory_order_acquire)) { return; } } consumed.at(i)->store(false, std::memory_order_release); } }}; for (auto i 0; i numThreads; ++i) { consumers.emplace_back([&, i]() { auto counter 0; while (true) { while (produced.at(i)->load(std::memory_order_acquire)) { if (stop.load(std::memory_order_acquire)) { return; } } produced.at(i)->store(false, std::memory_order_release); try { auto storage &exceptions.at(i); auto exc folly::launder( reinterpret_cast<std::exception_ptr*>(storage)); auto copy std::move(*exc); exc->std::exception_ptr::~exception_ptr(); std::rethrow_exception(std::move(copy)); } catch (std::exception& exc) { auto value std::stoi(exc.what()); EXPECT_EQ(value, counter++); } consumed.at(i)->store(true, std::memory_order_release); } }); } std::this_thread::sleep_for(milliseconds); stop.store(true); producer.join(); for (auto& thread : consumers) { thread.join(); } } } // namespace ``` Pull Request resolved: Differential Revision: D16746077 Pulled By: miasantreble fbshipit-source-id: 8af88dcf9161c05daec1a76290f577918638f79d/"
,,0.2912,rocksdb,"Refactor/consolidate legacy Bloom implementation details (#5784) Summary: Refactoring to consolidate implementation details of legacy Bloom filters. This helps to organize and document some related, obscure code. Also added make/cpp var TEST_CACHE_LINE_SIZE so that its easy to compile and run unit tests for non-native cache line size. (Fixed a related test failure in db_properties_test.) Pull Request resolved: Test Plan: make check, including Recently added Bloom schema unit tests (in ./plain_table_db_test && ./bloom_test), and including with TEST_CACHE_LINE_SIZE=128U and TEST_CACHE_LINE_SIZE=256U. Tested the schema tests with temporary fault injection into new implementations. Some performance testing with modified unit tests suggest a small to moderate improvement in speed. Differential Revision: D17381384 Pulled By: pdillinger fbshipit-source-id: ee42586da996798910fc45ac0b6289147f16d8df/Revert changes from PR#5784 accidentally in PR#5780 (#5810) Summary: This will allow us to fix history by having the code changes for PR#5784 properly attributed to it. Pull Request resolved: Differential Revision: D17400231 Pulled By: pdillinger fbshipit-source-id: 2da8b1cdf2533cfedb35b5526eadefb38c291f09/"
,,0.4828,rocksdb,"Fix TSAN failures in DistributedMutex tests (#5684) Summary: TSAN was not able to correctly instrument atomic bts and btr instructions, so when TSAN is enabled implement those with std::atomic::fetch_or and std::atomic::fetch_and. Also disable tests that fail on TSAN with false negatives (we know these are false negatives because this other verifiably correct program fails with the same TSAN error ``` make clean TEST_TMPDIR=/dev/shm/rocksdb OPT=-g COMPILE_WITH_TSAN=1 make J=1 folly_synchronization_distributed_mutex_test ``` This is the code that fails with the same false-negative with TSAN ``` namespace { class ExceptionWithConstructionTrack : public std::exception { public: explicit ExceptionWithConstructionTrack(int id) : id_{folly::to<std::string>(id)}, constructionTrack_{id} {} const char* what() const noexcept override { return id_.c_str(); } private: std::string id_; TestConstruction constructionTrack_; }; template Storage, typename Atomic> void transferCurrentException(Storage& storage, Atomic& produced) { assert(std::current_exception()); new (&storage) std::exception_ptr(std::current_exception()); produced->store(true, std::memory_order_release); } void concurrentExceptionPropagationStress( int numThreads, std::chrono::milliseconds milliseconds) { auto&& stop std::atomic<bool>{false}; auto&& exceptions std::vector<std::aligned_storage<48, 8>::type>{}; auto&& produced std::vector<std::unique_ptr<std::atomic<bool>>>{}; auto&& consumed std::vector<std::unique_ptr<std::atomic<bool>>>{}; auto&& consumers std::vector<std::thread>{}; for (auto i 0; i numThreads; ++i) { produced.emplace_back(new std::atomic<bool>{false}); consumed.emplace_back(new std::atomic<bool>{false}); exceptions.push_back({}); } auto producer std::thread{[&]() { auto counter std::vector<int>(numThreads, 0); for (auto i 0; true; i ((i + 1) % numThreads)) { try { throw ExceptionWithConstructionTrack{counter.at(i)++}; } catch (...) { transferCurrentException(exceptions.at(i), produced.at(i)); } while (consumed.at(i)->load(std::memory_order_acquire)) { if (stop.load(std::memory_order_acquire)) { return; } } consumed.at(i)->store(false, std::memory_order_release); } }}; for (auto i 0; i numThreads; ++i) { consumers.emplace_back([&, i]() { auto counter 0; while (true) { while (produced.at(i)->load(std::memory_order_acquire)) { if (stop.load(std::memory_order_acquire)) { return; } } produced.at(i)->store(false, std::memory_order_release); try { auto storage &exceptions.at(i); auto exc folly::launder( reinterpret_cast<std::exception_ptr*>(storage)); auto copy std::move(*exc); exc->std::exception_ptr::~exception_ptr(); std::rethrow_exception(std::move(copy)); } catch (std::exception& exc) { auto value std::stoi(exc.what()); EXPECT_EQ(value, counter++); } consumed.at(i)->store(true, std::memory_order_release); } }); } std::this_thread::sleep_for(milliseconds); stop.store(true); producer.join(); for (auto& thread : consumers) { thread.join(); } } } // namespace ``` Pull Request resolved: Differential Revision: D16746077 Pulled By: miasantreble fbshipit-source-id: 8af88dcf9161c05daec1a76290f577918638f79d/"
,,0.5006,rocksdb,"Fix TSAN failures in DistributedMutex tests (#5684) Summary: TSAN was not able to correctly instrument atomic bts and btr instructions, so when TSAN is enabled implement those with std::atomic::fetch_or and std::atomic::fetch_and. Also disable tests that fail on TSAN with false negatives (we know these are false negatives because this other verifiably correct program fails with the same TSAN error ``` make clean TEST_TMPDIR=/dev/shm/rocksdb OPT=-g COMPILE_WITH_TSAN=1 make J=1 folly_synchronization_distributed_mutex_test ``` This is the code that fails with the same false-negative with TSAN ``` namespace { class ExceptionWithConstructionTrack : public std::exception { public: explicit ExceptionWithConstructionTrack(int id) : id_{folly::to<std::string>(id)}, constructionTrack_{id} {} const char* what() const noexcept override { return id_.c_str(); } private: std::string id_; TestConstruction constructionTrack_; }; template Storage, typename Atomic> void transferCurrentException(Storage& storage, Atomic& produced) { assert(std::current_exception()); new (&storage) std::exception_ptr(std::current_exception()); produced->store(true, std::memory_order_release); } void concurrentExceptionPropagationStress( int numThreads, std::chrono::milliseconds milliseconds) { auto&& stop std::atomic<bool>{false}; auto&& exceptions std::vector<std::aligned_storage<48, 8>::type>{}; auto&& produced std::vector<std::unique_ptr<std::atomic<bool>>>{}; auto&& consumed std::vector<std::unique_ptr<std::atomic<bool>>>{}; auto&& consumers std::vector<std::thread>{}; for (auto i 0; i numThreads; ++i) { produced.emplace_back(new std::atomic<bool>{false}); consumed.emplace_back(new std::atomic<bool>{false}); exceptions.push_back({}); } auto producer std::thread{[&]() { auto counter std::vector<int>(numThreads, 0); for (auto i 0; true; i ((i + 1) % numThreads)) { try { throw ExceptionWithConstructionTrack{counter.at(i)++}; } catch (...) { transferCurrentException(exceptions.at(i), produced.at(i)); } while (consumed.at(i)->load(std::memory_order_acquire)) { if (stop.load(std::memory_order_acquire)) { return; } } consumed.at(i)->store(false, std::memory_order_release); } }}; for (auto i 0; i numThreads; ++i) { consumers.emplace_back([&, i]() { auto counter 0; while (true) { while (produced.at(i)->load(std::memory_order_acquire)) { if (stop.load(std::memory_order_acquire)) { return; } } produced.at(i)->store(false, std::memory_order_release); try { auto storage &exceptions.at(i); auto exc folly::launder( reinterpret_cast<std::exception_ptr*>(storage)); auto copy std::move(*exc); exc->std::exception_ptr::~exception_ptr(); std::rethrow_exception(std::move(copy)); } catch (std::exception& exc) { auto value std::stoi(exc.what()); EXPECT_EQ(value, counter++); } consumed.at(i)->store(true, std::memory_order_release); } }); } std::this_thread::sleep_for(milliseconds); stop.store(true); producer.join(); for (auto& thread : consumers) { thread.join(); } } } // namespace ``` Pull Request resolved: Differential Revision: D16746077 Pulled By: miasantreble fbshipit-source-id: 8af88dcf9161c05daec1a76290f577918638f79d/"
,,0.2803,rocksdb,"Refactor/consolidate legacy Bloom implementation details (#5784) Summary: Refactoring to consolidate implementation details of legacy Bloom filters. This helps to organize and document some related, obscure code. Also added make/cpp var TEST_CACHE_LINE_SIZE so that its easy to compile and run unit tests for non-native cache line size. (Fixed a related test failure in db_properties_test.) Pull Request resolved: Test Plan: make check, including Recently added Bloom schema unit tests (in ./plain_table_db_test && ./bloom_test), and including with TEST_CACHE_LINE_SIZE=128U and TEST_CACHE_LINE_SIZE=256U. Tested the schema tests with temporary fault injection into new implementations. Some performance testing with modified unit tests suggest a small to moderate improvement in speed. Differential Revision: D17381384 Pulled By: pdillinger fbshipit-source-id: ee42586da996798910fc45ac0b6289147f16d8df/Revert changes from PR#5784 accidentally in PR#5780 (#5810) Summary: This will allow us to fix history by having the code changes for PR#5784 properly attributed to it. Pull Request resolved: Differential Revision: D17400231 Pulled By: pdillinger fbshipit-source-id: 2da8b1cdf2533cfedb35b5526eadefb38c291f09/"
,,0.7946,rocksdb,"Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/Allow fractional bits/key in BloomFilterPolicy (#6092) Summary: Theres no technological impediment to allowing the Bloom filter bits/key to be non-integer (fractional/decimal) values, and it provides finer control over the memory vs. accuracy trade-off. This is especially handy in using the format_version=5 Bloom filter in place of the old one, because bits_per_key=9.55 provides the same accuracy as the old bits_per_key=10. This change not only requires refining the logic for choosing the best num_probes for a given bits/key setting, it revealed a flaw in that logic. As bits/key gets higher, the best num_probes for a cache-local Bloom filter is closer to bpk / 2 than to bpk * 0.69, the best choice for a standard Bloom filter. For example, at 16 bits per key, the best num_probes is 9 (FP rate 0.0843%) not 11 (FP rate 0.0884%). This change fixes and refines that logic (for the format_version=5 Bloom filter only, just in case) based on empirical tests to find accuracy inflection points between each num_probes. Although bits_per_key is now specified as a double, the new Bloom filter converts/rounds this to ""millibits / key"" for predictable/precise internal computations. Just in case of unforeseen compatibility issues, we round to the nearest whole number bits / key for the legacy Bloom filter, so as not to unlock new behaviors for it. Pull Request resolved: Test Plan: unit tests included Differential Revision: D18711313 Pulled By: pdillinger fbshipit-source-id: 1aa73295f152a995328cb846ef9157ae8a05522a/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.8171,rocksdb,"Warn on excessive keys for legacy Bloom filter with 32-bit hash (#6317) Summary: With many millions of keys, the old Bloom filter implementation for the block-based table (format_version 4) would have excessive FP rate due to the limitations of feeding the Bloom filter with a 32-bit hash. This change computes an estimated inflated FP rate due to this effect and warns in the log whenever an SST filter is constructed (almost certainly a ""full"" not ""partitioned"" filter) that exceeds 1.5x FP rate due to this effect. The detailed condition is only checked if 3 million keys or more have been added to a filter, as this should be a lower bound for common bits/key settings (< 20). Recommended remedies include smaller SST file size, using format_version >= 5 (for new Bloom filter), or using partitioned filters. This does not change behavior other than generating warnings for some constructed filters using the old implementation. Pull Request resolved: Test Plan: Example with warning, 15M keys 15 bits / key: (working_mem_size_mb is just to stop after building one filter if its large) $ ./filter_bench 2>&1 | grep FP rate [WARN] [/block_based/filter_policy.cc:292] Using legacy SST/BBT Bloom filter with excessive key count (15.0M 15bpk), causing estimated 1.8x higher filter FP rate. Consider using new Bloom with format_version>=5, smaller SST file size, or partitioned filters. Predicted FP rate %: 0.766702 Average FP rate %: 0.66846 Example without warning (150K keys): $ ./filter_bench 2>&1 | grep FP rate Predicted FP rate %: 0.422857 Average FP rate %: 0.379301 $ With more samples at 15 bits/key: 150K keys no warning; actual: 0.379% FP rate (baseline) 1M keys no warning; actual: 0.396% FP rate, 1.045x 9M keys no warning; actual: 0.563% FP rate, 1.485x 10M keys warning (1.5x); actual: 0.564% FP rate, 1.488x 15M keys warning (1.8x); actual: 0.668% FP rate, 1.76x 25M keys warning (2.4x); actual: 0.880% FP rate, 2.32x At 10 bits/key: 150K keys no warning; actual: 1.17% FP rate (baseline) 1M keys no warning; actual: 1.16% FP rate 10M keys no warning; actual: 1.32% FP rate, 1.13x 25M keys no warning; actual: 1.63% FP rate, 1.39x 35M keys warning (1.6x); actual: 1.81% FP rate, 1.55x At 5 bits/key: 150K keys no warning; actual: 9.32% FP rate (baseline) 25M keys no warning; actual: 9.62% FP rate, 1.03x 200M keys no warning; actual: 12.2% FP rate, 1.31x 250M keys warning (1.5x); actual: 12.8% FP rate, 1.37x 300M keys warning (1.6x); actual: 13.4% FP rate, 1.43x The reason for the modest inaccuracy at low bits/key is that the assumption of independence between a collision between 32-hash values feeding the filter and an FP in the filter is not quite true for implementations using ""simple"" logic to compute indices from the stock hash result. Theres math on this in my dissertation, but I dont think its worth the effort just for these extreme cases (> 100 million keys and low-ish bits/key). Differential Revision: D19471715 Pulled By: pdillinger fbshipit-source-id: f80c96893a09bf1152630ff0b964e5cdd7e35c68/Allow fractional bits/key in BloomFilterPolicy (#6092) Summary: Theres no technological impediment to allowing the Bloom filter bits/key to be non-integer (fractional/decimal) values, and it provides finer control over the memory vs. accuracy trade-off. This is especially handy in using the format_version=5 Bloom filter in place of the old one, because bits_per_key=9.55 provides the same accuracy as the old bits_per_key=10. This change not only requires refining the logic for choosing the best num_probes for a given bits/key setting, it revealed a flaw in that logic. As bits/key gets higher, the best num_probes for a cache-local Bloom filter is closer to bpk / 2 than to bpk * 0.69, the best choice for a standard Bloom filter. For example, at 16 bits per key, the best num_probes is 9 (FP rate 0.0843%) not 11 (FP rate 0.0884%). This change fixes and refines that logic (for the format_version=5 Bloom filter only, just in case) based on empirical tests to find accuracy inflection points between each num_probes. Although bits_per_key is now specified as a double, the new Bloom filter converts/rounds this to ""millibits / key"" for predictable/precise internal computations. Just in case of unforeseen compatibility issues, we round to the nearest whole number bits / key for the legacy Bloom filter, so as not to unlock new behaviors for it. Pull Request resolved: Test Plan: unit tests included Differential Revision: D18711313 Pulled By: pdillinger fbshipit-source-id: 1aa73295f152a995328cb846ef9157ae8a05522a/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.7779,rocksdb,"fix build warnnings on MSVC (#6309) Summary: Fix build warnings on MSVC. siying Pull Request resolved: Differential Revision: D19455012 Pulled By: ltamasi fbshipit-source-id: 940739f2c92de60e47cc2bed8dd7f921459545a9/Warn on excessive keys for legacy Bloom filter with 32-bit hash (#6317) Summary: With many millions of keys, the old Bloom filter implementation for the block-based table (format_version 4) would have excessive FP rate due to the limitations of feeding the Bloom filter with a 32-bit hash. This change computes an estimated inflated FP rate due to this effect and warns in the log whenever an SST filter is constructed (almost certainly a ""full"" not ""partitioned"" filter) that exceeds 1.5x FP rate due to this effect. The detailed condition is only checked if 3 million keys or more have been added to a filter, as this should be a lower bound for common bits/key settings (< 20). Recommended remedies include smaller SST file size, using format_version >= 5 (for new Bloom filter), or using partitioned filters. This does not change behavior other than generating warnings for some constructed filters using the old implementation. Pull Request resolved: Test Plan: Example with warning, 15M keys 15 bits / key: (working_mem_size_mb is just to stop after building one filter if its large) $ ./filter_bench 2>&1 | grep FP rate [WARN] [/block_based/filter_policy.cc:292] Using legacy SST/BBT Bloom filter with excessive key count (15.0M 15bpk), causing estimated 1.8x higher filter FP rate. Consider using new Bloom with format_version>=5, smaller SST file size, or partitioned filters. Predicted FP rate %: 0.766702 Average FP rate %: 0.66846 Example without warning (150K keys): $ ./filter_bench 2>&1 | grep FP rate Predicted FP rate %: 0.422857 Average FP rate %: 0.379301 $ With more samples at 15 bits/key: 150K keys no warning; actual: 0.379% FP rate (baseline) 1M keys no warning; actual: 0.396% FP rate, 1.045x 9M keys no warning; actual: 0.563% FP rate, 1.485x 10M keys warning (1.5x); actual: 0.564% FP rate, 1.488x 15M keys warning (1.8x); actual: 0.668% FP rate, 1.76x 25M keys warning (2.4x); actual: 0.880% FP rate, 2.32x At 10 bits/key: 150K keys no warning; actual: 1.17% FP rate (baseline) 1M keys no warning; actual: 1.16% FP rate 10M keys no warning; actual: 1.32% FP rate, 1.13x 25M keys no warning; actual: 1.63% FP rate, 1.39x 35M keys warning (1.6x); actual: 1.81% FP rate, 1.55x At 5 bits/key: 150K keys no warning; actual: 9.32% FP rate (baseline) 25M keys no warning; actual: 9.62% FP rate, 1.03x 200M keys no warning; actual: 12.2% FP rate, 1.31x 250M keys warning (1.5x); actual: 12.8% FP rate, 1.37x 300M keys warning (1.6x); actual: 13.4% FP rate, 1.43x The reason for the modest inaccuracy at low bits/key is that the assumption of independence between a collision between 32-hash values feeding the filter and an FP in the filter is not quite true for implementations using ""simple"" logic to compute indices from the stock hash result. Theres math on this in my dissertation, but I dont think its worth the effort just for these extreme cases (> 100 million keys and low-ish bits/key). Differential Revision: D19471715 Pulled By: pdillinger fbshipit-source-id: f80c96893a09bf1152630ff0b964e5cdd7e35c68/Optimize memory and CPU for building new Bloom filter (#6175) Summary: The filter bits builder collects all the hashes to add in memory before adding them (because the number of keys is not known until weve walked over all the keys). Existing code uses a std::vector for this, which can mean up to 2x than necessary space allocated (and not freed) and up to ~2x write amplification in memory. Using std::deque uses close to minimal space (for large filters, the only time it matters), no write amplification, frees memory while building, and no need for large contiguous memory area. The only cost is more calls to allocator, which does not appear to matter, at least in benchmark test. For now, this change only applies to the new (format_version=5) Bloom filter implementation, to ease before-and-after comparison downstream. Temporary memory use during build is about the only way the new Bloom filter could regress vs. the old (because of upgrade to 64-bit hash) and that should only matter for full filters. This change should largely mitigate that potential regression. Pull Request resolved: Test Plan: Using filter_bench with option and 6M keys per filter is like large full filter (improvement). 10k keys and no is like partitioned filters (about the same). (Corresponding configurations run simultaneously on devserver.) std::vector impl (before) $ /usr/bin/time ./filter_bench average_keys_per_filter=6000000 Build avg ns/key: 52.2027 Maximum resident set size (kbytes): 1105016 $ /usr/bin/time ./filter_bench average_keys_per_filter=10000 Build avg ns/key: 30.5694 Maximum resident set size (kbytes): 1208152 std::deque impl (after) $ /usr/bin/time ./filter_bench average_keys_per_filter=6000000 Build avg ns/key: 39.0697 Maximum resident set size (kbytes): 1087196 $ /usr/bin/time ./filter_bench average_keys_per_filter=10000 Build avg ns/key: 30.9348 Maximum resident set size (kbytes): 1207980 Differential Revision: D19053431 Pulled By: pdillinger fbshipit-source-id: 2888e748723a19d9ea40403934f13cbb8483430c/Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/Allow fractional bits/key in BloomFilterPolicy (#6092) Summary: Theres no technological impediment to allowing the Bloom filter bits/key to be non-integer (fractional/decimal) values, and it provides finer control over the memory vs. accuracy trade-off. This is especially handy in using the format_version=5 Bloom filter in place of the old one, because bits_per_key=9.55 provides the same accuracy as the old bits_per_key=10. This change not only requires refining the logic for choosing the best num_probes for a given bits/key setting, it revealed a flaw in that logic. As bits/key gets higher, the best num_probes for a cache-local Bloom filter is closer to bpk / 2 than to bpk * 0.69, the best choice for a standard Bloom filter. For example, at 16 bits per key, the best num_probes is 9 (FP rate 0.0843%) not 11 (FP rate 0.0884%). This change fixes and refines that logic (for the format_version=5 Bloom filter only, just in case) based on empirical tests to find accuracy inflection points between each num_probes. Although bits_per_key is now specified as a double, the new Bloom filter converts/rounds this to ""millibits / key"" for predictable/precise internal computations. Just in case of unforeseen compatibility issues, we round to the nearest whole number bits / key for the legacy Bloom filter, so as not to unlock new behaviors for it. Pull Request resolved: Test Plan: unit tests included Differential Revision: D18711313 Pulled By: pdillinger fbshipit-source-id: 1aa73295f152a995328cb846ef9157ae8a05522a/Fixes for g++ 4.9.2 compatibility (#6053) Summary: Taken from merryChris in Stackoverflow ref on {{}} vs. {}: Note to reader: .clear() does not empty out an ostringstream, but .str("""") suffices because we dont have to worry about clearing error flags. Pull Request resolved: Test Plan: make check, manual run of filter_bench Differential Revision: D18602259 Pulled By: pdillinger fbshipit-source-id: f6190f83b8eab4e80e7c107348839edabe727841/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.7927,rocksdb,"Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/Allow fractional bits/key in BloomFilterPolicy (#6092) Summary: Theres no technological impediment to allowing the Bloom filter bits/key to be non-integer (fractional/decimal) values, and it provides finer control over the memory vs. accuracy trade-off. This is especially handy in using the format_version=5 Bloom filter in place of the old one, because bits_per_key=9.55 provides the same accuracy as the old bits_per_key=10. This change not only requires refining the logic for choosing the best num_probes for a given bits/key setting, it revealed a flaw in that logic. As bits/key gets higher, the best num_probes for a cache-local Bloom filter is closer to bpk / 2 than to bpk * 0.69, the best choice for a standard Bloom filter. For example, at 16 bits per key, the best num_probes is 9 (FP rate 0.0843%) not 11 (FP rate 0.0884%). This change fixes and refines that logic (for the format_version=5 Bloom filter only, just in case) based on empirical tests to find accuracy inflection points between each num_probes. Although bits_per_key is now specified as a double, the new Bloom filter converts/rounds this to ""millibits / key"" for predictable/precise internal computations. Just in case of unforeseen compatibility issues, we round to the nearest whole number bits / key for the legacy Bloom filter, so as not to unlock new behaviors for it. Pull Request resolved: Test Plan: unit tests included Differential Revision: D18711313 Pulled By: pdillinger fbshipit-source-id: 1aa73295f152a995328cb846ef9157ae8a05522a/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.7716,rocksdb,"New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.4312,rocksdb,"Allow fractional bits/key in BloomFilterPolicy (#6092) Summary: Theres no technological impediment to allowing the Bloom filter bits/key to be non-integer (fractional/decimal) values, and it provides finer control over the memory vs. accuracy trade-off. This is especially handy in using the format_version=5 Bloom filter in place of the old one, because bits_per_key=9.55 provides the same accuracy as the old bits_per_key=10. This change not only requires refining the logic for choosing the best num_probes for a given bits/key setting, it revealed a flaw in that logic. As bits/key gets higher, the best num_probes for a cache-local Bloom filter is closer to bpk / 2 than to bpk * 0.69, the best choice for a standard Bloom filter. For example, at 16 bits per key, the best num_probes is 9 (FP rate 0.0843%) not 11 (FP rate 0.0884%). This change fixes and refines that logic (for the format_version=5 Bloom filter only, just in case) based on empirical tests to find accuracy inflection points between each num_probes. Although bits_per_key is now specified as a double, the new Bloom filter converts/rounds this to ""millibits / key"" for predictable/precise internal computations. Just in case of unforeseen compatibility issues, we round to the nearest whole number bits / key for the legacy Bloom filter, so as not to unlock new behaviors for it. Pull Request resolved: Test Plan: unit tests included Differential Revision: D18711313 Pulled By: pdillinger fbshipit-source-id: 1aa73295f152a995328cb846ef9157ae8a05522a/"
,,0.8143,rocksdb,"Warn on excessive keys for legacy Bloom filter with 32-bit hash (#6317) Summary: With many millions of keys, the old Bloom filter implementation for the block-based table (format_version 4) would have excessive FP rate due to the limitations of feeding the Bloom filter with a 32-bit hash. This change computes an estimated inflated FP rate due to this effect and warns in the log whenever an SST filter is constructed (almost certainly a ""full"" not ""partitioned"" filter) that exceeds 1.5x FP rate due to this effect. The detailed condition is only checked if 3 million keys or more have been added to a filter, as this should be a lower bound for common bits/key settings (< 20). Recommended remedies include smaller SST file size, using format_version >= 5 (for new Bloom filter), or using partitioned filters. This does not change behavior other than generating warnings for some constructed filters using the old implementation. Pull Request resolved: Test Plan: Example with warning, 15M keys 15 bits / key: (working_mem_size_mb is just to stop after building one filter if its large) $ ./filter_bench 2>&1 | grep FP rate [WARN] [/block_based/filter_policy.cc:292] Using legacy SST/BBT Bloom filter with excessive key count (15.0M 15bpk), causing estimated 1.8x higher filter FP rate. Consider using new Bloom with format_version>=5, smaller SST file size, or partitioned filters. Predicted FP rate %: 0.766702 Average FP rate %: 0.66846 Example without warning (150K keys): $ ./filter_bench 2>&1 | grep FP rate Predicted FP rate %: 0.422857 Average FP rate %: 0.379301 $ With more samples at 15 bits/key: 150K keys no warning; actual: 0.379% FP rate (baseline) 1M keys no warning; actual: 0.396% FP rate, 1.045x 9M keys no warning; actual: 0.563% FP rate, 1.485x 10M keys warning (1.5x); actual: 0.564% FP rate, 1.488x 15M keys warning (1.8x); actual: 0.668% FP rate, 1.76x 25M keys warning (2.4x); actual: 0.880% FP rate, 2.32x At 10 bits/key: 150K keys no warning; actual: 1.17% FP rate (baseline) 1M keys no warning; actual: 1.16% FP rate 10M keys no warning; actual: 1.32% FP rate, 1.13x 25M keys no warning; actual: 1.63% FP rate, 1.39x 35M keys warning (1.6x); actual: 1.81% FP rate, 1.55x At 5 bits/key: 150K keys no warning; actual: 9.32% FP rate (baseline) 25M keys no warning; actual: 9.62% FP rate, 1.03x 200M keys no warning; actual: 12.2% FP rate, 1.31x 250M keys warning (1.5x); actual: 12.8% FP rate, 1.37x 300M keys warning (1.6x); actual: 13.4% FP rate, 1.43x The reason for the modest inaccuracy at low bits/key is that the assumption of independence between a collision between 32-hash values feeding the filter and an FP in the filter is not quite true for implementations using ""simple"" logic to compute indices from the stock hash result. Theres math on this in my dissertation, but I dont think its worth the effort just for these extreme cases (> 100 million keys and low-ish bits/key). Differential Revision: D19471715 Pulled By: pdillinger fbshipit-source-id: f80c96893a09bf1152630ff0b964e5cdd7e35c68/Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/Allow fractional bits/key in BloomFilterPolicy (#6092) Summary: Theres no technological impediment to allowing the Bloom filter bits/key to be non-integer (fractional/decimal) values, and it provides finer control over the memory vs. accuracy trade-off. This is especially handy in using the format_version=5 Bloom filter in place of the old one, because bits_per_key=9.55 provides the same accuracy as the old bits_per_key=10. This change not only requires refining the logic for choosing the best num_probes for a given bits/key setting, it revealed a flaw in that logic. As bits/key gets higher, the best num_probes for a cache-local Bloom filter is closer to bpk / 2 than to bpk * 0.69, the best choice for a standard Bloom filter. For example, at 16 bits per key, the best num_probes is 9 (FP rate 0.0843%) not 11 (FP rate 0.0884%). This change fixes and refines that logic (for the format_version=5 Bloom filter only, just in case) based on empirical tests to find accuracy inflection points between each num_probes. Although bits_per_key is now specified as a double, the new Bloom filter converts/rounds this to ""millibits / key"" for predictable/precise internal computations. Just in case of unforeseen compatibility issues, we round to the nearest whole number bits / key for the legacy Bloom filter, so as not to unlock new behaviors for it. Pull Request resolved: Test Plan: unit tests included Differential Revision: D18711313 Pulled By: pdillinger fbshipit-source-id: 1aa73295f152a995328cb846ef9157ae8a05522a/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.3108,rocksdb,"Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/"
,,0.7023,rocksdb,"Fix wrong ExtractUserKey usage in BlockBasedTableBuilder::EnterUnbuffÖ (#6100) Summary: BlockBasedTableBuilder uses ExtractUserKey in EnterUnbuffered. This would cause index filter building error, since user-provided timestamp is supported by ExtractUserKeyAndStripTimestamp, and its used in Add. This commit changes ExtractUserKey to ExtractUserKeyAndStripTimestamp. A test case is also added by modifying DBBasicTestWithTimestampWithParam_ PutAndGet test in db_basic_test to cover ExtractUserKeyAndStripTimestamp usage in both kBuffered and kUnbuffered state of BlockBasedTableBuilder. Before the ExtractUserKeyAndStripTimstamp fix: ``` $ ./db_basic_test Note: Google Test filter *PutAndGet* [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from Timestamp/DBBasicTestWithTimestampWithParam [ RUN ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/0 db/db_basic_test.cc:2109: Failure db_->Get(ropts, cfh, ""key"" + std::to_string(j), &value) NotFound: db/db_basic_test.cc:2109: Failure db_->Get(ropts, cfh, ""key"" + std::to_string(j), &value) NotFound: db/db_basic_test.cc:2109: Failure db_->Get(ropts, cfh, ""key"" + std::to_string(j), &value) NotFound: db/db_basic_test.cc:2109: Failure db_->Get(ropts, cfh, ""key"" + std::to_string(j), &value) NotFound: db/db_basic_test.cc:2109: Failure db_->Get(ropts, cfh, ""key"" + std::to_string(j), &value) NotFound: [ FAILED ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/0, where GetParam() false (1177 ms) [ RUN ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/1 [ OK ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/1 (1056 ms) [----------] 2 tests from Timestamp/DBBasicTestWithTimestampWithParam (2233 ms total) [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. (2233 ms total) [ PASSED ] 1 test. [ FAILED ] 1 test, listed below: [ FAILED ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/0, where GetParam() false 1 FAILED TEST ``` After the ExtractUserKeyAndStripTimstamp fix: ``` $ ./db_basic_test Note: Google Test filter *PutAndGet* [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from Timestamp/DBBasicTestWithTimestampWithParam [ RUN ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/0 [ OK ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/0 (1417 ms) [ RUN ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/1 [ OK ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/1 (1041 ms) [----------] 2 tests from Timestamp/DBBasicTestWithTimestampWithParam (2458 ms total) [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. (2458 ms total) [ PASSED ] 2 tests. ``` Pull Request resolved: Differential Revision: D18769654 Pulled By: riversand963 fbshipit-source-id: 76c2cf2c9a5e0d85db95d98e812e6af0c2a15c6b/Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.7743,rocksdb,"Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.7694,rocksdb,"New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.7802,rocksdb,"Warn on excessive keys for legacy Bloom filter with 32-bit hash (#6317) Summary: With many millions of keys, the old Bloom filter implementation for the block-based table (format_version 4) would have excessive FP rate due to the limitations of feeding the Bloom filter with a 32-bit hash. This change computes an estimated inflated FP rate due to this effect and warns in the log whenever an SST filter is constructed (almost certainly a ""full"" not ""partitioned"" filter) that exceeds 1.5x FP rate due to this effect. The detailed condition is only checked if 3 million keys or more have been added to a filter, as this should be a lower bound for common bits/key settings (< 20). Recommended remedies include smaller SST file size, using format_version >= 5 (for new Bloom filter), or using partitioned filters. This does not change behavior other than generating warnings for some constructed filters using the old implementation. Pull Request resolved: Test Plan: Example with warning, 15M keys 15 bits / key: (working_mem_size_mb is just to stop after building one filter if its large) $ ./filter_bench 2>&1 | grep FP rate [WARN] [/block_based/filter_policy.cc:292] Using legacy SST/BBT Bloom filter with excessive key count (15.0M 15bpk), causing estimated 1.8x higher filter FP rate. Consider using new Bloom with format_version>=5, smaller SST file size, or partitioned filters. Predicted FP rate %: 0.766702 Average FP rate %: 0.66846 Example without warning (150K keys): $ ./filter_bench 2>&1 | grep FP rate Predicted FP rate %: 0.422857 Average FP rate %: 0.379301 $ With more samples at 15 bits/key: 150K keys no warning; actual: 0.379% FP rate (baseline) 1M keys no warning; actual: 0.396% FP rate, 1.045x 9M keys no warning; actual: 0.563% FP rate, 1.485x 10M keys warning (1.5x); actual: 0.564% FP rate, 1.488x 15M keys warning (1.8x); actual: 0.668% FP rate, 1.76x 25M keys warning (2.4x); actual: 0.880% FP rate, 2.32x At 10 bits/key: 150K keys no warning; actual: 1.17% FP rate (baseline) 1M keys no warning; actual: 1.16% FP rate 10M keys no warning; actual: 1.32% FP rate, 1.13x 25M keys no warning; actual: 1.63% FP rate, 1.39x 35M keys warning (1.6x); actual: 1.81% FP rate, 1.55x At 5 bits/key: 150K keys no warning; actual: 9.32% FP rate (baseline) 25M keys no warning; actual: 9.62% FP rate, 1.03x 200M keys no warning; actual: 12.2% FP rate, 1.31x 250M keys warning (1.5x); actual: 12.8% FP rate, 1.37x 300M keys warning (1.6x); actual: 13.4% FP rate, 1.43x The reason for the modest inaccuracy at low bits/key is that the assumption of independence between a collision between 32-hash values feeding the filter and an FP in the filter is not quite true for implementations using ""simple"" logic to compute indices from the stock hash result. Theres math on this in my dissertation, but I dont think its worth the effort just for these extreme cases (> 100 million keys and low-ish bits/key). Differential Revision: D19471715 Pulled By: pdillinger fbshipit-source-id: f80c96893a09bf1152630ff0b964e5cdd7e35c68/Optimize memory and CPU for building new Bloom filter (#6175) Summary: The filter bits builder collects all the hashes to add in memory before adding them (because the number of keys is not known until weve walked over all the keys). Existing code uses a std::vector for this, which can mean up to 2x than necessary space allocated (and not freed) and up to ~2x write amplification in memory. Using std::deque uses close to minimal space (for large filters, the only time it matters), no write amplification, frees memory while building, and no need for large contiguous memory area. The only cost is more calls to allocator, which does not appear to matter, at least in benchmark test. For now, this change only applies to the new (format_version=5) Bloom filter implementation, to ease before-and-after comparison downstream. Temporary memory use during build is about the only way the new Bloom filter could regress vs. the old (because of upgrade to 64-bit hash) and that should only matter for full filters. This change should largely mitigate that potential regression. Pull Request resolved: Test Plan: Using filter_bench with option and 6M keys per filter is like large full filter (improvement). 10k keys and no is like partitioned filters (about the same). (Corresponding configurations run simultaneously on devserver.) std::vector impl (before) $ /usr/bin/time ./filter_bench average_keys_per_filter=6000000 Build avg ns/key: 52.2027 Maximum resident set size (kbytes): 1105016 $ /usr/bin/time ./filter_bench average_keys_per_filter=10000 Build avg ns/key: 30.5694 Maximum resident set size (kbytes): 1208152 std::deque impl (after) $ /usr/bin/time ./filter_bench average_keys_per_filter=6000000 Build avg ns/key: 39.0697 Maximum resident set size (kbytes): 1087196 $ /usr/bin/time ./filter_bench average_keys_per_filter=10000 Build avg ns/key: 30.9348 Maximum resident set size (kbytes): 1207980 Differential Revision: D19053431 Pulled By: pdillinger fbshipit-source-id: 2888e748723a19d9ea40403934f13cbb8483430c/Fix & test rocksdb_filterpolicy_create_bloom_full (#6132) Summary: Add overrides needed in FilterPolicy wrapper to fix rocksdb_filterpolicy_create_bloom_full (see issue Re-enabled assertion in BloomFilterPolicy::CreateFilter that was being violated. Expanded c_test to identify Bloom filter implementations by FP counts. (Without the fix, updated test will trigger assertion and fail otherwise without the assertion.) Fixes Pull Request resolved: Test Plan: updated c_test, also run under valgrind. Differential Revision: D18864911 Pulled By: pdillinger fbshipit-source-id: 08e81d7b5368b08e501cd402ef5583f2650c19fa/Disable new Bloom filter assertion (#6128) Summary: A longstanding bug in our C interface can trigger this assertion; see issue Disabling the assertion for now (for 6.6.0) and will re-enable on fix of that bug. Pull Request resolved: Differential Revision: D18854899 Pulled By: pdillinger fbshipit-source-id: 9eb5294b9f11b208dc1a8cc148aaa31e47ff892b/Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/Allow fractional bits/key in BloomFilterPolicy (#6092) Summary: Theres no technological impediment to allowing the Bloom filter bits/key to be non-integer (fractional/decimal) values, and it provides finer control over the memory vs. accuracy trade-off. This is especially handy in using the format_version=5 Bloom filter in place of the old one, because bits_per_key=9.55 provides the same accuracy as the old bits_per_key=10. This change not only requires refining the logic for choosing the best num_probes for a given bits/key setting, it revealed a flaw in that logic. As bits/key gets higher, the best num_probes for a cache-local Bloom filter is closer to bpk / 2 than to bpk * 0.69, the best choice for a standard Bloom filter. For example, at 16 bits per key, the best num_probes is 9 (FP rate 0.0843%) not 11 (FP rate 0.0884%). This change fixes and refines that logic (for the format_version=5 Bloom filter only, just in case) based on empirical tests to find accuracy inflection points between each num_probes. Although bits_per_key is now specified as a double, the new Bloom filter converts/rounds this to ""millibits / key"" for predictable/precise internal computations. Just in case of unforeseen compatibility issues, we round to the nearest whole number bits / key for the legacy Bloom filter, so as not to unlock new behaviors for it. Pull Request resolved: Test Plan: unit tests included Differential Revision: D18711313 Pulled By: pdillinger fbshipit-source-id: 1aa73295f152a995328cb846ef9157ae8a05522a/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/Fix BloomFilterPolicy changes for unsigned char (ARM) (#6024) Summary: Bug in PR when char is unsigned that should only affect assertion on unused/invalid filter metadata. Pull Request resolved: Test Plan: on ARM: ./bloom_test && ./db_bloom_filter_test && ./block_based_filter_block_test && ./full_filter_block_test && ./partitioned_filter_block_test Differential Revision: D18461206 Pulled By: pdillinger fbshipit-source-id: 68a7c813a0b5791c05265edc03cdf52c78880e9a/"
,,0.28300000000000003,rocksdb,"Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/"
,,0.4236,rocksdb,"Revert ""Add kHashSearch to stress tests (#6210)"" (#6220) Summary: This reverts commit 54f9092b0c12d99971f340e180d42fffc9f73bc1. It making our daily stress tests fail. Revert it until the issues are fixed. Pull Request resolved: Differential Revision: D19179881 Pulled By: maysamyabandeh fbshipit-source-id: 99de0eaf776567fa81110b9ad2608234a16083ce/Add kHashSearch to stress tests (#6210) Summary: Beside extending index_type to kHashSearch, it clarifies in the code base that this feature is incompatible with index_block_restart_interval > 1. Pull Request resolved: Test Plan: ``` make crash_test Differential Revision: D19166567 Pulled By: maysamyabandeh fbshipit-source-id: 3aaf75a70a8b462d372d43aac69dbd10df303ec7/Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/Allow fractional bits/key in BloomFilterPolicy (#6092) Summary: Theres no technological impediment to allowing the Bloom filter bits/key to be non-integer (fractional/decimal) values, and it provides finer control over the memory vs. accuracy trade-off. This is especially handy in using the format_version=5 Bloom filter in place of the old one, because bits_per_key=9.55 provides the same accuracy as the old bits_per_key=10. This change not only requires refining the logic for choosing the best num_probes for a given bits/key setting, it revealed a flaw in that logic. As bits/key gets higher, the best num_probes for a cache-local Bloom filter is closer to bpk / 2 than to bpk * 0.69, the best choice for a standard Bloom filter. For example, at 16 bits per key, the best num_probes is 9 (FP rate 0.0843%) not 11 (FP rate 0.0884%). This change fixes and refines that logic (for the format_version=5 Bloom filter only, just in case) based on empirical tests to find accuracy inflection points between each num_probes. Although bits_per_key is now specified as a double, the new Bloom filter converts/rounds this to ""millibits / key"" for predictable/precise internal computations. Just in case of unforeseen compatibility issues, we round to the nearest whole number bits / key for the legacy Bloom filter, so as not to unlock new behaviors for it. Pull Request resolved: Test Plan: unit tests included Differential Revision: D18711313 Pulled By: pdillinger fbshipit-source-id: 1aa73295f152a995328cb846ef9157ae8a05522a/"
,,0.7696,rocksdb,"Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.7331,rocksdb,"New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/Fix a buffer overrun problem in BlockBasedTable::MultiGet (#6014) Summary: The calculation in BlockBasedTable::MultiGet for the required buffer length for reading in compressed blocks is incorrect. It needs to take the 5-byte block trailer into account. Pull Request resolved: Test Plan: Add a unit test DBBasicTest.MultiGetBufferOverrun that fails in asan_check before the fix, and passes after. Differential Revision: D18412753 Pulled By: anand1976 fbshipit-source-id: 754dfb66be1d5f161a7efdf87be872198c7e3b72/"
,,0.1928,rocksdb,"Check KeyContext status in MultiGet (#6387) Summary: Currently, any IO errors and checksum mismatches while reading data blocks, are being ignored by the batched MultiGet. Its only looking at the GetContext state. Fix that. Pull Request resolved: Test Plan: Add unit tests Differential Revision: D19799819 Pulled By: anand1976 fbshipit-source-id: 46133dccbb04e64067b9fe6cda73e282203db969/Fix compilation under LITE (#6277) Summary: Fix compilation under LITE by putting `#ifndef ROCKSDB_LITE` around a code block. Pull Request resolved: Differential Revision: D19334157 Pulled By: riversand963 fbshipit-source-id: 947111ed68aa550f5ea424b216c1442a8af9e32b/Update file indexer to take timestamp into consideration (#6205) Summary: Exclude timestamp in key comparison during boundary calculation to avoid key versions being excluded. Pull Request resolved: Differential Revision: D19166765 Pulled By: riversand963 fbshipit-source-id: bbe08816fef8de349a83ebd59a595ad844021f24/Fix wrong ExtractUserKey usage in BlockBasedTableBuilder::EnterUnbuffÖ (#6100) Summary: BlockBasedTableBuilder uses ExtractUserKey in EnterUnbuffered. This would cause index filter building error, since user-provided timestamp is supported by ExtractUserKeyAndStripTimestamp, and its used in Add. This commit changes ExtractUserKey to ExtractUserKeyAndStripTimestamp. A test case is also added by modifying DBBasicTestWithTimestampWithParam_ PutAndGet test in db_basic_test to cover ExtractUserKeyAndStripTimestamp usage in both kBuffered and kUnbuffered state of BlockBasedTableBuilder. Before the ExtractUserKeyAndStripTimstamp fix: ``` $ ./db_basic_test Note: Google Test filter *PutAndGet* [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from Timestamp/DBBasicTestWithTimestampWithParam [ RUN ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/0 db/db_basic_test.cc:2109: Failure db_->Get(ropts, cfh, ""key"" + std::to_string(j), &value) NotFound: db/db_basic_test.cc:2109: Failure db_->Get(ropts, cfh, ""key"" + std::to_string(j), &value) NotFound: db/db_basic_test.cc:2109: Failure db_->Get(ropts, cfh, ""key"" + std::to_string(j), &value) NotFound: db/db_basic_test.cc:2109: Failure db_->Get(ropts, cfh, ""key"" + std::to_string(j), &value) NotFound: db/db_basic_test.cc:2109: Failure db_->Get(ropts, cfh, ""key"" + std::to_string(j), &value) NotFound: [ FAILED ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/0, where GetParam() false (1177 ms) [ RUN ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/1 [ OK ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/1 (1056 ms) [----------] 2 tests from Timestamp/DBBasicTestWithTimestampWithParam (2233 ms total) [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. (2233 ms total) [ PASSED ] 1 test. [ FAILED ] 1 test, listed below: [ FAILED ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/0, where GetParam() false 1 FAILED TEST ``` After the ExtractUserKeyAndStripTimstamp fix: ``` $ ./db_basic_test Note: Google Test filter *PutAndGet* [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from Timestamp/DBBasicTestWithTimestampWithParam [ RUN ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/0 [ OK ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/0 (1417 ms) [ RUN ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/1 [ OK ] Timestamp/DBBasicTestWithTimestampWithParam.PutAndGet/1 (1041 ms) [----------] 2 tests from Timestamp/DBBasicTestWithTimestampWithParam (2458 ms total) [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. (2458 ms total) [ PASSED ] 2 tests. ``` Pull Request resolved: Differential Revision: D18769654 Pulled By: riversand963 fbshipit-source-id: 76c2cf2c9a5e0d85db95d98e812e6af0c2a15c6b/Fix test failure in LITE mode (#6050) Summary: GetSupportedCompressions() is not available in LITE build, so check and use Snappy compression in db_basic_test.cc. Pull Request resolved: Test Plan: make LITE=1 check make check Differential Revision: D18588114 Pulled By: anand1976 fbshipit-source-id: a193de58c44f91bcc237107f25dbc1b9458eef3d/Fix a test failure on systems that dont have Snappy compression libraries (#6038) Summary: The ParallelIO/DBBasicTestWithParallelIO.MultiGet/11 test fails if Snappy compression library is not installed, since RocksDB defaults to Snappy if none is specified. So dynamically determine the supported compression types and pick the first one. Pull Request resolved: Differential Revision: D18532370 Pulled By: anand1976 fbshipit-source-id: a0a735114d1f8892ea09f7c4af8688d7bcc5b075/Batched MultiGet API for multiple column families (#5816) Summary: Add a new API that allows a user to call MultiGet specifying multiple keys belonging to different column families. This is mainly useful for users who want to do a consistent read of keys across column families, with the added performance benefits of batching and returning values using PinnableSlice. As part of this change, the code in the original multi-column family MultiGet for acquiring the super versions has been refactored into a separate function that can be used by both, the batching and the non-batching versions of MultiGet. Pull Request resolved: Test Plan: make check make asan_check asan_crash_test Differential Revision: D18408676 Pulled By: anand1976 fbshipit-source-id: 933e7bec91dd70e7b633be4ff623a1116cc28c8d/Fix a buffer overrun problem in BlockBasedTable::MultiGet (#6014) Summary: The calculation in BlockBasedTable::MultiGet for the required buffer length for reading in compressed blocks is incorrect. It needs to take the 5-byte block trailer into account. Pull Request resolved: Test Plan: Add a unit test DBBasicTest.MultiGetBufferOverrun that fails in asan_check before the fix, and passes after. Differential Revision: D18412753 Pulled By: anand1976 fbshipit-source-id: 754dfb66be1d5f161a7efdf87be872198c7e3b72/"
,,0.7343,rocksdb,"Fix db_bloom_filter_test clang LITE build (#6340) Summary: db_bloom_filter_test break with clang LITE build with following message: db/db_bloom_filter_test.cc:23:29: error: unused variable kPlainTable [-Werror,-Wunused-const-variable] static constexpr PseudoMode kPlainTable ^ Fix it by moving the declaration out of LITE build Pull Request resolved: Test Plan: USE_CLANG=1 LITE=1 make db_bloom_filter_test and without LITE=1 Differential Revision: D19609834 fbshipit-source-id: 0e88f5c6759238a94f9880d84c785ac18e7cdd7e/Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/Abandon use of folly::Optional (#6036) Summary: Had complications with LITE build and valgrind test. Reverts/fixes small parts of PR Pull Request resolved: Test Plan: make LITE=1 all check and ROCKSDB_VALGRIND_RUN=1 DISABLE_JEMALLOC=1 make db_bloom_filter_test && ROCKSDB_VALGRIND_RUN=1 DISABLE_JEMALLOC=1 ./db_bloom_filter_test Differential Revision: D18512238 Pulled By: pdillinger fbshipit-source-id: 37213cf0d309edf11c483fb4b2fb6c02c2cf2b28/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
,,0.25,rocksdb,"Expose and elaborate FilterBuildingContext (#6088) Summary: This change enables custom implementations of FilterPolicy to wrap a variety of NewBloomFilterPolicy and select among them based on contextual information such as table level and compaction style. * Moves FilterBuildingContext to public API and elaborates it with more useful data. (It would be nice to put more general options-like data, but at the time this object is constructed, we are using internal APIs ImmutableCFOptions and MutableCFOptions and dont have easy access to ColumnFamilyOptions that I can tell.) * Renames BloomFilterPolicy::GetFilterBitsBuilderInternal to GetBuilderWithContext, because its now public. * Plumbs through the tables ""level_at_creation"" for filter building context. * Simplified some tests by adding GetBuilder() to MockBlockBasedTableTester. * Adds test as DBBloomFilterTest.ContextCustomFilterPolicy, including sample wrapper class LevelAndStyleCustomFilterPolicy. * Fixes a cross-test bug in DBBloomFilterTest.OptimizeFiltersForHits where it does not reset perf context. Pull Request resolved: Test Plan: make check, valgrind on db_bloom_filter_test Differential Revision: D18697817 Pulled By: pdillinger fbshipit-source-id: 5f987a2d7b07cc7a33670bc08ca6b4ca698c1cf4/Batched MultiGet API for multiple column families (#5816) Summary: Add a new API that allows a user to call MultiGet specifying multiple keys belonging to different column families. This is mainly useful for users who want to do a consistent read of keys across column families, with the added performance benefits of batching and returning values using PinnableSlice. As part of this change, the code in the original multi-column family MultiGet for acquiring the super versions has been refactored into a separate function that can be used by both, the batching and the non-batching versions of MultiGet. Pull Request resolved: Test Plan: make check make asan_check asan_crash_test Differential Revision: D18408676 Pulled By: anand1976 fbshipit-source-id: 933e7bec91dd70e7b633be4ff623a1116cc28c8d/"
,,0.1338,rocksdb,"Fix error message (#6264) Summary: Fix an error message when CURRENT is not found. Test plan (dev server) ``` make check ``` Pull Request resolved: Differential Revision: D19300699 Pulled By: riversand963 fbshipit-source-id: 303fa206386a125960ecca1dbdeff07422690caf/Fix c_test:filter for various CACHE_LINE_SIZEs (#6153) Summary: This test was recently updated but failed to account for Bloom schema variance by CACHE_LINE_SIZE. (Since CACHE_LINE_SIZE is not defined in our C code, the test now simply allows a valid result for any CACHE_LINE_SIZE, not just the current one.) Unblock Pull Request resolved: Test Plan: ran unit test with builds TEST_CACHE_LINE_SIZE=128, and unset (64 on Intel) Differential Revision: D18936015 Pulled By: pdillinger fbshipit-source-id: e5e3852f95283d34d624632c1ae8d3adb2f2662c/Fix & test rocksdb_filterpolicy_create_bloom_full (#6132) Summary: Add overrides needed in FilterPolicy wrapper to fix rocksdb_filterpolicy_create_bloom_full (see issue Re-enabled assertion in BloomFilterPolicy::CreateFilter that was being violated. Expanded c_test to identify Bloom filter implementations by FP counts. (Without the fix, updated test will trigger assertion and fail otherwise without the assertion.) Fixes Pull Request resolved: Test Plan: updated c_test, also run under valgrind. Differential Revision: D18864911 Pulled By: pdillinger fbshipit-source-id: 08e81d7b5368b08e501cd402ef5583f2650c19fa/"
,,0.5031,rocksdb,"Add an option to prevent DB::Open() from querying sizes of all sst files (#6353) Summary: When paranoid_checks is on, DBImpl::CheckConsistency() iterates over all sst files and calls Env::GetFileSize() for each of them. As far as I could understand, this is pretty arbitrary and doesnt affect correctness if filesystem doesnt corrupt fsynced files, the file sizes will always match; if it does, it may as well corrupt contents as well as sizes, and rocksdb doesnt check contents on open. If there are thousands of sst files, getting all their sizes takes a while. If, on top of that, Env is overridden to use some remote storage instead of local filesystem, it can be *really* slow and overload the remote storage service. This PR adds an option to not do GetFileSize(); instead it does GetChildren() for parent directory to check that all the expected sst files are at least present, but doesnt check their sizes. We cant just disable paranoid_checks instead because paranoid_checks do a few other important things: make the DB read-only on write errors, print error messages on read errors, etc. Pull Request resolved: Test Plan: ran the added sanity check unit test. Will try it out in a LogDevice test cluster where the GetFileSize() calls are causing a lot of trouble. Differential Revision: D19656425 Pulled By: al13n321 fbshipit-source-id: c2c421b367633033760d1f56747bad206d1fbf82/unordered_write incompatible with max_successive_merges (#6284) Summary: unordered_write is incompatible with non-zero max_successive_merges. Although we check this at runtime, we currently dont prevent the user from setting this combination in options. This has led to stress tests to fail with this combination is tried in ::SetOptions. The patch fixes that and also reverts the changes performed by in which max_successive_merges was mistakenly declared incompatible with unordered_write. Pull Request resolved: Differential Revision: D19356115 Pulled By: maysamyabandeh fbshipit-source-id: f06dadec777622bd75f267361c022735cf8cecb6/Prevent an incompatible combination of options (#6254) Summary: allow_concurrent_memtable_write is incompatible with non-zero max_successive_merges. Although we check this at runtime, we currently dont prevent the user from setting this combination in options. This has led to stress tests to fail with this combination is tried in ::SetOptions. The patch fixes that. Pull Request resolved: Differential Revision: D19265819 Pulled By: maysamyabandeh fbshipit-source-id: 47f2e2dc26fe0972c7152f4da15dadb9703f1179/Introduce a new storage specific Env API (#5761) Summary: The current Env API encompasses both storage/file operations, as well as OS related operations. Most of the APIs return a Status, which does not have enough metadata about an error, such as whether its retry-able or not, scope (i.e fault domain) of the error etc., that may be required in order to properly handle a storage error. The file APIs also do not provide enough control over the IO SLA, such as timeout, prioritization, hinting about placement and redundancy etc. This PR separates out the file/storage APIs from Env into a new FileSystem class. The APIs are updated to return an IOStatus with metadata about the error, as well as to take an IOOptions structure as input in order to allow more control over the IO. The user can set both ```options.env``` and ```options.file_system``` to specify that RocksDB should use the former for OS related operations and the latter for storage operations. Internally, a ```CompositeEnvWrapper``` has been introduced that inherits from ```Env``` and redirects individual methods to either an ```Env``` implementation or the ```FileSystem``` as appropriate. When options are sanitized during ```DB::Open```, ```options.env``` is replaced with a newly allocated ```CompositeEnvWrapper``` instance if both env and file_system have been specified. This way, the rest of the RocksDB code can continue to function as before. This PR also ports PosixEnv to the new API by splitting it into two PosixEnv and PosixFileSystem. PosixEnv is defined as a sub-class of CompositeEnvWrapper, and threading/time functions are overridden with Posix specific implementations in order to avoid an extra level of indirection. The ```CompositeEnvWrapper``` translates ```IOStatus``` return code to ```Status```, and sets the severity to ```kSoftError``` if the io_status is retryable. The error handling code in RocksDB can then recover the DB automatically. Pull Request resolved: Differential Revision: D18868376 Pulled By: anand1976 fbshipit-source-id: 39efe18a162ea746fabac6360ff529baba48486f/New Bloom filter implementation for full and partitioned filters (#6007) Summary: Adds an improved, replacement Bloom filter implementation (FastLocalBloom) for full and partitioned filters in the block-based table. This replacement is faster and more accurate, especially for high bits per key or millions of keys in a single filter. Speed The improved speed, at least on recent x86_64, comes from * Using fastrange instead of modulo (%) * Using our new hash function (XXH3 preview, added in a previous commit), which is much faster for large keys and only *slightly* slower on keys around 12 bytes if hashing the same size many thousands of times in a row. * Optimizing the Bloom filter queries with AVX2 SIMD operations. (Added AVX2 to the USE_SSE=1 build.) Careful design was required to support (a) SIMD-optimized queries, (b) compatible non-SIMD code thats simple and efficient, (c) flexible choice of number of probes, and (d) essentially maximized accuracy for a cache-local Bloom filter. Probes are made eight at a time, so any number of probes up to 8 is the same speed, then up to 16, etc. * Prefetching cache lines when building the filter. Although this optimization could be applied to the old structure as well, it seems to balance out the small added cost of accumulating 64 bit hashes for adding to the filter rather than 32 bit hashes. Heres nominal speed data from filter_bench (200MB in filters, about 10k keys each, 10 bits filter data / key, 6 probes, avg key size 24 bytes, includes hashing time) on Skylake DE (relatively low clock speed): $ ./filter_bench New Bloom filter Build avg ns/key: 47.7135 Mixed inside/outside queries... Single filter net ns/op: 26.2825 Random filter net ns/op: 150.459 Average FP rate %: 0.954651 $ ./filter_bench Old Bloom filter Build avg ns/key: 47.2245 Mixed inside/outside queries... Single filter net ns/op: 63.2978 Random filter net ns/op: 188.038 Average FP rate %: 1.13823 Similar build time but dramatically faster query times on hot data (63 ns to 26 ns), and somewhat faster on stale data (188 ns to 150 ns). Performance differences on batched and skewed query loads are between these extremes as expected. The only other interesting thing about speed is ""inside"" (query key was added to filter) vs. ""outside"" (query key was not added to filter) query times. The non-SIMD implementations are substantially slower when most queries are ""outside"" vs. ""inside"". This goes against what one might expect or would have observed years ago, as ""outside"" queries only need about two probes on average, due to short-circuiting, while ""inside"" always have num_probes (say 6). The problem is probably the nastily unpredictable branch. The SIMD implementation has few branches (very predictable) and has pretty consistent running time regardless of query outcome. Accuracy The generally improved accuracy (re: Issue comes from a better design for probing indices within a cache line (re: Issue and improved accuracy for millions of keys in a single filter from using a 64-bit hash function (XXH3p). Design details in code comments. Accuracy data (generalizes, except old impl gets worse with millions of keys): Memory bits per key: FP rate percent old impl FP rate percent new impl 6: 5.70953 5.69888 8: 2.45766 2.29709 10: 1.13977 0.959254 12: 0.662498 0.411593 16: 0.353023 0.0873754 24: 0.261552 0.0060971 50: 0.225453 ~0.00003 (less than 1 in a million queries are FP) Fixes Fixes Unlike the old implementation, this implementation has a fixed cache line size (64 bytes). At 10 bits per key, the accuracy of this new implementation is very close to the old implementation with 128-byte cache line size. If theres sufficient demand, this implementation could be generalized. Compatibility Although old releases would see the new structure as corrupt filter data and read the table as if theres no filter, weve decided only to enable the new Bloom filter with new format_version=5. This provides a smooth path for automatic adoption over time, with an option for early opt-in. Pull Request resolved: Test Plan: filter_bench has been used thoroughly to validate speed, accuracy, and correctness. Unit tests have been carefully updated to exercise new and old implementations, as well as the logic to select an implementation based on context (format_version). Differential Revision: D18294749 Pulled By: pdillinger fbshipit-source-id: d44c9db3696e4d0a17caaec47075b7755c262c5f/"
