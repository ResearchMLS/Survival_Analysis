Topic_no,Keywords,Contrib,System,Text
0,"revision_pulle, summary, closes_differential, option, yiwu_arbug, tools_modeling, memtable, set, run, true, feature, immutable_memtable, ajkr_fbshipit, resolved_differential, fail, pull_request, open, sagar_fbshipit, case, memory_usage",0.05,frostwire,[android] avoid npe on log./
,,0.05,frostwire,[common] Bitsnoop/torcache/magnet fix/
,,0.05,frostwire,[common] Bitsnoop/torcache/magnet fix/
,,0.05,frostwire,[common] avoid NPE in CopyrightLicenseBroker/
,,0.05,frostwire,[desktop] BugManager revision/
,,0.05,frostwire,[desktop] BugManager revision/
,,0.05,frostwire,[desktop] BugManager revision/
,,0.05,frostwire,[desktop] BugManager revision/
,,0.05,frostwire,[desktop] BugManager revision/
,,0.05,frostwire,[desktop] BugManager revision/
,,0.066,frostwire,[desktop] fixed deprecated API use in SkinTitledBorder/
,,0.05,frostwire,[desktop] LibraryExplorer min dimensions fix. cleanup/
,,0.05,frostwire,[desktop] LibraryExplorer min dimensions fix. cleanup/
,,0.05,frostwire,"[common] getCallingMethodInfo() fix, now its the 6th elem/"
,,0.05,frostwire,[desktop] grammar fix/
,,0.0673,frostwire,[android] NPE issue with FWVibrator/
,,0.0673,frostwire,[android] NPE issue with FWVibrator/
,,0.05,frostwire,[android] fix ANR on PromotionsAdapter loading mopub view/
,,0.05,frostwire,"[android] InHouseBannerFactory NPE Fix, kiss/"
,,0.05,frostwire,[android] NPE on MusicPlaybackService::notifyChangeTask()/[android] MusicPlaybackService NPE/
,,0.05,frostwire,[android] NPE in TransferDetailPiecesFragment/
,,0.05,frostwire,[android] ProdutctPaymentOptionsView NPE/
,,0.0815,javacpp,"* Offer the Apache License, Version 2.0, as a new choice of license, in addition to the GPLv2 with Classpath exception/ * Fix `Parser` errors on unnamed `namespace` blocks, preprocessor directives with comments, and empty macros/"
,,0.05,javacpp,* Fix `NullPointerException` in `Parser` on variadic templates (issue
,,0.05,javacpp,* Fix `NullPointerException` in `Parser` on variadic templates (issue
,,0.05,jna,fix varargs on ARM/
,,0.05,jna,Fix javadoc generation with openjdk 1.8.0_121/
,,0.05,jna,down to 24 failures (4 crashes) on w32ce-arm/
,,0.05,jna,Fix JavaDoc comment (#1014)/
,,0.05,jna,fix typo git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.05,jna,fix compile under msvc git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0599,jna,fix contrib reference to deprecated method/
,,0.05,jna,Fix JavaDoc warning/
,,0.1332,jna,fix merge conflicts for merge conflicts in javadoc errors/Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.05,jna,"Merge pull request from dbwiddis/master Add GetPerformanceInfo(), GetTickCount64(), and SetErrorMode()/Add SetErrorMode/"
,,0.05,jna,fix shell32 alignment on win64/
,,0.05,jna,fix typo in javadoc/
,,0.05,jna,down to 24 failures (4 crashes) on w32ce-arm/
,,0.05,jna,down to 24 failures (4 crashes) on w32ce-arm/
,,0.0636,jna,"fix varargs on ARM/updated ""Promote float varargs to double"" bug fix/"
,,0.05,jna,Fix typo git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.05,jna,Fix header inclusion on Visual Studio/MinGW/
,,0.05,jna,Fix NPE in NativeMappedConverter git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.05,jna,down to 24 failures (4 crashes) on w32ce-arm/
,,0.05,jna,down to 24 failures (4 crashes) on w32ce-arm/
,,0.05,jna,More cygwin fixes/
,,0.066,jna,Repair ppc build regression./
,,0.05,jna,Fix for sunpro compiler on Solaris/
,,0.05,jna,Revert debug code changes/Fix building with Clang for Darwin (OS X 10.6+ and iOS 4.0+)/
,,0.05,jna,Fix javadoc/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.05,jna,fix javadoc error/
,,0.1398,jna,fix javadoc error/fix javadoc errors/Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.05,jna,"Merge pull request from dbwiddis/master Add GetPerformanceInfo(), GetTickCount64(), and SetErrorMode()/Add SetErrorMode/"
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1083,jna,Fix merge conflicts in StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1403,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.05,jna,fix shell32 alignment on win64/
,,0.05,jna,Fix javadoc generation with openjdk 1.8.0_121/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.0605,jna,use correct type mapper for SERVICE_FAILURE_ACTIONS/implemented failure configuration of windows services/
,,0.1065,jna,fix merge conflicts for StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.05,jna,Fix javadoc/
,,0.1349,jna,Merge pull request from lgoldstein/load-library-fix Fix return type of Native#loadLibrary to match unconstrained generic Ã–/Fix return type of Native#loadLibrary to match unconstrained generic type/Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.0609,jna,address issue
,,0.1422,jna,fix javadoc errors/Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.05,jna,fix getpid perf test on w32/
,,0.05,jna,Fix vararg float test/
,,0.05,jna,More cygwin fixes/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.05,jna,fix some broken links/
,,0.05,jna,Fix header inclusion on Visual Studio/MinGW/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1441,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.05,jna,* fix class name casing/
,,0.05,jna,* fix class name casing/
,,0.05,jna,"Merge pull request from dbwiddis/master Add GetPerformanceInfo(), GetTickCount64(), and SetErrorMode()/"
,,0.05,jna,"Merge pull request from dbwiddis/master Add GetPerformanceInfo(), GetTickCount64(), and SetErrorMode()/"
,,0.1422,jna,Drop StdCallLibrary inheritance from non-library interfaces Prefer String over WString Fix some w32 API callback types to be stdcall Fix platform test execution from top level test-platform target/
,,0.05,jna,fix varargs on ARM/
,,0.05,jna,Fix javadoc/
,,0.05,jna,Fix ELFAnalyserTest for java 6 (remove usage of post-java6 NIO code)/
,,0.05,OpenDDS,Tue Jun 12 19:06:13 UTC 2007 Wallace Zhang
,,0.05,OpenDDS,Tue Jun 12 19:06:13 UTC 2007 Wallace Zhang
,,0.05,OpenDDS,Tue Jun 12 20:30:05 UTC 2007 Wallace Zhang
,,0.05,OpenDDS,fuzz fix/
,,0.05,OpenDDS,fuzz fix/
,,0.05,OpenDDS,fuzz fixes/
,,0.0665,OpenDDS,Changed to size_t to avoid warning during comparison with capacity_/
,,0.05,OpenDDS,"fix for fuzz, remove tab/"
,,0.1238,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1218,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1199,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1218,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1199,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1238,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.11599999999999999,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1218,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1277,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1257,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1218,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1296,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1257,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1238,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1218,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.0998,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./Merge pull request from objectcomputing/writer_reader_impl_templates Fix build error for safety profile./Fix build errors./
,,0.1199,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1199,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1257,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1179,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1218,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1257,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1277,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1179,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1277,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1218,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1199,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1257,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1257,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1218,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.1179,OpenDDS,Added missing headers from ZeroCopyInfoSeq_T.h absence. Added CORBA::String_var setter to unions to avoid implicit conversion. Fixed SampleInfoSeq constructors./
,,0.05,OpenDDS,Fix typo and format specifier/
,,0.05,OpenDDS,Fixed typo in TS_common.hpp/
,,0.05,OpenDDS,Fuzz fixes for itl/rapidjson./
,,0.05,OpenDDS,Spdp: fix Spdp local addr if rtsp is not used/
,,0.05,OpenDDS,Spdp: fix Spdp local addr if rtsp is not used/
,,0.4167,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4167,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4193,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4218,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.418,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4205,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4176,OpenDDS,When we have C++11 we also use noexcept(false) * tools/modeling/codegen/model/Sync.h:/Merge pull request from jwillemsen/master Add guard include to fix issue versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4218,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4167,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4193,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4154,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4193,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4183,OpenDDS,Merge pull request from jwillemsen/master Minor typo and doxygen fixes/Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.3925,OpenDDS,Merge pull request from jwillemsen/master Add guard include to fix issue missing include for versioned namespace * tools/modeling/codegen/model/Exceptions.h:/Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4193,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4231,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4106,OpenDDS,Merge pull request from jwillemsen/master Minor typo and doxygen fixes/Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.418,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4205,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4205,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4195,OpenDDS,Merge pull request from jwillemsen/master Minor typo and doxygen fixes/Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.3877,OpenDDS,"Merge branch master of into jwi-gendirbug/In tests and tools, dont leak memory from TypeSupport::get_type_name()/Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/"
,,0.4141,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4231,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4231,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.42700000000000005,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4154,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4154,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4193,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4154,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4167,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4244,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4154,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.4218,OpenDDS,Add versioned namespace macros * tools/modeling/codegen/model/Application.cpp: * tools/modeling/codegen/model/Application.h: * tools/modeling/codegen/model/Config.cpp: * tools/modeling/codegen/model/Config.h: * tools/modeling/codegen/model/Config.inl: * tools/modeling/codegen/model/CopyQos.h: * tools/modeling/codegen/model/Delegate.cpp: * tools/modeling/codegen/model/Delegate.h: * tools/modeling/codegen/model/Entities.cpp: * tools/modeling/codegen/model/Entities.h: * tools/modeling/codegen/model/Entities.inl: * tools/modeling/codegen/model/EntityProfiles.cpp: * tools/modeling/codegen/model/EntityProfiles.h: * tools/modeling/codegen/model/Exceptions.h: * tools/modeling/codegen/model/NullListener.cpp: * tools/modeling/codegen/model/NullListener.h: * tools/modeling/codegen/model/NullParticipantListener.cpp: * tools/modeling/codegen/model/NullParticipantListener.h: * tools/modeling/codegen/model/NullPublisherListener.cpp: * tools/modeling/codegen/model/NullPublisherListener.h: * tools/modeling/codegen/model/NullReaderListener.cpp: * tools/modeling/codegen/model/NullReaderListener.h: * tools/modeling/codegen/model/NullSubscriberListener.cpp: * tools/modeling/codegen/model/NullSubscriberListener.h: * tools/modeling/codegen/model/NullTopicListener.cpp: * tools/modeling/codegen/model/NullTopicListener.h: * tools/modeling/codegen/model/NullWriterListener.cpp: * tools/modeling/codegen/model/NullWriterListener.h: * tools/modeling/codegen/model/Service_T.cpp: * tools/modeling/codegen/model/Service_T.h: * tools/modeling/codegen/model/Sync.cpp: * tools/modeling/codegen/model/Sync.h: * tools/modeling/codegen/model/TransportDirectives.cpp: * tools/modeling/codegen/model/TransportDirectives.h: * tools/modeling/codegen/model/Utilities.cpp: * tools/modeling/codegen/model/Utilities.h:/
,,0.05,OpenDDS,Merge branch master of into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch master of into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch master into coverity_issues_fixes/
,,0.05,OpenDDS,Merge branch master into coverity_issues_fixes/
,,0.05,OpenDDS,Fix DurablityCache bug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge pull request from huangminghuang/StaticDiscovery_Fix Static discovery fix/
,,0.05,OpenDDS,Merge pull request from huangminghuang/StaticDiscovery_Fix Static discovery fix/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug/
,,0.05,OpenDDS,Merge branch jwi-gendirbug/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.05,OpenDDS,Merge branch master into jwi-endhistorysampleleak/
,,0.066,OpenDDS,Merge pull request from huangminghuang/master memory leak fixes/Fix memory leak for tools/modeling/tests/RemoteInstHandles/
,,0.0653,OpenDDS,Merge pull request from PaulCalabrese/master Fix Monitor library and test/Add new DCPSMonitor configuration option to initialize monitor functionality. Fix bug in DCPSRTISerialization command line option./
,,0.05,OpenDDS,Merge pull request from huangminghuang/master Fix memory leak in ZeroCopySeq/Fix memory leak in ZeroCopySeq/
,,0.05,OpenDDS,Fix memory leak in ZeroCopySeq/
,,0.05,OpenDDS,Merge pull request from huangminghuang/master Fix memory leak in ZeroCopySeq/Fix memory leak in ZeroCopySeq/
,,0.05,OpenDDS,Merge pull request from huangminghuang/master Fix memory leak in ZeroCopySeq/Fix memory leak in ZeroCopySeq/
,,0.05,OpenDDS,Merge pull request from huangminghuang/master Fix memory leak in ZeroCopySeq/Fix memory leak in ZeroCopySeq/
,,0.05,OpenDDS,DomainParticipantImpl.h: Fix Indent/
,,0.05,OpenDDS,Fix opendds_idl
,,0.05,OpenDDS,MultiTopicTest: fix string concat again/MultiTopicTest: fix string concat/
,,0.05,OpenDDS,Fix typo in stack_subscriber/
,,0.05,OpenDDS,Bug fix/
,,0.066,OpenDDS,Fix get_index(). Remove unimplemented function declaration./
,,0.066,pljava,Fixes to some problems reported by Filip Hrbek/
,,0.05,pljava,Fix bug
,,0.05,pljava,Fix of bug 915 and 916/
,,0.05,pljava,Fix bug
,,0.05,pljava,Fix for bug
,,0.05,pljava,Fix for bug
,,0.05,pljava,Fix for bug
,,0.05,pljava,Fix for bug
,,0.05,pljava,Fix of bug
,,0.05,pljava,Fix of bug
,,0.05,pljava,Fix of bug
,,0.05,pljava,Fix of bug
,,0.0625,pljava,Merge pull request from tada/bug/REL1_5_STABLE/issue134 Accomodate upstream SPI_push/pop API changes (issue
,,0.0625,pljava,Merge pull request from tada/bug/REL1_5_STABLE/issue134 Accomodate upstream SPI_push/pop API changes (issue
,,0.05,realm-java,Merge branch master of github.com:Tightdb/tightdb_java into group-readonly-bug/
,,0.05,realm-java,Merge branch master of github.com:Tightdb/tightdb_java into group-readonly-bug/
,,0.05,realm-java,Merge branch master of github.com:Tightdb/tightdb_java into group-readonly-bug/
,,0.05,realm-java,Merge branch master of github.com:Tightdb/tightdb_java into group-readonly-bug/
,,0.05,realm-java,Merge branch master of github.com:Tightdb/tightdb_java into group-readonly-bug/
,,0.05,realm-java,Merge branch master of github.com:Tightdb/tightdb_java into group-readonly-bug/
,,0.05,realm-java,Merge branch master of github.com:Tightdb/tightdb_java into group-readonly-bug/
,,0.05,realm-java,Merge branch master of github.com:Tightdb/tightdb_java into group-readonly-bug/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from Tightdb/doc-fixes Doc fixes/
,,0.05,realm-java,Merge pull request from realm/kg-bugfix-sort Changing sort()/Merge branch master of github.com:realm/realm-java into kg-bugfix-maxdate Conflicts: changelog.txt/
,,0.05,realm-java,Merge pull request from zaki50/fix_typo fix typos/fix typos in RealmModule.java/
,,0.05,realm-java,Merge pull request from zaki50/fix_typo fix typos/fix typos/
,,0.05,realm-java,Merge pull request from zaki50/fix_typo fix typos/fix typos/
,,0.05,realm-java,Merge pull request from zaki50/fix_typo fix typos/fix typos/
,,0.05,realm-java,Merge pull request from zaki50/fix_typo fix typos/fix typos/
,,0.05,realm-java,Merge pull request from zaki50/fix_typo fix typos/fix typos/
,,0.0586,realm-java,fix copilation error in model classes in unit test/
,,0.0699,realm-java,"hashCode behavior javadoc (#2750) The behavior of hashCode is documented as it is pointed in issue Until realm-java has an immutable identifier, the behavior of hashCode should remain documented./"
,,0.05,realm-java,Fix JNI declaration for UncheckedRow (#3104)/
,,0.05,realm-java,Fix JNI declaration for UncheckedRow (#3104)/
,,0.05,realm-java,Merge pull request from realm/merge-e62d11-to-master Fix merge from e62d11 to master/
,,0.05,realm-java,Merge pull request from realm/merge-a8d647-to-master Fix merge from a8d647 to master/
,,0.05,realm-java,Merge pull request from realm/merge-5d1d1f-to-master Fix merge from 5d1d1f to master/
,,0.05,realm-java,Typo fix/
,,0.05,realm-java,Merge pull request from realm/merge-a8d647-to-master Fix merge from a8d647 to master/
,,0.05,realm-java,Merge pull request from realm/merge-a8d647-to-master Fix merge from a8d647 to master/
,,0.05,realm-java,Merge pull request from realm/merge-a8d647-to-master Fix merge from a8d647 to master/
,,0.2204,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.2204,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.2133,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.22399999999999998,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.218,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.2168,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.218,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.218,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.05,realm-java,Merge pull request from realm/merge-f3f8ab-to-master Fix merge from f3f8ab to master/
,,0.2314,realm-java,"Feature/backlinks (#4406) * Refactor RealmObjectSchema Clean up Proxy generation Comments addressed * Fix Test, Checkstyle and Findbugs errors * Respond to comments/Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.05,realm-java,Merge pull request from realm/merge-52ba43-to-master Fix merge from 52ba43 to master/Introduce ErrorProne plugin (#4342)/
,,0.2107,realm-java,"Fix threading bugs in RunInLooperThread rule (#4563) * Fix threading bugs in RunInLooperThread rule * Respond to comments Fix spelling errors Clean up multi-error recovery./Feature/backlinks (#4406) * Refactor RealmObjectSchema Clean up Proxy generation Comments addressed * Fix Test, Checkstyle and Findbugs errors * Respond to comments/Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.2204,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.05,realm-java,Merge pull request from realm/merge-f3f8ab-to-master Fix merge from f3f8ab to master/
,,0.2216,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.05,realm-java,Merge pull request from realm/merge-f3f8ab-to-master Fix merge from f3f8ab to master/
,,0.2157,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.2216,realm-java,"Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.2217,realm-java,"Feature/backlinks (#4406) * Refactor RealmObjectSchema Clean up Proxy generation Comments addressed * Fix Test, Checkstyle and Findbugs errors * Respond to comments/Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-dc09d5-to-master Fix merge from dc09d5 to master/
,,0.05,realm-java,Merge pull request from realm/merge-ea02c9-to-master Fix merge from ea02c9 to master/
,,0.05,realm-java,Merge pull request from realm/merge-ea02c9-to-master Fix merge from ea02c9 to master/
,,0.05,realm-java,Merge pull request from realm/merge-ea02c9-to-master Fix merge from ea02c9 to master/
,,0.05,realm-java,Merge pull request from realm/merge-ea02c9-to-master Fix merge from ea02c9 to master/
,,0.05,realm-java,fixes (#4862) * fixes
,,0.05,realm-java,fixes (#4862) * fixes
,,0.0636,realm-java,Exponential Back Off now retry query in case of `ConnectionException` (#4805) * fixes
,,0.0657,realm-java,Exponential Back Off now retry query in case of `ConnectionException` (#4805) * fixes
,,0.05,realm-java,Merge pull request from realm/merge-f2144d-to-master-4.0 Fix merge from f2144d to master-4.0/fixes (#4862) * fixes
,,0.05,realm-java,PR fix/
,,0.05,realm-java,Merge pull request from realm/merge-c9206c-to-master-4.0 Fix merge from c9206c to master-4.0/Merge pull request from realm/merge-39bb67-to-master-4.0 Fix merge from 39bb67 to master-4.0/Merge pull request from realm/merge-b59c82-to-master Fix merge from b59c82 to master/
,,0.05,realm-java,Merge pull request from realm/merge-39bb67-to-master-4.0 Fix merge from 39bb67 to master-4.0/Merge pull request from realm/merge-b59c82-to-master Fix merge from b59c82 to master/
,,0.05,realm-java,Merge pull request from realm/merge-c9206c-to-master-4.0 Fix merge from c9206c to master-4.0/
,,0.05,realm-java,Merge pull request from realm/merge-c9206c-to-master-4.0 Fix merge from c9206c to master-4.0/
,,0.05,realm-java,Merge pull request from realm/merge-c9206c-to-master-4.0 Fix merge from c9206c to master-4.0/
,,0.05,realm-java,Merge pull request from realm/merge-c9206c-to-master-4.0 Fix merge from c9206c to master-4.0/
,,0.05,realm-java,Merge pull request from realm/merge-39bb67-to-master-4.0 Fix merge from 39bb67 to master-4.0/
,,0.05,realm-java,Merge pull request from realm/merge-39bb67-to-master-4.0 Fix merge from 39bb67 to master-4.0/
,,0.05,realm-java,Maybe a fix for flaky CI/
,,0.05,realm-java,Merge pull request from realm/merge-3a36b1-to-master Fix merge from 3a36b1 to master/
,,0.05,realm-java,Merge pull request from realm/merge-3a36b1-to-master Fix merge from 3a36b1 to master/
,,0.05,realm-java,fixes (#6010) * Fixes
,,0.05,realm-java,fixes (#6010) * Fixes
,,0.05,realm-java,Fix Realm.deleteAll() and Realm.isEmpty() (#6024)/
,,0.05,realm-java,Merge pull request from realm/merge-3a36b1-to-master Fix merge from 3a36b1 to master/
,,0.05,realm-java,fixes (#6010) * Fixes
,,0.05,realm-java,Fix Realm.deleteAll() and Realm.isEmpty() (#6024)/
,,0.05,realm-java,Fix Realm.deleteAll() and Realm.isEmpty() (#6024)/
,,0.0618,realm-java,IllegalStateException when opening old synchronized Realm (#6621)/
,,0.05,rocksdb,Fix compaction_filter.h typos/
,,0.05,rocksdb,Fix implicit compare/
,,0.05,rocksdb,Fix assert in histogramData/
,,0.0621,rocksdb,Merge pull request from fyrz/RocksJava-Deprecate-SkipLogError [RocksJava] Deprecate setSkipLogErrorOnRecovery/[RocksJava] Deprecate setSkipLogErrorOnRecovery see: 62ad0a9b19f0be4cefa70b6b32876e764b7f3c11/
,,0.05,rocksdb,Merge pull request from fyrz/RocksJava-Sigsegv-MergeOperatorName [RocksJava] Fixes in MergeOperatorByName/[RocksJava] Sigsegv fix for MergerOperatorByName/
,,0.05,rocksdb,Fix java build/
,,0.0929,rocksdb,"Fix leak when create_missing_column_families=true on ThreadStatus Summary: An entry of ConstantColumnFamilyInfo is created when: 1. DB::Open 2. CreateColumnFamily. However, there are cases that DB::Open could also call CreateColumnFamily when create_missing_column_families=true. As a result, it will create duplicate ConstantColumnFamilyInfo and one of them would be leaked. Test Plan: ./deletefile_test Reviewers: igor, sdong, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision:"
,,0.05,rocksdb,Fix stack trace on mac/
,,0.3207,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
,,0.3168,rocksdb,"Print memory allocation counters Summary: Introduced option to dump malloc statistics using new option flag. Added new command line option to db_bench tool to enable this funtionality. Also extended build to support environments with/without jemalloc. Test Plan: 1) Build rocksdb using `make` command. Launch the following command `./db_bench end verified that jemalloc dump is present in LOG file. 2) Build rocksdb using `DISABLE_JEMALLOC=1 make db_bench and ran the same db_bench tool and found the following message in LOG file: ""Please compile with jemalloc to enable malloc dump"". 3) Also built rocksdb using `make` command on MacOS to verify behavior in non-FB environment. Also to debug build configuration change temporary changed AM_DEFAULT_VERBOSITY 1 in Makefile to see compiler and build tools output. For case 1) was present in compiler command line. For both 2) and 3) this flag was not present. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.48700000000000004,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision: pull request from bureau14/master Fixes warnings and ensure correct int behavior on 32-bit platforms./Fixes warnings and ensure correct int behavior on 32-bit platforms./"
,,0.1081,rocksdb,Merge pull request from bureau14/master Fixes warnings and ensure correct int behavior on 32-bit platforms./Fixes warnings and ensure correct int behavior on 32-bit platforms./
,,0.3739,rocksdb,"Dont run DBOptionsAllFieldsSettable under valgrind Summary: Test DBOptionsAllFieldsSettable sometimes fails under valgrind. Move option settable tests to a separate test file and disable it in valgrind.. Test Plan: Run valgrind test and make sure the test doesnt run. Reviewers: andrewkr, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: kradhakrishnan, yiwu, yhchiang, leveldb, andrewkr, dhruba Differential Revision: info logger Summary: Adapted a stderr logger from the option tests. Moved it to a separate header so we can reuse it, e.g., from ldb subcommands for faster debugging. This is especially useful to make errors/warnings more visible when running ""ldb repair"", which involves potential data loss. Test Plan: ran options_test and ""ldb repair"" $ ./ldb repair [WARN] **** Repaired rocksdb ./tmp/; recovered 1 files; 588bytes. Some data may have been lost. **** OK Reviewers: IslamAbdelRahman, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.3132,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: to have an option to fail Cache::Insert() when full Summary: Cache to have an option to fail Cache::Insert() when full. Update call sites to check status and handle error. I totally have no idea whats correct behavior of all the call sites when they encounter error. Please let me know if you see something wrong or more unit test is needed. Test Plan: make check see tests pass. Reviewers: anthony, yhchiang, andrewkr, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.1201,rocksdb,"Cache to have an option to fail Cache::Insert() when full Summary: Cache to have an option to fail Cache::Insert() when full. Update call sites to check status and handle error. I totally have no idea whats correct behavior of all the call sites when they encounter error. Please let me know if you see something wrong or more unit test is needed. Test Plan: make check see tests pass. Reviewers: anthony, yhchiang, andrewkr, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.096,rocksdb,Merge pull request from bureau14/master Fixes warnings and ensure correct int behavior on 32-bit platforms./Fixes warnings and ensure correct int behavior on 32-bit platforms./
,,0.3141,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: to have an option to fail Cache::Insert() when full Summary: Cache to have an option to fail Cache::Insert() when full. Update call sites to check status and handle error. I totally have no idea whats correct behavior of all the call sites when they encounter error. Please let me know if you see something wrong or more unit test is needed. Test Plan: make check see tests pass. Reviewers: anthony, yhchiang, andrewkr, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.4337,rocksdb,"Fix compression dictionary clang errors Summary: There were a few narrowing conversions that clang didnt like. Test Plan: $ make clean && USE_CLANG=1 DISABLE_JEMALLOC=1 TEST_TMPDIR=/dev/shm/rocksdb OPT=-g make check Reviewers: IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba, leveldb Differential Revision: pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.1146,rocksdb,"Cache to have an option to fail Cache::Insert() when full Summary: Cache to have an option to fail Cache::Insert() when full. Update call sites to check status and handle error. I totally have no idea whats correct behavior of all the call sites when they encounter error. Please let me know if you see something wrong or more unit test is needed. Test Plan: make check see tests pass. Reviewers: anthony, yhchiang, andrewkr, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.4673,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.4666,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.368,rocksdb,"BlockBasedTable::Get() not to use prefix bloom if read_options.total_order_seek true Summary: This is to provide a way for users to skip prefix bloom in point look-up. Test Plan: Add a new unit test scenario. Reviewers: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision: pull request from flyd1005/wip-fix-typo fix typos and remove duplicated words/Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision: Reader should not be reused after DB restart Summary: In block based table reader, wow we put index reader to block cache, which can be retrieved after DB restart. However, index reader may reference internal comparator, which can be destroyed after DB restarts, causing problems. Fix it by making cache key identical per table reader. Test Plan: Add a new test which failed with out the commit but now pass. Reviewers: IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: maro, yhchiang, kradhakrishnan, leveldb, andrewkr, dhruba Differential Revision:"
,,0.4644,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.4659,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.4891,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision: pull request from bureau14/master Fixes warnings and ensure correct int behavior on 32-bit platforms./Fixes warnings and ensure correct int behavior on 32-bit platforms./"
,,0.4305,rocksdb,"Fix calling GetCurrentMutableCFOptions in CompactionJob::ProcessKeyValueCompaction() Summary: GetCurrentMutableCFOptions() can only be called when DB mutex is held so we cannot call it in CompactionJob::ProcessKeyValueCompaction() since its not holding the db mutex Test Plan: make check Reviewers: sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.4586,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.4357,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision: to have an option to fail Cache::Insert() when full Summary: Cache to have an option to fail Cache::Insert() when full. Update call sites to check status and handle error. I totally have no idea whats correct behavior of all the call sites when they encounter error. Please let me know if you see something wrong or more unit test is needed. Test Plan: make check see tests pass. Reviewers: anthony, yhchiang, andrewkr, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.096,rocksdb,Merge pull request from bureau14/master Fixes warnings and ensure correct int behavior on 32-bit platforms./Fixes warnings and ensure correct int behavior on 32-bit platforms./
,,0.3546,rocksdb,"Added EventListener::OnTableFileCreationStarted() callback Summary: Added EventListener::OnTableFileCreationStarted. EventListener::OnTableFileCreated will be called on failure case. User can check creation status via TableFileCreationInfo::status. Test Plan: unit test. Reviewers: dhruba, yhchiang, ott, sdong Reviewed By: sdong Subscribers: sdong, kradhakrishnan, IslamAbdelRahman, andrewkr, yhchiang, leveldb, ott, dhruba Differential Revision: Statistics FLUSH_WRITE_BYTES Summary: In we fixed an issue where we consider flush as compaction. However, that makes us mistakenly count FLUSH_WRITE_BYTES twice (one in flush_job and one in db_impl.) This patch removes the one incremented in db_impl. Test Plan: db_test Reviewers: yiwu, andrewkr, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: a bug in RocksDB Statistics where flush is considered as compaction Summary: Fixed a bug in RocksDB Statistics where flush is considered as compaction Test Plan: unit test Reviewers: sdong, IslamAbdelRahman, rven, kradhakrishnan, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, leveldb Differential Revision: pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.102,rocksdb,Merge pull request from bureau14/master Fixes warnings and ensure correct int behavior on 32-bit platforms./Fixes warnings and ensure correct int behavior on 32-bit platforms./
,,0.4325,rocksdb,"Added EventListener::OnTableFileCreationStarted() callback Summary: Added EventListener::OnTableFileCreationStarted. EventListener::OnTableFileCreated will be called on failure case. User can check creation status via TableFileCreationInfo::status. Test Plan: unit test. Reviewers: dhruba, yhchiang, ott, sdong Reviewed By: sdong Subscribers: sdong, kradhakrishnan, IslamAbdelRahman, andrewkr, yhchiang, leveldb, ott, dhruba Differential Revision: pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.3255,rocksdb,"Adding pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
,,0.4409,rocksdb,"Added EventListener::OnTableFileCreationStarted() callback Summary: Added EventListener::OnTableFileCreationStarted. EventListener::OnTableFileCreated will be called on failure case. User can check creation status via TableFileCreationInfo::status. Test Plan: unit test. Reviewers: dhruba, yhchiang, ott, sdong Reviewed By: sdong Subscribers: sdong, kradhakrishnan, IslamAbdelRahman, andrewkr, yhchiang, leveldb, ott, dhruba Differential Revision: pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
,,0.0694,rocksdb,fix delete file bug when do checkpoint (#1138)/
,,0.1939,rocksdb,"Add avoid_flush_during_shutdown DB option Summary: Add avoid_flush_during_shutdown DB option. Closes Differential Revision: D4108643 Pulled By: yiwu-arbug fbshipit-source-id: abdaf4d/Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/Support running consistency checks in release mode Summary: We always run consistency checks when compiling in debug mode allow users to set Options::force_consistency_checks to true to be able to run such checks even when compiling in release mode Test Plan: make check make release Reviewers: lightmark, sdong, yiwu Reviewed By: yiwu Subscribers: hermanlee4, andrewkr, yoshinorim, jkedgar, dhruba Differential Revision: compiling under Summary: MyRocks build is broken because they are using ""-Werror=missing-field-initializers"" We should fix that by explicitly passing these arguments Test Plan: Build MyRocks Reviewers: sdong, yiwu Reviewed By: yiwu Subscribers: andrewkr, dhruba Differential Revision:"
,,0.1452,rocksdb,Add avoid_flush_during_shutdown DB option Summary: Add avoid_flush_during_shutdown DB option. Closes Differential Revision: D4108643 Pulled By: yiwu-arbug fbshipit-source-id: abdaf4d/
,,0.2037,rocksdb,"Add avoid_flush_during_shutdown DB option Summary: Add avoid_flush_during_shutdown DB option. Closes Differential Revision: D4108643 Pulled By: yiwu-arbug fbshipit-source-id: abdaf4d/Support running consistency checks in release mode Summary: We always run consistency checks when compiling in debug mode allow users to set Options::force_consistency_checks to true to be able to run such checks even when compiling in release mode Test Plan: make check make release Reviewers: lightmark, sdong, yiwu Reviewed By: yiwu Subscribers: hermanlee4, andrewkr, yoshinorim, jkedgar, dhruba Differential Revision:"
,,0.085,rocksdb,"Show More DB Stats in info logs Summary: DB Stats now are truncated if there are too many CFs. Extend the buffer size to allow more to be printed out. Also, separate out malloc to another log line. Closes Differential Revision: D4100943 Pulled By: yiwu-arbug fbshipit-source-id: 79f7218/"
,,0.0982,rocksdb,Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/
,,0.1941,rocksdb,"Add avoid_flush_during_shutdown DB option Summary: Add avoid_flush_during_shutdown DB option. Closes Differential Revision: D4108643 Pulled By: yiwu-arbug fbshipit-source-id: abdaf4d/Support running consistency checks in release mode Summary: We always run consistency checks when compiling in debug mode allow users to set Options::force_consistency_checks to true to be able to run such checks even when compiling in release mode Test Plan: make check make release Reviewers: lightmark, sdong, yiwu Reviewed By: yiwu Subscribers: hermanlee4, andrewkr, yoshinorim, jkedgar, dhruba Differential Revision:"
,,0.2254,rocksdb,"Add avoid_flush_during_shutdown DB option Summary: Add avoid_flush_during_shutdown DB option. Closes Differential Revision: D4108643 Pulled By: yiwu-arbug fbshipit-source-id: abdaf4d/Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/Support running consistency checks in release mode Summary: We always run consistency checks when compiling in debug mode allow users to set Options::force_consistency_checks to true to be able to run such checks even when compiling in release mode Test Plan: make check make release Reviewers: lightmark, sdong, yiwu Reviewed By: yiwu Subscribers: hermanlee4, andrewkr, yoshinorim, jkedgar, dhruba Differential Revision:"
,,0.05,rocksdb,Fix Mac build/
,,0.2239,rocksdb,"Add avoid_flush_during_shutdown DB option Summary: Add avoid_flush_during_shutdown DB option. Closes Differential Revision: D4108643 Pulled By: yiwu-arbug fbshipit-source-id: abdaf4d/Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/Support running consistency checks in release mode Summary: We always run consistency checks when compiling in debug mode allow users to set Options::force_consistency_checks to true to be able to run such checks even when compiling in release mode Test Plan: make check make release Reviewers: lightmark, sdong, yiwu Reviewed By: yiwu Subscribers: hermanlee4, andrewkr, yoshinorim, jkedgar, dhruba Differential Revision: typo (#1349)/fix typo in option.hs comment (#1321)/Fixed typo (#1279) Users shouldnt reply on users shouldnt rely on/"
,,0.1849,rocksdb,Add C api for RateLimiter Summary: Add C api for RateLimiter. Closes Differential Revision: D4116362 Pulled By: yiwu-arbug fbshipit-source-id: cb05a8d/Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/expose IngestExternalFile to c abi Summary: IngestExternalFile is very useful when doing bulk load. This pr expose this API to c so many bindings can benefit from it too. Closes Differential Revision: D4113420 Pulled By: yiwu-arbug fbshipit-source-id: 307c6ae/Remove extraneous function prototypes from c.h (#1326) * Fix function prototypes from upstream commit 32149059. * Fix removed function. * Readd removed function./
,,0.0982,rocksdb,Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/
,,0.0903,rocksdb,Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/
,,0.0948,rocksdb,Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/Fix in two Java functions (#1396)/
,,0.1022,rocksdb,Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/
,,0.1015,rocksdb,Disable auto compactions in memory_test and re-enable the test (#1408) Summary: Auto-compactions will change memory usage of DB but memory_test didnt take it into account. This PR disable auto compactions in the test and hopefully it fixes its flakyness. Test Plan: UBSAN build used to catch the flakyness. Run `make ubsan_check` and it passes./
,,0.05,rocksdb,Fix typo (#903) Presumably a leftover from optimistic_transaction_example.cc./
,,0.0898,rocksdb,Fix rocksdb::Status::getState Summary: This fixes the Java API for Status#getState use in Native code and also simplifies the implementation of rocksdb::Status::getState. Closes Closes Differential Revision: D4364181 Pulled By: yiwu-arbug fbshipit-source-id: 8e073b4/
,,0.1234,rocksdb,Print cache options to info log Summary: Improve cache options logging to info log. Also print the value of cache_index_and_filter_blocks_with_high_priority. Closes Differential Revision: D4358776 Pulled By: yiwu-arbug fbshipit-source-id: 8f030a0/
,,0.1051,rocksdb,Gcc-7 buffer size insufficient Summary: Bunch of commits related to insufficient buffer size. Errors in individual commits. Closes Differential Revision: D4332127 Pulled By: IslamAbdelRahman fbshipit-source-id: 878f73c/
,,0.1102,rocksdb,Print cache options to info log Summary: Improve cache options logging to info log. Also print the value of cache_index_and_filter_blocks_with_high_priority. Closes Differential Revision: D4358776 Pulled By: yiwu-arbug fbshipit-source-id: 8f030a0/
,,0.0965,rocksdb,fixed typo Summary: I fixed exisit exist Closes Differential Revision: D4451466 Pulled By: yiwu-arbug fbshipit-source-id: b447c3a/
,,0.1505,rocksdb,Report memory usage by memtable insert hints map. Summary: It is hard to measure acutal memory usage by std containers. Even providing a custom allocator will miss count some of the usage. Here we only do a wild guess on its memory usage. Closes Differential Revision: D4179945 Pulled By: yiwu-arbug fbshipit-source-id: 32ab929/
,,0.1139,rocksdb,Print cache options to info log Summary: Improve cache options logging to info log. Also print the value of cache_index_and_filter_blocks_with_high_priority. Closes Differential Revision: D4358776 Pulled By: yiwu-arbug fbshipit-source-id: 8f030a0/
,,0.141,rocksdb,"Fix LRU Ref() for handles with external references only Summary: For case handle->InCache() && handle->refs >= 1 (the third case mentioned in lru_cache.h), the key was overwritten by Insert(). In this case, the refcount can still be incremented, and the cache handle will never enter LRU list. Fix Ref() logic for this case. Closes Differential Revision: D4467656 Pulled By: ajkr fbshipit-source-id: c0784d8/Print cache options to info log Summary: Improve cache options logging to info log. Also print the value of cache_index_and_filter_blocks_with_high_priority. Closes Differential Revision: D4358776 Pulled By: yiwu-arbug fbshipit-source-id: 8f030a0/"
,,0.0984,rocksdb,fixed typo Summary: I fixed exisit exist Closes Differential Revision: D4451466 Pulled By: yiwu-arbug fbshipit-source-id: b447c3a/
,,0.0972,rocksdb,Fix rocksdb::Status::getState Summary: This fixes the Java API for Status#getState use in Native code and also simplifies the implementation of rocksdb::Status::getState. Closes Closes Differential Revision: D4364181 Pulled By: yiwu-arbug fbshipit-source-id: 8e073b4/
,,0.1143,rocksdb,"improving the C wrapper Summary: rocksdb_property_int (so that we dont have to parse strings) and rocksdb_set_options (to allow controlling options via strings) a few other missing options exposed a documentation comment fix Closes Differential Revision: D4456569 Pulled By: yiwu-arbug fbshipit-source-id: 9f1fac1/Fix bug of Checkpoint loses recent transactions with 2PC Summary: If 2PC is enabled, checkpoint may not copy previous log files that contain uncommitted prepare records. In this diff we keep those files. Closes Differential Revision: D4368319 Pulled By: siying fbshipit-source-id: cc2c746/"
,,0.1158,rocksdb,Print cache options to info log Summary: Improve cache options logging to info log. Also print the value of cache_index_and_filter_blocks_with_high_priority. Closes Differential Revision: D4358776 Pulled By: yiwu-arbug fbshipit-source-id: 8f030a0/
,,0.10800000000000001,rocksdb,Dump persistent cache options Summary: Dump persistent cache options Closes Differential Revision: D4337019 Pulled By: yiwu-arbug fbshipit-source-id: 3812f8a/
,,0.1869,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/"
,,0.0917,rocksdb,Fix rocksdb::Status::getState Summary: This fixes the Java API for Status#getState use in Native code and also simplifies the implementation of rocksdb::Status::getState. Closes Closes Differential Revision: D4364181 Pulled By: yiwu-arbug fbshipit-source-id: 8e073b4/
,,0.1121,rocksdb,Fixes for MSVC compilation Summary: Closes Differential Revision: D4327421 Pulled By: yiwu-arbug fbshipit-source-id: 661ee0b/
,,0.08800000000000001,rocksdb,Remove Ticker::SEQUENCE_NUMBER Summary: Remove the ticker count because: * Having to reset the ticker count in WriteImpl is ineffiecent; * It doesnt make sense to have it as a ticker count if multiple db instance share a statistics object. Closes Differential Revision: D4194442 Pulled By: yiwu-arbug fbshipit-source-id: e2110a9/
,,0.0738,rocksdb,"fix batchresult handle leak Summary: This is related to PR Sorry about this extra PR, as my workflow was messed up when I checked in another work item by accident. Closes Differential Revision: D4379513 Pulled By: yiwu-arbug fbshipit-source-id: a668d4c/"
,,0.1557,rocksdb,Print cache options to info log Summary: Improve cache options logging to info log. Also print the value of cache_index_and_filter_blocks_with_high_priority. Closes Differential Revision: D4358776 Pulled By: yiwu-arbug fbshipit-source-id: 8f030a0/Gcc-7 buffer size insufficient Summary: Bunch of commits related to insufficient buffer size. Errors in individual commits. Closes Differential Revision: D4332127 Pulled By: IslamAbdelRahman fbshipit-source-id: 878f73c/
,,0.1047,rocksdb,"fix non-portable behavior in encoder Summary: using ~0UL for mask uses a uint32_t at least in MSVC, but a uint64_t is required for it to work properly Closes Differential Revision: D4444004 Pulled By: yiwu-arbug fbshipit-source-id: 057cc42/"
,,0.1034,rocksdb,Gcc-7 buffer size insufficient Summary: Bunch of commits related to insufficient buffer size. Errors in individual commits. Closes Differential Revision: D4332127 Pulled By: IslamAbdelRahman fbshipit-source-id: 878f73c/
,,0.1121,rocksdb,Dump persistent cache options Summary: Dump persistent cache options Closes Differential Revision: D4337019 Pulled By: yiwu-arbug fbshipit-source-id: 3812f8a/
,,0.1101,rocksdb,Dump persistent cache options Summary: Dump persistent cache options Closes Differential Revision: D4337019 Pulled By: yiwu-arbug fbshipit-source-id: 3812f8a/
,,0.106,rocksdb,Dump persistent cache options Summary: Dump persistent cache options Closes Differential Revision: D4337019 Pulled By: yiwu-arbug fbshipit-source-id: 3812f8a/
,,0.1119,rocksdb,Range deletion microoptimizations Summary: Made RangeDelAggregators InternalKeyComparator member a reference-to-const so we dont need to copy-construct it. Also added InternalKeyComparator to ImmutableCFOptions so we dont need to construct one for each DBIter. Made MemTable::NewRangeTombstoneIterator and the table readers NewRangeTombstoneIterator() functions return nullptr instead of NewEmptyInternalIterator to avoid the allocation. Updated callers accordingly. Closes Differential Revision: D4208169 Pulled By: ajkr fbshipit-source-id: 2fd65cf/
,,0.1718,rocksdb,Print cache options to info log Summary: Improve cache options logging to info log. Also print the value of cache_index_and_filter_blocks_with_high_priority. Closes Differential Revision: D4358776 Pulled By: yiwu-arbug fbshipit-source-id: 8f030a0/Dump persistent cache options Summary: Dump persistent cache options Closes Differential Revision: D4337019 Pulled By: yiwu-arbug fbshipit-source-id: 3812f8a/
,,0.1172,rocksdb,"Add WriteOptions.no_slowdown Summary: If the WriteOptions.no_slowdown flag is set AND we need to wait or sleep for the write request, then fail immediately with Status::Incomplete(). Closes Differential Revision: D4191405 Pulled By: maysamyabandeh fbshipit-source-id: 7f3ce3f/"
,,0.2672,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/Fixed CompactionFilter::Decision::kRemoveAndSkipUntil Summary: Embarassingly enough, the first time I tried to use my new feature in logdevice it crashed with this assertion failure: db/pinned_iterators_manager.h:30: void rocksdb::PinnedIteratorsManager::StartPinning(): Assertion `pinning_enabled false failed The issue was that `pinned_iters_mgr_.StartPinning()` was called but `pinned_iters_mgr_.ReleasePinnedData()` wasnt. Closes Differential Revision: D4265622 Pulled By: al13n321 fbshipit-source-id: 747b10f/fix clang build Summary: override is missing for FilterV2 Closes Differential Revision: D4263832 Pulled By: IslamAbdelRahman fbshipit-source-id: d8b337a/"
,,0.1256,rocksdb,"Flush job should release reference current version if sync log failed Summary: Fix the bug when sync log fail, FlushJob::Run() will not be execute and reference to cfd->current() will not be release. Closes Differential Revision: D4441316 Pulled By: yiwu-arbug fbshipit-source-id: 5523e28/"
,,0.1172,rocksdb,"Add WriteOptions.no_slowdown Summary: If the WriteOptions.no_slowdown flag is set AND we need to wait or sleep for the write request, then fail immediately with Status::Incomplete(). Closes Differential Revision: D4191405 Pulled By: maysamyabandeh fbshipit-source-id: 7f3ce3f/"
,,0.1325,rocksdb,"Flush job should release reference current version if sync log failed Summary: Fix the bug when sync log fail, FlushJob::Run() will not be execute and reference to cfd->current() will not be release. Closes Differential Revision: D4441316 Pulled By: yiwu-arbug fbshipit-source-id: 5523e28/"
,,0.131,rocksdb,DeleteRange unsupported in non-block-based tables Summary: Return an error from DeleteRange() (or Write() if the user is using the low-level WriteBatch API) if an unsupported table type is configured. Closes Differential Revision: D4185933 Pulled By: ajkr fbshipit-source-id: abcdf84/
,,0.0987,rocksdb,Performance: Iterate vector by reference Summary: Closes Differential Revision: D4398796 Pulled By: yiwu-arbug fbshipit-source-id: b82636d/
,,0.1768,rocksdb,"Flush job should release reference current version if sync log failed Summary: Fix the bug when sync log fail, FlushJob::Run() will not be execute and reference to cfd->current() will not be release. Closes Differential Revision: D4441316 Pulled By: yiwu-arbug fbshipit-source-id: 5523e28/Fixed a crash in debug build in flush_job.cc Summary: It was doing `&range_del_iters[0]` on an empty vector. Even though the resulting pointer is never dereferenced, its still bad for two reasons: * the practical reason: it crashes with `std::out_of_range` exception in our debug build, * the ""C++ standard lawyer"" reason: its undefined behavior because, in `std::vector` implementation, it probably ""dereferences"" a null pointer, which is invalid even though it doesnt actually read the pointed memory, just converts a pointer into a reference (and then flush_job.cc converts it back to pointer); nullptr references are undefined behavior. Closes Differential Revision: D4265625 Pulled By: al13n321 fbshipit-source-id: db26fb9/Range deletion microoptimizations Summary: Made RangeDelAggregators InternalKeyComparator member a reference-to-const so we dont need to copy-construct it. Also added InternalKeyComparator to ImmutableCFOptions so we dont need to construct one for each DBIter. Made MemTable::NewRangeTombstoneIterator and the table readers NewRangeTombstoneIterator() functions return nullptr instead of NewEmptyInternalIterator to avoid the allocation. Updated callers accordingly. Closes Differential Revision: D4208169 Pulled By: ajkr fbshipit-source-id: 2fd65cf/Remove Arena in RangeDelAggregator Summary: The Arena construction/destruction introduced significant overhead to read-heavy workload just by creating empty vectors for its blocks, so avoid it in RangeDelAggregator. Closes Differential Revision: D4207781 Pulled By: ajkr fbshipit-source-id: 9d1c130/"
,,0.11199999999999999,rocksdb,Compaction::IsTrivialMove relaxing Summary: IsTrivialMove returns true if no input file overlaps with output_level+1 with more than max_compaction_bytes_ bytes. Closes Differential Revision: D4278338 Pulled By: yiwu-arbug fbshipit-source-id: 994c001/
,,0.1955,rocksdb,"Fix for 2PC causing WAL to grow too large Summary: Consider the following single column family scenario: prepare in log A commit in log B *WAL is too large, flush all CFs to releast log A* *CFA is on log B so we do not see CFA is depending on log A so no flush is requested* To fix this we must also consider the log containing the prepare section when determining what log a CF is dependent on. Closes Differential Revision: D4403265 Pulled By: reidHoruff fbshipit-source-id: ce800ff/Fix bug of Checkpoint loses recent transactions with 2PC Summary: If 2PC is enabled, checkpoint may not copy previous log files that contain uncommitted prepare records. In this diff we keep those files. Closes Differential Revision: D4368319 Pulled By: siying fbshipit-source-id: cc2c746/Add WriteOptions.no_slowdown Summary: If the WriteOptions.no_slowdown flag is set AND we need to wait or sleep for the write request, then fail immediately with Status::Incomplete(). Closes Differential Revision: D4191405 Pulled By: maysamyabandeh fbshipit-source-id: 7f3ce3f/Dynamic max_total_wal_size option Summary: Closes Differential Revision: D4176426 Pulled By: yiwu-arbug fbshipit-source-id: b57689d/"
,,0.1635,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/DeleteRange compaction statistics Summary: ""rocksdb.compaction.key.drop.range_del"" number of keys dropped during compaction due to a range tombstone covering them ""rocksdb.compaction.range_del.drop.obsolete"" number of range tombstones dropped due to compaction to bottom level and no snapshot saving them s/CompactionIteratorStats/CompactionIterationStats/g since this class is no longer specific to CompactionIterator its also updated for range tombstone iteration during compaction Move the above class into a separate .h file to avoid circular dependency. Closes Differential Revision: D4187179 Pulled By: ajkr fbshipit-source-id: 10c2103/"
,,0.0897,rocksdb,Fix repair issues Summary: Record the first parsed sequence number as the minimum so we can find the true minimum otherwise everything is larger than zero. Fix the comparator name comparision. Closes Differential Revision: D4544365 Pulled By: ajkr fbshipit-source-id: 439cbc2/
,,0.1137,rocksdb,"Flush job should release reference current version if sync log failed Summary: Fix the bug when sync log fail, FlushJob::Run() will not be execute and reference to cfd->current() will not be release. Closes Differential Revision: D4441316 Pulled By: yiwu-arbug fbshipit-source-id: 5523e28/"
,,0.121,rocksdb,Compaction::IsTrivialMove relaxing Summary: IsTrivialMove returns true if no input file overlaps with output_level+1 with more than max_compaction_bytes_ bytes. Closes Differential Revision: D4278338 Pulled By: yiwu-arbug fbshipit-source-id: 994c001/
,,0.1181,rocksdb,Dynamic max_total_wal_size option Summary: Closes Differential Revision: D4176426 Pulled By: yiwu-arbug fbshipit-source-id: b57689d/
,,0.201,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/Fixed CompactionFilter::Decision::kRemoveAndSkipUntil Summary: Embarassingly enough, the first time I tried to use my new feature in logdevice it crashed with this assertion failure: db/pinned_iterators_manager.h:30: void rocksdb::PinnedIteratorsManager::StartPinning(): Assertion `pinning_enabled false failed The issue was that `pinned_iters_mgr_.StartPinning()` was called but `pinned_iters_mgr_.ReleasePinnedData()` wasnt. Closes Differential Revision: D4265622 Pulled By: al13n321 fbshipit-source-id: 747b10f/Fix double-counted deletion stat Summary: Both the single deletion and the value are included in compaction outputs, so no need to update the stat for the values deletion yet, otherwise itd be double-counted. Closes Differential Revision: D4241181 Pulled By: ajkr fbshipit-source-id: c9aaa15/DeleteRange compaction statistics Summary: ""rocksdb.compaction.key.drop.range_del"" number of keys dropped during compaction due to a range tombstone covering them ""rocksdb.compaction.range_del.drop.obsolete"" number of range tombstones dropped due to compaction to bottom level and no snapshot saving them s/CompactionIteratorStats/CompactionIterationStats/g since this class is no longer specific to CompactionIterator its also updated for range tombstone iteration during compaction Move the above class into a separate .h file to avoid circular dependency. Closes Differential Revision: D4187179 Pulled By: ajkr fbshipit-source-id: 10c2103/"
,,0.1263,rocksdb,Compaction::IsTrivialMove relaxing Summary: IsTrivialMove returns true if no input file overlaps with output_level+1 with more than max_compaction_bytes_ bytes. Closes Differential Revision: D4278338 Pulled By: yiwu-arbug fbshipit-source-id: 994c001/
,,0.121,rocksdb,Compaction::IsTrivialMove relaxing Summary: IsTrivialMove returns true if no input file overlaps with output_level+1 with more than max_compaction_bytes_ bytes. Closes Differential Revision: D4278338 Pulled By: yiwu-arbug fbshipit-source-id: 994c001/
,,0.1779,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/"
,,0.1604,rocksdb,"fixed typo Summary: I fixed exisit exist Closes Differential Revision: D4451466 Pulled By: yiwu-arbug fbshipit-source-id: b447c3a/Add WriteOptions.no_slowdown Summary: If the WriteOptions.no_slowdown flag is set AND we need to wait or sleep for the write request, then fail immediately with Status::Incomplete(). Closes Differential Revision: D4191405 Pulled By: maysamyabandeh fbshipit-source-id: 7f3ce3f/"
,,0.0964,rocksdb,"Remove unnecessary header include Summary: Remove ""util/testharness.h"" from list of includes for ""db/db_filesnapshot.cc"", as it wasnt being used and thus caused an extraneous dependency on gtest. Closes Differential Revision: D4302146 Pulled By: yiwu-arbug fbshipit-source-id: e900c0b/"
,,0.1884,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/"
,,0.1082,rocksdb,Disable test to unblock travis build Summary: The two tests keep failing in travis. Disable them and will fix later. Closes Differential Revision: D4316389 Pulled By: yiwu-arbug fbshipit-source-id: 0a370e7/
,,0.1026,rocksdb,dynamic setting of stats_dump_period_sec through SetDBOption() Summary: Resolved the following issue: Closes Differential Revision: D4736764 Pulled By: yiwu-arbug fbshipit-source-id: 64fe0b7/
,,0.0685,rocksdb,Add missing include for `abort()` Summary: Fixes (again). Closes Differential Revision: D4625289 Pulled By: ajkr fbshipit-source-id: 70e774e/
,,0.1102,rocksdb,dynamic setting of stats_dump_period_sec through SetDBOption() Summary: Resolved the following issue: Closes Differential Revision: D4736764 Pulled By: yiwu-arbug fbshipit-source-id: 64fe0b7/
,,0.1578,rocksdb,"Blob storage pr Summary: The final pull request for Blob Storage. Closes Differential Revision: D5033189 Pulled By: yiwu-arbug fbshipit-source-id: 6356b683ccd58cbf38a1dc55e2ea400feecd5d06/fix WritableFile buffer size in direct IO Summary: ?fix the buffer size in case of ppl use buffer size as their block_size. Closes Differential Revision: D4956878 Pulled By: lightmark fbshipit-source-id: 8bb0dc9c133887aadcd625d5261a3d1110b71473/Statistic for how often rate limiter is drained Summary: This is the metric I plan to use for adaptive rate limiting. The statistics are updated only if the rate limiter is drained by flush or compaction. I believe (but am not certain) that this is the normal case. The Statistics object is passed in RateLimiter::Request() to avoid requiring changes to client code, which wouldve been necessary if we passed it in the RateLimiter constructor. Closes Differential Revision: D4646489 Pulled By: ajkr fbshipit-source-id: d8e0161/avoid direct io in rocksdb_lite Summary: fix lite bugs disable direct io in lite mode Closes Differential Revision: D4559866 Pulled By: yiwu-arbug fbshipit-source-id: 3761c51/"
,,0.1656,rocksdb,"Roundup read bytes in ReadaheadRandomAccessFile Summary: Fix alignment in ReadaheadRandomAccessFile Closes Differential Revision: D5012336 Pulled By: lightmark fbshipit-source-id: 10d2c829520cb787227ef653ef63d5d701725778/avoid ftruncate twice in buffered io Summary: in buffered io, the filesize_ is the real size. Closes Differential Revision: D4711433 Pulled By: lightmark fbshipit-source-id: ad604b9/Statistic for how often rate limiter is drained Summary: This is the metric I plan to use for adaptive rate limiting. The statistics are updated only if the rate limiter is drained by flush or compaction. I believe (but am not certain) that this is the normal case. The Statistics object is passed in RateLimiter::Request() to avoid requiring changes to client code, which wouldve been necessary if we passed it in the RateLimiter constructor. Closes Differential Revision: D4646489 Pulled By: ajkr fbshipit-source-id: d8e0161/add use_direct_io() to ReadaheadRandomAccessFile Summary: Missing this function will cause RandomAccessFileReader not doing alignment in Direct IO mode, which introduce an IOError: invalid argument. Closes Differential Revision: D4601261 Pulled By: lightmark fbshipit-source-id: c3eadf1/avoid direct io in rocksdb_lite Summary: fix lite bugs disable direct io in lite mode Closes Differential Revision: D4559866 Pulled By: yiwu-arbug fbshipit-source-id: 3761c51/"
,,0.1384,rocksdb,avoid direct io in rocksdb_lite Summary: fix lite bugs disable direct io in lite mode Closes Differential Revision: D4559866 Pulled By: yiwu-arbug fbshipit-source-id: 3761c51/
,,0.0859,rocksdb,Blob storage helper methods Summary: Split out interfaces needed for blob storage from including * CompactionEventListener and OnFlushBegin listener interfaces. * Blob filename support. Closes Differential Revision: D4905463 Pulled By: yiwu-arbug fbshipit-source-id: 564e73448f1b7a367e5e46216a521e57ea9011b5/
,,0.1004,rocksdb,Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/
,,0.1008,rocksdb,"Fix build with MinGW Summary: There still are many warnings (most of them about invalid printf format for long long), but it builds if FAIL_ON_WARNINGS is disabled. Closes Differential Revision: D4807355 Pulled By: siying fbshipit-source-id: ef03786/"
,,0.0795,rocksdb,"fix compile for VS2015 Summary: Without the cast, the build will break on Windows. Closes Differential Revision: D4690462 Pulled By: ajkr fbshipit-source-id: c493b6c/"
,,0.1683,rocksdb,Move memtable related files into memtable directory Summary: Move memtable related files into memtable directory. Closes Differential Revision: D4829242 Pulled By: yiwu-arbug fbshipit-source-id: ca70ab6/
,,0.1026,rocksdb,dynamic setting of stats_dump_period_sec through SetDBOption() Summary: Resolved the following issue: Closes Differential Revision: D4736764 Pulled By: yiwu-arbug fbshipit-source-id: 64fe0b7/
,,0.1347,rocksdb,avoid direct io in rocksdb_lite Summary: fix lite bugs disable direct io in lite mode Closes Differential Revision: D4559866 Pulled By: yiwu-arbug fbshipit-source-id: 3761c51/
,,0.2521,rocksdb,Move memtable related files into memtable directory Summary: Move memtable related files into memtable directory. Closes Differential Revision: D4829242 Pulled By: yiwu-arbug fbshipit-source-id: ca70ab6/update IterKey that can get user key and internal key explicitly Summary: to void future bug that caused by the mix of userkey/internalkey Closes Differential Revision: D4825889 Pulled By: lightmark fbshipit-source-id: 28411db/
,,0.0825,rocksdb,Blob storage helper methods Summary: Split out interfaces needed for blob storage from including * CompactionEventListener and OnFlushBegin listener interfaces. * Blob filename support. Closes Differential Revision: D4905463 Pulled By: yiwu-arbug fbshipit-source-id: 564e73448f1b7a367e5e46216a521e57ea9011b5/
,,0.1,rocksdb,support PopSavePoint for WriteBatch Summary: Try to fix Closes Differential Revision: D4907333 Pulled By: yiwu-arbug fbshipit-source-id: 417b420ff668e6c2fd0dad42a94c57385012edc5/
,,0.0987,rocksdb,Object lifetime in cache Summary: Any non-raw-data dependent object must be destructed before the table closes. There was a bug of not doing that for filter object. This patch fixes the bug and adds a unit test to prevent such bugs in future. Closes Differential Revision: D5001318 Pulled By: maysamyabandeh fbshipit-source-id: 6d8772e58765485868094b92964da82ef9730b6d/
,,0.0919,rocksdb,support PopSavePoint for WriteBatch Summary: Try to fix Closes Differential Revision: D4907333 Pulled By: yiwu-arbug fbshipit-source-id: 417b420ff668e6c2fd0dad42a94c57385012edc5/
,,0.233,rocksdb,"fix WritableFile buffer size in direct IO Summary: ?fix the buffer size in case of ppl use buffer size as their block_size. Closes Differential Revision: D4956878 Pulled By: lightmark fbshipit-source-id: 8bb0dc9c133887aadcd625d5261a3d1110b71473/Option to fail a request as incomplete when skipping too many internal keys Summary: Operations like Seek/Next/Prev sometimes take too long to complete when there are many internal keys to be skipped. Adding an option, max_skippable_internal_keys which could be used to set a threshold for the maximum number of keys that can be skipped, will help to address these cases where it is much better to fail a request (as incomplete) than to wait for a considerable time for the request to complete. This feature to fail an iterator seek request as incomplete, is disabled by default when max_skippable_internal_keys 0. It is enabled only when max_skippable_internal_keys > 0. This feature is based on the discussion mentioned in the PR Closes Differential Revision: D4753223 Pulled By: sagar0 fbshipit-source-id: 1c973f7/Remove timeout_hint_us from WriteOptions Summary: The option has been deprecated for two years and has no effect. Removing. Closes Differential Revision: D4555203 Pulled By: yiwu-arbug fbshipit-source-id: c48f627/avoid direct io in rocksdb_lite Summary: fix lite bugs disable direct io in lite mode Closes Differential Revision: D4559866 Pulled By: yiwu-arbug fbshipit-source-id: 3761c51/"
,,0.0919,rocksdb,support PopSavePoint for WriteBatch Summary: Try to fix Closes Differential Revision: D4907333 Pulled By: yiwu-arbug fbshipit-source-id: 417b420ff668e6c2fd0dad42a94c57385012edc5/
,,0.0939,rocksdb,support PopSavePoint for WriteBatch Summary: Try to fix Closes Differential Revision: D4907333 Pulled By: yiwu-arbug fbshipit-source-id: 417b420ff668e6c2fd0dad42a94c57385012edc5/
,,0.1207,rocksdb,"Fixing Solaris Sparc crash due to cached TLS Summary: Workaround for Solaris gcc binary. Program is crashing, because when TLS of perf context that is used twice on same frame, it is damaged thus Segmentation fault. Issue: Closes Differential Revision: D4922274 Pulled By: siying fbshipit-source-id: 549105ebce9a8ce08a737f4d6b9f2312ebcde9a8/Fix build with MinGW Summary: There still are many warnings (most of them about invalid printf format for long long), but it builds if FAIL_ON_WARNINGS is disabled. Closes Differential Revision: D4807355 Pulled By: siying fbshipit-source-id: ef03786/"
,,0.0792,rocksdb,Expose DB::DeleteRange and WriteBath::DeleteRange in Java Summary: Added JNI wrapper from `DeleteRange` methods Closes Differential Revision: D4657746 Pulled By: yiwu-arbug fbshipit-source-id: 3fc7ab8/
,,0.0751,rocksdb,Expose DB::DeleteRange and WriteBath::DeleteRange in Java Summary: Added JNI wrapper from `DeleteRange` methods Closes Differential Revision: D4657746 Pulled By: yiwu-arbug fbshipit-source-id: 3fc7ab8/
,,0.0772,rocksdb,Expose DB::DeleteRange and WriteBath::DeleteRange in Java Summary: Added JNI wrapper from `DeleteRange` methods Closes Differential Revision: D4657746 Pulled By: yiwu-arbug fbshipit-source-id: 3fc7ab8/
,,0.0751,rocksdb,Expose DB::DeleteRange and WriteBath::DeleteRange in Java Summary: Added JNI wrapper from `DeleteRange` methods Closes Differential Revision: D4657746 Pulled By: yiwu-arbug fbshipit-source-id: 3fc7ab8/
,,0.0895,rocksdb,Blob storage pr Summary: The final pull request for Blob Storage. Closes Differential Revision: D5033189 Pulled By: yiwu-arbug fbshipit-source-id: 6356b683ccd58cbf38a1dc55e2ea400feecd5d06/
,,0.0838,rocksdb,Fix WriteBatchWithIndex address use after scope error Summary: Fix use after scope error caught by ASAN. Closes Differential Revision: D4968028 Pulled By: yiwu-arbug fbshipit-source-id: a2a266c98634237494ab4fb2d666bc938127aeb2/
,,0.2275,rocksdb,support PopSavePoint for WriteBatch Summary: Try to fix Closes Differential Revision: D4907333 Pulled By: yiwu-arbug fbshipit-source-id: 417b420ff668e6c2fd0dad42a94c57385012edc5/Move memtable related files into memtable directory Summary: Move memtable related files into memtable directory. Closes Differential Revision: D4829242 Pulled By: yiwu-arbug fbshipit-source-id: ca70ab6/
,,0.1633,rocksdb,Move memtable related files into memtable directory Summary: Move memtable related files into memtable directory. Closes Differential Revision: D4829242 Pulled By: yiwu-arbug fbshipit-source-id: ca70ab6/
,,0.1011,rocksdb,Fix unaligned reads in read cache Summary: Fix unaligned reads in read cache by using RandomAccessFileReader Allow read cache flags in db_bench Closes Differential Revision: D4610885 Pulled By: IslamAbdelRahman fbshipit-source-id: 2aa1dc8/
,,0.2215,rocksdb,Move memtable related files into memtable directory Summary: Move memtable related files into memtable directory. Closes Differential Revision: D4829242 Pulled By: yiwu-arbug fbshipit-source-id: ca70ab6/Fix unaligned reads in read cache Summary: Fix unaligned reads in read cache by using RandomAccessFileReader Allow read cache flags in db_bench Closes Differential Revision: D4610885 Pulled By: IslamAbdelRahman fbshipit-source-id: 2aa1dc8/
,,0.0857,rocksdb,Blob storage pr Summary: The final pull request for Blob Storage. Closes Differential Revision: D5033189 Pulled By: yiwu-arbug fbshipit-source-id: 6356b683ccd58cbf38a1dc55e2ea400feecd5d06/
,,0.0876,rocksdb,Blob storage pr Summary: The final pull request for Blob Storage. Closes Differential Revision: D5033189 Pulled By: yiwu-arbug fbshipit-source-id: 6356b683ccd58cbf38a1dc55e2ea400feecd5d06/
,,0.09699999999999999,rocksdb,Object lifetime in cache Summary: Any non-raw-data dependent object must be destructed before the table closes. There was a bug of not doing that for filter object. This patch fixes the bug and adds a unit test to prevent such bugs in future. Closes Differential Revision: D5001318 Pulled By: maysamyabandeh fbshipit-source-id: 6d8772e58765485868094b92964da82ef9730b6d/
,,0.1332,rocksdb,update IterKey that can get user key and internal key explicitly Summary: to void future bug that caused by the mix of userkey/internalkey Closes Differential Revision: D4825889 Pulled By: lightmark fbshipit-source-id: 28411db/
,,0.1279,rocksdb,update IterKey that can get user key and internal key explicitly Summary: to void future bug that caused by the mix of userkey/internalkey Closes Differential Revision: D4825889 Pulled By: lightmark fbshipit-source-id: 28411db/
,,0.0807,rocksdb,Blob storage helper methods Summary: Split out interfaces needed for blob storage from including * CompactionEventListener and OnFlushBegin listener interfaces. * Blob filename support. Closes Differential Revision: D4905463 Pulled By: yiwu-arbug fbshipit-source-id: 564e73448f1b7a367e5e46216a521e57ea9011b5/
,,0.0807,rocksdb,Blob storage helper methods Summary: Split out interfaces needed for blob storage from including * CompactionEventListener and OnFlushBegin listener interfaces. * Blob filename support. Closes Differential Revision: D4905463 Pulled By: yiwu-arbug fbshipit-source-id: 564e73448f1b7a367e5e46216a521e57ea9011b5/
,,0.1544,rocksdb,"Option to fail a request as incomplete when skipping too many internal keys Summary: Operations like Seek/Next/Prev sometimes take too long to complete when there are many internal keys to be skipped. Adding an option, max_skippable_internal_keys which could be used to set a threshold for the maximum number of keys that can be skipped, will help to address these cases where it is much better to fail a request (as incomplete) than to wait for a considerable time for the request to complete. This feature to fail an iterator seek request as incomplete, is disabled by default when max_skippable_internal_keys 0. It is enabled only when max_skippable_internal_keys > 0. This feature is based on the discussion mentioned in the PR Closes Differential Revision: D4753223 Pulled By: sagar0 fbshipit-source-id: 1c973f7/Reset DBIter::saved_key_ with proper user key anywhere before pass to DBIter::FindNextUserEntry Summary: fix db_iter bug introduced by [facebook#1413]( Closes Differential Revision: D4672369 Pulled By: lightmark fbshipit-source-id: 6a22953/"
,,0.1243,rocksdb,update IterKey that can get user key and internal key explicitly Summary: to void future bug that caused by the mix of userkey/internalkey Closes Differential Revision: D4825889 Pulled By: lightmark fbshipit-source-id: 28411db/
,,0.1899,rocksdb,Move memtable related files into memtable directory Summary: Move memtable related files into memtable directory. Closes Differential Revision: D4829242 Pulled By: yiwu-arbug fbshipit-source-id: ca70ab6/
,,0.1367,rocksdb,update IterKey that can get user key and internal key explicitly Summary: to void future bug that caused by the mix of userkey/internalkey Closes Differential Revision: D4825889 Pulled By: lightmark fbshipit-source-id: 28411db/
,,0.1367,rocksdb,update IterKey that can get user key and internal key explicitly Summary: to void future bug that caused by the mix of userkey/internalkey Closes Differential Revision: D4825889 Pulled By: lightmark fbshipit-source-id: 28411db/
,,0.1872,rocksdb,Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/avoid direct io in rocksdb_lite Summary: fix lite bugs disable direct io in lite mode Closes Differential Revision: D4559866 Pulled By: yiwu-arbug fbshipit-source-id: 3761c51/
,,0.102,rocksdb,support PopSavePoint for WriteBatch Summary: Try to fix Closes Differential Revision: D4907333 Pulled By: yiwu-arbug fbshipit-source-id: 417b420ff668e6c2fd0dad42a94c57385012edc5/
,,0.1199,rocksdb,Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/
,,0.1243,rocksdb,update IterKey that can get user key and internal key explicitly Summary: to void future bug that caused by the mix of userkey/internalkey Closes Differential Revision: D4825889 Pulled By: lightmark fbshipit-source-id: 28411db/
,,0.14800000000000002,rocksdb,"Option to fail a request as incomplete when skipping too many internal keys Summary: Operations like Seek/Next/Prev sometimes take too long to complete when there are many internal keys to be skipped. Adding an option, max_skippable_internal_keys which could be used to set a threshold for the maximum number of keys that can be skipped, will help to address these cases where it is much better to fail a request (as incomplete) than to wait for a considerable time for the request to complete. This feature to fail an iterator seek request as incomplete, is disabled by default when max_skippable_internal_keys 0. It is enabled only when max_skippable_internal_keys > 0. This feature is based on the discussion mentioned in the PR Closes Differential Revision: D4753223 Pulled By: sagar0 fbshipit-source-id: 1c973f7/"
,,0.1706,rocksdb,"avoid direct io in rocksdb_lite Summary: fix lite bugs disable direct io in lite mode Closes Differential Revision: D4559866 Pulled By: yiwu-arbug fbshipit-source-id: 3761c51/Fix repair_test on ROCKSDB_LITE Summary: RepairDB isnt included in rocksdb lite, so dont test it. Closes Differential Revision: D4565094 Pulled By: ajkr fbshipit-source-id: 8cc0898/"
,,0.114,rocksdb,Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/
,,0.1799,rocksdb,Move memtable related files into memtable directory Summary: Move memtable related files into memtable directory. Closes Differential Revision: D4829242 Pulled By: yiwu-arbug fbshipit-source-id: ca70ab6/
,,0.2229,rocksdb,"Max open files mutable Summary: Makes max_open_files db option dynamically set-able by SetDBOptions. During the call of SetDBOptions we call SetCapacity on the table cache, which is a LRUCache. Closes Differential Revision: D4979189 Pulled By: yiwu-arbug fbshipit-source-id: ca7e8dc5e3619c79434f579be4847c0f7e56afda/dynamic setting of stats_dump_period_sec through SetDBOption() Summary: Resolved the following issue: Closes Differential Revision: D4736764 Pulled By: yiwu-arbug fbshipit-source-id: 64fe0b7/Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/"
,,0.1179,rocksdb,Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/
,,0.1179,rocksdb,Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/
,,0.1847,rocksdb,Blob storage pr Summary: The final pull request for Blob Storage. Closes Differential Revision: D5033189 Pulled By: yiwu-arbug fbshipit-source-id: 6356b683ccd58cbf38a1dc55e2ea400feecd5d06/set compaction_iterator earliest_snapshot to max if no snapshot Summary: It is a potential bug that will be triggered if we ingest files before inserting the first key into an empty db. 0 is a special value reserved to indicate the concept of non-existence. But not good for seqno in this case because 0 is a valid seqno for ingestion(bulk loading) Closes Differential Revision: D4919827 Pulled By: lightmark fbshipit-source-id: 237eea40f88bd6487b66806109d90065dc02c362/Hide event listeners from lite build Summary: Fixing lite build failure introduce by Closes Reviewed By: sagar0 Differential Revision: D4910619 Pulled By: yiwu-arbug fbshipit-source-id: 5213b7b7431cc258688793c8c28153025588d8d9/Blob storage helper methods Summary: Split out interfaces needed for blob storage from including * CompactionEventListener and OnFlushBegin listener interfaces. * Blob filename support. Closes Differential Revision: D4905463 Pulled By: yiwu-arbug fbshipit-source-id: 564e73448f1b7a367e5e46216a521e57ea9011b5/update IterKey that can get user key and internal key explicitly Summary: to void future bug that caused by the mix of userkey/internalkey Closes Differential Revision: D4825889 Pulled By: lightmark fbshipit-source-id: 28411db/
,,0.1103,rocksdb,"Make db_wal_test slightly faster Summary: Avoid to run db_wal_test in all the DB test options, and some small changes. Closes Differential Revision: D4622054 Pulled By: siying fbshipit-source-id: 890fd64/"
,,0.1101,rocksdb,Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/
,,0.0877,rocksdb,Blob storage helper methods Summary: Split out interfaces needed for blob storage from including * CompactionEventListener and OnFlushBegin listener interfaces. * Blob filename support. Closes Differential Revision: D4905463 Pulled By: yiwu-arbug fbshipit-source-id: 564e73448f1b7a367e5e46216a521e57ea9011b5/
,,0.158,rocksdb,"Avoid calling fallocate with UINT64_MAX Summary: When user doesnt set a limit on compaction output file size, lets use the sum of the input files sizes. This will avoid passing UINT64_MAX as fallocate()s length. Reported in Test setup: command: `TEST_TMPDIR=/data/rocksdb-test/ strace fallocate ./db_compaction_test filesystem: xfs before this diff: `fallocate(10, 01, 0, 1844674407370955160) ENOSPC (No space left on device)` after this diff: `fallocate(10, 01, 0, 1977) 0` Closes Differential Revision: D5007275 Pulled By: ajkr fbshipit-source-id: 4491404a6ae8a41328aede2e2d6f4d9ac3e38880/Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/Make db_wal_test slightly faster Summary: Avoid to run db_wal_test in all the DB test options, and some small changes. Closes Differential Revision: D4622054 Pulled By: siying fbshipit-source-id: 890fd64/"
,,0.1043,rocksdb,Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/
,,0.1314,rocksdb,update IterKey that can get user key and internal key explicitly Summary: to void future bug that caused by the mix of userkey/internalkey Closes Differential Revision: D4825889 Pulled By: lightmark fbshipit-source-id: 28411db/
,,0.114,rocksdb,Fix some bugs in MockEnv Summary: Fixing some bugs in MockEnv so it be actually used. Closes Differential Revision: D4609923 Pulled By: maysamyabandeh fbshipit-source-id: ca25735/
,,0.16,rocksdb,Move memtable related files into memtable directory Summary: Move memtable related files into memtable directory. Closes Differential Revision: D4829242 Pulled By: yiwu-arbug fbshipit-source-id: ca70ab6/
,,0.09699999999999999,rocksdb,"Fix build with MinGW Summary: There still are many warnings (most of them about invalid printf format for long long), but it builds if FAIL_ON_WARNINGS is disabled. Closes Differential Revision: D4807355 Pulled By: siying fbshipit-source-id: ef03786/"
,,0.0932,rocksdb,"Fix build with MinGW Summary: There still are many warnings (most of them about invalid printf format for long long), but it builds if FAIL_ON_WARNINGS is disabled. Closes Differential Revision: D4807355 Pulled By: siying fbshipit-source-id: ef03786/"
,,0.0932,rocksdb,"Fix build with MinGW Summary: There still are many warnings (most of them about invalid printf format for long long), but it builds if FAIL_ON_WARNINGS is disabled. Closes Differential Revision: D4807355 Pulled By: siying fbshipit-source-id: ef03786/"
,,0.0989,rocksdb,"Fix build with MinGW Summary: There still are many warnings (most of them about invalid printf format for long long), but it builds if FAIL_ON_WARNINGS is disabled. Closes Differential Revision: D4807355 Pulled By: siying fbshipit-source-id: ef03786/"
,,0.0932,rocksdb,"Fix build with MinGW Summary: There still are many warnings (most of them about invalid printf format for long long), but it builds if FAIL_ON_WARNINGS is disabled. Closes Differential Revision: D4807355 Pulled By: siying fbshipit-source-id: ef03786/"
,,0.0944,rocksdb,"Downgrade option sanitiy check level for prefix_extractor Summary: With c7004840d2f4ad5fc1bdce042902b822492f3a0e, its safe to open a DB with different prefix extractor. So its safe to skip prefix extractor check. Closes Differential Revision: D5294700 Pulled By: siying fbshipit-source-id: eeb500da795eecb29b8c9c56a14cfd4afda12ecc/"
,,0.1087,rocksdb,"add VerifyChecksum() to db.h Summary: We need a tool to check any sst file corruption in the db. It will check all the sst files in current version and read all the blocks (data, meta, index) with checksum verification. If any verification fails, the function will return non-OK status. Closes Differential Revision: D5324269 Pulled By: lightmark fbshipit-source-id: 6f8a272008b722402a772acfc804524c9d1a483b/"
,,0.0891,rocksdb,fixed typo Summary: fixed exisitng existing Closes Differential Revision: D5070169 Pulled By: yiwu-arbug fbshipit-source-id: 8c8450acf50757b767cf78b78314018395738d96/
,,0.1161,rocksdb,fixed typo Summary: fixed typo Closes Differential Revision: D5183630 Pulled By: ajkr fbshipit-source-id: 133cfd0445959e70aa2cd1a12151bf3c0c5c3ac5/
,,0.0809,rocksdb,"Do not run RateLimiterTest.Rate test on Travis+Mac OSX. Summary: RateLimiterTest.Rate test has been failing continuously since many days on travis in Mac OSX PLATFORM_DEPENDENT test suite. Check Disabling this test for now, so that we can investigate more in depth. Closes Differential Revision: D5250147 Pulled By: sagar0 fbshipit-source-id: d58476a3c2792d20e875754d1516c4bc7174e86c/"
,,0.1641,rocksdb,"Fix undefined behavior in Hash Summary: Instead of ignoring UBSan checks, fix the negative shifts in Hash(). Also add test to make sure the hash values are stable over time. The values were computed before this change, so the test also verifies the correctness of the change. Closes Differential Revision: D5386369 Pulled By: yiwu-arbug fbshipit-source-id: 6de4b44461a544d6222cc5d72d8cda2c0373d17e/Avoid unsupported attributes when not building with UBSAN Summary: yiwu-arbug see individual commits. Closes Differential Revision: D5141520 Pulled By: yiwu-arbug fbshipit-source-id: 7987c92ab4461eef36afce5a133d3a0ee0c96300/"
,,0.1437,rocksdb,Avoid unsupported attributes when not building with UBSAN Summary: yiwu-arbug see individual commits. Closes Differential Revision: D5141520 Pulled By: yiwu-arbug fbshipit-source-id: 7987c92ab4461eef36afce5a133d3a0ee0c96300/
,,0.0859,rocksdb,fix valgrind init complaint Summary: Closes Differential Revision: D5386307 Pulled By: maysamyabandeh fbshipit-source-id: 3032c95c54755053b6450765ec4dacbecb734f9d/
,,0.0751,rocksdb,fixed typo Summary: fixed typo Closes Differential Revision: D5242471 Pulled By: IslamAbdelRahman fbshipit-source-id: 832eb3a4c70221444ccd2ae63217823fec56c748/
,,0.122,rocksdb,fixed typo Summary: fixed typo Closes Differential Revision: D5183630 Pulled By: ajkr fbshipit-source-id: 133cfd0445959e70aa2cd1a12151bf3c0c5c3ac5/
,,0.0838,rocksdb,Address MS Visual Studio 2017 issue with autovector Summary: This addresses Closes Differential Revision: D5097941 Pulled By: siying fbshipit-source-id: fb33582bfe7883ecc3f6da028703982522b5f75f/
,,0.1161,rocksdb,Fixed some spelling mistakes Summary: Closes Differential Revision: D5079601 Pulled By: sagar0 fbshipit-source-id: ae5696fd735718f544435c64c3179c49b8c04349/
,,0.1081,rocksdb,Fixed some spelling mistakes Summary: Closes Differential Revision: D5079601 Pulled By: sagar0 fbshipit-source-id: ae5696fd735718f544435c64c3179c49b8c04349/
,,0.0807,rocksdb,fixed typo Summary: fixed typo Closes Differential Revision: D5079631 Pulled By: sagar0 fbshipit-source-id: e4c8d1d89b244ee69e9dea1dd013227cc5241026/
,,0.0833,rocksdb,Fix license headers in Cassandra related files Summary: I might have missed these while doing some recent cassandra code reviews. Closes Differential Revision: D5520138 Pulled By: sagar0 fbshipit-source-id: 340930afe9efe03c75f535a1da1f89bd3e53c1f9/
,,0.0778,rocksdb,Fix valgrind complaint about initialization Summary: Closes Differential Revision: D5573894 Pulled By: maysamyabandeh fbshipit-source-id: 8fc03ea8ea6f3f3bc0f68b64cf90243a70562dc4/
,,0.12,rocksdb,Fixed some spelling mistakes Summary: Closes Differential Revision: D5079601 Pulled By: sagar0 fbshipit-source-id: ae5696fd735718f544435c64c3179c49b8c04349/
,,0.1161,rocksdb,Fixed some spelling mistakes Summary: Closes Differential Revision: D5079601 Pulled By: sagar0 fbshipit-source-id: ae5696fd735718f544435c64c3179c49b8c04349/
,,0.1121,rocksdb,fixed typo Summary: fixed typo Closes Differential Revision: D5183630 Pulled By: ajkr fbshipit-source-id: 133cfd0445959e70aa2cd1a12151bf3c0c5c3ac5/
,,0.1346,rocksdb,"fixed typo Summary: fixed typo Closes Differential Revision: D5183630 Pulled By: ajkr fbshipit-source-id: 133cfd0445959e70aa2cd1a12151bf3c0c5c3ac5/Replace deprecated RocksDB#addFile with RocksDB#ingestExternalFile Summary: Previously the Java implementation of `RocksDB#addFile` was both incomplete and not inline with the C++ API. Rather than fix it, as I see that `rocksdb::DB::AddFile` is now deprecated in favour of `rocksdb::DB::IngestExternalFile`, I have removed the old broken implementation and implemented `RocksDB#ingestExternalFile`. Closes Closes Differential Revision: D5061264 Pulled By: sagar0 fbshipit-source-id: 85df0899fa1b1fc3535175cac4f52353511d4104/"
,,0.12,rocksdb,fixed typo Summary: fixed typo Closes Differential Revision: D5183630 Pulled By: ajkr fbshipit-source-id: 133cfd0445959e70aa2cd1a12151bf3c0c5c3ac5/
,,0.1062,rocksdb,Fix the Java build which was broken by a4d9c02 Summary: Closes Differential Revision: D5181091 Pulled By: ajkr fbshipit-source-id: fd72525da4fb1d50143080a210f8d824cbb968d6/
,,0.0782,rocksdb,"Fix statistics in RocksJava sample Summary: I observed while doing a `make jtest` that the java sample was broken, due to the changes in . Closes Differential Revision: D5539807 Pulled By: sagar0 fbshipit-source-id: 2c7e9d84778099dfa1c611996b444efe3c9fd466/"
,,0.1011,rocksdb,"Fix jni WriteBatchThreadedTest Summary: WriteBatchThreadedTest is failing, at least on Mac. The problem seems to be `wb` is getting GC before we finish write. Explicitly close it seems to fix it. Closes Differential Revision: D5307379 Pulled By: yiwu-arbug fbshipit-source-id: 8ff7f8170451078c941951f5aafae83afffb7933/"
,,0.0792,rocksdb,fixed typo Summary: fixed typo Closes Differential Revision: D5242471 Pulled By: IslamAbdelRahman fbshipit-source-id: 832eb3a4c70221444ccd2ae63217823fec56c748/
,,0.0958,rocksdb,Improve the error message for I/O related errors. Summary: Force people to write something other than file name while returning status for IOError. Closes Differential Revision: D5321309 Pulled By: siying fbshipit-source-id: 38bcf6c19e80831cd3e300a047e975cbb131d822/
,,0.142,rocksdb,Avoid unsupported attributes when not building with UBSAN Summary: yiwu-arbug see individual commits. Closes Differential Revision: D5141520 Pulled By: yiwu-arbug fbshipit-source-id: 7987c92ab4461eef36afce5a133d3a0ee0c96300/
,,0.1161,rocksdb,Fixed some spelling mistakes Summary: Closes Differential Revision: D5079601 Pulled By: sagar0 fbshipit-source-id: ae5696fd735718f544435c64c3179c49b8c04349/
,,0.1286,rocksdb,"block_cache_tier: fix gcc-7 warnings Summary: Error was: utilities/persistent_cache/block_cache_tier.cc: In instantiation of Ã«void rocksdb::Add(std::map<std::__cxx11::basic_string<char>, double>*, const string&, const T&) [with T double; std::__cxx11::string std::__cxx11::basic_string<char>]Ã­: utilities/persistent_cache/block_cache_tier.cc:147:40: required from here utilities/persistent_cache/block_cache_tier.cc:141:23: error: type qualifiers ignored on cast result type [-Werror=ignored-qualifiers] stats->insert({key, static_cast<const double>(t)}); fixing like Closes Differential Revision: D5600910 Pulled By: yiwu-arbug fbshipit-source-id: 891a5ec7e451d2dec6ad1b6b7fac545657f87363/"
,,0.1393,rocksdb,Fix data races caught by tsan Summary: This fixes the tsan build failures in: write_callback_test persistent_cache_test.* Closes Differential Revision: D5101190 Pulled By: sagar0 fbshipit-source-id: 537e19ed05272b1f34cfbf793aa822b2264a1643/
,,0.1339,rocksdb,Fix data races caught by tsan Summary: This fixes the tsan build failures in: write_callback_test persistent_cache_test.* Closes Differential Revision: D5101190 Pulled By: sagar0 fbshipit-source-id: 537e19ed05272b1f34cfbf793aa822b2264a1643/
,,0.1689,rocksdb,"Update all blob db TTL and timestamps to uint64_t Summary: The current blob db implementation use mix of int32_t, uint32_t and uint64_t for TTL and expiration. Update all timestamps to uint64_t for consistency. Closes Differential Revision: D5557103 Pulled By: yiwu-arbug fbshipit-source-id: e4eab2691629a755e614e8cf1eed9c3a681d0c42/Simple blob file dumper Summary: A simple blob file dumper. Closes Differential Revision: D5097553 Pulled By: yiwu-arbug fbshipit-source-id: c6e00d949fcd3658f9f68da9352f06339fac418d/"
,,0.23600000000000002,rocksdb,"Dump Blob DB options to info log Summary: * Dump blob db options to info log * Remove BlobDBOptionsImpl to disallow dynamic cast *BlobDBOptions into *BlobDBOptionsImpl. Move options there to be constants or into BlobDBOptions. The dynamic cast is broken after * Change some of the default options * Remove blob_db_options.min_blob_size, which is unimplemented. Will implement it soon. Closes Differential Revision: D5529912 Pulled By: yiwu-arbug fbshipit-source-id: dcd58ca981db5bcc7f123b65a0d6f6ae0dc703c7/Update blob_db_test Summary: Im trying to improve unit test of blob db. Im rewriting blob db test. In this patch: * Rewrite tests of basic put/write/delete operations. * Add disable_background_tasks to BlobDBOptionsImpl to allow me not running any background job for basic unit tests. * Move DestroyBlobDB out from BlobDBImpl to be a standalone function. * Remove all garbage collection related tests. Will rewrite them in following patch. * Disabled compression test since it is failing. Will fix in a followup patch. Closes Differential Revision: D5243306 Pulled By: yiwu-arbug fbshipit-source-id: 157c71ad3b699307cb88baa3830e9b6e74f8e939/"
,,0.2083,rocksdb,"Dump Blob DB options to info log Summary: * Dump blob db options to info log * Remove BlobDBOptionsImpl to disallow dynamic cast *BlobDBOptions into *BlobDBOptionsImpl. Move options there to be constants or into BlobDBOptions. The dynamic cast is broken after * Change some of the default options * Remove blob_db_options.min_blob_size, which is unimplemented. Will implement it soon. Closes Differential Revision: D5529912 Pulled By: yiwu-arbug fbshipit-source-id: dcd58ca981db5bcc7f123b65a0d6f6ae0dc703c7/Blob DB TTL extractor Summary: Introducing blob_db::TTLExtractor to replace extract_ttl_fn. The TTL extractor can be use to extract TTL from keys insert with Put or WriteBatch. Change over existing extract_ttl_fn are: * If value is changed, it will be return via std::string* (rather than Slice*). With Slice* the new value has to be part of the existing value. With std::string* the limitation is removed. * It can optionally return TTL or expiration. Other changes in this PR: * replace `std::chrono::system_clock` with `Env::NowMicros` so that I can mock time in tests. * add several TTL tests. * other minor naming change. Closes Differential Revision: D5512627 Pulled By: yiwu-arbug fbshipit-source-id: 0dfcb00d74d060b8534c6130c808e4d5d0a54440/"
,,0.1102,rocksdb,"Update all blob db TTL and timestamps to uint64_t Summary: The current blob db implementation use mix of int32_t, uint32_t and uint64_t for TTL and expiration. Update all timestamps to uint64_t for consistency. Closes Differential Revision: D5557103 Pulled By: yiwu-arbug fbshipit-source-id: e4eab2691629a755e614e8cf1eed9c3a681d0c42/"
,,0.2258,rocksdb,"Dump Blob DB options to info log Summary: * Dump blob db options to info log * Remove BlobDBOptionsImpl to disallow dynamic cast *BlobDBOptions into *BlobDBOptionsImpl. Move options there to be constants or into BlobDBOptions. The dynamic cast is broken after * Change some of the default options * Remove blob_db_options.min_blob_size, which is unimplemented. Will implement it soon. Closes Differential Revision: D5529912 Pulled By: yiwu-arbug fbshipit-source-id: dcd58ca981db5bcc7f123b65a0d6f6ae0dc703c7/Update blob_db_test Summary: Im trying to improve unit test of blob db. Im rewriting blob db test. In this patch: * Rewrite tests of basic put/write/delete operations. * Add disable_background_tasks to BlobDBOptionsImpl to allow me not running any background job for basic unit tests. * Move DestroyBlobDB out from BlobDBImpl to be a standalone function. * Remove all garbage collection related tests. Will rewrite them in following patch. * Disabled compression test since it is failing. Will fix in a followup patch. Closes Differential Revision: D5243306 Pulled By: yiwu-arbug fbshipit-source-id: 157c71ad3b699307cb88baa3830e9b6e74f8e939/"
,,0.1864,rocksdb,"Update all blob db TTL and timestamps to uint64_t Summary: The current blob db implementation use mix of int32_t, uint32_t and uint64_t for TTL and expiration. Update all timestamps to uint64_t for consistency. Closes Differential Revision: D5557103 Pulled By: yiwu-arbug fbshipit-source-id: e4eab2691629a755e614e8cf1eed9c3a681d0c42/Blob DB TTL extractor Summary: Introducing blob_db::TTLExtractor to replace extract_ttl_fn. The TTL extractor can be use to extract TTL from keys insert with Put or WriteBatch. Change over existing extract_ttl_fn are: * If value is changed, it will be return via std::string* (rather than Slice*). With Slice* the new value has to be part of the existing value. With std::string* the limitation is removed. * It can optionally return TTL or expiration. Other changes in this PR: * replace `std::chrono::system_clock` with `Env::NowMicros` so that I can mock time in tests. * add several TTL tests. * other minor naming change. Closes Differential Revision: D5512627 Pulled By: yiwu-arbug fbshipit-source-id: 0dfcb00d74d060b8534c6130c808e4d5d0a54440/Simple blob file dumper Summary: A simple blob file dumper. Closes Differential Revision: D5097553 Pulled By: yiwu-arbug fbshipit-source-id: c6e00d949fcd3658f9f68da9352f06339fac418d/"
,,0.2546,rocksdb,"Fix blob DB transaction usage while GC Summary: While GC, blob DB use optimistic transaction to delete or replace the index entry in LSM, to guarantee correctness if theres a normal write writing to the same key. However, the previous implementation doesnt call SetSnapshot() nor use GetForUpdate() of transaction API, instead it do its own sequence number checking before beginning the transaction. A normal write can sneak in after the sequence number check and overwrite the key, and the GC will delete or relocate the old version of the key by mistake. Update the code to property use GetForUpdate() to check the existing index entry. After the patch the sequence number store with each blob record is useless, So Im considering remove the sequence number from blob record, in another patch. Closes Differential Revision: D5589178 Pulled By: yiwu-arbug fbshipit-source-id: 8dc960cd5f4e61b36024ba7c32d05584ce149c24/Update all blob db TTL and timestamps to uint64_t Summary: The current blob db implementation use mix of int32_t, uint32_t and uint64_t for TTL and expiration. Update all timestamps to uint64_t for consistency. Closes Differential Revision: D5557103 Pulled By: yiwu-arbug fbshipit-source-id: e4eab2691629a755e614e8cf1eed9c3a681d0c42/Allow concurrent writes to blob db Summary: Im going with brute-force solution, just letting Put() and Write() holding a mutex before writing. May improve concurrent writing with finer granularity locking later. Closes Differential Revision: D5552690 Pulled By: yiwu-arbug fbshipit-source-id: 039abd675b5d274a7af6428198d1733cafecef4c/Dump Blob DB options to info log Summary: * Dump blob db options to info log * Remove BlobDBOptionsImpl to disallow dynamic cast *BlobDBOptions into *BlobDBOptionsImpl. Move options there to be constants or into BlobDBOptions. The dynamic cast is broken after * Change some of the default options * Remove blob_db_options.min_blob_size, which is unimplemented. Will implement it soon. Closes Differential Revision: D5529912 Pulled By: yiwu-arbug fbshipit-source-id: dcd58ca981db5bcc7f123b65a0d6f6ae0dc703c7/Blob DB TTL extractor Summary: Introducing blob_db::TTLExtractor to replace extract_ttl_fn. The TTL extractor can be use to extract TTL from keys insert with Put or WriteBatch. Change over existing extract_ttl_fn are: * If value is changed, it will be return via std::string* (rather than Slice*). With Slice* the new value has to be part of the existing value. With std::string* the limitation is removed. * It can optionally return TTL or expiration. Other changes in this PR: * replace `std::chrono::system_clock` with `Env::NowMicros` so that I can mock time in tests. * add several TTL tests. * other minor naming change. Closes Differential Revision: D5512627 Pulled By: yiwu-arbug fbshipit-source-id: 0dfcb00d74d060b8534c6130c808e4d5d0a54440/Fix BlobDB::Get which only get out the value offset Summary: Blob db use StackableDB::get which only get out the value offset, but not the value. Fix by making BlobDB::Get override the designated getter. Closes Differential Revision: D5396823 Pulled By: yiwu-arbug fbshipit-source-id: 5a7d1cf77ee44490f836a6537225955382296878/Fix blob db compression bug Summary: `CompressBlock()` will return the uncompressed slice (i.e. `Slice(value_unc)`) if compression ratio is not good enough. This is undesired. We need to always assign the compressed slice to `value`. Closes Differential Revision: D5244682 Pulled By: yiwu-arbug fbshipit-source-id: 6989dd8852c9622822ba9acec9beea02007dff09/Update blob_db_test Summary: Im trying to improve unit test of blob db. Im rewriting blob db test. In this patch: * Rewrite tests of basic put/write/delete operations. * Add disable_background_tasks to BlobDBOptionsImpl to allow me not running any background job for basic unit tests. * Move DestroyBlobDB out from BlobDBImpl to be a standalone function. * Remove all garbage collection related tests. Will rewrite them in following patch. * Disabled compression test since it is failing. Will fix in a followup patch. Closes Differential Revision: D5243306 Pulled By: yiwu-arbug fbshipit-source-id: 157c71ad3b699307cb88baa3830e9b6e74f8e939/Fixing blob db sequence number handling Summary: Blob db rely on base db returning sequence number through write batch after DB::Write(). However after recent changes to the write path, DB::Writ()e no longer return sequence number in some cases. Fixing it by have WriteBatchInternal::InsertInto() always encode sequence number into write batch. Stacking on Closes Differential Revision: D5148358 Pulled By: yiwu-arbug fbshipit-source-id: 8bda0aa07b9334ed03ed381548b39d167dc20c33/"
,,0.1138,rocksdb,"Update all blob db TTL and timestamps to uint64_t Summary: The current blob db implementation use mix of int32_t, uint32_t and uint64_t for TTL and expiration. Update all timestamps to uint64_t for consistency. Closes Differential Revision: D5557103 Pulled By: yiwu-arbug fbshipit-source-id: e4eab2691629a755e614e8cf1eed9c3a681d0c42/"
,,0.2981,rocksdb,"Update all blob db TTL and timestamps to uint64_t Summary: The current blob db implementation use mix of int32_t, uint32_t and uint64_t for TTL and expiration. Update all timestamps to uint64_t for consistency. Closes Differential Revision: D5557103 Pulled By: yiwu-arbug fbshipit-source-id: e4eab2691629a755e614e8cf1eed9c3a681d0c42/Dump Blob DB options to info log Summary: * Dump blob db options to info log * Remove BlobDBOptionsImpl to disallow dynamic cast *BlobDBOptions into *BlobDBOptionsImpl. Move options there to be constants or into BlobDBOptions. The dynamic cast is broken after * Change some of the default options * Remove blob_db_options.min_blob_size, which is unimplemented. Will implement it soon. Closes Differential Revision: D5529912 Pulled By: yiwu-arbug fbshipit-source-id: dcd58ca981db5bcc7f123b65a0d6f6ae0dc703c7/Move blob_db/ttl_extractor.h into blob_db/blob_db.h Summary: Move blob_db/ttl_extractor.h into blob_db/blob_db.h Also exclude TTLExtractor from LITE build. Closes Differential Revision: D5520009 Pulled By: yiwu-arbug fbshipit-source-id: 4813dcc272c7cc4bf2cdac285256d9a17d78c7b7/Blob DB TTL extractor Summary: Introducing blob_db::TTLExtractor to replace extract_ttl_fn. The TTL extractor can be use to extract TTL from keys insert with Put or WriteBatch. Change over existing extract_ttl_fn are: * If value is changed, it will be return via std::string* (rather than Slice*). With Slice* the new value has to be part of the existing value. With std::string* the limitation is removed. * It can optionally return TTL or expiration. Other changes in this PR: * replace `std::chrono::system_clock` with `Env::NowMicros` so that I can mock time in tests. * add several TTL tests. * other minor naming change. Closes Differential Revision: D5512627 Pulled By: yiwu-arbug fbshipit-source-id: 0dfcb00d74d060b8534c6130c808e4d5d0a54440/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Fix BlobDB::Get which only get out the value offset Summary: Blob db use StackableDB::get which only get out the value offset, but not the value. Fix by making BlobDB::Get override the designated getter. Closes Differential Revision: D5396823 Pulled By: yiwu-arbug fbshipit-source-id: 5a7d1cf77ee44490f836a6537225955382296878/Update blob_db_test Summary: Im trying to improve unit test of blob db. Im rewriting blob db test. In this patch: * Rewrite tests of basic put/write/delete operations. * Add disable_background_tasks to BlobDBOptionsImpl to allow me not running any background job for basic unit tests. * Move DestroyBlobDB out from BlobDBImpl to be a standalone function. * Remove all garbage collection related tests. Will rewrite them in following patch. * Disabled compression test since it is failing. Will fix in a followup patch. Closes Differential Revision: D5243306 Pulled By: yiwu-arbug fbshipit-source-id: 157c71ad3b699307cb88baa3830e9b6e74f8e939/"
,,0.1681,rocksdb,"Fix LITE unit tests Summary: Closes Differential Revision: D5505778 Pulled By: siying fbshipit-source-id: 7e935603ede3d958ea087ed6b8cfc4121e8797bc/Fix Windows build broken by 5c97a7c0664d4071768113814e9ba71fe87e18cf Summary: A typo conversion fails Windows build. Fix it. Closes Differential Revision: D5325962 Pulled By: siying fbshipit-source-id: 2cefdafc9afbc85f856f403af7c876b622400630/Unit Tests for sync, range sync and file close failures Summary: Closes Differential Revision: D5255320 Pulled By: siying fbshipit-source-id: 0080830fa8eb5da6de25e17ba68aee91018c7913/"
,,0.1013,rocksdb,Fix the reported asan issues Summary: This is to resolve the asan complains. In the meanwhile I am working on clarifying/revisiting the sync rules. Closes Differential Revision: D5338660 Pulled By: yiwu-arbug fbshipit-source-id: ce6f6e0826d43a2c0bfa4328a00c78f73cd6498a/
,,0.1497,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Avoid unsupported attributes when not building with UBSAN Summary: yiwu-arbug see individual commits. Closes Differential Revision: D5141520 Pulled By: yiwu-arbug fbshipit-source-id: 7987c92ab4461eef36afce5a133d3a0ee0c96300/Suppress clang-analyzer false positive Summary: Fixing two types of clang-analyzer false positives: * db is deleted and then reopen, and clang-analyzer thinks we are reusing the pointer after it has been deleted. Adding asserts to hint clang-analyzer the pointer is recreated. * ParsedInternalKey is (intentionally) uninitialized. Initialize the struct only when clang-analyzer is running. Closes Differential Revision: D5093801 Pulled By: yiwu-arbug fbshipit-source-id: f51355382098eb3da5ab9f64e094c6d03e6bdf7d/"
,,0.1349,rocksdb,fixed typo Summary: fixed typo Closes Differential Revision: D5242471 Pulled By: IslamAbdelRahman fbshipit-source-id: 832eb3a4c70221444ccd2ae63217823fec56c748/fixed typo Summary: fixed exisitng existing Closes Differential Revision: D5070169 Pulled By: yiwu-arbug fbshipit-source-id: 8c8450acf50757b767cf78b78314018395738d96/
,,0.1246,rocksdb,"fix WinEnv assertions Summary: Closes Differential Revision: D5585389 Pulled By: ajkr fbshipit-source-id: cb54041eb481d0d759c440f82a8a2c5b34534173/Implement ReopenWritibaleFile on Windows and other fixes Summary: Make default impl return NoSupported so the db_blob tests exist in a meaningful manner. Replace std::thread to port::Thread Closes Differential Revision: D5275563 Pulled By: yiwu-arbug fbshipit-source-id: cedf1a18a2c05e20d768c1308b3f3224dbd70ab6/Fix mingw compilation with Summary: This was exposed by a48a62d, which made NDEBUG the default for cmake builds. Closes Differential Revision: D5079583 Pulled By: sagar0 fbshipit-source-id: c614e96a40df016a834a62b6236852265e7ee4db/"
,,0.1487,rocksdb,"Allow upgrades from nullptr to some merge operator Summary: Currently, RocksDB does not allow reopening a preexisting DB with no merge operator defined, with a merge operator defined. This means that if a DB ever want to add a merge operator, theres no way to do so currently. Fix this by adding a new verification type `kByNameAllowFromNull` which will allow old values to be nullptr, and new values to be non-nullptr. Closes Differential Revision: D5961131 Pulled By: lth fbshipit-source-id: 06179bebd0d90db3d43690b5eb7345e2d5bab1eb/Fix memory leak in OptionsTest::OptionsComposeDecompose Summary: Fixing asan error. Closes Differential Revision: D5838895 Pulled By: yiwu-arbug fbshipit-source-id: 1662ce9856eb5e6877675347dc2240f2acb6fae8/regression test for missing init options Summary: test the `DBOptions(const Options&)` and `ColumnFamilyOptions(const Options&)` constructors. Actually thisll work better once we refactor `RandomInitDBOptions` / `RandomInitCFOptions` to use the authoritative sources of struct members: `db_options_type_info` / `cf_options_type_info` (internal task T21804189 for this). Closes Differential Revision: D5817141 Pulled By: ajkr fbshipit-source-id: 8567c20feced9d1751fdf1f4383e2af30f7e3591/"
,,0.1102,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.0938,rocksdb,Overload new[] to properly align LRUCacheShard Summary: Also verify it fixes gcc7 compile failure (see also Closes Differential Revision: D5620348 Pulled By: yiwu-arbug fbshipit-source-id: 87db657ab734f23b1bfaaa9db9b9956d10eaef59/
,,0.215,rocksdb,"Fix coverity uninitialized fields warnings in lru_cache Summary: Coverity uninitialized member variable warnings in lru_cache Closes Differential Revision: D6173062 Pulled By: sagar0 fbshipit-source-id: 7bcfc653457bd362d46045d06527838c9a6adad6/Fix unstable floating point exception Summary: Fix unstable floating point exception, tested on Windows, 64-bit build. The problem appeared in `SetCapacity()` method at line `high_pri_pool_capacity_ capacity_ * high_pri_pool_ratio_;` `high_pri_pool_ratio_` was not initialized at that moment, because `SetHighPriorityPoolRatio()` is called after `SetCapacity()`. So, `high_pri_pool_ratio_` contained garbage, which caused ""Floating point exception"" sometimes. Closes Differential Revision: D6111161 Pulled By: yiwu-arbug fbshipit-source-id: d170329111ad12b4bf9bbcf37bcb6411523438ae/Not using aligned_alloc with gcc4 + asan Summary: GCC 5 + ASAN does not instrument aligned_alloc, which can make ASAN report false-positive with ""free on address which was not malloc"" error. Also suppress leak warning with LRUCache::DisownData(). Closes Differential Revision: D5696465 Pulled By: yiwu-arbug fbshipit-source-id: 87c607c002511fa089b18cc35e24909bee0e74b4/Overload new[] to properly align LRUCacheShard Summary: Also verify it fixes gcc7 compile failure (see also Closes Differential Revision: D5620348 Pulled By: yiwu-arbug fbshipit-source-id: 87db657ab734f23b1bfaaa9db9b9956d10eaef59/"
,,0.1121,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.1178,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.1467,rocksdb,Fix clang build error Summary: Fix cast from size_t to unsigned int. Closes Differential Revision: D6232863 Pulled By: yiwu-arbug fbshipit-source-id: 4c6131168b1faec26f7820b2cf4a09c242d323b7/Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.0744,rocksdb,fix CopyFile status checks Summary: copied from internal diff D6156261 Closes Differential Revision: D6230167 Pulled By: ajkr fbshipit-source-id: 17926bb1152d607556364e3aacfec0ef3c115748/
,,0.114,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.1102,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.1625,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/Fix CLANG Analyze Summary: clang analyze shows warnings after we upgrade the CLANG version. Fix them. Closes Differential Revision: D5769060 Pulled By: siying fbshipit-source-id: 3f8e4df715590d8984f6564b608fa08cfdfa5f14/
,,0.1709,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/Added CPU prefetch for skiplist Summary: This change causes following changes result of test: ./db_bench 10000000 none from fillrandom : 3.177 micros/op 314804 ops/sec; 34.8 MB/s to fillrandom : 2.777 micros/op 360087 ops/sec; 39.8 MB/s Closes Differential Revision: D5977822 Pulled By: yiwu-arbug fbshipit-source-id: 1ea77707bffa978b1592b0c5d0fe76bfa1930f8d/Fix CLANG Analyze Summary: clang analyze shows warnings after we upgrade the CLANG version. Fix them. Closes Differential Revision: D5769060 Pulled By: siying fbshipit-source-id: 3f8e4df715590d8984f6564b608fa08cfdfa5f14/
,,0.1544,rocksdb,"Fix PinnableSlice move assignment Summary: After move assignment, we need to re-initialized the moved PinnableSlice. Also update blob_db_impl.cc to not reuse the moved PinnableSlice since it is supposed to be in an undefined state after move. Closes Differential Revision: D6238585 Pulled By: yiwu-arbug fbshipit-source-id: bd99f2e37406c4f7de160c7dee6a2e8126bc224e/PinnableSlice move assignment Summary: Allow `std::move(pinnable_slice)`. Closes Differential Revision: D6036782 Pulled By: yiwu-arbug fbshipit-source-id: 583fb0419a97e437ff530f4305822341cd3381fa/stop calling memcmp with nullptrs Summary: it doesnt take nullptr according to its declaration in glibc, and calling it in this way causes our sanitizers (ubsan, clang analyze) to fail. Closes Differential Revision: D5683260 Pulled By: ajkr fbshipit-source-id: 114b137ee188172f96eedc43139255cae7bee80a/"
,,0.1643,rocksdb,"Make bytes_per_sync and wal_bytes_per_sync mutable Summary: SUMMARY Moves the bytes_per_sync and wal_bytes_per_sync options from immutableoptions to mutable options. Also if wal_bytes_per_sync is changed, the wal file and memtables are flushed. TEST PLAN ran make check all passed Two new tests SetBytesPerSync, SetWalBytesPerSync check that after issuing setoptions with a new value for the var, the db options have the new value. Closes Reviewed By: yiwu-arbug Differential Revision: D5845814 Pulled By: TheRushingWookie fbshipit-source-id: 93b52d779ce623691b546679dcd984a06d2ad1bd/Added save points for transactions C API Summary: Added possibility to set save points in transactions and then rollback to them Closes Differential Revision: D5825829 Pulled By: yiwu-arbug fbshipit-source-id: 62168992340bbcddecdaea3baa2a678475d1429d/Additions for `OptimisticTransactionDB` in C API Summary: Added some bindings for `OptimisticTransactionDB` in C API Closes Differential Revision: D5820672 Pulled By: yiwu-arbug fbshipit-source-id: 7efd17f619cc0741feddd2050b8fc856f9288350/Improved transactions support in C API Summary: Solves Added OptimisticTransactionDB to the C API. Added missing merge operations to Transaction. Added missing get_for_update operation to transaction If required I will create tests for this another day. Closes Differential Revision: D5600906 Pulled By: yiwu-arbug fbshipit-source-id: da23e4484433d8f59d471f778ff2ae210e3fe4eb/"
,,0.0899,rocksdb,PinnableSlice move assignment Summary: Allow `std::move(pinnable_slice)`. Closes Differential Revision: D6036782 Pulled By: yiwu-arbug fbshipit-source-id: 583fb0419a97e437ff530f4305822341cd3381fa/
,,0.1159,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.1177,rocksdb,"Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/Fix missing stdlib include required for abort() Summary: If ROCKSDB_LITE is defined, a call to abort() is introduced. This call requires stdlib.h. Build log of unpatched 5.7.1: Closes Reviewed By: yiwu-arbug Differential Revision: D5632372 Pulled By: lxcode fbshipit-source-id: b2a8e692bf14ccf1f875f3a00463e87bba310a2b/"
,,0.1032,rocksdb,"add counter for deletion dropping optimization Summary: add this counter stat to track usage of deletion-dropping optimization. if usage is low, we can delete it to prevent bugs like Closes Differential Revision: D5665421 Pulled By: ajkr fbshipit-source-id: 881befa2d199838dac88709e7b376a43d304e3d4/"
,,0.1068,rocksdb,Blob DB: Add compaction filter to remove expired blob index entries Summary: After adding expiration to blob index in we are now able to add a compaction filter to cleanup expired blob index entries. Closes Differential Revision: D6183812 Pulled By: yiwu-arbug fbshipit-source-id: 9cb03267a9702975290e758c9c176a2c03530b83/
,,0.1797,rocksdb,"Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
,,0.1393,rocksdb,"added missing subcodes and improved error message for missing enum values Summary: Javas `Status.SubCode` was out of sync with `include/rocksdb/status.h:SubCode`. When running out of disc space this led to an `IllegalArgumentException` because of an invalid status code, rather than just returning the corresponding status code without an exception. I added the missing status codes. By this, we keep the behaviour of throwing an `IllegalArgumentException` in case of newly added status codes that are defined in C but not in Java. We could think of an alternative strategy: add in Java another code ""UnknownCode"" which acts as a catch-all for all those status codes that are not yet mirrored from C to Java. This approach would never throw an exception but simply return a non-OK status-code. I think the current approach of throwing an Exception in case of a C/Java inconsistency is fine, but if you have some opinion on the alternative strategy, then feel free to comment here. Closes Differential Revision: D6129682 Pulled By: sagar0 fbshipit-source-id: f2bf44caad650837cffdcb1f93eb793b43580c66/Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
,,0.171,rocksdb,"Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
,,0.1727,rocksdb,"Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
,,0.1745,rocksdb,"Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
,,0.1762,rocksdb,"Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
,,0.1727,rocksdb,"Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
,,0.1658,rocksdb,"Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
,,0.1797,rocksdb,"Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
,,0.1727,rocksdb,"Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
,,0.2897,rocksdb,"Fix crashes, address test issues and adjust windows test script Summary: Add per-exe execution capability Add fix parsing of groups/tests Add timer test exclusion Fix unit tests Ifdef threadpool specific tests that do not pass on Vista threadpool. Remove spurious outout from prefix_test so test case listing works properly. Fix not using standard test directories results in file creation errors in sst_dump_test. BlobDb fixes: In C++ end() iterators can not be dereferenced. They are not valid. When deleting blob_db_ set it to nullptr before any other code executes. Not fixed:. On Windows you can not delete a file while it is open. [ RUN ] BlobDBTest.ReadWhileGC d:\dev\rocksdb\rocksdb\utilities\blob_db\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options) IO error: Failed to delete: d:/mnt/db\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied d:\dev\rocksdb\rocksdb\utilities\blob_db\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options) IO error: Failed to delete: d:/mnt/db\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied write_batch Should not call front() if there is a chance the container is empty Closes Differential Revision: D6293274 Pulled By: sagar0 fbshipit-source-id: 318c3717c22087fae13b18715dffb24565dbd956/Fix build on OpenBSD Summary: A few simple changes to allow RocksDB to be built on OpenBSD. Let me know if any further changes are needed. Closes Differential Revision: D6138800 Pulled By: ajkr fbshipit-source-id: a13a17b5dc051e6518bd56a8c5efd1d24dd81b0c/Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/"
,,0.2144,rocksdb,"Fix removed structurally dead return statement Summary: There seems to be a typo mistake in env ReuseWritableFile func where status is being returned twice. Closes Differential Revision: D6196204 Pulled By: ajkr fbshipit-source-id: abb6e3e1c1e772dd485fc39e7f1b9d502fa188fe/Repair DBs with trailing slash in name Summary: Problem: `DB::SanitizeOptions` strips trailing slash from `wal_dir` but not `dbname` We check whether `wal_dir` and `dbname` refer to the same directory using string equality: Providing `dbname` with trailing slash causes default `wal_dir` to be misidentified as a separate directory. Then the repair tries to add all SST files to the `VersionEdit` twice (once for `dbname` dir, once for `wal_dir`) and fails with coredump. Solution: Add a new `Env` function, `AreFilesSame`, which uses device and inode number to check whether files are the same. Its currently only implemented in `PosixEnv`. Migrate repair to use `AreFilesSame` to check whether `dbname` and `wal_dir` are same. If unsupported, falls back to string comparison. Closes Differential Revision: D5761349 Pulled By: ajkr fbshipit-source-id: c839d548678b742af1166d60b09abd94e5476238/"
,,0.1178,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.2116,rocksdb,"WritePrepared Txn: Disable GC during recovery Summary: Disables GC during recovery of a WritePrepared txn db to avoid GCing uncommitted key values. Closes Differential Revision: D6000191 Pulled By: maysamyabandeh fbshipit-source-id: fc4d522c643d24ebf043f811fe4ecd0dd0294675/WritePrepared Txn: Iterator Summary: On iterator create, take a snapshot, create a ReadCallback and pass the ReadCallback to the underlying DBIter to check if key is committed. Closes Differential Revision: D6001471 Pulled By: yiwu-arbug fbshipit-source-id: 3565c4cdaf25370ba47008b0e0cb65b31dfe79fe/WritePrepare Txn: Cancel flush/compaction before destruction Summary: On WritePreparedTxnDB destruct there could be running compaction/flush holding a SnapshotChecker, which holds a pointer back to WritePreparedTxnDB. Make sure those jobs finished before destructing WritePreparedTxnDB. This is caught by TransactionTest::SeqAdvanceTest. Closes Differential Revision: D6002957 Pulled By: yiwu-arbug fbshipit-source-id: f1e70390c9798d1bd7959f5c8e2a1c14100773c3/WritePrepared Txn: Compaction/Flush Summary: Update Compaction/Flush to support WritePreparedTxnDB: Add SnapshotChecker which is a proxy to query WritePreparedTxnDB::IsInSnapshot. Pass SnapshotChecker to DBImpl on WritePreparedTxnDB open. CompactionIterator use it to check if a key has been committed and if it is visible to a snapshot. In CompactionIterator: * check if key has been committed. If not, output uncommitted keys AS-IS. * use SnapshotChecker to check if key is visible to a snapshot when in need. * do not output key with seq 0 if the key is not committed. Closes Differential Revision: D5902907 Pulled By: yiwu-arbug fbshipit-source-id: 945e037fdf0aa652dc5ba0ad879461040baa0320/WritePrepared Txn: Test sequence number 0 is visible Summary: Compaction will output keys with sequence number 0, if it is visible to earliest snapshot. Adding a test to make sure IsInSnapshot() report sequence number 0 is visible to any snapshot. Closes Differential Revision: D5990665 Pulled By: yiwu-arbug fbshipit-source-id: ef50ebc777ff8ca688771f3ab598c7a609b0b65e/Add TransactionDB::SingleDelete() Summary: Looks like the API is simply missing. Adding it. Closes Differential Revision: D5919955 Pulled By: yiwu-arbug fbshipit-source-id: 6e2e9c96c29882b0bb4113d1f8efb72bffc57878/fix some misspellings Summary: PTAL ajkr Closes Differential Revision: D5648052 Pulled By: ajkr fbshipit-source-id: 7cd1ddd61364d5a55a10fdd293fa74b2bf89dd98/"
,,0.0863,rocksdb,Add TransactionDB::SingleDelete() Summary: Looks like the API is simply missing. Adding it. Closes Differential Revision: D5919955 Pulled By: yiwu-arbug fbshipit-source-id: 6e2e9c96c29882b0bb4113d1f8efb72bffc57878/
,,0.0883,rocksdb,Add TransactionDB::SingleDelete() Summary: Looks like the API is simply missing. Adding it. Closes Differential Revision: D5919955 Pulled By: yiwu-arbug fbshipit-source-id: 6e2e9c96c29882b0bb4113d1f8efb72bffc57878/
,,0.0977,rocksdb,"WritePrepared Txn: Iterator Summary: On iterator create, take a snapshot, create a ReadCallback and pass the ReadCallback to the underlying DBIter to check if key is committed. Closes Differential Revision: D6001471 Pulled By: yiwu-arbug fbshipit-source-id: 3565c4cdaf25370ba47008b0e0cb65b31dfe79fe/"
,,0.0706,rocksdb,Remove unused TransactionCallback Summary: TransactionCallback was never used. Remove it to avoid confusion. Closes Differential Revision: D5787219 Pulled By: maysamyabandeh fbshipit-source-id: e2b6a89537e3770a269ad38be71c4b0b160a88ac/
,,0.1714,rocksdb,"WritePrepared Txn: Iterator Summary: On iterator create, take a snapshot, create a ReadCallback and pass the ReadCallback to the underlying DBIter to check if key is committed. Closes Differential Revision: D6001471 Pulled By: yiwu-arbug fbshipit-source-id: 3565c4cdaf25370ba47008b0e0cb65b31dfe79fe/WritePrepare Txn: Cancel flush/compaction before destruction Summary: On WritePreparedTxnDB destruct there could be running compaction/flush holding a SnapshotChecker, which holds a pointer back to WritePreparedTxnDB. Make sure those jobs finished before destructing WritePreparedTxnDB. This is caught by TransactionTest::SeqAdvanceTest. Closes Differential Revision: D6002957 Pulled By: yiwu-arbug fbshipit-source-id: f1e70390c9798d1bd7959f5c8e2a1c14100773c3/WritePrepared Txn: Compaction/Flush Summary: Update Compaction/Flush to support WritePreparedTxnDB: Add SnapshotChecker which is a proxy to query WritePreparedTxnDB::IsInSnapshot. Pass SnapshotChecker to DBImpl on WritePreparedTxnDB open. CompactionIterator use it to check if a key has been committed and if it is visible to a snapshot. In CompactionIterator: * check if key has been committed. If not, output uncommitted keys AS-IS. * use SnapshotChecker to check if key is visible to a snapshot when in need. * do not output key with seq 0 if the key is not committed. Closes Differential Revision: D5902907 Pulled By: yiwu-arbug fbshipit-source-id: 945e037fdf0aa652dc5ba0ad879461040baa0320/Add TransactionDB::SingleDelete() Summary: Looks like the API is simply missing. Adding it. Closes Differential Revision: D5919955 Pulled By: yiwu-arbug fbshipit-source-id: 6e2e9c96c29882b0bb4113d1f8efb72bffc57878/"
,,0.1792,rocksdb,"Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/Make bytes_per_sync and wal_bytes_per_sync mutable Summary: SUMMARY Moves the bytes_per_sync and wal_bytes_per_sync options from immutableoptions to mutable options. Also if wal_bytes_per_sync is changed, the wal file and memtables are flushed. TEST PLAN ran make check all passed Two new tests SetBytesPerSync, SetWalBytesPerSync check that after issuing setoptions with a new value for the var, the db options have the new value. Closes Reviewed By: yiwu-arbug Differential Revision: D5845814 Pulled By: TheRushingWookie fbshipit-source-id: 93b52d779ce623691b546679dcd984a06d2ad1bd/Fix TransactionTest::SeqAdvanceTest ASAN failure Summary: The test didnt delete txn before creating a new one. Closes Differential Revision: D5880236 Pulled By: yiwu-arbug fbshipit-source-id: 7a4fcaada3d86332292754502cd8f4341143bf4f/"
,,0.0959,rocksdb,"WritePrepared Txn: Iterator Summary: On iterator create, take a snapshot, create a ReadCallback and pass the ReadCallback to the underlying DBIter to check if key is committed. Closes Differential Revision: D6001471 Pulled By: yiwu-arbug fbshipit-source-id: 3565c4cdaf25370ba47008b0e0cb65b31dfe79fe/"
,,0.301,rocksdb,"Fix crashes, address test issues and adjust windows test script Summary: Add per-exe execution capability Add fix parsing of groups/tests Add timer test exclusion Fix unit tests Ifdef threadpool specific tests that do not pass on Vista threadpool. Remove spurious outout from prefix_test so test case listing works properly. Fix not using standard test directories results in file creation errors in sst_dump_test. BlobDb fixes: In C++ end() iterators can not be dereferenced. They are not valid. When deleting blob_db_ set it to nullptr before any other code executes. Not fixed:. On Windows you can not delete a file while it is open. [ RUN ] BlobDBTest.ReadWhileGC d:\dev\rocksdb\rocksdb\utilities\blob_db\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options) IO error: Failed to delete: d:/mnt/db\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied d:\dev\rocksdb\rocksdb\utilities\blob_db\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options) IO error: Failed to delete: d:/mnt/db\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied write_batch Should not call front() if there is a chance the container is empty Closes Differential Revision: D6293274 Pulled By: sagar0 fbshipit-source-id: 318c3717c22087fae13b18715dffb24565dbd956/Fix unused var warnings in Release mode Summary: MSVC does not support unused attribute at this time. A separate assignment line fixes the issue probably by being counted as usage for MSVC and it no longer complains about unused var. Closes Differential Revision: D6126272 Pulled By: maysamyabandeh fbshipit-source-id: 4907865db45fd75a39a15725c0695aaa17509c1f/Fix WriteBatchWithIndex::GetFromBatchAndDB not allowing StackableDB Summary: Closes Differential Revision: D5829682 Pulled By: yiwu-arbug fbshipit-source-id: abb8fa14b58cea7c416282f9be19e8b1a7961c6e/"
,,0.1159,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.1005,rocksdb,Fix unused var warnings in Release mode Summary: MSVC does not support unused attribute at this time. A separate assignment line fixes the issue probably by being counted as usage for MSVC and it no longer complains about unused var. Closes Differential Revision: D6126272 Pulled By: maysamyabandeh fbshipit-source-id: 4907865db45fd75a39a15725c0695aaa17509c1f/
,,0.1102,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.2454,rocksdb,"Fix crashes, address test issues and adjust windows test script Summary: Add per-exe execution capability Add fix parsing of groups/tests Add timer test exclusion Fix unit tests Ifdef threadpool specific tests that do not pass on Vista threadpool. Remove spurious outout from prefix_test so test case listing works properly. Fix not using standard test directories results in file creation errors in sst_dump_test. BlobDb fixes: In C++ end() iterators can not be dereferenced. They are not valid. When deleting blob_db_ set it to nullptr before any other code executes. Not fixed:. On Windows you can not delete a file while it is open. [ RUN ] BlobDBTest.ReadWhileGC d:\dev\rocksdb\rocksdb\utilities\blob_db\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options) IO error: Failed to delete: d:/mnt/db\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied d:\dev\rocksdb\rocksdb\utilities\blob_db\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options) IO error: Failed to delete: d:/mnt/db\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied write_batch Should not call front() if there is a chance the container is empty Closes Differential Revision: D6293274 Pulled By: sagar0 fbshipit-source-id: 318c3717c22087fae13b18715dffb24565dbd956/Blob DB: use compression in file header instead of global options Summary: To fix the issue of failing to decompress existing value after reopen DB with a different compression settings. Closes Differential Revision: D6267260 Pulled By: yiwu-arbug fbshipit-source-id: c7cf7f3e33b0cd25520abf4771cdf9180cc02a5f/Blob DB: Fix BlobDBTest::SnapshotAndGarbageCollection asan failure Summary: Fix unreleased snapshot at the end of the test. Closes Differential Revision: D6232867 Pulled By: yiwu-arbug fbshipit-source-id: 651ca3144fc573ea2ab0ab20f0a752fb4a101d26/Blob DB: Add compaction filter to remove expired blob index entries Summary: After adding expiration to blob index in we are now able to add a compaction filter to cleanup expired blob index entries. Closes Differential Revision: D6183812 Pulled By: yiwu-arbug fbshipit-source-id: 9cb03267a9702975290e758c9c176a2c03530b83/Blob DB: fix snapshot handling Summary: Blob db will keep blob file if data in the file is visible to an active snapshot. Before this patch it checks whether there is an active snapshot has sequence number greater than the earliest sequence in the file. This is problematic since we take snapshot on every read, if it keep having reads, old blob files will not be cleanup. Change to check if there is an active snapshot falls in the range of [earliest_sequence, obsolete_sequence) where obsolete sequence is 1. if data is relocated to another file by garbage collection, it is the latest sequence at the time garbage collection finish 2. otherwise, it is the latest sequence of the file Closes Differential Revision: D6182519 Pulled By: yiwu-arbug fbshipit-source-id: cdf4c35281f782eb2a9ad6a87b6727bbdff27a45/Blob DB: Fix flaky BlobDBTest::GCExpiredKeyWhileOverwriting test Summary: The test intent to wait until key being overwritten until proceed with garbage collection. It failed to wait for `PutUntil` finally finish. Fixing it. Closes Differential Revision: D6222833 Pulled By: yiwu-arbug fbshipit-source-id: fa9b57a772b92a66cf250b44e7975c43f62f45c5/Blob DB: cleanup unused options Summary: * cleanup num_concurrent_simple_blobs. We dont do concurrent writes (by taking write_mutex_) so it doesnt make sense to have multiple non TTL files open. We can revisit later when we want to improve writes. * cleanup eviction callback. we dont have plan to use it now. * rename s/open_simple_blob_files_/open_non_ttl_file_/ and s/open_blob_files_/open_ttl_files_/ to avoid confusion. Closes Differential Revision: D6182598 Pulled By: yiwu-arbug fbshipit-source-id: 99e6f5e01fa66d31309cdb06ce48502464bac6ad/Blob DB: update blob file format Summary: Changing blob file format and some code cleanup around the change. The change with blob log format are: * Remove timestamp field in blob file header, blob file footer and blob records. The field is not being use and often confuse with expiration field. * Blob file header now come with column family id, which always equal to default column family id. It leaves room for future support of column family. * Compression field in blob file header now is a standalone byte (instead of compact encode with flags field) * Blob file footer now come with its own crc. * Key length now being uint64_t instead of uint32_t * Blob CRC now checksum both key and value (instead of value only). * Some reordering of the fields. The list of cleanups: * Better inline comments in blob_log_format.h * rename ttlrange_t and snrange_t to ExpirationRange and SequenceRange respectively. * simplify blob_db::Reader * Move crc checking logic to inside blob_log_format.cc Closes Differential Revision: D6171304 Pulled By: yiwu-arbug fbshipit-source-id: e4373e0d39264441b7e2fbd0caba93ddd99ea2af/Blob DB: Inline small values in base DB Summary: Adding the `min_blob_size` option to allow storing small values in base db (in LSM tree) together with the key. The goal is to improve performance for small values, while taking advantage of blob dbs low write amplification for large values. Also adding expiration timestamp to blob index. It will be useful to evict stale blob indexes in base db by adding a compaction filter. Ill work on the compaction filter in future patches. See blob_index.h for the new blob index format. There are 4 cases when writing a new key: * small value w/o TTL: put in base db as normal value (i.e. ValueType::kTypeValue) * small value w/ TTL: put (type, expiration, value) to base db. * large value w/o TTL: write value to blob log and put (type, file, offset, size, compression) to base db. * large value w/TTL: write value to blob log and put (type, expiration, file, offset, size, compression) to base db. Closes Differential Revision: D6142115 Pulled By: yiwu-arbug fbshipit-source-id: 9526e76e19f0839310a3f5f2a43772a4ad182cd0/Return write error on reaching blob dir size limit Summary: I found that we continue accepting writes even when the blob db goes beyond the configured blob directory size limit. Now, we return an error for writes on reaching `blob_dir_size` limit and if `is_fifo` is set to false. (We cannot just drop any file when `is_fifo` is true.) Deleting the oldest file when `is_fifo` is true will be handled in a later PR. Closes Differential Revision: D6136156 Pulled By: sagar0 fbshipit-source-id: 2f11cb3f2eedfa94524fbfa2613dd64bfad7a23c/Add DB::Properties::kEstimateOldestKeyTime Summary: With FIFO compaction we would like to get the oldest data time for monitoring. The problem is we dont have timestamp for each key in the DB. As an approximation, we expose the earliest of sst file ""creation_time"" property. My plan is to override the property with a more accurate value with blob db, where we actually have timestamp. Closes Differential Revision: D5770600 Pulled By: yiwu-arbug fbshipit-source-id: 03833c8f10bbfbee62f8ea5c0d03c0cafb5d853a/Blob DB: Store blob index as kTypeBlobIndex in base db Summary: Blob db insert blob index to base db as kTypeBlobIndex type, to tell apart values written by plain rocksdb or blob db. This is to make it possible to migrate from existing rocksdb to blob db. Also with the patch blob db garbage collection get away from OptimisticTransaction. Instead it use a custom write callback to achieve similar behavior as OptimisticTransaction. This is because we need to pass the is_blob_index flag to DBImpl::Get but OptimisticTransaction dont support it. Closes Differential Revision: D6050044 Pulled By: yiwu-arbug fbshipit-source-id: 61dc72ab9977625e75f78cd968e7d8a3976e3632/Blob DB: not writing sequence number as blob record footer Summary: Previously each time we write a blob we write blog_record_header + key + value + blob_record_footer to blob log. The footer only contains a sequence and a crc for the sequence number. The sequence number was used in garbage collection to verify the value is recent. After we moved to use optimistic transaction and no longer use sequence number from the footer. Remove the footer altogether. Theres another usage of sequence number and we are keeping it: Each blob log file keep track of sequence number range of keys in it, and use it to check if it is reference by a snapshot, before being deleted. Closes Differential Revision: D6057585 Pulled By: yiwu-arbug fbshipit-source-id: d6da53c457a316e9723f359a1b47facfc3ffe090/Make it explicit blob db doesnt support CF Summary: Blob db doesnt currently support column families. Return NotSupported status explicitly. Closes Differential Revision: D5757438 Pulled By: yiwu-arbug fbshipit-source-id: 44de9408fd032c98e8ae337d4db4ed37169bd9fa/make blob file close synchronous Summary: Fixing flaky blob_db_test. To close a blob file, blob db used to add a CloseSeqWrite job to the background thread to close it. Changing file close to be synchronous in order to simplify logic, and fix flaky blob_db_test. Closes Differential Revision: D5699387 Pulled By: yiwu-arbug fbshipit-source-id: dd07a945cd435cd3808fce7ee4ea57817409474a/Blob db create a snapshot before every read Summary: If GC kicks in between * A Get() reads index entry from base db. * The Get() read from a blob file The GC can delete the corresponding blob file, making the key not found. Fortunately we have existing logic to avoid deleting a blob file if it is referenced by a snapshot. So the fix is to explicitly create a snapshot before reading index entry from base db. Closes Differential Revision: D5655956 Pulled By: yiwu-arbug fbshipit-source-id: e4ccbc51331362542e7343175bbcbdea5830f544/GC the oldest file when out of space Summary: When out of space, blob db should GC the oldest file. The current implementation GC the newest one instead. Fixing it. Closes Differential Revision: D5657611 Pulled By: yiwu-arbug fbshipit-source-id: 56c30a4c52e6ab04551dda8c5c46006d4070b28d/fix some misspellings Summary: PTAL ajkr Closes Differential Revision: D5648052 Pulled By: ajkr fbshipit-source-id: 7cd1ddd61364d5a55a10fdd293fa74b2bf89dd98/"
,,0.2135,rocksdb,"Blob DB: Add compaction filter to remove expired blob index entries Summary: After adding expiration to blob index in we are now able to add a compaction filter to cleanup expired blob index entries. Closes Differential Revision: D6183812 Pulled By: yiwu-arbug fbshipit-source-id: 9cb03267a9702975290e758c9c176a2c03530b83/Blob DB: option to enable garbage collection Summary: Add an option to enable/disable auto garbage collection, where we keep counting how many keys have been evicted by either deletion or compaction and decide whether to garbage collect a blob file. Default disable auto garbage collection for now since the whole logic is not fully tested and we plan to make major change to it. Closes Differential Revision: D6224756 Pulled By: yiwu-arbug fbshipit-source-id: cdf53bdccec96a4580a2b3a342110ad9e8864dfe/Blob DB: cleanup unused options Summary: * cleanup num_concurrent_simple_blobs. We dont do concurrent writes (by taking write_mutex_) so it doesnt make sense to have multiple non TTL files open. We can revisit later when we want to improve writes. * cleanup eviction callback. we dont have plan to use it now. * rename s/open_simple_blob_files_/open_non_ttl_file_/ and s/open_blob_files_/open_ttl_files_/ to avoid confusion. Closes Differential Revision: D6182598 Pulled By: yiwu-arbug fbshipit-source-id: 99e6f5e01fa66d31309cdb06ce48502464bac6ad/Fix memory leak on blob db open Summary: Fixes Closes Differential Revision: D5757527 Pulled By: yiwu-arbug fbshipit-source-id: f495b63700495aeaade30a1da5e3675848f3d72f/"
,,0.1102,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.1121,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.1197,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.098,rocksdb,PinnableSlice move assignment Summary: Allow `std::move(pinnable_slice)`. Closes Differential Revision: D6036782 Pulled By: yiwu-arbug fbshipit-source-id: 583fb0419a97e437ff530f4305822341cd3381fa/
,,0.1465,rocksdb,"TableProperty::oldest_key_time defaults to 0 Summary: We dont propagate TableProperty::oldest_key_time on compaction and just write the default value to SST files. It is more natural to default the value to 0. Also revert db_sst_test back to before Closes Differential Revision: D6165702 Pulled By: yiwu-arbug fbshipit-source-id: ca3ce5928d96ae79a5beb12bb7d8c640a71478a0/Add DB::Properties::kEstimateOldestKeyTime Summary: With FIFO compaction we would like to get the oldest data time for monitoring. The problem is we dont have timestamp for each key in the DB. As an approximation, we expose the earliest of sst file ""creation_time"" property. My plan is to override the property with a more accurate value with blob db, where we actually have timestamp. Closes Differential Revision: D5770600 Pulled By: yiwu-arbug fbshipit-source-id: 03833c8f10bbfbee62f8ea5c0d03c0cafb5d853a/"
,,0.1396,rocksdb,"TableProperty::oldest_key_time defaults to 0 Summary: We dont propagate TableProperty::oldest_key_time on compaction and just write the default value to SST files. It is more natural to default the value to 0. Also revert db_sst_test back to before Closes Differential Revision: D6165702 Pulled By: yiwu-arbug fbshipit-source-id: ca3ce5928d96ae79a5beb12bb7d8c640a71478a0/Add DB::Properties::kEstimateOldestKeyTime Summary: With FIFO compaction we would like to get the oldest data time for monitoring. The problem is we dont have timestamp for each key in the DB. As an approximation, we expose the earliest of sst file ""creation_time"" property. My plan is to override the property with a more accurate value with blob db, where we actually have timestamp. Closes Differential Revision: D5770600 Pulled By: yiwu-arbug fbshipit-source-id: 03833c8f10bbfbee62f8ea5c0d03c0cafb5d853a/"
,,0.1067,rocksdb,"Add DB::Properties::kEstimateOldestKeyTime Summary: With FIFO compaction we would like to get the oldest data time for monitoring. The problem is we dont have timestamp for each key in the DB. As an approximation, we expose the earliest of sst file ""creation_time"" property. My plan is to override the property with a more accurate value with blob db, where we actually have timestamp. Closes Differential Revision: D5770600 Pulled By: yiwu-arbug fbshipit-source-id: 03833c8f10bbfbee62f8ea5c0d03c0cafb5d853a/print more table_options to info log Summary: print more table_options to info log Closes Differential Revision: D6054490 Pulled By: yiwu-arbug fbshipit-source-id: 8e6f96e08bdc906077b6c62ade419d7cb739110f/Allow upgrades from nullptr to some merge operator Summary: Currently, RocksDB does not allow reopening a preexisting DB with no merge operator defined, with a merge operator defined. This means that if a DB ever want to add a merge operator, theres no way to do so currently. Fix this by adding a new verification type `kByNameAllowFromNull` which will allow old values to be nullptr, and new values to be non-nullptr. Closes Differential Revision: D5961131 Pulled By: lth fbshipit-source-id: 06179bebd0d90db3d43690b5eb7345e2d5bab1eb/"
,,0.1023,rocksdb,Fix CLANG Analyze Summary: clang analyze shows warnings after we upgrade the CLANG version. Fix them. Closes Differential Revision: D5769060 Pulled By: siying fbshipit-source-id: 3f8e4df715590d8984f6564b608fa08cfdfa5f14/
,,0.124,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/support disabling checksum in block-based table Summary: store a zero as the checksum when disabled since its easier to keep block trailer a fixed length. Closes Differential Revision: D5694702 Pulled By: ajkr fbshipit-source-id: 69cea9da415778ba2b600dfd9d0dfc8cb5188ecd/
,,0.11199999999999999,rocksdb,"add counter for deletion dropping optimization Summary: add this counter stat to track usage of deletion-dropping optimization. if usage is low, we can delete it to prevent bugs like Closes Differential Revision: D5665421 Pulled By: ajkr fbshipit-source-id: 881befa2d199838dac88709e7b376a43d304e3d4/"
,,0.2682,rocksdb,"Return Status::InvalidArgument if user request sync write while disabling WAL Summary: write_options.sync true and write_options.disableWAL is incompatible. When WAL is disabled, we are not able to persist the write immediately. Return an error in this case to avoid misuse of the options. Closes Differential Revision: D6176822 Pulled By: yiwu-arbug fbshipit-source-id: 1eb10028c14fe7d7c13c8bc12c0ef659f75aa071/Make bytes_per_sync and wal_bytes_per_sync mutable Summary: SUMMARY Moves the bytes_per_sync and wal_bytes_per_sync options from immutableoptions to mutable options. Also if wal_bytes_per_sync is changed, the wal file and memtables are flushed. TEST PLAN ran make check all passed Two new tests SetBytesPerSync, SetWalBytesPerSync check that after issuing setoptions with a new value for the var, the db options have the new value. Closes Reviewed By: yiwu-arbug Differential Revision: D5845814 Pulled By: TheRushingWookie fbshipit-source-id: 93b52d779ce623691b546679dcd984a06d2ad1bd/Fix missing BYTES_PER_WRITE for pipeline write Summary: Closes Differential Revision: D5805638 Pulled By: yiwu-arbug fbshipit-source-id: 72d38c74395690023a719f400daff01527645a17/fix some misspellings Summary: PTAL ajkr Closes Differential Revision: D5648052 Pulled By: ajkr fbshipit-source-id: 7cd1ddd61364d5a55a10fdd293fa74b2bf89dd98/"
,,0.2042,rocksdb,"dynamically change current memtable size Summary: Previously setting `write_buffer_size` with `SetOptions` would only apply to new memtables. An internal user wanted it to take effect immediately, instead of at an arbitrary future point, to prevent OOM. This PR makes the memtables size mutable, and makes `SetOptions()` mutate it. There is one case when we preserve the old behavior, which is when memtable prefix bloom filter is enabled and the user is increasing the memtables capacity. Thats because the prefix bloom filters size is fixed and wouldnt work as well on a larger memtable. Closes Differential Revision: D6228304 Pulled By: ajkr fbshipit-source-id: e44bd9d10a5f8c9d8c464bf7436070bb3eafdfc9/Fix unused var warnings in Release mode Summary: MSVC does not support unused attribute at this time. A separate assignment line fixes the issue probably by being counted as usage for MSVC and it no longer complains about unused var. Closes Differential Revision: D6126272 Pulled By: maysamyabandeh fbshipit-source-id: 4907865db45fd75a39a15725c0695aaa17509c1f/"
,,0.2717,rocksdb,"Fix crashes, address test issues and adjust windows test script Summary: Add per-exe execution capability Add fix parsing of groups/tests Add timer test exclusion Fix unit tests Ifdef threadpool specific tests that do not pass on Vista threadpool. Remove spurious outout from prefix_test so test case listing works properly. Fix not using standard test directories results in file creation errors in sst_dump_test. BlobDb fixes: In C++ end() iterators can not be dereferenced. They are not valid. When deleting blob_db_ set it to nullptr before any other code executes. Not fixed:. On Windows you can not delete a file while it is open. [ RUN ] BlobDBTest.ReadWhileGC d:\dev\rocksdb\rocksdb\utilities\blob_db\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options) IO error: Failed to delete: d:/mnt/db\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied d:\dev\rocksdb\rocksdb\utilities\blob_db\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options) IO error: Failed to delete: d:/mnt/db\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied write_batch Should not call front() if there is a chance the container is empty Closes Differential Revision: D6293274 Pulled By: sagar0 fbshipit-source-id: 318c3717c22087fae13b18715dffb24565dbd956/"
,,0.2372,rocksdb,"fix file numbers after repair Summary: The file numbers assigned post-repair were sometimes smaller than older files numbers due to `LogAndApply` saving the wrong next file number in the manifest. Mark the highest file seen during repair as used before `LogAndApply` so the correct next file number will be stored. Renamed `MarkFileNumberUsedDuringRecovery` to `MarkFileNumberUsed` since now its used during repair in addition to during recovery Added `TEST_Current_Next_FileNo` to expose the next file number for the unit test. Closes Differential Revision: D6018083 Pulled By: ajkr fbshipit-source-id: 3f25cbf74439cb8f16dd12af90b67f9f9f75e718/Repair DBs with trailing slash in name Summary: Problem: `DB::SanitizeOptions` strips trailing slash from `wal_dir` but not `dbname` We check whether `wal_dir` and `dbname` refer to the same directory using string equality: Providing `dbname` with trailing slash causes default `wal_dir` to be misidentified as a separate directory. Then the repair tries to add all SST files to the `VersionEdit` twice (once for `dbname` dir, once for `wal_dir`) and fails with coredump. Solution: Add a new `Env` function, `AreFilesSame`, which uses device and inode number to check whether files are the same. Its currently only implemented in `PosixEnv`. Migrate repair to use `AreFilesSame` to check whether `dbname` and `wal_dir` are same. If unsupported, falls back to string comparison. Closes Differential Revision: D5761349 Pulled By: ajkr fbshipit-source-id: c839d548678b742af1166d60b09abd94e5476238/"
,,0.1197,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.1306,rocksdb,"Fix DBOptionsTest.SetBytesPerSync test when run with no compression Summary: Also made the test more easier to understand: changed the value size to ~1MB. switched to NoCompression. We dont anyway need compression in this test for dynamic options. The test failures started happening starting from: . Closes Differential Revision: D5959392 Pulled By: sagar0 fbshipit-source-id: 2d55641e429246328bc6d10fcb9ef540d6ce07da/Make bytes_per_sync and wal_bytes_per_sync mutable Summary: SUMMARY Moves the bytes_per_sync and wal_bytes_per_sync options from immutableoptions to mutable options. Also if wal_bytes_per_sync is changed, the wal file and memtables are flushed. TEST PLAN ran make check all passed Two new tests SetBytesPerSync, SetWalBytesPerSync check that after issuing setoptions with a new value for the var, the db options have the new value. Closes Reviewed By: yiwu-arbug Differential Revision: D5845814 Pulled By: TheRushingWookie fbshipit-source-id: 93b52d779ce623691b546679dcd984a06d2ad1bd/"
,,0.1274,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.1696,rocksdb,"Add test kPointInTimeRecoveryCFConsistency Summary: Context/problem: CFs may be flushed at different times A WAL can only be deleted after all CFs have flushed beyond end of that WAL. Point-in-time recovery might stop upon reaching the first corruption. Some CFs may have already flushed beyond that point, while others havent. We should fail the Open() instead of proceeding with inconsistent CFs. Closes Differential Revision: D5863281 Pulled By: miasantreble fbshipit-source-id: 180dbaf83d96c804cff49b3c406312a4ae61313e/"
,,0.2303,rocksdb,"WritePrepared Txn: Disable GC during recovery Summary: Disables GC during recovery of a WritePrepared txn db to avoid GCing uncommitted key values. Closes Differential Revision: D6000191 Pulled By: maysamyabandeh fbshipit-source-id: fc4d522c643d24ebf043f811fe4ecd0dd0294675/fix file numbers after repair Summary: The file numbers assigned post-repair were sometimes smaller than older files numbers due to `LogAndApply` saving the wrong next file number in the manifest. Mark the highest file seen during repair as used before `LogAndApply` so the correct next file number will be stored. Renamed `MarkFileNumberUsedDuringRecovery` to `MarkFileNumberUsed` since now its used during repair in addition to during recovery Added `TEST_Current_Next_FileNo` to expose the next file number for the unit test. Closes Differential Revision: D6018083 Pulled By: ajkr fbshipit-source-id: 3f25cbf74439cb8f16dd12af90b67f9f9f75e718/WritePrepared Txn: Compaction/Flush Summary: Update Compaction/Flush to support WritePreparedTxnDB: Add SnapshotChecker which is a proxy to query WritePreparedTxnDB::IsInSnapshot. Pass SnapshotChecker to DBImpl on WritePreparedTxnDB open. CompactionIterator use it to check if a key has been committed and if it is visible to a snapshot. In CompactionIterator: * check if key has been committed. If not, output uncommitted keys AS-IS. * use SnapshotChecker to check if key is visible to a snapshot when in need. * do not output key with seq 0 if the key is not committed. Closes Differential Revision: D5902907 Pulled By: yiwu-arbug fbshipit-source-id: 945e037fdf0aa652dc5ba0ad879461040baa0320/Make bytes_per_sync and wal_bytes_per_sync mutable Summary: SUMMARY Moves the bytes_per_sync and wal_bytes_per_sync options from immutableoptions to mutable options. Also if wal_bytes_per_sync is changed, the wal file and memtables are flushed. TEST PLAN ran make check all passed Two new tests SetBytesPerSync, SetWalBytesPerSync check that after issuing setoptions with a new value for the var, the db options have the new value. Closes Reviewed By: yiwu-arbug Differential Revision: D5845814 Pulled By: TheRushingWookie fbshipit-source-id: 93b52d779ce623691b546679dcd984a06d2ad1bd/Repair DBs with trailing slash in name Summary: Problem: `DB::SanitizeOptions` strips trailing slash from `wal_dir` but not `dbname` We check whether `wal_dir` and `dbname` refer to the same directory using string equality: Providing `dbname` with trailing slash causes default `wal_dir` to be misidentified as a separate directory. Then the repair tries to add all SST files to the `VersionEdit` twice (once for `dbname` dir, once for `wal_dir`) and fails with coredump. Solution: Add a new `Env` function, `AreFilesSame`, which uses device and inode number to check whether files are the same. Its currently only implemented in `PosixEnv`. Migrate repair to use `AreFilesSame` to check whether `dbname` and `wal_dir` are same. If unsupported, falls back to string comparison. Closes Differential Revision: D5761349 Pulled By: ajkr fbshipit-source-id: c839d548678b742af1166d60b09abd94e5476238/"
,,0.1745,rocksdb,"Return Status::InvalidArgument if user request sync write while disabling WAL Summary: write_options.sync true and write_options.disableWAL is incompatible. When WAL is disabled, we are not able to persist the write immediately. Return an error in this case to avoid misuse of the options. Closes Differential Revision: D6176822 Pulled By: yiwu-arbug fbshipit-source-id: 1eb10028c14fe7d7c13c8bc12c0ef659f75aa071/"
,,0.1438,rocksdb,"TableProperty::oldest_key_time defaults to 0 Summary: We dont propagate TableProperty::oldest_key_time on compaction and just write the default value to SST files. It is more natural to default the value to 0. Also revert db_sst_test back to before Closes Differential Revision: D6165702 Pulled By: yiwu-arbug fbshipit-source-id: ca3ce5928d96ae79a5beb12bb7d8c640a71478a0/Add DB::Properties::kEstimateOldestKeyTime Summary: With FIFO compaction we would like to get the oldest data time for monitoring. The problem is we dont have timestamp for each key in the DB. As an approximation, we expose the earliest of sst file ""creation_time"" property. My plan is to override the property with a more accurate value with blob db, where we actually have timestamp. Closes Differential Revision: D5770600 Pulled By: yiwu-arbug fbshipit-source-id: 03833c8f10bbfbee62f8ea5c0d03c0cafb5d853a/"
,,0.1197,rocksdb,Enable MSVC W4 with a few exceptions. Fix warnings and bugs Summary: Closes Differential Revision: D6079011 Pulled By: yiwu-arbug fbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57/
,,0.0972,rocksdb,Fix unused var warnings in Release mode Summary: MSVC does not support unused attribute at this time. A separate assignment line fixes the issue probably by being counted as usage for MSVC and it no longer complains about unused var. Closes Differential Revision: D6126272 Pulled By: maysamyabandeh fbshipit-source-id: 4907865db45fd75a39a15725c0695aaa17509c1f/
,,0.1052,rocksdb,Enable cacheline_aligned_alloc() to allocate from jemalloc if enabled. Summary: Reuse WITH_JEMALLOC option in preparation for module search unification. Move jemalloc overrides into a separate .cc Remote obsolete JEMALLOC_NOINIT option. Closes Differential Revision: D6174826 Pulled By: yiwu-arbug fbshipit-source-id: 9970a0289b4490272d15853920d9d7531af91140/
,,0.1002,rocksdb,Enable cacheline_aligned_alloc() to allocate from jemalloc if enabled. Summary: Reuse WITH_JEMALLOC option in preparation for module search unification. Move jemalloc overrides into a separate .cc Remote obsolete JEMALLOC_NOINIT option. Closes Differential Revision: D6174826 Pulled By: yiwu-arbug fbshipit-source-id: 9970a0289b4490272d15853920d9d7531af91140/
,,0.1184,rocksdb,"Enable cacheline_aligned_alloc() to allocate from jemalloc if enabled. Summary: Reuse WITH_JEMALLOC option in preparation for module search unification. Move jemalloc overrides into a separate .cc Remote obsolete JEMALLOC_NOINIT option. Closes Differential Revision: D6174826 Pulled By: yiwu-arbug fbshipit-source-id: 9970a0289b4490272d15853920d9d7531af91140/Fix MinGW build Summary: snprintf is defined as _snprintf, which doesnt exist in the std namespace. Closes Differential Revision: D5070457 Pulled By: yiwu-arbug fbshipit-source-id: 6e1659ac3e86170653b174578da5a8ed16812cbb/"
,,0.1271,rocksdb,"HistogramStat: Handle divide by zero situation Summary: The num() might return cur_num as 0 and we are making sure that cur_num will not be 0 down the path. The mult variable is being set to 100.0/cur_num which makes program crash when cur_num is 0. Closes Differential Revision: D6222594 Pulled By: ajkr fbshipit-source-id: 986154709897ff4dbbeb0e8aa81eb8c0b2a2db76/fix inclusive-exclusiveness of histogram ToString Summary: I spent too much time thinking about histograms lately and realized boundary values fall into the lower bucket, not the upper bucket. Its because were using `std::map::lower_bound` here: Fixed histograms `ToString()` to reflect this. Closes Differential Revision: D5751159 Pulled By: ajkr fbshipit-source-id: 67432bb45849eec9b5bcc0d095551dbc0ee81766/"
,,0.114,rocksdb,"fix HistogramWindowingImpl copy-{assignment,constructor} Summary: their arguments had a typo. Closes Differential Revision: D5752408 Pulled By: ajkr fbshipit-source-id: f2d84489c0c615b12a790f04c42d35cc0dccb02d/"
,,0.0885,rocksdb,DB::DumpSupportInfo should log all supported compression types Summary: DB::DumpSupportInfo should log all supported compression types. Closes Closes Differential Revision: D6777019 Pulled By: yiwu-arbug fbshipit-source-id: 5b17f1ffb2d71224e52f7d9c045434746c789fb0/Move static variables out of the header file Summary: Static variables in header files will be instantiated in every file that includes the header file. This patch moves some of them from options_helper.h to its .cc files. It also moves the static variable out of the offset_of since the template function could also lead to multiple instantiation perhaps due to inlining. Fixes Closes Differential Revision: D6363794 Pulled By: maysamyabandeh fbshipit-source-id: d0a07f061b4d992ab4e0de2706e622131d258fdd/
,,0.0946,rocksdb,DB::DumpSupportInfo should log all supported compression types Summary: DB::DumpSupportInfo should log all supported compression types. Closes Closes Differential Revision: D6777019 Pulled By: yiwu-arbug fbshipit-source-id: 5b17f1ffb2d71224e52f7d9c045434746c789fb0/Move static variables out of the header file Summary: Static variables in header files will be instantiated in every file that includes the header file. This patch moves some of them from options_helper.h to its .cc files. It also moves the static variable out of the offset_of since the template function could also lead to multiple instantiation perhaps due to inlining. Fixes Closes Differential Revision: D6363794 Pulled By: maysamyabandeh fbshipit-source-id: d0a07f061b4d992ab4e0de2706e622131d258fdd/
,,0.2754,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.2126,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/Move static variables out of the header file Summary: Static variables in header files will be instantiated in every file that includes the header file. This patch moves some of them from options_helper.h to its .cc files. It also moves the static variable out of the offset_of since the template function could also lead to multiple instantiation perhaps due to inlining. Fixes Closes Differential Revision: D6363794 Pulled By: maysamyabandeh fbshipit-source-id: d0a07f061b4d992ab4e0de2706e622131d258fdd/"
,,0.0855,rocksdb,Update DBOptions::IncreaseParallelism to use newer background settings Summary: The Options header file recommends using max_background_jobs rather than directly setting max_background_compactions or max_background_flushes. Ive personally seen a performance problem where stalls were happening because the one background flushing thread was blocked that was fixed by this change Closes Differential Revision: D6473178 Pulled By: ajkr fbshipit-source-id: 67c892ceb7b1909d251492640cb15a0f2262b7ed/
,,0.2944,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.2881,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.2881,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.2694,rocksdb,"Compilation fixes for powerpc build, error and missing header guards Summary: This pull request contains miscellaneous compilation fixes. Thanks, Chinmay Closes Differential Revision: D6941424 Pulled By: sagar0 fbshipit-source-id: fe9c26507bf131221f2466740204bff40a15614a/fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.2897,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.2913,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.1262,rocksdb,fix ThreadStatus for bottom-pri compaction threads Summary: added `ThreadType::BOTTOM_PRIORITY` which is used in the `ThreadStatus` object to indicate the thread is used for bottom-pri compactions. Previously there was a bug where we mislabeled such threads as `ThreadType::LOW_PRIORITY`. Closes Differential Revision: D6559428 Pulled By: ajkr fbshipit-source-id: 96b1a50a9c19492b1a5fd1b77cf7061a6f9f1d1c/
,,0.2849,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.2944,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.0898,rocksdb,Update endif/else behavior for unreachable code error on Windows. Summary: Per Closes Differential Revision: D6766126 Pulled By: gfosco fbshipit-source-id: e441a15e8aec6747c613d68f4f0621b605eb48a0/
,,0.1221,rocksdb,Fix DeleteScheduler::MarkAsTrash() handling existing trash Summary: DeleteScheduler::MarkAsTrash() dont handle existing .trash files correctly This cause rocksdb to not being able to delete existing .trash files on restart Closes Differential Revision: D6548003 Pulled By: IslamAbdelRahman fbshipit-source-id: c3800639412e587a690062c63076a5a08881e0e6/
,,0.2881,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.2849,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.11199999999999999,rocksdb,Fix WriteBatch rep_ format for RangeDeletion records Summary: This is a small amount of general cleanup I made while experimenting with Closes Differential Revision: D6788365 Pulled By: yiwu-arbug fbshipit-source-id: 2716e5aabd5424a4dfdaa954361a62c8eb721ae2/
,,0.1341,rocksdb,fix ThreadStatus for bottom-pri compaction threads Summary: added `ThreadType::BOTTOM_PRIORITY` which is used in the `ThreadStatus` object to indicate the thread is used for bottom-pri compactions. Previously there was a bug where we mislabeled such threads as `ThreadType::LOW_PRIORITY`. Closes Differential Revision: D6559428 Pulled By: ajkr fbshipit-source-id: 96b1a50a9c19492b1a5fd1b77cf7061a6f9f1d1c/
,,0.1053,rocksdb,"add WriteBatch::WriteBatch(std::string&&) Summary: to save a string copy for some use cases. The change is pretty straightforward, please feel free to let me know if you want to suggest any tests for it. Closes Differential Revision: D6706828 Pulled By: yiwu-arbug fbshipit-source-id: 873ce4442937bdc030b395c7f99228eda7f59eb7/"
,,0.1104,rocksdb,fix a typo (of a potential vi user) Summary: Closes Differential Revision: D6939089 Pulled By: siying fbshipit-source-id: ccce3ae10cc5ff50a74b85804afd044b21a3c3e2/Rewrite comments on use_fsync option Summary: This replaces a vague warning about the mostly-obsolete ext3 filesystem with a more detailed note about a historical bug in the still-relevant ext4. Fixes Closes Differential Revision: D6834881 Pulled By: siying fbshipit-source-id: 7771ef5c89a54c0ac17821680779c48178d0b400/
,,0.0966,rocksdb,"Fixed get version on windows, moved throwing exceptions into cc file. Summary: Fixes for msys2 and mingw, hide exceptions into cpp file. Closes Differential Revision: D6746707 Pulled By: yiwu-arbug fbshipit-source-id: 456b38df80bc48b8386a2cf87f669b5a4f9999a4/"
,,0.21899999999999997,rocksdb,"Blob DB: miscellaneous changes Summary: * Expose garbage collection related options * Minor logging and counter name update * Remove unused constants. Closes Differential Revision: D6867077 Pulled By: yiwu-arbug fbshipit-source-id: 6c3272a9c9d78b125a0bd6b2e56d00d087cdd6c8/Blob DB: Add statistics Summary: Adding a list of blob db counters. Also remove WaStats() which doesnt expose the stats and can be substitute by (BLOB_DB_BYTES_WRITTEN / BLOB_DB_BLOB_FILE_BYTES_WRITTEN). Closes Differential Revision: D6394216 Pulled By: yiwu-arbug fbshipit-source-id: 017508c8ff3fcd7ea7403c64d0f9834b24816803/Add a ticker stat for number of keys skipped during iteration Summary: This diff adds a new ticker stat, NUMBER_ITER_SKIP, to count the number of internal keys skipped during iteration. Keys can be skipped due to deletes, or lower sequence number, or higher sequence number than the one requested. Also, fix the issue when StatisticsData is naturally aligned on cacheline boundary, padding becomes a zero size array, which the Windows compiler doesnt like. So add a cacheline worth of padding in that case to keep it happy. We cannot conditionally add padding as gcc doesnt allow using sizeof in preprocessor directives. Closes Differential Revision: D6353897 Pulled By: anand1976 fbshipit-source-id: 441d5a09af9c4e22e7355242dfc0c7b27aa0a6c2/"
,,0.1085,rocksdb,Remove incorrect comment Summary: We actually create individual compaction filter from compaction filter factory per sub-compaction in `CompactionJob::ProcessKeyValueCompaction`: The comment seems incorrect. Closes Differential Revision: D6598455 Pulled By: yiwu-arbug fbshipit-source-id: a6bc059a9103b87a73ae6ec4bb01ca33f5d48cf5/
,,0.13699999999999998,rocksdb,Java: Add copy constructors for various option classes Summary: Add Java-side copy constructors for: Options DBOptions ColumnFamilyOptions WriteOptions along with unit tests to assert the copy worked. NOTE: Unit tests are failing in travis but it looks like a global timeout issue. These tests pass. Closes Differential Revision: D6874425 Pulled By: sagar0 fbshipit-source-id: 5bde68ea5b5225e071faea2628bf8bbf10bd65ab/
,,0.1237,rocksdb,Java: Add copy constructors for various option classes Summary: Add Java-side copy constructors for: Options DBOptions ColumnFamilyOptions WriteOptions along with unit tests to assert the copy worked. NOTE: Unit tests are failing in travis but it looks like a global timeout issue. These tests pass. Closes Differential Revision: D6874425 Pulled By: sagar0 fbshipit-source-id: 5bde68ea5b5225e071faea2628bf8bbf10bd65ab/
,,0.13699999999999998,rocksdb,Java: Add copy constructors for various option classes Summary: Add Java-side copy constructors for: Options DBOptions ColumnFamilyOptions WriteOptions along with unit tests to assert the copy worked. NOTE: Unit tests are failing in travis but it looks like a global timeout issue. These tests pass. Closes Differential Revision: D6874425 Pulled By: sagar0 fbshipit-source-id: 5bde68ea5b5225e071faea2628bf8bbf10bd65ab/
,,0.1326,rocksdb,Java: Add copy constructors for various option classes Summary: Add Java-side copy constructors for: Options DBOptions ColumnFamilyOptions WriteOptions along with unit tests to assert the copy worked. NOTE: Unit tests are failing in travis but it looks like a global timeout issue. These tests pass. Closes Differential Revision: D6874425 Pulled By: sagar0 fbshipit-source-id: 5bde68ea5b5225e071faea2628bf8bbf10bd65ab/
,,0.1311,rocksdb,Java: Add copy constructors for various option classes Summary: Add Java-side copy constructors for: Options DBOptions ColumnFamilyOptions WriteOptions along with unit tests to assert the copy worked. NOTE: Unit tests are failing in travis but it looks like a global timeout issue. These tests pass. Closes Differential Revision: D6874425 Pulled By: sagar0 fbshipit-source-id: 5bde68ea5b5225e071faea2628bf8bbf10bd65ab/
,,0.2833,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.1403,rocksdb,"fix for checkpoint directory with trailing slash(es) Summary: previously if `checkpoint_dir` contained a trailing slash, wed attempt to create the `.tmp` directory under `checkpoint_dir` due to simply concatenating `checkpoint_dir + "".tmp""`. This failed because `checkpoint_dir` hadnt been created yet and our directory creation is non-recursive. This PR fixes the issue by always creating the `.tmp` directory in the same parent as `checkpoint_dir` by stripping trailing slashes before concatenating. Closes Differential Revision: D6574952 Pulled By: ajkr fbshipit-source-id: a6daa6777a901eac2460cd0140c9515f7241aefc/"
,,0.0968,rocksdb,"Fix checkpoint_test directory setup/cleanup Summary: Change directory name from ""db_test"" to ""checkpoint_test"". Previously it used the same directory as `db_test` Systematically cleanup snapshot and snapshot staging directories before each test. Previously a failed test run caused subsequent runs to fail, particularly when the first failure caused ""snapshot.tmp"" to not be cleaned up. Closes Differential Revision: D6691015 Pulled By: ajkr fbshipit-source-id: 4fc2ac2e21ff2617ea0e96297c5132b5f2eefd79/"
,,0.1843,rocksdb,"Fix UBSAN Error in WritePreparedTransactionTest Summary: WritePreparedTransactionTest has the UBSAN error because the wrong order of its parent class construction. Fix it. Closes Differential Revision: D6928975 Pulled By: siying fbshipit-source-id: 13edfd5cb9cf73f1ac5ae3b6f53061d32783733d/Make WithParamInterface virtual in transaction_test Summary: Without this patch, ubsan_check is currently failing with this error: ``` utilities/transactions/write_prepared_transaction_test.cc:369:63: runtime error: member call on address 0x0000051649f8 which does not point to an object of type WithParamInterface 0x0000051649f8: note: object has invalid vptr ``` Tested by `COMPILE_WITH_UBSAN=1 make transaction_test` and running `./write_prepared_transaction_test Closes Differential Revision: D6850087 Pulled By: maysamyabandeh fbshipit-source-id: 5b254da8504b8757f7aec8a820ad464154da1a1d/Split SnapshotConcurrentAccessTest into 20 sub tests Summary: SnapshotConcurrentAccessTest sometimes times out when running on the test infra. This patch splits the test into smaller sub-tests to avoid the timeout. It also benefits from lower run-time of each sub-test and increases the coverage of the test. The overall run-time of each final sub-test is at most half of the original test so we should no longer see a timeout. Closes Differential Revision: D6839427 Pulled By: maysamyabandeh fbshipit-source-id: d53fdb157109e2438ca7fe447d0cf4b71f304bd8/WritePrepared Txn: Return NotSupported on iterator refresh Summary: A proper implementation of Iterator::Refresh() for WritePreparedTxnDB would require release and acquire another snapshot. Since MyRocks dont make use of Iterator::Refresh(), we just simply mark it as not supported. Closes Differential Revision: D6599931 Pulled By: yiwu-arbug fbshipit-source-id: 4e1632d967316431424f6e458254ecf9a97567cf/WritePrepared Txn: fix bug with Rollback seq Summary: The sequence number was not properly advanced after a rollback marker. The patch extends the existing unit tests to detect the bug and also fixes it. Closes Differential Revision: D6304291 Pulled By: maysamyabandeh fbshipit-source-id: 1b519c44a5371b802da49c9e32bd00087a8da401/"
,,0.1786,rocksdb,"WritePrepared Txn: Return NotSupported on iterator refresh Summary: A proper implementation of Iterator::Refresh() for WritePreparedTxnDB would require release and acquire another snapshot. Since MyRocks dont make use of Iterator::Refresh(), we just simply mark it as not supported. Closes Differential Revision: D6599931 Pulled By: yiwu-arbug fbshipit-source-id: 4e1632d967316431424f6e458254ecf9a97567cf/WritePrepared Txn: stress test Summary: Augment the existing MySQLStyleTransactionTest to check for more core case scenarios. The changes showed effective in revealing the bugs reported in and Closes Differential Revision: D6476862 Pulled By: maysamyabandeh fbshipit-source-id: 5068497702d67ffc206a58ed96f8578fbb510137/WritePrepared Txn: fix bug with Rollback seq Summary: The sequence number was not properly advanced after a rollback marker. The patch extends the existing unit tests to detect the bug and also fixes it. Closes Differential Revision: D6304291 Pulled By: maysamyabandeh fbshipit-source-id: 1b519c44a5371b802da49c9e32bd00087a8da401/"
,,0.1444,rocksdb,"Make WithParamInterface virtual in transaction_test Summary: Without this patch, ubsan_check is currently failing with this error: ``` utilities/transactions/write_prepared_transaction_test.cc:369:63: runtime error: member call on address 0x0000051649f8 which does not point to an object of type WithParamInterface 0x0000051649f8: note: object has invalid vptr ``` Tested by `COMPILE_WITH_UBSAN=1 make transaction_test` and running `./write_prepared_transaction_test Closes Differential Revision: D6850087 Pulled By: maysamyabandeh fbshipit-source-id: 5b254da8504b8757f7aec8a820ad464154da1a1d/Split SnapshotConcurrentAccessTest into 20 sub tests Summary: SnapshotConcurrentAccessTest sometimes times out when running on the test infra. This patch splits the test into smaller sub-tests to avoid the timeout. It also benefits from lower run-time of each sub-test and increases the coverage of the test. The overall run-time of each final sub-test is at most half of the original test so we should no longer see a timeout. Closes Differential Revision: D6839427 Pulled By: maysamyabandeh fbshipit-source-id: d53fdb157109e2438ca7fe447d0cf4b71f304bd8/Remove assert(s.ok()) from ::DeleteFile Summary: DestroyDB that is used in tests loops over the files returned by ::GetChildren and delete them one by one. Such files might be already deleted in the file system (during DeleteObsoleteFileImpl for example) but will get actually deleted with a delay sometimes before ::DeleteFile is called on the file name. We have some test failures where FaultInjectionTestEnv::DeleteFile fails on assert(s.ok()) during DestroyDB. This patch removes the assert statement to fix that. Closes Differential Revision: D6659545 Pulled By: maysamyabandeh fbshipit-source-id: 4c9552fbcd494dcf3e61d475c11fc965c4388b2c/WritePrepared Txn: fix bug with Rollback seq Summary: The sequence number was not properly advanced after a rollback marker. The patch extends the existing unit tests to detect the bug and also fixes it. Closes Differential Revision: D6304291 Pulled By: maysamyabandeh fbshipit-source-id: 1b519c44a5371b802da49c9e32bd00087a8da401/"
,,0.1072,rocksdb,Split SnapshotConcurrentAccessTest into 20 sub tests Summary: SnapshotConcurrentAccessTest sometimes times out when running on the test infra. This patch splits the test into smaller sub-tests to avoid the timeout. It also benefits from lower run-time of each sub-test and increases the coverage of the test. The overall run-time of each final sub-test is at most half of the original test so we should no longer see a timeout. Closes Differential Revision: D6839427 Pulled By: maysamyabandeh fbshipit-source-id: d53fdb157109e2438ca7fe447d0cf4b71f304bd8/WritePrepared Txn: fix bug with Rollback seq Summary: The sequence number was not properly advanced after a rollback marker. The patch extends the existing unit tests to detect the bug and also fixes it. Closes Differential Revision: D6304291 Pulled By: maysamyabandeh fbshipit-source-id: 1b519c44a5371b802da49c9e32bd00087a8da401/
,,0.1289,rocksdb,Fix leak report by asan on DuplicateKeys test Summary: Deletes the transaction object at the end of the test. Verified by: COMPILE_WITH_ASAN=1 make transaction_test ./transaction_test Closes Differential Revision: D6916473 Pulled By: maysamyabandeh fbshipit-source-id: 8303df25408635d5d3ac2b25f309a3d15957c937/WritePrepared Txn: stress test Summary: Augment the existing MySQLStyleTransactionTest to check for more core case scenarios. The changes showed effective in revealing the bugs reported in and Closes Differential Revision: D6476862 Pulled By: maysamyabandeh fbshipit-source-id: 5068497702d67ffc206a58ed96f8578fbb510137/WritePrepared Txn: fix bug with Rollback seq Summary: The sequence number was not properly advanced after a rollback marker. The patch extends the existing unit tests to detect the bug and also fixes it. Closes Differential Revision: D6304291 Pulled By: maysamyabandeh fbshipit-source-id: 1b519c44a5371b802da49c9e32bd00087a8da401/
,,0.2913,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.3592,rocksdb,"Fix build for linux Summary: * Include `unistd.h` for `sleep(3)` * Include `sys/time.h` for `gettimeofday(3)` * Include `utils/random.h` for `Random64` Error messages: utilities/persistent_cache/hash_table_bench.cc: In constructor Ã«rocksdb::HashTableBenchmark::HashTableBenchmark(rocksdb::HashTableImpl<long unsigned int, std::__cxx11::basic_string<char> >*, size_t, size_t, size_t, size_t)Ã­: utilities/persistent_cache/hash_table_bench.cc:76:28: error: Ã«sleepÃ­ was not declared in this scope /* sleep override */ sleep(1); ^~~~~ utilities/persistent_cache/hash_table_bench.cc:76:28: note: suggested alternative: Ã«strsepÃ­ /* sleep override */ sleep(1); ^~~~~ strsep utilities/persistent_cache/hash_table_bench.cc: In member function Ã«void rocksdb::HashTableBenchmark::RunRead()Ã­: utilities/persistent_cache/hash_table_bench.cc:107:5: error: Ã«Random64Ã­ was not declared in this scope Random64 rgen(time(nullptr)); ^~~~~~~~ utilities/persistent_cache/hash_table_bench.cc:107:5: note: suggested alternative: Ã«random_rÃ­ Random64 rgen(time(nullptr)); ^~~~~~~~ random_r utilities/persistent_cache/hash_table_bench.cc:110:18: error: Ã«rgenÃ­ was not declared in this scope size_t k rgen.Next() % max_prepop_key; ^~~~ utilities/persistent_cache/hash_table_bench.cc: In static member function Ã«static uint64_t rocksdb::HashTableBenchmark::NowInMillSec()Ã­: utilities/persistent_cache/hash_table_bench.cc:153:5: error: Ã«gettimeofdayÃ­ was not declared in this scope gettimeofday(&tv, /*tz=*/nullptr); ^~~~~~~~~~~~ make[2]: *** [CMakeFiles/hash_table_bench.dir/build.make:63: CMakeFiles/hash_table_bench.dir/utilities/persistent_cache/hash_table_bench.cc.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:3346: CMakeFiles/hash_table_bench.dir/all] Error 2 make[1]: *** Waiting for unfinished jobs.... Closes Differential Revision: D6594850 Pulled By: ajkr fbshipit-source-id: fd83957338c210cdfd253763347aafd39476824f/fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.124,rocksdb,"BlobDB: Remove the need to get sequence number per write Summary: Previously we store sequence number range of each blob files, and use the sequence number range to check if the file can be possibly visible by a snapshot. But it adds complexity to the code, since the sequence number is only available after a write. (The current implementation get sequence number by calling GetLatestSequenceNumber(), which is wrong.) With the patch, we are not storing sequence number range, and check if snapshot_sequence obsolete_sequence to decide if the file is visible by a snapshot (previously we check if first_sequence snapshot_sequence obsolete_sequence). Closes Differential Revision: D6571497 Pulled By: yiwu-arbug fbshipit-source-id: ca06479dc1fcd8782f6525b62b7762cd47d61909/"
,,0.1636,rocksdb,"Blob DB: fix crash when DB full but no candidate file to evict Summary: When blob_files is empty, std::min_element will return blobfiles.end(), which cannot be dereference. Fixing it. Closes Differential Revision: D6764927 Pulled By: yiwu-arbug fbshipit-source-id: 86f78700132be95760d35ac63480dfd3a8bbe17a/BlobDB: Remove the need to get sequence number per write Summary: Previously we store sequence number range of each blob files, and use the sequence number range to check if the file can be possibly visible by a snapshot. But it adds complexity to the code, since the sequence number is only available after a write. (The current implementation get sequence number by calling GetLatestSequenceNumber(), which is wrong.) With the patch, we are not storing sequence number range, and check if snapshot_sequence obsolete_sequence to decide if the file is visible by a snapshot (previously we check if first_sequence snapshot_sequence obsolete_sequence). Closes Differential Revision: D6571497 Pulled By: yiwu-arbug fbshipit-source-id: ca06479dc1fcd8782f6525b62b7762cd47d61909/Blob DB: Add statistics Summary: Adding a list of blob db counters. Also remove WaStats() which doesnt expose the stats and can be substitute by (BLOB_DB_BYTES_WRITTEN / BLOB_DB_BLOB_FILE_BYTES_WRITTEN). Closes Differential Revision: D6394216 Pulled By: yiwu-arbug fbshipit-source-id: 017508c8ff3fcd7ea7403c64d0f9834b24816803/Blob DB: not using PinnableSlice move assignment Summary: The current implementation of PinnableSlice move assignment have an issue We are moving away from it instead of try to get the move assignment right, since it is too tricky. Closes Differential Revision: D6319201 Pulled By: yiwu-arbug fbshipit-source-id: 8f3279021f3710da4a4caa14fd238ed2df902c48/"
,,0.0816,rocksdb,"NUMBER_BLOCK_COMPRESSED, etc, shouldnt be treated as timer counter Summary: NUMBER_BLOCK_DECOMPRESSED and NUMBER_BLOCK_COMPRESSED are not reported unless the stats level contain detailed timers, which is wrong. They are normal counters. Fix it. Closes Differential Revision: D6552519 Pulled By: siying fbshipit-source-id: 40899ccea7b2856bb39752616657c0bfd432f6f9/"
,,0.2849,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.1104,rocksdb,"Tests for dynamic universal compaction options Summary: Added a test for three dynamic universal compaction options, in the realm of read amplification: size_ratio min_merge_width max_merge_width Also updated DynamicUniversalCompactionSizeAmplification by adding a check on compaction reason. Found a bug in compaction reason setting while working on this PR, and fixed in . TODO for later: Still to add tests for these options: compression_size_percent, stop_style and trivial_move. Closes Differential Revision: D6822217 Pulled By: sagar0 fbshipit-source-id: 074573fca6389053cbac229891a0163f38bb56c4/"
,,0.1443,rocksdb,"fix Seek with lower_bound Summary: When Seek a key less than `lower_bound`, should return `lower_bound`. ajkr PTAL Closes Differential Revision: D6421126 Pulled By: ajkr fbshipit-source-id: a06c825830573e0040630704f6bcb3f7f48626f7/"
,,0.2786,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.217,rocksdb,"Explictly fail writes if key or value is not smaller than 4GB Summary: Right now, users will encounter unexpected bahavior if they use key or value larger than 4GB. We should explicitly fail the queriers. Closes Differential Revision: D6953895 Pulled By: siying fbshipit-source-id: b60491e1af064fc5d52971956661f6c18ceac24f/Fix WriteBatch rep_ format for RangeDeletion records Summary: This is a small amount of general cleanup I made while experimenting with Closes Differential Revision: D6788365 Pulled By: yiwu-arbug fbshipit-source-id: 2716e5aabd5424a4dfdaa954361a62c8eb721ae2/add WriteBatch::WriteBatch(std::string&&) Summary: to save a string copy for some use cases. The change is pretty straightforward, please feel free to let me know if you want to suggest any tests for it. Closes Differential Revision: D6706828 Pulled By: yiwu-arbug fbshipit-source-id: 873ce4442937bdc030b395c7f99228eda7f59eb7/Fix coverity issues version, write_batch Summary: db/version_builder.cc: 117 base_vstorage_->InternalComparator(); CID 1351713 (#1 of 1): Uninitialized pointer field (UNINIT_CTOR) 2. uninit_member: Non-static class member field level_zero_cmp_.internal_comparator is not initialized in this constructor nor in any functions that it calls. db/version_edit.h: 145 FdWithKeyRange() 146 : fd(), 147 smallest_key(), 148 largest_key() { CID 1418254 (#1 of 1): Uninitialized pointer field (UNINIT_CTOR) 2. uninit_member: Non-static class member file_metadata is not initialized in this constructor nor in any functions that it calls. 149 } db/version_set.cc: 120 } CID 1322789 (#1 of 1): Uninitialized pointer field (UNINIT_CTOR) 4. uninit_member: Non-static class member curr_file_level_ is not initialized in this constructor nor in any functions that it calls. 121 } db/write_batch.cc: 939 assert(cf_mems_); CID 1419862 (#1 of 1): Uninitialized scalar field (UNINIT_CTOR) 3. uninit_member: Non-static class member rebuilding_trx_seq_ is not initialized in this constructor nor in any functions that it calls. 940 } Closes Differential Revision: D6505666 Pulled By: yiwu-arbug fbshipit-source-id: fd2c68948a0280772691a419d72ac7e190951d86/WritePrepared Txn: fix bug with Rollback seq Summary: The sequence number was not properly advanced after a rollback marker. The patch extends the existing unit tests to detect the bug and also fixes it. Closes Differential Revision: D6304291 Pulled By: maysamyabandeh fbshipit-source-id: 1b519c44a5371b802da49c9e32bd00087a8da401/Properly destruct rebuilding_trx_ Summary: When testing rebuilding_trx_ in MemTableInserter might still be set before the tests finishes which would cause ASAN alarms for leaks. This patch deletes the pointers in MemTableInserter destructor. Closes Differential Revision: D6317113 Pulled By: maysamyabandeh fbshipit-source-id: a68be70709a4fff7ac2b768660119311968f9c21/"
,,0.2754,rocksdb,"fix gflags namespace Summary: I started adding gflags support for cmake on linux and got frustrated that Id need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently use the GFLAGS_NAMESPACE macro if available, and if not, that indicates its an old gflags version without configurable namespace so we can simply hardcode ""google"". Closes Differential Revision: D6456973 Pulled By: ajkr fbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656/"
,,0.075,rocksdb,"Fix memleak when DB::DeleteFile() Summary: Because the corresponding read_first_record_cache_ item wasnt erased, memory leaked. Closes Differential Revision: D4363654 Pulled By: ajkr fbshipit-source-id: 7da1adcfc8c380e4ffe05b8769fc2221ad17a225/"
,,0.08800000000000001,rocksdb,"Fix memleak when DB::DeleteFile() Summary: Because the corresponding read_first_record_cache_ item wasnt erased, memory leaked. Closes Differential Revision: D4363654 Pulled By: ajkr fbshipit-source-id: 7da1adcfc8c380e4ffe05b8769fc2221ad17a225/"
,,0.1502,rocksdb,"WritePrepared Txn: Return NotSupported on iterator refresh Summary: A proper implementation of Iterator::Refresh() for WritePreparedTxnDB would require release and acquire another snapshot. Since MyRocks dont make use of Iterator::Refresh(), we just simply mark it as not supported. Closes Differential Revision: D6599931 Pulled By: yiwu-arbug fbshipit-source-id: 4e1632d967316431424f6e458254ecf9a97567cf/"
,,0.0933,rocksdb,"Prevent unnecessary calls to PurgeObsoleteFiles Summary: Split `JobContext::HaveSomethingToDelete` into two functions: itself and `JobContext::HaveSomethingToClean`. Now we wont call `DBImpl::PurgeObsoleteFiles` in cases where we really just need to call `JobContext::Clean`. The change is needed because I want to track pending calls to `PurgeObsoleteFiles` for a bug fix, which is much simpler if we only call it after `FindObsoleteFiles` finds files to delete. Closes Differential Revision: D6690609 Pulled By: ajkr fbshipit-source-id: 61502e7469288afe16a663a1b7df345baeaf246f/"
,,0.115,rocksdb,DB::DumpSupportInfo should log all supported compression types Summary: DB::DumpSupportInfo should log all supported compression types. Closes Closes Differential Revision: D6777019 Pulled By: yiwu-arbug fbshipit-source-id: 5b17f1ffb2d71224e52f7d9c045434746c789fb0/Fix building on FreeBSD Summary: FreeBSD uses jemalloc as the base malloc implementation. The patch has been functional on FreeBSD as of the MariaDB 10.2 port. Closes Differential Revision: D6765742 Pulled By: yiwu-arbug fbshipit-source-id: d55dbc082eecf640ef3df9a21f26064ebe6587e8/
,,0.1395,rocksdb,"WritePrepared Txn: Support merge operator Summary: CompactionIterator invoke MergeHelper::MergeUntil() to do partial merge between snapshot boundaries. Previously it only depend on sequence number to tell snapshot boundary, but we also need to make use of snapshot_checker to verify visibility of the merge operands to the snapshots. For example, say there is a snapshot with seq 2 but only can see data with seq 1. There are three merges, each with seq 1, 2, 3. A correct compaction output would be (1),(2+3). Without taking snapshot_checker into account when generating merge result, compaction will generate output (1+2),(3). By filtering uncommitted keys with read callback, the read path already take care of merges well and dont need additional updates. Closes Differential Revision: D6926087 Pulled By: yiwu-arbug fbshipit-source-id: 8f539d6f897cfe29b6dc27a8992f68c2a629d40a/WritePrepared Txn: update compaction_iterator_test and db_iterator_test Summary: Update compaction_iterator_test with write-prepared transaction DB related tests. Transaction related tests are group in CompactionIteratorWithSnapshotCheckerTest. The existing test are duplicated to make them also test with dummy SnapshotChecker that will say every key is visible to every snapshot (this is okay, we still compare sequence number to verify visibility). Merge related tests are disabled and will be revisit in another PR. Existing db_iterator_tests are also duplicated to test with dummy read_callback that will say every key is committed. Closes Differential Revision: D6909253 Pulled By: yiwu-arbug fbshipit-source-id: 2ae4656b843a55e2e9ff8beecf21f2832f96cd25/WritePrepared Txn: fix compaction filter snapshot checks Summary: Add snapshot_checker check whenever we need to check sequence against snapshots and decide what to do with an input key. The changes are related to one of: * compaction filter * single delete * delete at bottom level * merge Closes Differential Revision: D6537850 Pulled By: yiwu-arbug fbshipit-source-id: 3faba40ed5e37779f4a0cb7ae78af9546659c7f2/"
,,0.2128,rocksdb,"Disable options_settable_test in UBSAN and fix UBSAN failure in blob_Ã– Summary: Ã–db_test options_settable_test wont pass UBSAN so disable it. blob_db_test fails in UBSAN as SnapshotList doesnt initialize all the fields in dummy snapshot. Fix it. I dont understand why only blob_db_test fails though. Closes Differential Revision: D6928681 Pulled By: siying fbshipit-source-id: e31dd300fcdecdfd4f6af279a0987fd0cdec5122/BlobDB: Remove the need to get sequence number per write Summary: Previously we store sequence number range of each blob files, and use the sequence number range to check if the file can be possibly visible by a snapshot. But it adds complexity to the code, since the sequence number is only available after a write. (The current implementation get sequence number by calling GetLatestSequenceNumber(), which is wrong.) With the patch, we are not storing sequence number range, and check if snapshot_sequence obsolete_sequence to decide if the file is visible by a snapshot (previously we check if first_sequence snapshot_sequence obsolete_sequence). Closes Differential Revision: D6571497 Pulled By: yiwu-arbug fbshipit-source-id: ca06479dc1fcd8782f6525b62b7762cd47d61909/"
,,0.0983,rocksdb,"Explictly fail writes if key or value is not smaller than 4GB Summary: Right now, users will encounter unexpected bahavior if they use key or value larger than 4GB. We should explicitly fail the queriers. Closes Differential Revision: D6953895 Pulled By: siying fbshipit-source-id: b60491e1af064fc5d52971956661f6c18ceac24f/"
,,0.2677,rocksdb,"Fix use-after-free in tailing iterator with merge operator Summary: ForwardIterator::SVCleanup() sometimes didnt pin superversion when it was supposed to. See the added test for the scenario. Heres the ASAN output of the added test without the fix (using `COMPILE_WITH_ASAN=1 make`): Closes Differential Revision: D6817414 Pulled By: al13n321 fbshipit-source-id: bc80c44ea78a3a1fa885dfa448a26111f91afb24/WritePrepared Txn: Fix DBIterator and add test Summary: In DBIter, Prev() calls FindValueForCurrentKey() to search the current value backward. If it finds that there are too many stale value being skipped, it falls back to FindValueForCurrentKeyUsingSeek(), seeking directly to the key with snapshot sequence. After introducing read_callback, however, the key it seeks to might not be visible, according to read_callback. It thus needs to keep searching forward until the first visible value. Closes Differential Revision: D6756148 Pulled By: yiwu-arbug fbshipit-source-id: 064e39b1eec5e083af1c10142600f26d1d2697be/WritePrepared Txn: Return NotSupported on iterator refresh Summary: A proper implementation of Iterator::Refresh() for WritePreparedTxnDB would require release and acquire another snapshot. Since MyRocks dont make use of Iterator::Refresh(), we just simply mark it as not supported. Closes Differential Revision: D6599931 Pulled By: yiwu-arbug fbshipit-source-id: 4e1632d967316431424f6e458254ecf9a97567cf/Make iterator invalid on Merge error Summary: Since on merge error, iterator will be set to corrupted status, but it doesnt invalidate the iterator. Fixing it. Closes Differential Revision: D6499094 Pulled By: yiwu-arbug fbshipit-source-id: 80222930f949e31f90a6feaa37ddc3529b510d2c/fix Seek with lower_bound Summary: When Seek a key less than `lower_bound`, should return `lower_bound`. ajkr PTAL Closes Differential Revision: D6421126 Pulled By: ajkr fbshipit-source-id: a06c825830573e0040630704f6bcb3f7f48626f7/Add a ticker stat for number of keys skipped during iteration Summary: This diff adds a new ticker stat, NUMBER_ITER_SKIP, to count the number of internal keys skipped during iteration. Keys can be skipped due to deletes, or lower sequence number, or higher sequence number than the one requested. Also, fix the issue when StatisticsData is naturally aligned on cacheline boundary, padding becomes a zero size array, which the Windows compiler doesnt like. So add a cacheline worth of padding in that case to keep it happy. We cannot conditionally add padding as gcc doesnt allow using sizeof in preprocessor directives. Closes Differential Revision: D6353897 Pulled By: anand1976 fbshipit-source-id: 441d5a09af9c4e22e7355242dfc0c7b27aa0a6c2/"
,,0.2739,rocksdb,"WritePrepared Txn: update compaction_iterator_test and db_iterator_test Summary: Update compaction_iterator_test with write-prepared transaction DB related tests. Transaction related tests are group in CompactionIteratorWithSnapshotCheckerTest. The existing test are duplicated to make them also test with dummy SnapshotChecker that will say every key is visible to every snapshot (this is okay, we still compare sequence number to verify visibility). Merge related tests are disabled and will be revisit in another PR. Existing db_iterator_tests are also duplicated to test with dummy read_callback that will say every key is committed. Closes Differential Revision: D6909253 Pulled By: yiwu-arbug fbshipit-source-id: 2ae4656b843a55e2e9ff8beecf21f2832f96cd25/WritePrepared Txn: Fix DBIterator and add test Summary: In DBIter, Prev() calls FindValueForCurrentKey() to search the current value backward. If it finds that there are too many stale value being skipped, it falls back to FindValueForCurrentKeyUsingSeek(), seeking directly to the key with snapshot sequence. After introducing read_callback, however, the key it seeks to might not be visible, according to read_callback. It thus needs to keep searching forward until the first visible value. Closes Differential Revision: D6756148 Pulled By: yiwu-arbug fbshipit-source-id: 064e39b1eec5e083af1c10142600f26d1d2697be/fix Seek with lower_bound Summary: When Seek a key less than `lower_bound`, should return `lower_bound`. ajkr PTAL Closes Differential Revision: D6421126 Pulled By: ajkr fbshipit-source-id: a06c825830573e0040630704f6bcb3f7f48626f7/Add a ticker stat for number of keys skipped during iteration Summary: This diff adds a new ticker stat, NUMBER_ITER_SKIP, to count the number of internal keys skipped during iteration. Keys can be skipped due to deletes, or lower sequence number, or higher sequence number than the one requested. Also, fix the issue when StatisticsData is naturally aligned on cacheline boundary, padding becomes a zero size array, which the Windows compiler doesnt like. So add a cacheline worth of padding in that case to keep it happy. We cannot conditionally add padding as gcc doesnt allow using sizeof in preprocessor directives. Closes Differential Revision: D6353897 Pulled By: anand1976 fbshipit-source-id: 441d5a09af9c4e22e7355242dfc0c7b27aa0a6c2/"
,,0.1271,rocksdb,Fix DeleteScheduler::MarkAsTrash() handling existing trash Summary: DeleteScheduler::MarkAsTrash() dont handle existing .trash files correctly This cause rocksdb to not being able to delete existing .trash files on restart Closes Differential Revision: D6548003 Pulled By: IslamAbdelRahman fbshipit-source-id: c3800639412e587a690062c63076a5a08881e0e6/
,,0.0897,rocksdb,"Fixed get version on windows, moved throwing exceptions into cc file. Summary: Fixes for msys2 and mingw, hide exceptions into cpp file. Closes Differential Revision: D6746707 Pulled By: yiwu-arbug fbshipit-source-id: 456b38df80bc48b8386a2cf87f669b5a4f9999a4/"
,,0.1569,rocksdb,Fix multiple build failures Summary: * Fix DBTest.CompactRangeWithEmptyBottomLevel lite build failure * Fix DBTest.AutomaticConflictsWithManualCompaction failure introduce by * Fix BlockBasedTableTest::IndexUncompressed should be disabled if snappy is disabled * Fix ASAN failure with DBBasicTest::DBClose test Closes Differential Revision: D6732313 Pulled By: yiwu-arbug fbshipit-source-id: 1eb9b9d9a8d795f56188fa9770db9353f6fdedc5/make compaction_readahead_size_ thread safe Summary: this should fix the failing tsan_check Closes Differential Revision: D6390004 Pulled By: miasantreble fbshipit-source-id: 6cadfc6f68febb1a77b0abcdb5416570dad926a5/
,,0.08800000000000001,rocksdb,Fix write_callback_test compile error Summary: Rename shadow variable name db_impl. Fixing Closes Differential Revision: D6504051 Pulled By: yiwu-arbug fbshipit-source-id: 186c9378dabb11f8d6db56f45c95cc3b029fcb88/
,,0.0949,rocksdb,"Explictly fail writes if key or value is not smaller than 4GB Summary: Right now, users will encounter unexpected bahavior if they use key or value larger than 4GB. We should explicitly fail the queriers. Closes Differential Revision: D6953895 Pulled By: siying fbshipit-source-id: b60491e1af064fc5d52971956661f6c18ceac24f/"
,,0.0829,rocksdb,"Explictly fail writes if key or value is not smaller than 4GB Summary: Right now, users will encounter unexpected bahavior if they use key or value larger than 4GB. We should explicitly fail the queriers. Closes Differential Revision: D6953895 Pulled By: siying fbshipit-source-id: b60491e1af064fc5d52971956661f6c18ceac24f/"
,,0.2142,rocksdb,"Add a ticker stat for number of keys skipped during iteration Summary: This diff adds a new ticker stat, NUMBER_ITER_SKIP, to count the number of internal keys skipped during iteration. Keys can be skipped due to deletes, or lower sequence number, or higher sequence number than the one requested. Also, fix the issue when StatisticsData is naturally aligned on cacheline boundary, padding becomes a zero size array, which the Windows compiler doesnt like. So add a cacheline worth of padding in that case to keep it happy. We cannot conditionally add padding as gcc doesnt allow using sizeof in preprocessor directives. Closes Differential Revision: D6353897 Pulled By: anand1976 fbshipit-source-id: 441d5a09af9c4e22e7355242dfc0c7b27aa0a6c2/"
,,0.1135,rocksdb,fix ThreadStatus for bottom-pri compaction threads Summary: added `ThreadType::BOTTOM_PRIORITY` which is used in the `ThreadStatus` object to indicate the thread is used for bottom-pri compactions. Previously there was a bug where we mislabeled such threads as `ThreadType::LOW_PRIORITY`. Closes Differential Revision: D6559428 Pulled By: ajkr fbshipit-source-id: 96b1a50a9c19492b1a5fd1b77cf7061a6f9f1d1c/
,,0.2668,rocksdb,"fix calling SetOptions on deprecated options Summary: In `cf_options_type_info`, the deprecated options are all considered to have offset zero in the `MutableCFOptions` struct. Previously we werent checking in `GetMutableOptionsFromStrings` whether the provided option was deprecated or not and simply writing the provided value to the offset specified by `cf_options_type_info`. That meant setting any deprecated option would overwrite the first element in the struct, which is `write_buffer_size`. `db_stress` hit this often since it calls `SetOptions` with `soft_rate_limit=0` and `hard_rate_limit=0`, which are both deprecated so cause `write_buffer_size` to be set to zero, which causes it to crash on the following assertion: ``` db_stress: db/memtable.cc:106: rocksdb::MemTable::MemTable(const rocksdb::InternalKeyComparator&, const rocksdb::ImmutableCFOptions&, const rocksdb::MutableCFOptions&, rocksdb::WriteBufferManager*, rocksdb::SequenceNumber, uint32_t): Assertion `ShouldScheduleFlush() failed. ``` We fix it by skipping deprecated options (and logging a warning) when users provide them to `SetOptions`. I didnt want to fail the call for compatibility reasons. Closes Differential Revision: D7572596 Pulled By: ajkr fbshipit-source-id: bd5d84e14c0c39f30c5d4c6df7c1503d2c28ecf1/"
,,0.115,rocksdb,Blob DB: blob_dump to show uncompressed values Summary: Make blob_dump tool able to show uncompressed values if the blob file is compressed. Also show total compressed vs. raw size at the end if is provided. Closes Differential Revision: D7348926 Pulled By: yiwu-arbug fbshipit-source-id: ca709cb4ed5cf6a550ff2987df8033df81516f8e/
,,0.1296,rocksdb,make MockTimeEnv::current_time_ atomic to fix data race Summary: fix a new TSAN failure Closes Differential Revision: D7565310 Pulled By: miasantreble fbshipit-source-id: f672c96e925797b34dec6e20b59527e8eebaa825/Fix up backupable_db stack corruption. Summary: Fix up OACR(Lint) warnings. Closes Differential Revision: D7563869 Pulled By: ajkr fbshipit-source-id: 8c1e5045c8a6a2d85b2933fdbc60fde93bf0c9de/
,,0.0833,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.1582,rocksdb,"Align SST file data blocks to avoid spanning multiple pages Summary: Provide a block_align option in BlockBasedTableOptions to allow alignment of SST file data blocks. This will avoid higher IOPS/throughput load due to 4KB data blocks spanning 2 4KB pages. When this option is set to true, the block alignment is set to lower of block size and 4KB. Closes Differential Revision: D7400897 Pulled By: anand1976 fbshipit-source-id: 04cc3bd144e88e3431a4f97604e63ad7a0f06d44/"
,,0.26,rocksdb,"Align SST file data blocks to avoid spanning multiple pages Summary: Provide a block_align option in BlockBasedTableOptions to allow alignment of SST file data blocks. This will avoid higher IOPS/throughput load due to 4KB data blocks spanning 2 4KB pages. When this option is set to true, the block alignment is set to lower of block size and 4KB. Closes Differential Revision: D7400897 Pulled By: anand1976 fbshipit-source-id: 04cc3bd144e88e3431a4f97604e63ad7a0f06d44/uint64_t and size_t changes to compile for iOS Summary: In attempting to build a static lib for use in iOS, I ran in to lots of type errors between uint64_t and size_t. This PR contains the changes I made to get `TARGET_OS=IOS make static_lib` to succeed while also getting Xcode to build successfully with the resulting `librocksdb.a` library imported. This also compiles for me on macOS and tests fine, but Im really not sure if I made the correct decisions about where to `static_cast` and where to change types. Also up for discussion: is iOS worth supporting? Getting the static lib is just part one, we arent providing any bridging headers or wrappers like the ObjectiveRocks project, it wont be a great experience. Closes Differential Revision: D7106457 Pulled By: gfosco fbshipit-source-id: 82ac2073de7e1f09b91f6b4faea91d18bd311f8e/"
,,0.2433,rocksdb,"WritePrepared Txn: enable rollback in stress test Summary: Rollback was disabled in stress test since there was a concurrency issue in WritePrepared rollback algorithm. The issue is fixed by caching the column family handles in WritePrepared to skip getting them from the db when needed for rollback. Tested by running transaction stress test under tsan. Closes Differential Revision: D7793727 Pulled By: maysamyabandeh fbshipit-source-id: d81ab6fda0e53186ca69944cfe0712ce4869451e/WritePrepared Txn: disable rollback in stress test Summary: WritePrepared rollback implementation is not ready to be invoked in the middle of workload. This is due the lack of synchronization to obtain the cf handle from db. Temporarily disabling this until the problem with rollback is fixed. Closes Differential Revision: D7769041 Pulled By: maysamyabandeh fbshipit-source-id: 0e3b0ce679bc2afba82e653a40afa3f045722754/Windows cumulative patch Summary: This patch addressed several issues. Portability including db_test std::thread port::Thread Cc: and %z to ROCKSDB portable macro. Cc: maysamyabandeh Implement Env::AreFilesSame Make the implementation of file unique number more robust Get rid of C-runtime and go directly to Windows API when dealing with file primitives. Implement GetSectorSize() and aling unbuffered read on the value if available. Adjust Windows Logger for the new interface, implement CloseImpl() Cc: anand1976 Fix test running script issue where $status var was of incorrect scope so the failures were swallowed and not reported. DestroyDB() creates a logger and opens a LOG file in the directory being cleaned up. This holds a lock on the folder and the cleanup is prevented. This fails one of the checkpoin tests. We observe the same in production. We close the log file in this change. Fix DBTest2.ReadAmpBitmapLiveInCacheAfterDBClose failure where the test attempts to open a directory with NewRandomAccessFile which does not work on Windows. Fix DBTest.SoftLimit as it is dependent on thread timing. CC: yiwu-arbug Closes Differential Revision: D7156304 Pulled By: siying fbshipit-source-id: 43db0a757f1dfceffeb2b7988043156639173f5b/"
,,0.0987,rocksdb,"Fix FaultInjectionTestEnv to work with DirectIO Summary: Implemented PositionedAppend() and use_direct_io() for TestWritableFile. With these changes, FaultInjectionTestEnv can be used with DirectIO enabled. Closes Differential Revision: D7244305 Pulled By: yiwu-arbug fbshipit-source-id: f6b7aece53daa0f9977bc684164a0693693e514c/"
,,0.1359,rocksdb,Use nullptr instead of NULL / 0 more consistently. Summary: Closes Differential Revision: D7170968 Pulled By: yiwu-arbug fbshipit-source-id: 308a6b7dd358a04fd9a7de3d927bfd8abd57d348/
,,0.1088,rocksdb,"check return status for Sync() and Append() calls to avoid corruption Summary: Right now in `SyncClosedLogs`, `CopyFile`, and `AddRecord`, where `Sync` and `Append` are invoked in a loop, the error status are not checked. This could lead to potential corruption as later calls will overwrite the error status. Closes Differential Revision: D7678848 Pulled By: miasantreble fbshipit-source-id: 4b0b412975989dfe80348f73217b9c4122a4bd77/"
,,0.1667,rocksdb,"Align SST file data blocks to avoid spanning multiple pages Summary: Provide a block_align option in BlockBasedTableOptions to allow alignment of SST file data blocks. This will avoid higher IOPS/throughput load due to 4KB data blocks spanning 2 4KB pages. When this option is set to true, the block alignment is set to lower of block size and 4KB. Closes Differential Revision: D7400897 Pulled By: anand1976 fbshipit-source-id: 04cc3bd144e88e3431a4f97604e63ad7a0f06d44/"
,,0.0792,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.1617,rocksdb,Update comments about MergeOperator::AllowSingleOperand Summary: Updated comments around AllowSingleOperand. Reason: A couple of users were confused and encountered issues due to no overriding PartialMerge with AllowSingleOperand=true. Ill also look into modifying the default merge operator implementation so that overriding PartialMerge is not mandatory when AllowSingleOp=true. Closes Differential Revision: D7422691 Pulled By: sagar0 fbshipit-source-id: 3d075a6ced0120f5d65cb7ae5412936f1862f342/Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.0813,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.0833,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.0792,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.0772,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.0792,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.0813,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.0813,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.1845,rocksdb,"Align SST file data blocks to avoid spanning multiple pages Summary: Provide a block_align option in BlockBasedTableOptions to allow alignment of SST file data blocks. This will avoid higher IOPS/throughput load due to 4KB data blocks spanning 2 4KB pages. When this option is set to true, the block alignment is set to lower of block size and 4KB. Closes Differential Revision: D7400897 Pulled By: anand1976 fbshipit-source-id: 04cc3bd144e88e3431a4f97604e63ad7a0f06d44/Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/"
,,0.073,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.0772,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.0867,rocksdb,"Improve the output of the RocksJava JUnit runner Summary: This changes the console output when the RocksJava tests are run. It makes spotting the errors and failures much easier; perviously the output was malformed with results like ""ERun"" where the ""E"" represented an error in the preceding test. Closes Differential Revision: D7306172 Pulled By: sagar0 fbshipit-source-id: 3fa6f6e1ca6c6ea7ceef55a23ca81903716132b7/"
,,0.0813,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.1085,rocksdb,Support StringAppendOperator(delimiter_char) constructor in java-api Summary: Fixes Closes Differential Revision: D7196585 Pulled By: sagar0 fbshipit-source-id: a854f3fc906862ecba685b31946e4ef7c0b421c5/Added bytes XOR merge operator Summary: Closes I fixed the merge conflicts etc. Closes Differential Revision: D7128233 Pulled By: sagar0 fbshipit-source-id: 2c23a48c9f0432c290b0cd16a12fb691bb37820c/
,,0.13,rocksdb,Use nullptr instead of NULL / 0 more consistently. Summary: Closes Differential Revision: D7170968 Pulled By: yiwu-arbug fbshipit-source-id: 308a6b7dd358a04fd9a7de3d927bfd8abd57d348/
,,0.1319,rocksdb,Use nullptr instead of NULL / 0 more consistently. Summary: Closes Differential Revision: D7170968 Pulled By: yiwu-arbug fbshipit-source-id: 308a6b7dd358a04fd9a7de3d927bfd8abd57d348/
,,0.085,rocksdb,fix wrong indentation Summary: Somehow the indentation was incorrect in this file. The only change in this PR is to get it right again in order to make the code more readable. Please reject if you think its not worth it. Closes Differential Revision: D6996011 Pulled By: miasantreble fbshipit-source-id: 060514a3a8c910d34bad795b36eb4d278512b154/
,,0.1103,rocksdb,Blob DB: blob_dump to show uncompressed values Summary: Make blob_dump tool able to show uncompressed values if the blob file is compressed. Also show total compressed vs. raw size at the end if is provided. Closes Differential Revision: D7348926 Pulled By: yiwu-arbug fbshipit-source-id: ca709cb4ed5cf6a550ff2987df8033df81516f8e/
,,0.2356,rocksdb,"Fix the memory leak with pinned partitioned filters Summary: The existing unit test did not set the level so the check for pinned partitioned filter/index being properly released from the block cache was not properly exercised as they only take effect in level 0. As a result a memory leak in pinned partitioned filters was hidden. The patch fix the test as well as the bug. Closes Differential Revision: D7559763 Pulled By: maysamyabandeh fbshipit-source-id: 55eff274945838af983c764a7d71e8daff092e4a/uint64_t and size_t changes to compile for iOS Summary: In attempting to build a static lib for use in iOS, I ran in to lots of type errors between uint64_t and size_t. This PR contains the changes I made to get `TARGET_OS=IOS make static_lib` to succeed while also getting Xcode to build successfully with the resulting `librocksdb.a` library imported. This also compiles for me on macOS and tests fine, but Im really not sure if I made the correct decisions about where to `static_cast` and where to change types. Also up for discussion: is iOS worth supporting? Getting the static lib is just part one, we arent providing any bridging headers or wrappers like the ObjectiveRocks project, it wont be a great experience. Closes Differential Revision: D7106457 Pulled By: gfosco fbshipit-source-id: 82ac2073de7e1f09b91f6b4faea91d18bd311f8e/"
,,0.1186,rocksdb,Fix the memory leak with pinned partitioned filters Summary: The existing unit test did not set the level so the check for pinned partitioned filter/index being properly released from the block cache was not properly exercised as they only take effect in level 0. As a result a memory leak in pinned partitioned filters was hidden. The patch fix the test as well as the bug. Closes Differential Revision: D7559763 Pulled By: maysamyabandeh fbshipit-source-id: 55eff274945838af983c764a7d71e8daff092e4a/
,,0.1709,rocksdb,"Align SST file data blocks to avoid spanning multiple pages Summary: Provide a block_align option in BlockBasedTableOptions to allow alignment of SST file data blocks. This will avoid higher IOPS/throughput load due to 4KB data blocks spanning 2 4KB pages. When this option is set to true, the block alignment is set to lower of block size and 4KB. Closes Differential Revision: D7400897 Pulled By: anand1976 fbshipit-source-id: 04cc3bd144e88e3431a4f97604e63ad7a0f06d44/"
,,0.1709,rocksdb,"Align SST file data blocks to avoid spanning multiple pages Summary: Provide a block_align option in BlockBasedTableOptions to allow alignment of SST file data blocks. This will avoid higher IOPS/throughput load due to 4KB data blocks spanning 2 4KB pages. When this option is set to true, the block alignment is set to lower of block size and 4KB. Closes Differential Revision: D7400897 Pulled By: anand1976 fbshipit-source-id: 04cc3bd144e88e3431a4f97604e63ad7a0f06d44/"
,,0.1582,rocksdb,"Align SST file data blocks to avoid spanning multiple pages Summary: Provide a block_align option in BlockBasedTableOptions to allow alignment of SST file data blocks. This will avoid higher IOPS/throughput load due to 4KB data blocks spanning 2 4KB pages. When this option is set to true, the block alignment is set to lower of block size and 4KB. Closes Differential Revision: D7400897 Pulled By: anand1976 fbshipit-source-id: 04cc3bd144e88e3431a4f97604e63ad7a0f06d44/"
,,0.1568,rocksdb,"Align SST file data blocks to avoid spanning multiple pages Summary: Provide a block_align option in BlockBasedTableOptions to allow alignment of SST file data blocks. This will avoid higher IOPS/throughput load due to 4KB data blocks spanning 2 4KB pages. When this option is set to true, the block alignment is set to lower of block size and 4KB. Closes Differential Revision: D7400897 Pulled By: anand1976 fbshipit-source-id: 04cc3bd144e88e3431a4f97604e63ad7a0f06d44/"
,,0.2258,rocksdb,"check return status for Sync() and Append() calls to avoid corruption Summary: Right now in `SyncClosedLogs`, `CopyFile`, and `AddRecord`, where `Sync` and `Append` are invoked in a loop, the error status are not checked. This could lead to potential corruption as later calls will overwrite the error status. Closes Differential Revision: D7678848 Pulled By: miasantreble fbshipit-source-id: 4b0b412975989dfe80348f73217b9c4122a4bd77/uint64_t and size_t changes to compile for iOS Summary: In attempting to build a static lib for use in iOS, I ran in to lots of type errors between uint64_t and size_t. This PR contains the changes I made to get `TARGET_OS=IOS make static_lib` to succeed while also getting Xcode to build successfully with the resulting `librocksdb.a` library imported. This also compiles for me on macOS and tests fine, but Im really not sure if I made the correct decisions about where to `static_cast` and where to change types. Also up for discussion: is iOS worth supporting? Getting the static lib is just part one, we arent providing any bridging headers or wrappers like the ObjectiveRocks project, it wont be a great experience. Closes Differential Revision: D7106457 Pulled By: gfosco fbshipit-source-id: 82ac2073de7e1f09b91f6b4faea91d18bd311f8e/"
,,0.1487,rocksdb,"Split FaultInjectionTest.FaultTest to avoid timeout Summary: tsan flavor of this test occasionally times out in our test infra. The patch split the test to two, each working on half of the option range. Before: [ OK ] FaultTest/FaultInjectionTest.FaultTest/0 (5918 ms) [ OK ] FaultTest/FaultInjectionTest.FaultTest/1 (5336 ms) After: [ OK ] FaultTest/FaultInjectionTestSplitted.FaultTest/0 (2930 ms) [ OK ] FaultTest/FaultInjectionTestSplitted.FaultTest/1 (2676 ms) [ OK ] FaultTest/FaultInjectionTestSplitted.FaultTest/2 (2759 ms) [ OK ] FaultTest/FaultInjectionTestSplitted.FaultTest/3 (2546 ms) Closes Differential Revision: D7894975 Pulled By: maysamyabandeh fbshipit-source-id: 809f1411cbcc27f8aa71a6b29a16b039f51b67c9/Use nullptr instead of NULL / 0 more consistently. Summary: Closes Differential Revision: D7170968 Pulled By: yiwu-arbug fbshipit-source-id: 308a6b7dd358a04fd9a7de3d927bfd8abd57d348/fix UBSAN errors in fault_injection_test Summary: This fixes shift and signed-integer-overflow UBSAN checks in fault_injection_test by using a larger and unsigned type. Closes Reviewed By: siying Differential Revision: D6981116 Pulled By: igorsugak fbshipit-source-id: 3688f62cce570534b161e9b5f42109ebc9ae5a2c/"
,,0.0813,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.1404,rocksdb,"Fix TSAN timeout in MergeOperatorPinningTest.Randomized/x test Summary: [FB Internal] MergeOperatorPinningTest.Randomized/x tests are frequently failing with timeouts when run with tsan, as they are exceeding 10 minute limit for tests. The tests are in turn getting disabled due to frequent failures. I halved the number of rounds to make the test complete sooner. This reduces the number of testing iterations a little, but it still is much better than totally letting the test be disabled. Closes Differential Revision: D7031498 Pulled By: sagar0 fbshipit-source-id: 9a694f2176b235259920a42bf24bca5346f7cff1/"
,,0.0621,rocksdb,doc: fix a typo Summary: s/synchromization/synchronization/ Closes Differential Revision: D7276596 Pulled By: sagar0 fbshipit-source-id: 552ec6d6935f642e1a3a7c552de6c94441ac50e0/
,,0.1319,rocksdb,Use nullptr instead of NULL / 0 more consistently. Summary: Closes Differential Revision: D7170968 Pulled By: yiwu-arbug fbshipit-source-id: 308a6b7dd358a04fd9a7de3d927bfd8abd57d348/
,,0.0887,rocksdb,"avoid double delete on dummy record insertion failure Summary: When the dummy record insertion fails, there is no need to explicitly delete the block as it will be registered for cleanup regardless. Closes Differential Revision: D7537741 Pulled By: miasantreble fbshipit-source-id: fcd3a3d3d382ee8e2c7ced0a4980e683d93a16d6/"
,,0.0874,rocksdb,Fix some typos in comments and docs. Summary: Closes Differential Revision: D7170953 Pulled By: siying fbshipit-source-id: 9cfb8dd88b7266da920c0e0c1e10fb2c5af0641c/
,,0.0891,rocksdb,Fix formatting in log message Summary: Add missing space. Closes Differential Revision: D7956059 Pulled By: miasantreble fbshipit-source-id: 3aeba76385f8726399a3086c46de710636a31191/
,,0.146,rocksdb,"Unbreak MemTableRep API change Summary: The MemTableRep API was broken by this commit: 813719e9525f647aaebf19ca3d4bb6f1c63e2648 This patch reverts the changes and instead adds InsertKey (and etc.) overloads to extend the MemTableRep API without breaking the existing classes that inherit from it. Closes Differential Revision: D7004134 Pulled By: maysamyabandeh fbshipit-source-id: e568d91fe1e17dd76c0c1f6c7dd51a18633b1c4f/Several small ""fixes"" Summary: removed a few unneeded variables fused some variable declarations and their assignments fixed right-trimming code in string_util.cc to not underflow simplifed an assertion move non-nullptr check assertion before dereferencing of that pointer pass an std::string function parameter by const reference instead of by value (avoiding potential copy) Closes Differential Revision: D7004679 Pulled By: sagar0 fbshipit-source-id: 52944952d9b56dfcac3bea3cd7878e315bb563c4/Fix 2 more unused reference errors VS2017 Summary: As in Closes Differential Revision: D6979588 Pulled By: gfosco fbshipit-source-id: e9fb32d04ad45575dfe9de1d79348d158e474197/"
,,0.0789,rocksdb,"fix behavior does not match name for ""IsFileDeletionsEnabled"" Summary: for PR I deleted the original repo for some reason. Sorry for the inconvenience. Closes Differential Revision: D7291671 Pulled By: ajkr fbshipit-source-id: 918490ba86b13fe450d232af436cbe259d847c64/"
,,0.213,rocksdb,"MaxFileSizeForLevel: adjust max_file_size for dynamic level compaction Summary: `MutableCFOptions::RefreshDerivedOptions` always assume base level is L1, which is not true when `level_compaction_dynamic_level_bytes=true` and Level based compaction is used. This PR fixes this by recomputing `max_file_size` at query time (in `MaxFileSizeForLevel`) Fixes In master: ``` Level Files Size(MB) 0 14 846 1 0 0 2 0 0 3 0 0 4 0 0 5 15 366 6 11 481 Cumulative compaction: 3.83 GB write, 2.27 GB read ``` In branch: ``` Level Files Size(MB) 0 9 544 1 0 0 2 0 0 3 0 0 4 0 0 5 0 0 6 445 935 Cumulative compaction: 2.91 GB write, 1.46 GB read ``` db_bench command used: ``` ./db_bench ``` Closes Differential Revision: D7721381 Pulled By: miasantreble fbshipit-source-id: 39afb8503190bac3b466adf9bbf2a9b3655789f8/Fix typo Summary: regrad regard Closes Differential Revision: D7540952 Pulled By: miasantreble fbshipit-source-id: e08c9389f7fccf401c962a4441b62cd5e73a33ad/Throw NoSpace instead of IOError when out of space. Summary: Replaces and is updated from feedback. Closes Differential Revision: D7457395 Pulled By: gfosco fbshipit-source-id: 25a21dd8cfa5a6e42e024208b444d9379d920c82/uint64_t and size_t changes to compile for iOS Summary: In attempting to build a static lib for use in iOS, I ran in to lots of type errors between uint64_t and size_t. This PR contains the changes I made to get `TARGET_OS=IOS make static_lib` to succeed while also getting Xcode to build successfully with the resulting `librocksdb.a` library imported. This also compiles for me on macOS and tests fine, but Im really not sure if I made the correct decisions about where to `static_cast` and where to change types. Also up for discussion: is iOS worth supporting? Getting the static lib is just part one, we arent providing any bridging headers or wrappers like the ObjectiveRocks project, it wont be a great experience. Closes Differential Revision: D7106457 Pulled By: gfosco fbshipit-source-id: 82ac2073de7e1f09b91f6b4faea91d18bd311f8e/Blob DB: remove existing garbage collection implementation Summary: Red diff to remove existing implementation of garbage collection. The current approach is reference counting kind of approach and require a lot of effort to get the size counter right on compaction and deletion. Im going to go with a simple mark-sweep kind of approach and will send another PR for that. CompactionEventListener was added solely for blob db and it adds complexity and overhead to compaction iterator. Removing it as well. Closes Differential Revision: D7130190 Pulled By: yiwu-arbug fbshipit-source-id: c3a375ad2639a3f6ed179df6eda602372cc5b8df/"
,,0.1043,rocksdb,Fix up backupable_db stack corruption. Summary: Fix up OACR(Lint) warnings. Closes Differential Revision: D7563869 Pulled By: ajkr fbshipit-source-id: 8c1e5045c8a6a2d85b2933fdbc60fde93bf0c9de/
,,0.1359,rocksdb,Use nullptr instead of NULL / 0 more consistently. Summary: Closes Differential Revision: D7170968 Pulled By: yiwu-arbug fbshipit-source-id: 308a6b7dd358a04fd9a7de3d927bfd8abd57d348/
,,0.3024,rocksdb,"Windows cumulative patch Summary: This patch addressed several issues. Portability including db_test std::thread port::Thread Cc: and %z to ROCKSDB portable macro. Cc: maysamyabandeh Implement Env::AreFilesSame Make the implementation of file unique number more robust Get rid of C-runtime and go directly to Windows API when dealing with file primitives. Implement GetSectorSize() and aling unbuffered read on the value if available. Adjust Windows Logger for the new interface, implement CloseImpl() Cc: anand1976 Fix test running script issue where $status var was of incorrect scope so the failures were swallowed and not reported. DestroyDB() creates a logger and opens a LOG file in the directory being cleaned up. This holds a lock on the folder and the cleanup is prevented. This fails one of the checkpoin tests. We observe the same in production. We close the log file in this change. Fix DBTest2.ReadAmpBitmapLiveInCacheAfterDBClose failure where the test attempts to open a directory with NewRandomAccessFile which does not work on Windows. Fix DBTest.SoftLimit as it is dependent on thread timing. CC: yiwu-arbug Closes Differential Revision: D7156304 Pulled By: siying fbshipit-source-id: 43db0a757f1dfceffeb2b7988043156639173f5b/"
,,0.3077,rocksdb,"Windows cumulative patch Summary: This patch addressed several issues. Portability including db_test std::thread port::Thread Cc: and %z to ROCKSDB portable macro. Cc: maysamyabandeh Implement Env::AreFilesSame Make the implementation of file unique number more robust Get rid of C-runtime and go directly to Windows API when dealing with file primitives. Implement GetSectorSize() and aling unbuffered read on the value if available. Adjust Windows Logger for the new interface, implement CloseImpl() Cc: anand1976 Fix test running script issue where $status var was of incorrect scope so the failures were swallowed and not reported. DestroyDB() creates a logger and opens a LOG file in the directory being cleaned up. This holds a lock on the folder and the cleanup is prevented. This fails one of the checkpoin tests. We observe the same in production. We close the log file in this change. Fix DBTest2.ReadAmpBitmapLiveInCacheAfterDBClose failure where the test attempts to open a directory with NewRandomAccessFile which does not work on Windows. Fix DBTest.SoftLimit as it is dependent on thread timing. CC: yiwu-arbug Closes Differential Revision: D7156304 Pulled By: siying fbshipit-source-id: 43db0a757f1dfceffeb2b7988043156639173f5b/"
,,0.2919,rocksdb,"Adjust pread/pwrite to return Status Summary: Returning bytes_read causes the caller to call GetLastError() to report failure but the lasterror may be overwritten by then so we lose the error code. Fix up CMake file to include xpress source code only when needed. Fix warning for the uninitialized var. Closes Differential Revision: D7832935 Pulled By: anand1976 fbshipit-source-id: 4be21affb9b85d361b96244f4ef459f492b7cb2b/Windows cumulative patch Summary: This patch addressed several issues. Portability including db_test std::thread port::Thread Cc: and %z to ROCKSDB portable macro. Cc: maysamyabandeh Implement Env::AreFilesSame Make the implementation of file unique number more robust Get rid of C-runtime and go directly to Windows API when dealing with file primitives. Implement GetSectorSize() and aling unbuffered read on the value if available. Adjust Windows Logger for the new interface, implement CloseImpl() Cc: anand1976 Fix test running script issue where $status var was of incorrect scope so the failures were swallowed and not reported. DestroyDB() creates a logger and opens a LOG file in the directory being cleaned up. This holds a lock on the folder and the cleanup is prevented. This fails one of the checkpoin tests. We observe the same in production. We close the log file in this change. Fix DBTest2.ReadAmpBitmapLiveInCacheAfterDBClose failure where the test attempts to open a directory with NewRandomAccessFile which does not work on Windows. Fix DBTest.SoftLimit as it is dependent on thread timing. CC: yiwu-arbug Closes Differential Revision: D7156304 Pulled By: siying fbshipit-source-id: 43db0a757f1dfceffeb2b7988043156639173f5b/"
,,0.3129,rocksdb,"Windows cumulative patch Summary: This patch addressed several issues. Portability including db_test std::thread port::Thread Cc: and %z to ROCKSDB portable macro. Cc: maysamyabandeh Implement Env::AreFilesSame Make the implementation of file unique number more robust Get rid of C-runtime and go directly to Windows API when dealing with file primitives. Implement GetSectorSize() and aling unbuffered read on the value if available. Adjust Windows Logger for the new interface, implement CloseImpl() Cc: anand1976 Fix test running script issue where $status var was of incorrect scope so the failures were swallowed and not reported. DestroyDB() creates a logger and opens a LOG file in the directory being cleaned up. This holds a lock on the folder and the cleanup is prevented. This fails one of the checkpoin tests. We observe the same in production. We close the log file in this change. Fix DBTest2.ReadAmpBitmapLiveInCacheAfterDBClose failure where the test attempts to open a directory with NewRandomAccessFile which does not work on Windows. Fix DBTest.SoftLimit as it is dependent on thread timing. CC: yiwu-arbug Closes Differential Revision: D7156304 Pulled By: siying fbshipit-source-id: 43db0a757f1dfceffeb2b7988043156639173f5b/"
,,0.3024,rocksdb,"Windows cumulative patch Summary: This patch addressed several issues. Portability including db_test std::thread port::Thread Cc: and %z to ROCKSDB portable macro. Cc: maysamyabandeh Implement Env::AreFilesSame Make the implementation of file unique number more robust Get rid of C-runtime and go directly to Windows API when dealing with file primitives. Implement GetSectorSize() and aling unbuffered read on the value if available. Adjust Windows Logger for the new interface, implement CloseImpl() Cc: anand1976 Fix test running script issue where $status var was of incorrect scope so the failures were swallowed and not reported. DestroyDB() creates a logger and opens a LOG file in the directory being cleaned up. This holds a lock on the folder and the cleanup is prevented. This fails one of the checkpoin tests. We observe the same in production. We close the log file in this change. Fix DBTest2.ReadAmpBitmapLiveInCacheAfterDBClose failure where the test attempts to open a directory with NewRandomAccessFile which does not work on Windows. Fix DBTest.SoftLimit as it is dependent on thread timing. CC: yiwu-arbug Closes Differential Revision: D7156304 Pulled By: siying fbshipit-source-id: 43db0a757f1dfceffeb2b7988043156639173f5b/"
,,0.3035,rocksdb,"Windows cumulative patch Summary: This patch addressed several issues. Portability including db_test std::thread port::Thread Cc: and %z to ROCKSDB portable macro. Cc: maysamyabandeh Implement Env::AreFilesSame Make the implementation of file unique number more robust Get rid of C-runtime and go directly to Windows API when dealing with file primitives. Implement GetSectorSize() and aling unbuffered read on the value if available. Adjust Windows Logger for the new interface, implement CloseImpl() Cc: anand1976 Fix test running script issue where $status var was of incorrect scope so the failures were swallowed and not reported. DestroyDB() creates a logger and opens a LOG file in the directory being cleaned up. This holds a lock on the folder and the cleanup is prevented. This fails one of the checkpoin tests. We observe the same in production. We close the log file in this change. Fix DBTest2.ReadAmpBitmapLiveInCacheAfterDBClose failure where the test attempts to open a directory with NewRandomAccessFile which does not work on Windows. Fix DBTest.SoftLimit as it is dependent on thread timing. CC: yiwu-arbug Closes Differential Revision: D7156304 Pulled By: siying fbshipit-source-id: 43db0a757f1dfceffeb2b7988043156639173f5b/"
,,0.2066,rocksdb,"uint64_t and size_t changes to compile for iOS Summary: In attempting to build a static lib for use in iOS, I ran in to lots of type errors between uint64_t and size_t. This PR contains the changes I made to get `TARGET_OS=IOS make static_lib` to succeed while also getting Xcode to build successfully with the resulting `librocksdb.a` library imported. This also compiles for me on macOS and tests fine, but Im really not sure if I made the correct decisions about where to `static_cast` and where to change types. Also up for discussion: is iOS worth supporting? Getting the static lib is just part one, we arent providing any bridging headers or wrappers like the ObjectiveRocks project, it wont be a great experience. Closes Differential Revision: D7106457 Pulled By: gfosco fbshipit-source-id: 82ac2073de7e1f09b91f6b4faea91d18bd311f8e/"
,,0.1056,rocksdb,Add SST ingestion to ldb (#4205) Summary: We add two subcommands `write_extern_sst` and `ingest_extern_sst` to ldb. This PR avoids changing existing code because we hope to cherry-pick to earlier releases to support compatibility check for external SST file ingestion. Pull Request resolved: Differential Revision: D9112711 Pulled By: riversand963 fbshipit-source-id: 7cae88380d4de86da8440230e87eca66755648e4/
,,0.3033,rocksdb,"Fix db_bench default compression level (#4248) Summary: db_benchs previous default compression level (-1) was not the default compression level in all libraries. In particular, in ZSTD negative values are valid compression levels, while ZSTDs default compression level is three. This PR changes db_benchs default to be RocksDBs library-independent default compression level (see I also changed a couple other flags to get their default values from an options object directly rather than hardcoding. Pull Request resolved: Differential Revision: D9235140 Pulled By: ajkr fbshipit-source-id: be4e0722d59fa1968832183db36d1d20fcf11e5b/Fix lite build failure in db_bench due to trace/replay (#4225) Summary: Fix lite build failure in db_bench due to trace/replay feature. Pull Request resolved: Differential Revision: D9153303 Pulled By: sagar0 fbshipit-source-id: 9f7a8035429d0dcdbe99616d11389ed7bccf44be/Fixed the db_bench MergeRandom only access CF_default (#4155) Summary: When running the tracing and analyzing, I found that MergeRandom benchmark in db_bench only access the default column family even the is specified > 1. changes: Using the db_with_cfh as DB to randomly select the column family to execute the Merge operation if is specified > 1. Tested with make asan_check and verified in tracing Pull Request resolved: Differential Revision: D8907888 Pulled By: zhichao-cao fbshipit-source-id: 2b4bc8fe0e99c8f262f5be6b986c7025d62cf850/Fix unsigned int flag in db_bench (#4129) Summary: `DEFINE_uint32` was unavailable on some platforms, e.g., Use `DEFINE_uint64` instead which should work as its used many times elsewhere in this file. Pull Request resolved: Differential Revision: D8830311 Pulled By: ajkr fbshipit-source-id: b4fc90ba3f50e649c070ce8069c68e530d731f05/Pin top-level index on partitioned index/filter blocks (#4037) Summary: Top-level index in partitioned index/filter blocks are small and could be pinned in memory. So far we use that by cache_index_and_filter_blocks to false. This however make it difficult to keep account of the total memory usage. This patch introduces pin_top_level_index_and_filter which in combination with cache_index_and_filter_blocks=true keeps the top-level index in cache and yet pinned them to avoid cache misses and also cache lookup overhead. Closes Differential Revision: D8596218 Pulled By: maysamyabandeh fbshipit-source-id: 3a5f7f9ca6b4b525b03ff6bd82354881ae974ad2/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/Compile error in db bench tool Summary: Small format error below causes build to fail. I believe that this : ``` fprintf(stderr, ""num reads to do %lu\n"", reads_); ``` Can be changed to this: ``` fprintf(stderr, ""num reads to do %"" PRIu64 ""\n"", reads_); ``` Successful build ``` CC utilities/blob_db/blob_dump_tool.o AR librocksdb_debug.a ar: creating archive librocksdb_debug.a /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb_debug.a(rocks_lua_compaction_filter.o) has no symbols CC tools/db_bench.o CC tools/db_bench_tool.o tools/db_bench_tool.cc:4532:46: error: format specifies type unsigned long but the argument has type int64_t (aka long long) [-Werror,-Wformat] fprintf(stderr, ""num reads to do %lu\n"", reads_); ~~~ ^~~~~~ %lld 1 error generated. make: *** [tools/db_bench_tool.o] Error 1 ``` ``` $ cd rocksdb $ make all $ g++ Configured with: Apple LLVM version 9.1.0 (clang-902.0.39.1) Target: x86_64-apple-darwin17.5.0 Thread model: posix InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin ``` Closes Differential Revision: D8215710 Pulled By: siying fbshipit-source-id: 15e49fb02a818fec846e9f9b2a50e372b6b67751/LRUCache midpoint insertion Summary: Implement midpoint insertion strategy where new blocks will be insert to the middle of LRU list, then move the head on the first hit in cache. Closes Differential Revision: D8100895 Pulled By: yiwu-arbug fbshipit-source-id: f4bd83cb8be469e5d02072cfc8bd66011391f3da/"
,,0.1197,rocksdb,Add SST ingestion to ldb (#4205) Summary: We add two subcommands `write_extern_sst` and `ingest_extern_sst` to ldb. This PR avoids changing existing code because we hope to cherry-pick to earlier releases to support compatibility check for external SST file ingestion. Pull Request resolved: Differential Revision: D9112711 Pulled By: riversand963 fbshipit-source-id: 7cae88380d4de86da8440230e87eca66755648e4/Fixed the fprintf of uint64_t by using PRIu64 (#3963) Summary: Fixed the fprintf format of uint64_t by using PRIu64 in file tools/ldb_cmd.cc Closes Differential Revision: D8306179 Pulled By: zhichao-cao fbshipit-source-id: 597dcd55321576801bbf2cf4714736ebc4750a0c/
,,0.272,rocksdb,"Support dictionary compression in stress/crash tests (#4234) Summary: Add `--compression_max_dict_bytes` and `--compression_zstd_max_train_bytes` flags to stress test Randomly enable/disable the above flags in crash test Set `--compression_type=zstd` in FB-specific crash test runs Pull Request resolved: Differential Revision: D9187207 Pulled By: ajkr fbshipit-source-id: 8d78cf8d8e1165f2cd1c32e069b73726b5bc1fd2/Protect external file when ingesting (#4099) Summary: If crash happen after a hard link established, Recover function may reuse the file number that has already assigned to the internal file, and this will overwrite the external file. To protect the external file, we have to make sure the file number will never being reused. Pull Request resolved: Differential Revision: D9034092 Pulled By: riversand963 fbshipit-source-id: 3f1a737440b86aa2ef01673e5013aacbb7c33e28/db_stress to cover upper bound in iterators (#4162) Summary: db_stress doesnt cover upper or lower bound in iterators. Try to cover it by randomly assigning a random one. Also in prefix scan tests, with 50% of the chance, set next prefix as the upper bound. Pull Request resolved: Differential Revision: D8953507 Pulled By: siying fbshipit-source-id: f0f04e9cb6c07cbebbb82b892ca23e0daeea708b/Fix dangling checkpoint pointer in db_stress (#4042) Summary: Fix db_stress failed to delete checkpoint pointer. Its caught by asan_crash test. Closes Differential Revision: D8592604 Pulled By: yiwu-arbug fbshipit-source-id: 7b2d67d5e3dfb05f71c33fcf320482303e97d3ef/Cleanup staging directory at start of checkpoint (#4035) Summary: Attempt to clean the checkpoint staging directory before starting a checkpoint. It was already cleaned up at the end of checkpoint. But it wasnt cleaned up in the edge case where the process crashed while staging checkpoint files. Attempt to clean the checkpoint directory before calling `Checkpoint::Create` in `db_stress`. This handles the case where checkpoint directory was created by a previous `db_stress` run but the process crashed before cleaning it up. Use `DestroyDB` for cleaning checkpoint directory since a checkpoint is a DB. Closes Reviewed By: yiwu-arbug Differential Revision: D8580223 Pulled By: ajkr fbshipit-source-id: 28c667400e249fad0fdedc664b349031b7b61599/Fix a warning (treated as error) caused by type mismatch. Summary: Closes Differential Revision: D8573061 Pulled By: riversand963 fbshipit-source-id: 112324dcb35956d6b3ec891073f4f21493933c8b/Support file ingestion in stress test (#4018) Summary: Once per `ingest_external_file_one_in` operations, uses SstFileWriter to create a file containing `ingest_external_file_width` consecutive keys. The file is named containing the thread ID to avoid clashes. The file is then added to the DB using `IngestExternalFile`. We cant enable it by default in crash test because `nooverwritepercent` and `test_batches_snapshot` both must be zero for the DBs whole lifetime. Perhaps we should setup a separate test with that config as range deletion also requires it. Closes Differential Revision: D8507698 Pulled By: ajkr fbshipit-source-id: 1437ea26fd989349a9ce8b94117241c65e40f10f/Support pipelined write in stress/crash tests Summary: Closes Differential Revision: D8508681 Pulled By: ajkr fbshipit-source-id: 23a3c07d642386446e322b02e69cdf70d12ef009/Support backup and checkpoint in db_stress (#4005) Summary: Add the `backup_one_in` and `checkpoint_one_in` options to periodically trigger backups and checkpoints. The directory names contain thread ID to avoid clashing with parallel backups/checkpoints. Enable checkpoint in crash test so our CI runs will use it. Didnt enable backup in crash test since it copies all the files which is too slow. Closes Differential Revision: D8472275 Pulled By: ajkr fbshipit-source-id: ff91bdc37caac4ffd97aea8df96b3983313ac1d5/Run manual compaction in stress/crash tests (#3936) Summary: Add support to `db_stress` for `CompactRange` Enable `CompactRange` and `CompactFiles` in crash tests Closes Differential Revision: D8230953 Pulled By: ajkr fbshipit-source-id: 208f9980b5bc8c204b1fa726e83791ad674e21e8/Choose unique keys faster in db_stress (#3990) Summary: db_stress initialization randomly chooses a set of keys to not overwrite. It was doing it separately for each column family. That caused 30+ second initialization times for the non-simple crash tests, which have 10 CFs. This PR: reuses the same set of randomly chosen no-overwrite keys across all CFs logs a couple more timestamps so we can more easily see initialization time Closes Differential Revision: D8393821 Pulled By: ajkr fbshipit-source-id: d0b263a298df607285ffdd8b0983ff6575cc6c34/Fix build errors. Summary: Closes Differential Revision: D8322775 Pulled By: riversand963 fbshipit-source-id: bd73067bd5d3ed4627348f0685bc499359ad6442/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Configure direct I/O statically in db_stress Summary: Previously `db_stress` attempted to configure direct I/O dynamically in `SetOptions()` which had multiple problems (ummm mustve never been tested): Its a DB option so SetDBOptions shouldve been called instead Its not a dynamic option so even SetDBOptions would fail It required enabling SyncPoint to mask O_DIRECT since it had no way to detect whether the DB directory was in tmpfs or not. This required locking that consumed ~80% of db_stress CPU. In this PR I delete the broken dynamic config and instead configure it statically, only enabling it if the DB directory truly supports O_DIRECT. Closes Differential Revision: D8238120 Pulled By: ajkr fbshipit-source-id: 60bb2deebe6c9b54a3f788079261715b4a229279/Catchup with posix features Summary: Catch up with Posix features NewWritableRWFile must fail when file does not exists Implement Env::Truncate() Adjust Env options optimization functions Implement MemoryMappedBuffer on Windows. Closes Differential Revision: D8053610 Pulled By: ajkr fbshipit-source-id: ccd0d46c29648a9f6f496873bc1c9d6c5547487e/Avoid single-deleting merge operands in db_stress Summary: I reprod some of the ""unexpected value"" failures showing up in our CI lately and they always happened on keys that have a mix of single deletes and merge operands. The `SingleDelete()` API comment mentions its incompatible with `Merge()`, so this PR prevents `db_stress` from mixing them. Closes Differential Revision: D8097346 Pulled By: ajkr fbshipit-source-id: 357a48c6a31156f4f8db3ce565638ad924c437a1/Set the default value of max_manifest_file_size. Summary: In the past, the default value of max_manifest_file_size is uint64_t::MAX, allowing a long running RocksDB process to grow its MANIFEST file to take up the entire disk, as reported in [issue 3851]( It is reasonable and common to provide a default non-max value for this option. Therefore, I set the value to 1GB. siying miasantreble Please let me know whether this looks good to you. Thanks Closes Differential Revision: D8051524 Pulled By: riversand963 fbshipit-source-id: 50251f0804b1fa933a19a30d19d261ea8b9d2b72/Fix db_stress build on mac Summary: I noticed, while debugging an unrelated issue, that db_stress is failing to build on mac, leading to a failed `make all`. ``` $ make db_stress ... tools/db_stress.cc:862:69: error: cannot initialize a parameter of type uint64_t * (aka unsigned long long *) with an rvalue of type size_t * (aka unsigned long *) status FLAGS_env->GetFileSize(FLAGS_expected_values_path, &size); ^~~~~ ./include/rocksdb/env.h:277:66: note: passing argument to parameter file_size here virtual Status GetFileSize(const std::string& fname, uint64_t* file_size) 0; ^ 1 error generated. make: *** [tools/db_stress.o] Error 1 make: *** Waiting for unfinished jobs.... ``` Closes Differential Revision: D7979236 Pulled By: sagar0 fbshipit-source-id: 0615e7bb5405bade71e4203803bf723720422d62/"
,,0.1009,rocksdb,Add SST ingestion to ldb (#4205) Summary: We add two subcommands `write_extern_sst` and `ingest_extern_sst` to ldb. This PR avoids changing existing code because we hope to cherry-pick to earlier releases to support compatibility check for external SST file ingestion. Pull Request resolved: Differential Revision: D9112711 Pulled By: riversand963 fbshipit-source-id: 7cae88380d4de86da8440230e87eca66755648e4/
,,0.2115,rocksdb,"Fix LRUCache missing null check on destruct Summary: Fix LRUCache missing null check on destruct. The check is needed if LRUCache::DisownData is called. Closes Differential Revision: D8191631 Pulled By: yiwu-arbug fbshipit-source-id: d5014f6e49b51692c18a25fb55ece935f5a023c4/LRUCache midpoint insertion Summary: Implement midpoint insertion strategy where new blocks will be insert to the middle of LRU list, then move the head on the first hit in cache. Closes Differential Revision: D8100895 Pulled By: yiwu-arbug fbshipit-source-id: f4bd83cb8be469e5d02072cfc8bd66011391f3da/refactor constructor of LRUCacheShard Summary: Update LRUCacheShard constructor so that adding new params to it dont need to add extra SetXXX() methods. Closes Differential Revision: D8128618 Pulled By: yiwu-arbug fbshipit-source-id: 6afa715de1493a50de413678761a765e3af9b83b/"
,,0.1667,rocksdb,"LRUCache midpoint insertion Summary: Implement midpoint insertion strategy where new blocks will be insert to the middle of LRU list, then move the head on the first hit in cache. Closes Differential Revision: D8100895 Pulled By: yiwu-arbug fbshipit-source-id: f4bd83cb8be469e5d02072cfc8bd66011391f3da/refactor constructor of LRUCacheShard Summary: Update LRUCacheShard constructor so that adding new params to it dont need to add extra SetXXX() methods. Closes Differential Revision: D8128618 Pulled By: yiwu-arbug fbshipit-source-id: 6afa715de1493a50de413678761a765e3af9b83b/"
,,0.1758,rocksdb,"Suppress leak warning for clang(LLVM) asan (#4066) Summary: Instead of __SANITIZE_ADDRESS__ macro, LLVM uses __has_feature(address_sanitzer) to check if ASAN is enabled for the build. I tested it with MySQL sanitizer build that uses RocksDB as a submodule. Closes Reviewed By: riversand963 Differential Revision: D8668941 Pulled By: taewookoh fbshipit-source-id: af4d1da180c1470d257a228f431eebc61490bc36/Fix LRUCache missing null check on destruct Summary: Fix LRUCache missing null check on destruct. The check is needed if LRUCache::DisownData is called. Closes Differential Revision: D8191631 Pulled By: yiwu-arbug fbshipit-source-id: d5014f6e49b51692c18a25fb55ece935f5a023c4/LRUCache midpoint insertion Summary: Implement midpoint insertion strategy where new blocks will be insert to the middle of LRU list, then move the head on the first hit in cache. Closes Differential Revision: D8100895 Pulled By: yiwu-arbug fbshipit-source-id: f4bd83cb8be469e5d02072cfc8bd66011391f3da/refactor constructor of LRUCacheShard Summary: Update LRUCacheShard constructor so that adding new params to it dont need to add extra SetXXX() methods. Closes Differential Revision: D8128618 Pulled By: yiwu-arbug fbshipit-source-id: 6afa715de1493a50de413678761a765e3af9b83b/"
,,0.1667,rocksdb,"In delete scheduler, before ftruncate file for slow delete, check whether there is other hard links (#4093) Summary: Right now slow deletion with ftruncate doesnt work well with checkpoints because it ruin hard linked files in checkpoints. To fix it, check the file has no other hard link before ftruncate it. Pull Request resolved: Differential Revision: D8730360 Pulled By: siying fbshipit-source-id: 756eea5bce8a87b9a2ea3a5bfa190b2cab6f75df/"
,,0.1026,rocksdb,"Fix singleton destruction order of PosixEnv and SyncPoint (#3951) Summary: Ensure the PosixEnv singleton is destroyed first since its destructor waits for background threads to all complete. This ensures background threads cannot hit sync points after the SyncPoint singleton is destroyed, which was previously possible. Closes Differential Revision: D8265295 Pulled By: ajkr fbshipit-source-id: 7738dd458c5d993a78377dd0420e82badada81ab/"
,,0.3973,rocksdb,"Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.0702,rocksdb,Add GCC 8 to Travis (#3433) Summary: Avoid `strdup` to use jemalloc on Windows Use `size_t` for consistency Add GCC 8 to Travis Add CMAKE_BUILD_TYPE=Release to Travis Pull Request resolved: Differential Revision: D6837948 Pulled By: sagar0 fbshipit-source-id: b8543c3a4da9cd07ee9a33f9f4623188e233261f/
,,0.1163,rocksdb,Cap concurrent arenas shard block size to 128KB (#4147) Summary: Users sometime see their memtable size far smaller than expected. They probably have hit a fragementation of shard blocks. Cap their size anyway to reduce the impact of problem. 128KB is conservative so I dont imagine it can cause any performance problem. Pull Request resolved: Differential Revision: D8886706 Pulled By: siying fbshipit-source-id: 8528a2a4196aa4457274522e2565fd3ff28f621e/
,,0.0802,rocksdb,Build and tests fixes for Solaris Sparc (#4000) Summary: Here are some fixes for build on Solaris Sparc. It is also fixing CRC test on BigEndian platforms. Closes Differential Revision: D8455394 Pulled By: ajkr fbshipit-source-id: c9289a7b541a5628139c6b77e84368e14dc3d174/
,,0.3961,rocksdb,"ZSTD compression: should also expect type kZSTDNotFinalCompression (#3964) Summary: Depending on the compression type, `CompressBlock` calls the compress method for each compression type. It calls ZSTD_Compress for both kZSTD and kZSTDNotFinalCompression ( However currently ZSTD_Compress only expects the type to be kZSTD and this is causing assert failures and crashes. The same also applies to ZSTD_Uncompress. Closes Differential Revision: D8308715 Pulled By: miasantreble fbshipit-source-id: e5125f53edb829c9c33733167bec74e4793d0782/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.1728,rocksdb,"In delete scheduler, before ftruncate file for slow delete, check whether there is other hard links (#4093) Summary: Right now slow deletion with ftruncate doesnt work well with checkpoints because it ruin hard linked files in checkpoints. To fix it, check the file has no other hard link before ftruncate it. Pull Request resolved: Differential Revision: D8730360 Pulled By: siying fbshipit-source-id: 756eea5bce8a87b9a2ea3a5bfa190b2cab6f75df/"
,,0.0744,rocksdb,Add GCC 8 to Travis (#3433) Summary: Avoid `strdup` to use jemalloc on Windows Use `size_t` for consistency Add GCC 8 to Travis Add CMAKE_BUILD_TYPE=Release to Travis Pull Request resolved: Differential Revision: D6837948 Pulled By: sagar0 fbshipit-source-id: b8543c3a4da9cd07ee9a33f9f4623188e233261f/
,,0.0744,rocksdb,Add GCC 8 to Travis (#3433) Summary: Avoid `strdup` to use jemalloc on Windows Use `size_t` for consistency Add GCC 8 to Travis Add CMAKE_BUILD_TYPE=Release to Travis Pull Request resolved: Differential Revision: D6837948 Pulled By: sagar0 fbshipit-source-id: b8543c3a4da9cd07ee9a33f9f4623188e233261f/
,,0.1682,rocksdb,"In delete scheduler, before ftruncate file for slow delete, check whether there is other hard links (#4093) Summary: Right now slow deletion with ftruncate doesnt work well with checkpoints because it ruin hard linked files in checkpoints. To fix it, check the file has no other hard link before ftruncate it. Pull Request resolved: Differential Revision: D8730360 Pulled By: siying fbshipit-source-id: 756eea5bce8a87b9a2ea3a5bfa190b2cab6f75df/"
,,0.18600000000000003,rocksdb,"In delete scheduler, before ftruncate file for slow delete, check whether there is other hard links (#4093) Summary: Right now slow deletion with ftruncate doesnt work well with checkpoints because it ruin hard linked files in checkpoints. To fix it, check the file has no other hard link before ftruncate it. Pull Request resolved: Differential Revision: D8730360 Pulled By: siying fbshipit-source-id: 756eea5bce8a87b9a2ea3a5bfa190b2cab6f75df/Fix a bug caused by not copying the block trailer. (#4096) Summary: This was caught by crash test, and the following is a simple way to reproduce it and verify the fix. One way to trigger this code path is to use the following configuration: Compress SST file Enable direct IO and prefetch buffer Do NOT use compressed block cache Closes Differential Revision: D8742009 Pulled By: riversand963 fbshipit-source-id: f13381078bbb0dce92f60bd313a78ab602bcacd2/Change default value of `bytes_max_delete_chunk` to 0 in NewSstFileManager() (#4092) Summary: Now by default, with NewSstFileManager, checkpoints may be corrupted. Disable this feature to avoid this issue. Closes Differential Revision: D8729856 Pulled By: siying fbshipit-source-id: 914c321d6eaf52d8c5981171322d85dd29088307/"
,,0.2228,rocksdb,"In delete scheduler, before ftruncate file for slow delete, check whether there is other hard links (#4093) Summary: Right now slow deletion with ftruncate doesnt work well with checkpoints because it ruin hard linked files in checkpoints. To fix it, check the file has no other hard link before ftruncate it. Pull Request resolved: Differential Revision: D8730360 Pulled By: siying fbshipit-source-id: 756eea5bce8a87b9a2ea3a5bfa190b2cab6f75df/Catchup with posix features Summary: Catch up with Posix features NewWritableRWFile must fail when file does not exists Implement Env::Truncate() Adjust Env options optimization functions Implement MemoryMappedBuffer on Windows. Closes Differential Revision: D8053610 Pulled By: ajkr fbshipit-source-id: ccd0d46c29648a9f6f496873bc1c9d6c5547487e/"
,,0.077,rocksdb,types: add kEntryBlobIndex for TablePropertiesCollector (#4233) Summary: So that we can act accordingly on blob index entries Pull Request resolved: Differential Revision: D9190205 Pulled By: yiwu-arbug fbshipit-source-id: e5b84d5b41e44fa7a76762f1f7b0305369bb3a0c/
,,0.11599999999999999,rocksdb,"Make rocksdb::Slice more interoperable with std::string_view (#4242) Summary: This change allows using std::string_view objects directly in the DB API: db->Get(some_string_view_object, ...); The conversion from std::string_view to rocksdb::Slice is done automatically, thanks to the added constructor. Im stopping short of adding an implicit conversion operator from rocksdb::Slice to std::string_view, as I dont think thats a good idea for PinnableSlices. Pull Request resolved: Differential Revision: D9224134 Pulled By: anand1976 fbshipit-source-id: f50aad04dd0b01737907c0fb88d495c83a81f4e4/Fix Issue Slice ctor checks for nullptr and creates empty string Summary: Fix Issue : Check for nullptr in Slice constructor Slice ctor checks for nullptr and creates empty string if the string does not exist Closes Differential Revision: D8098852 Pulled By: ajkr fbshipit-source-id: 04471077defa9776ce7b8c389a61312ce31002fb/"
,,0.1504,rocksdb,"LRUCache midpoint insertion Summary: Implement midpoint insertion strategy where new blocks will be insert to the middle of LRU list, then move the head on the first hit in cache. Closes Differential Revision: D8100895 Pulled By: yiwu-arbug fbshipit-source-id: f4bd83cb8be469e5d02072cfc8bd66011391f3da/"
,,0.0978,rocksdb,Add SST ingestion to ldb (#4205) Summary: We add two subcommands `write_extern_sst` and `ingest_extern_sst` to ldb. This PR avoids changing existing code because we hope to cherry-pick to earlier releases to support compatibility check for external SST file ingestion. Pull Request resolved: Differential Revision: D9112711 Pulled By: riversand963 fbshipit-source-id: 7cae88380d4de86da8440230e87eca66755648e4/
,,0.1269,rocksdb,"Windows JNI build fixes (#4015) Summary: Fixing compilation, unsatisfied link exceptions (updated list of files that needs to be linked) and warnings for Windows build. ```C++ //MSVC 2015 does not support dynamic arrays like: rocksdb::Slice key_parts[jkey_parts_len]; //I have converted to: std::vector<rocksdb::Slice> key_parts; ``` Also reusing `free_key_parts` that does the same as `free_key_value_parts` that was removed. Java elapsedTime unit test increase of sleep to 2 ms. Otherwise it was failing. Pull Request resolved: Differential Revision: D8558215 Pulled By: sagar0 fbshipit-source-id: d3c34f846343f9218424da2402a2bd367bbd0aa2/Fix an issue with unnecessary capture in lambda expressions Summary: Closes Replaces I needed this to build v5.12.4 on Mac OS X (10.13.3). Closes Differential Revision: D8169357 Pulled By: sagar0 fbshipit-source-id: 85faac42168796e7def9250d0c221a9a03b84476/"
,,0.2033,rocksdb,"In delete scheduler, before ftruncate file for slow delete, check whether there is other hard links (#4093) Summary: Right now slow deletion with ftruncate doesnt work well with checkpoints because it ruin hard linked files in checkpoints. To fix it, check the file has no other hard link before ftruncate it. Pull Request resolved: Differential Revision: D8730360 Pulled By: siying fbshipit-source-id: 756eea5bce8a87b9a2ea3a5bfa190b2cab6f75df/Remove unused arg which causes compilation failure (#4080) Summary: It seems that compilation has been made stricter about unused args. Closes Differential Revision: D8712049 Pulled By: sagar0 fbshipit-source-id: 984af1982638af3568aac1a167f565f4741badee/"
,,0.1065,rocksdb,"Crash on Windows, because of shared_ptr reinterpret cast (#3999) Summary: For more details see Closes Differential Revision: D8458905 Pulled By: sagar0 fbshipit-source-id: d6e09182933253a08eaf81ac7cfe50ed3b6576c5/"
,,0.0902,rocksdb,check if data size exceeds java array vm limit when it is copied in jni (#3850) Summary: to address issue Closes Differential Revision: D8695487 Pulled By: sagar0 fbshipit-source-id: 04baeb2127663934ed1321fe6d9a9ec23c86e16b/
,,0.1638,rocksdb,"Assert for Direct IO at the beginning in PositionedRead (#3891) Summary: Moved the direct-IO assertion to the top in `PosixSequentialFile::PositionedRead`, as it doesnt make sense to check for sector alignments before checking for direct IO. Closes Differential Revision: D8267972 Pulled By: sagar0 fbshipit-source-id: 0ecf77c0fb5c35747a4ddbc15e278918c0849af7/Fix Fadvise on closed file when reads use mmap Summary: ```PosixMmapReadableFile::fd_``` is closed after created, but needs to remain open for the lifetime of `PosixMmapReadableFile` since it is used whenever `InvalidateCache` is called. Closes Differential Revision: D8152515 Pulled By: ajkr fbshipit-source-id: b738a6a55ba4e392f9b0f374ff396a1e61c64f65/Catchup with posix features Summary: Catch up with Posix features NewWritableRWFile must fail when file does not exists Implement Env::Truncate() Adjust Env options optimization functions Implement MemoryMappedBuffer on Windows. Closes Differential Revision: D8053610 Pulled By: ajkr fbshipit-source-id: ccd0d46c29648a9f6f496873bc1c9d6c5547487e/"
,,0.1761,rocksdb,"Disable EnvPosixTest.RunImmediately, add EnvPosixTest.RunEventually. (#4126) Summary: The original `EnvPosixTest.RunImmediately` assumes that after scheduling a background thread, the thread is guaranteed to complete after 0.1 second. I do not know about any non-real-time OS/runtime providing this guarantee. Nor does C++11 standard say anything about this in the documentation of `std::thread`. In fact, we have observed this test failure multiple times on appveyor, and we havent been able to reproduce the failure deterministically. Therefore, I disable this test for now until we know for sure how it used to fail. Instead, I add another test `EnvPosixTest.RunEventually` that checks that a thread will be scheduled eventually. Pull Request resolved: Differential Revision: D8827086 Pulled By: riversand963 fbshipit-source-id: abc5cb655f90d50b791493da5eeb3716885dfe93/Catchup with posix features Summary: Catch up with Posix features NewWritableRWFile must fail when file does not exists Implement Env::Truncate() Adjust Env options optimization functions Implement MemoryMappedBuffer on Windows. Closes Differential Revision: D8053610 Pulled By: ajkr fbshipit-source-id: ccd0d46c29648a9f6f496873bc1c9d6c5547487e/"
,,0.19399999999999998,rocksdb,"Fix the build failure with OS_ANDROID (#4232) Summary: sysmacros.h should be included in OS_ANDROID build as well otherwise the compile would complain: error: use of undeclared identifier major. Fixes Pull Request resolved: Differential Revision: D9217350 Pulled By: maysamyabandeh fbshipit-source-id: 21f4b62dbbda3163120ac0b38b95d95d35d67dce/In delete scheduler, before ftruncate file for slow delete, check whether there is other hard links (#4093) Summary: Right now slow deletion with ftruncate doesnt work well with checkpoints because it ruin hard linked files in checkpoints. To fix it, check the file has no other hard link before ftruncate it. Pull Request resolved: Differential Revision: D8730360 Pulled By: siying fbshipit-source-id: 756eea5bce8a87b9a2ea3a5bfa190b2cab6f75df/Build and tests fixes for Solaris Sparc (#4000) Summary: Here are some fixes for build on Solaris Sparc. It is also fixing CRC test on BigEndian platforms. Closes Differential Revision: D8455394 Pulled By: ajkr fbshipit-source-id: c9289a7b541a5628139c6b77e84368e14dc3d174/Check with PosixEnv before opening LOCK file (#3993) Summary: Rebased and resubmitting on behalf of stevelittle. The problem is when a single process attempts to open the same DB twice, the second attempt fails due to LOCK file held. If the second attempt had opened the LOCK file, itll now need to close it, and closing causes the file to be unlocked. Then, any subsequent attempt to open the DB will succeed, which is the wrong behavior. The solution was to track which files a process has locked in PosixEnv, and check those before opening a LOCK file. Fixes Closes Differential Revision: D8398984 Pulled By: ajkr fbshipit-source-id: 2755fe66950a0c9de63075f932f9e15768041918/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Fix singleton destruction order of PosixEnv and SyncPoint (#3951) Summary: Ensure the PosixEnv singleton is destroyed first since its destructor waits for background threads to all complete. This ensures background threads cannot hit sync points after the SyncPoint singleton is destroyed, which was previously possible. Closes Differential Revision: D8265295 Pulled By: ajkr fbshipit-source-id: 7738dd458c5d993a78377dd0420e82badada81ab/Fix Fadvise on closed file when reads use mmap Summary: ```PosixMmapReadableFile::fd_``` is closed after created, but needs to remain open for the lifetime of `PosixMmapReadableFile` since it is used whenever `InvalidateCache` is called. Closes Differential Revision: D8152515 Pulled By: ajkr fbshipit-source-id: b738a6a55ba4e392f9b0f374ff396a1e61c64f65/"
,,0.1045,rocksdb,Fix geo_db may seek an error key when they have the same quadkey Summary: Closes Differential Revision: D7994326 Pulled By: miasantreble fbshipit-source-id: 84a81b35b97750360423a9d4eca5b5a14d002134/
,,0.1083,rocksdb,Fix geo_db may seek an error key when they have the same quadkey Summary: Closes Differential Revision: D7994326 Pulled By: miasantreble fbshipit-source-id: 84a81b35b97750360423a9d4eca5b5a14d002134/
,,0.2169,rocksdb,Cleanup staging directory at start of checkpoint (#4035) Summary: Attempt to clean the checkpoint staging directory before starting a checkpoint. It was already cleaned up at the end of checkpoint. But it wasnt cleaned up in the edge case where the process crashed while staging checkpoint files. Attempt to clean the checkpoint directory before calling `Checkpoint::Create` in `db_stress`. This handles the case where checkpoint directory was created by a previous `db_stress` run but the process crashed before cleaning it up. Use `DestroyDB` for cleaning checkpoint directory since a checkpoint is a DB. Closes Reviewed By: yiwu-arbug Differential Revision: D8580223 Pulled By: ajkr fbshipit-source-id: 28c667400e249fad0fdedc664b349031b7b61599/
,,0.2323,rocksdb,Cleanup staging directory at start of checkpoint (#4035) Summary: Attempt to clean the checkpoint staging directory before starting a checkpoint. It was already cleaned up at the end of checkpoint. But it wasnt cleaned up in the edge case where the process crashed while staging checkpoint files. Attempt to clean the checkpoint directory before calling `Checkpoint::Create` in `db_stress`. This handles the case where checkpoint directory was created by a previous `db_stress` run but the process crashed before cleaning it up. Use `DestroyDB` for cleaning checkpoint directory since a checkpoint is a DB. Closes Reviewed By: yiwu-arbug Differential Revision: D8580223 Pulled By: ajkr fbshipit-source-id: 28c667400e249fad0fdedc664b349031b7b61599/
,,0.1747,rocksdb,"SetOptions Backup Race Condition (#4108) Summary: Prior to this PR, there was a race condition between `DBImpl::SetOptions` and `BackupEngine::CreateNewBackup`, as illustrated below. ``` Time thread 1 thread 2 | CreateNewBackup GetLiveFiles | SetOptions RenameTempFileToOptionsFile | SetOptions RenameTempFileToOptionsFile | SetOptions RenameTempFileToOptionsFile // unlink oldest OPTIONS file | copy the oldest OPTIONS // IO error V ``` Proposed fix is to check the value of `DBImpl::disable_obsolete_files_deletion_` before calling `DeleteObsoleteOptionsFiles`. Pull Request resolved: Differential Revision: D8796360 Pulled By: riversand963 fbshipit-source-id: 02045317f793ea4c7d4400a5bf333b8502fa3e82/"
,,0.3843,rocksdb,"Pin mmap files in ReadOnlyDB (#4053) Summary: fixed a bug where PinnableSlice pin mmap files which could be deleted with background compaction. This is however a non-issue for ReadOnlyDB when there is no compaction running and max_open_files is This patch reenables the pinning feature for that case. Closes Differential Revision: D8662546 Pulled By: maysamyabandeh fbshipit-source-id: 402962602eb0f644e17822748332999c3af029fd/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.1006,rocksdb,"Blob DB: enable readahead for garbage collection (#3648) Summary: Enable readahead for blob DB garbage collection, which should improve GC performance a little bit. Closes Differential Revision: D7383791 Pulled By: yiwu-arbug fbshipit-source-id: 642b3327f7105eca85986d3fb2d8f960a3d83cf1/"
,,0.1006,rocksdb,"Blob DB: enable readahead for garbage collection (#3648) Summary: Enable readahead for blob DB garbage collection, which should improve GC performance a little bit. Closes Differential Revision: D7383791 Pulled By: yiwu-arbug fbshipit-source-id: 642b3327f7105eca85986d3fb2d8f960a3d83cf1/"
,,0.4212,rocksdb,"Blob DB: enable readahead for garbage collection (#3648) Summary: Enable readahead for blob DB garbage collection, which should improve GC performance a little bit. Closes Differential Revision: D7383791 Pulled By: yiwu-arbug fbshipit-source-id: 642b3327f7105eca85986d3fb2d8f960a3d83cf1/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.2384,rocksdb,"BlobDB: Can return expiration together with Get() (#4227) Summary: Add API to allow fetching expiration of a key with `Get()`. Pull Request resolved: Differential Revision: D9169897 Pulled By: yiwu-arbug fbshipit-source-id: 2a6f216c493dc75731ddcef1daa689b517fab31b/BlobDB: Cleanup TTLExtractor interface (#4229) Summary: Cleanup TTLExtractor interface. The original purpose of it is to allow our users keep using existing `Write()` interface but allow it to accept TTL via `TTLExtractor`. However the interface is confusing. Will replace it with something like `WriteWithTTL(batch, ttl)` in the future. Pull Request resolved: Differential Revision: D9174390 Pulled By: yiwu-arbug fbshipit-source-id: 68201703d784408b851336ab4dd9b84188245b2d/BlobDB: is_fifo=true also evict non-TTL blob files (#4049) Summary: Previously with is_fifo=true we only evict TTL file. Changing it to also evict non-TTL files from oldest to newest, after exhausted TTL files. Closes Differential Revision: D8604597 Pulled By: yiwu-arbug fbshipit-source-id: bc4209ee27c1528ce4b72833e6f1e1bff80082c1/"
,,0.1752,rocksdb,"BlobDB: is_fifo=true also evict non-TTL blob files (#4049) Summary: Previously with is_fifo=true we only evict TTL file. Changing it to also evict non-TTL files from oldest to newest, after exhausted TTL files. Closes Differential Revision: D8604597 Pulled By: yiwu-arbug fbshipit-source-id: bc4209ee27c1528ce4b72833e6f1e1bff80082c1/Blob DB: enable readahead for garbage collection (#3648) Summary: Enable readahead for blob DB garbage collection, which should improve GC performance a little bit. Closes Differential Revision: D7383791 Pulled By: yiwu-arbug fbshipit-source-id: 642b3327f7105eca85986d3fb2d8f960a3d83cf1/"
,,0.1021,rocksdb,"BlobDB: Cleanup TTLExtractor interface (#4229) Summary: Cleanup TTLExtractor interface. The original purpose of it is to allow our users keep using existing `Write()` interface but allow it to accept TTL via `TTLExtractor`. However the interface is confusing. Will replace it with something like `WriteWithTTL(batch, ttl)` in the future. Pull Request resolved: Differential Revision: D9174390 Pulled By: yiwu-arbug fbshipit-source-id: 68201703d784408b851336ab4dd9b84188245b2d/"
,,0.1005,rocksdb,"BlobDB: Cleanup TTLExtractor interface (#4229) Summary: Cleanup TTLExtractor interface. The original purpose of it is to allow our users keep using existing `Write()` interface but allow it to accept TTL via `TTLExtractor`. However the interface is confusing. Will replace it with something like `WriteWithTTL(batch, ttl)` in the future. Pull Request resolved: Differential Revision: D9174390 Pulled By: yiwu-arbug fbshipit-source-id: 68201703d784408b851336ab4dd9b84188245b2d/"
,,0.2431,rocksdb,"BlobDB: Can return expiration together with Get() (#4227) Summary: Add API to allow fetching expiration of a key with `Get()`. Pull Request resolved: Differential Revision: D9169897 Pulled By: yiwu-arbug fbshipit-source-id: 2a6f216c493dc75731ddcef1daa689b517fab31b/BlobDB: Cleanup TTLExtractor interface (#4229) Summary: Cleanup TTLExtractor interface. The original purpose of it is to allow our users keep using existing `Write()` interface but allow it to accept TTL via `TTLExtractor`. However the interface is confusing. Will replace it with something like `WriteWithTTL(batch, ttl)` in the future. Pull Request resolved: Differential Revision: D9174390 Pulled By: yiwu-arbug fbshipit-source-id: 68201703d784408b851336ab4dd9b84188245b2d/BlobDB: is_fifo=true also evict non-TTL blob files (#4049) Summary: Previously with is_fifo=true we only evict TTL file. Changing it to also evict non-TTL files from oldest to newest, after exhausted TTL files. Closes Differential Revision: D8604597 Pulled By: yiwu-arbug fbshipit-source-id: bc4209ee27c1528ce4b72833e6f1e1bff80082c1/"
,,0.2419,rocksdb,"BlobDB: Can return expiration together with Get() (#4227) Summary: Add API to allow fetching expiration of a key with `Get()`. Pull Request resolved: Differential Revision: D9169897 Pulled By: yiwu-arbug fbshipit-source-id: 2a6f216c493dc75731ddcef1daa689b517fab31b/BlobDB: Cleanup TTLExtractor interface (#4229) Summary: Cleanup TTLExtractor interface. The original purpose of it is to allow our users keep using existing `Write()` interface but allow it to accept TTL via `TTLExtractor`. However the interface is confusing. Will replace it with something like `WriteWithTTL(batch, ttl)` in the future. Pull Request resolved: Differential Revision: D9174390 Pulled By: yiwu-arbug fbshipit-source-id: 68201703d784408b851336ab4dd9b84188245b2d/BlobDB: is_fifo=true also evict non-TTL blob files (#4049) Summary: Previously with is_fifo=true we only evict TTL file. Changing it to also evict non-TTL files from oldest to newest, after exhausted TTL files. Closes Differential Revision: D8604597 Pulled By: yiwu-arbug fbshipit-source-id: bc4209ee27c1528ce4b72833e6f1e1bff80082c1/"
,,0.0967,rocksdb,"Blob DB: enable readahead for garbage collection (#3648) Summary: Enable readahead for blob DB garbage collection, which should improve GC performance a little bit. Closes Differential Revision: D7383791 Pulled By: yiwu-arbug fbshipit-source-id: 642b3327f7105eca85986d3fb2d8f960a3d83cf1/"
,,0.4007,rocksdb,"BlobDB: Can return expiration together with Get() (#4227) Summary: Add API to allow fetching expiration of a key with `Get()`. Pull Request resolved: Differential Revision: D9169897 Pulled By: yiwu-arbug fbshipit-source-id: 2a6f216c493dc75731ddcef1daa689b517fab31b/BlobDB: Fix VisibleToActiveSnapshot() (#4236) Summary: There are two issues with `VisibleToActiveSnapshot`: 1. If there are no snapshots, `oldest_snapshot` will be 0 and `VisibleToActiveSnapshot` will always return true. Since the method is used to decide whether it is safe to delete obsolete files, obsolete file wont be able to delete in this case. 2. The `auto` keyword of `auto snapshots db_impl_->snapshots()` translate to a copy of `const SnapshotList` instead of a reference. Since copy constructor of `SnapshotList` is not defined, using the copy may yield unexpected result. Issue 2 actually hide issue 1 from being catch by tests. During test `snapshots.empty()` can return false while it should actually be empty, and `snapshots.oldest()` return an invalid address, making `oldest_snapshot` being some random large number. The issue was originally reported by BlobDB early adopter at Kuaishou. Pull Request resolved: Differential Revision: D9188706 Pulled By: yiwu-arbug fbshipit-source-id: a0f2624b927cf9bf28c1bb534784fee5d106f5ea/BlobDB: Cleanup TTLExtractor interface (#4229) Summary: Cleanup TTLExtractor interface. The original purpose of it is to allow our users keep using existing `Write()` interface but allow it to accept TTL via `TTLExtractor`. However the interface is confusing. Will replace it with something like `WriteWithTTL(batch, ttl)` in the future. Pull Request resolved: Differential Revision: D9174390 Pulled By: yiwu-arbug fbshipit-source-id: 68201703d784408b851336ab4dd9b84188245b2d/BlobDB: is_fifo=true also evict non-TTL blob files (#4049) Summary: Previously with is_fifo=true we only evict TTL file. Changing it to also evict non-TTL files from oldest to newest, after exhausted TTL files. Closes Differential Revision: D8604597 Pulled By: yiwu-arbug fbshipit-source-id: bc4209ee27c1528ce4b72833e6f1e1bff80082c1/Blob DB: enable readahead for garbage collection (#3648) Summary: Enable readahead for blob DB garbage collection, which should improve GC performance a little bit. Closes Differential Revision: D7383791 Pulled By: yiwu-arbug fbshipit-source-id: 642b3327f7105eca85986d3fb2d8f960a3d83cf1/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.3167,rocksdb,"Write properties metablock last in block-based tables (#4158) Summary: The properties meta-block should come at the end since we always need to read it when opening a file, unlike index/filter/other meta-blocks, which are sometimes read depending on the users configuration. This ordering will allow us to (in a future PR) do a small readahead on the end of the file to read properties and meta-index blocks with one I/O. The bulk of this PR is a refactoring of the `BlockBasedTableBuilder::Finish` function. It was previously too large with inconsistent error handling, which made it difficult to change. So I broke it up into one function per meta-block write, and tried to make error handling consistent within those functions. Then reordering the metablocks was trivial just reorder the calls to these helper functions. Pull Request resolved: Differential Revision: D8921705 Pulled By: ajkr fbshipit-source-id: 96c9cc3182eb1adf11af46adab79dbeba7b12fcc/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.4295,rocksdb,"run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.3986,rocksdb,"Return correct usable_size for BlockContents (#4246) Summary: If jemalloc is disabled or the API is incorrectly referenced (jemalloc api on windows have a prefix je_) memory usage is incorrectly reported for all block sizes. This is because sizeof(char) is always 1. sizeof() is calculated at compile time and *(char*) is char. The patch uses the size of the slice to fix that. Fixes Pull Request resolved: Differential Revision: D9233958 Pulled By: maysamyabandeh fbshipit-source-id: 9646933b24504e2814c7379f06a31148829c6b4e/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.3667,rocksdb,"Fix a bug caused by not copying the block trailer. (#4096) Summary: This was caught by crash test, and the following is a simple way to reproduce it and verify the fix. One way to trigger this code path is to use the following configuration: Compress SST file Enable direct IO and prefetch buffer Do NOT use compressed block cache Closes Differential Revision: D8742009 Pulled By: riversand963 fbshipit-source-id: f13381078bbb0dce92f60bd313a78ab602bcacd2/Pin mmap files in ReadOnlyDB (#4053) Summary: fixed a bug where PinnableSlice pin mmap files which could be deleted with background compaction. This is however a non-issue for ReadOnlyDB when there is no compaction running and max_open_files is This patch reenables the pinning feature for that case. Closes Differential Revision: D8662546 Pulled By: maysamyabandeh fbshipit-source-id: 402962602eb0f644e17822748332999c3af029fd/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.0887,rocksdb,PrefixMayMatch: remove unnecessary check for prefix_extractor_ (#4067) Summary: with and `prefix_extractor_` is not really being used in block based filter and full filters version of `PrefixMayMatch` because now `prefix_extractor` is passed as an argument. Also it is now possible that prefix_extractor_ may be initialized to nullptr when a non-standard prefix_extractor is used and also for ROCKSDB_LITE. Removing these checks should not break any existing tests. Closes Differential Revision: D8669002 Pulled By: miasantreble fbshipit-source-id: 0e701ba912b8a26734fadb72d15bb1b266b6176a/
,,0.1858,rocksdb,"SetOptions Backup Race Condition (#4108) Summary: Prior to this PR, there was a race condition between `DBImpl::SetOptions` and `BackupEngine::CreateNewBackup`, as illustrated below. ``` Time thread 1 thread 2 | CreateNewBackup GetLiveFiles | SetOptions RenameTempFileToOptionsFile | SetOptions RenameTempFileToOptionsFile | SetOptions RenameTempFileToOptionsFile // unlink oldest OPTIONS file | copy the oldest OPTIONS // IO error V ``` Proposed fix is to check the value of `DBImpl::disable_obsolete_files_deletion_` before calling `DeleteObsoleteOptionsFiles`. Pull Request resolved: Differential Revision: D8796360 Pulled By: riversand963 fbshipit-source-id: 02045317f793ea4c7d4400a5bf333b8502fa3e82/DBImpl::FindObsoleteFiles() not to call GetChildren() on the same path Summary: DBImpl::FindObsoleteFiles() may call GetChildren() multiple times if different CFs are on the same path. Fix it. Closes Differential Revision: D8084634 Pulled By: siying fbshipit-source-id: b471fbc251f6a05e9243304dc14c0831060cc0b0/"
,,0.1027,rocksdb,Pass manual_wal_flush also to the first wal file Summary: Currently manual_wal_flush if set in the options will be used only for the wal files created during wal switch. The configuration thus does not affect the first wal file. The patch fixes that and also update the related unit tests. This PR is built on top of Closes Differential Revision: D7909153 Pulled By: maysamyabandeh fbshipit-source-id: 024ed99d2555db06bf096c902b998e432bb7b9ce/
,,0.0744,rocksdb,Add GCC 8 to Travis (#3433) Summary: Avoid `strdup` to use jemalloc on Windows Use `size_t` for consistency Add GCC 8 to Travis Add CMAKE_BUILD_TYPE=Release to Travis Pull Request resolved: Differential Revision: D6837948 Pulled By: sagar0 fbshipit-source-id: b8543c3a4da9cd07ee9a33f9f4623188e233261f/
,,0.1502,rocksdb,"Disable DBFlushTest.SyncFail and DBTest.GroupCommitTest on Travis (#4154) Summary: I am temporarily disabling DBFlushTest.SyncFail and DBTest.GroupCommitTest tests on Travis until we figure out the root-cause. These tests will still continue to run locally though. I havent been able to reproduce these failures locally so far (even on a [local Travis environment]( ). These tests are failing way too frequently causing everyone to wonder why their PR failed on travis, and waste time in debugging. Pull Request resolved: Differential Revision: D8907258 Pulled By: sagar0 fbshipit-source-id: f40068b16e9245fb3791b6a4796435d1ce1ed205/"
,,0.0992,rocksdb,Fix test for rocksdb_lite: hide incompatible option kDirectIO Summary: Previous commit unhide a few test options which includes kDirectIO. However its not supported by RocksDB lite. Need to hide this option from the lite build. Closes Differential Revision: D8242757 Pulled By: miasantreble fbshipit-source-id: 1edfad3a5d01a46bfb7eedee765981ebe02c500a/
,,0.0859,rocksdb,"Fix VersionStorageInfo::EstimateLiveDataSize seg fault Summary: `HandleEstimateLiveDataSize`s `need_out_of_mutex` is true so , is will ref a `SuperVersion` so , the param `version` of `InternalStats::HandleEstimateLiveDataSize` is safe , but `cfd_->current()` is not safe the `cfd_->current()` maybe invalid ... heres mongo-rocks crash backtrace ``` mongod(mongo::printStackTrace(std::basic_ostream<char, std::char_traits<char> >&)+0x41) [0x7fe3a3137c51] mongod(+0x2152E89) [0x7fe3a3136e89] mongod(+0x21534F6) [0x7fe3a31374f6] libpthread.so.0(+0xF5E0) [0x7fe39f5e45e0] mongod(rocksdb::InternalKeyComparator::Compare(rocksdb::Slice const&, rocksdb::Slice const&) const+0x17) [0x7fe3a22375a7] mongod(rocksdb::VersionStorageInfo::EstimateLiveDataSize() const+0x3AA) [0x7fe3a228daba] mongod(rocksdb::InternalStats::HandleEstimateLiveDataSize(unsigned long*, rocksdb::DBImpl*, rocksdb::Version*)+0x20) [0x7fe3a2250d70] mongod(rocksdb::DBImpl::GetIntPropertyInternal(rocksdb::ColumnFamilyData*, rocksdb::DBPropertyInfo const&, bool, unsigned long*)+0xEF) [0x7fe3a21e3dbf] ``` Closes Differential Revision: D8179944 Pulled By: yiwu-arbug fbshipit-source-id: 26f314a8f98f4c2dc4348745d759f26f0e8d95e1/"
,,0.1073,rocksdb,Pass manual_wal_flush also to the first wal file Summary: Currently manual_wal_flush if set in the options will be used only for the wal files created during wal switch. The configuration thus does not affect the first wal file. The patch fixes that and also update the related unit tests. This PR is built on top of Closes Differential Revision: D7909153 Pulled By: maysamyabandeh fbshipit-source-id: 024ed99d2555db06bf096c902b998e432bb7b9ce/
,,0.1576,rocksdb,"use user_key and iterate_upper_bound to determine compatibility of bloom filters (#3899) Summary: Previously in bloom filter will only be checked if `prefix_extractor` in the mutable_cf_options matches the one found in the SST file. This PR relaxes the requirement by checking if all keys in the range [user_key, iterate_upper_bound) all share the same prefix after transforming using the BF in the SST file. If so, the bloom filter is considered compatible and will continue to be looked at. Closes Differential Revision: D8157459 Pulled By: miasantreble fbshipit-source-id: 18d17cba56a1005162f8d5db7a27aba277089c41/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/"
,,0.077,rocksdb,types: add kEntryBlobIndex for TablePropertiesCollector (#4233) Summary: So that we can act accordingly on blob index entries Pull Request resolved: Differential Revision: D9190205 Pulled By: yiwu-arbug fbshipit-source-id: e5b84d5b41e44fa7a76762f1f7b0305369bb3a0c/
,,0.1301,rocksdb,"Modify verification logic of ObsoleteOptionsFileTest (#4218) Summary: The current verification logic does not consider the case in which multiple threads (foreground and background) may execute `PurgeObsoleteFiles` function simultaneously. Each invocation will trigger the callback adding elements to a vector. Then we verify the elements in the vector, which can fail sometimes. The solution is to give up checking the elements. Instead, we check the number of OPTIONS file in the database dir. Pull Request resolved: Differential Revision: D9128727 Pulled By: riversand963 fbshipit-source-id: 2b13b705fb21bc0ddd41940c4ec9b6b0c8d88224/SetOptions Backup Race Condition (#4108) Summary: Prior to this PR, there was a race condition between `DBImpl::SetOptions` and `BackupEngine::CreateNewBackup`, as illustrated below. ``` Time thread 1 thread 2 | CreateNewBackup GetLiveFiles | SetOptions RenameTempFileToOptionsFile | SetOptions RenameTempFileToOptionsFile | SetOptions RenameTempFileToOptionsFile // unlink oldest OPTIONS file | copy the oldest OPTIONS // IO error V ``` Proposed fix is to check the value of `DBImpl::disable_obsolete_files_deletion_` before calling `DeleteObsoleteOptionsFiles`. Pull Request resolved: Differential Revision: D8796360 Pulled By: riversand963 fbshipit-source-id: 02045317f793ea4c7d4400a5bf333b8502fa3e82/"
,,0.2004,rocksdb,"Disable DBFlushTest.SyncFail and DBTest.GroupCommitTest on Travis (#4154) Summary: I am temporarily disabling DBFlushTest.SyncFail and DBTest.GroupCommitTest tests on Travis until we figure out the root-cause. These tests will still continue to run locally though. I havent been able to reproduce these failures locally so far (even on a [local Travis environment]( ). These tests are failing way too frequently causing everyone to wonder why their PR failed on travis, and waste time in debugging. Pull Request resolved: Differential Revision: D8907258 Pulled By: sagar0 fbshipit-source-id: f40068b16e9245fb3791b6a4796435d1ce1ed205/use user_key and iterate_upper_bound to determine compatibility of bloom filters (#3899) Summary: Previously in bloom filter will only be checked if `prefix_extractor` in the mutable_cf_options matches the one found in the SST file. This PR relaxes the requirement by checking if all keys in the range [user_key, iterate_upper_bound) all share the same prefix after transforming using the BF in the SST file. If so, the bloom filter is considered compatible and will continue to be looked at. Closes Differential Revision: D8157459 Pulled By: miasantreble fbshipit-source-id: 18d17cba56a1005162f8d5db7a27aba277089c41/option for timing measurement of non-blocking ops during compaction (#4029) Summary: For example calling CompactionFilter is always timed and gives the user no way to disable. This PR will disable the timer if `Statistics::stats_level_` (which is part of DBOptions) is `kExceptDetailedTimers` Closes Differential Revision: D8583670 Pulled By: miasantreble fbshipit-source-id: 913be9fe433ae0c06e88193b59d41920a532307f/Avoid sleep in DBTest.GroupCommitTest to fix flakiness Summary: DBTest.GroupCommitTest would often fail when run under valgrind because its sleeps were insufficient to guarantee a group commit had multiple entries. Instead we can use sync point to force a leader to wait until a non-leader thread has enqueued its work, thus guaranteeing a leader can do group commit work for multiple threads. Closes Differential Revision: D8079429 Pulled By: ajkr fbshipit-source-id: 61dc50fad29d2c85547842f681288de60fa29049/"
,,0.1446,rocksdb,"LRUCache midpoint insertion Summary: Implement midpoint insertion strategy where new blocks will be insert to the middle of LRU list, then move the head on the first hit in cache. Closes Differential Revision: D8100895 Pulled By: yiwu-arbug fbshipit-source-id: f4bd83cb8be469e5d02072cfc8bd66011391f3da/"
,,0.1232,rocksdb,"Suppress tsan lock-order-inversion on FlushWAL Summary: TSAN reports a false alarm for lock-order-inversion in DBWriteTest.IOErrorOnWALWritePropagateToWriteThreadFollower but Open and FlushWAL are not run concurrently. Suppressing the error by skipping FlushWAL in the test until TSAN is fixed. The alternative would be to use ``` TSAN_OPTIONS=""suppressions=tsan-suppressions.txt"" ./db_write_test ``` but it does not seem straightforward to integrate it to our test infra. Closes Differential Revision: D8000202 Pulled By: maysamyabandeh fbshipit-source-id: fde33483d963a7ad84d3145123821f64960a4802/Pass manual_wal_flush also to the first wal file Summary: Currently manual_wal_flush if set in the options will be used only for the wal files created during wal switch. The configuration thus does not affect the first wal file. The patch fixes that and also update the related unit tests. This PR is built on top of Closes Differential Revision: D7909153 Pulled By: maysamyabandeh fbshipit-source-id: 024ed99d2555db06bf096c902b998e432bb7b9ce/"
,,0.0744,rocksdb,Add GCC 8 to Travis (#3433) Summary: Avoid `strdup` to use jemalloc on Windows Use `size_t` for consistency Add GCC 8 to Travis Add CMAKE_BUILD_TYPE=Release to Travis Pull Request resolved: Differential Revision: D6837948 Pulled By: sagar0 fbshipit-source-id: b8543c3a4da9cd07ee9a33f9f4623188e233261f/
,,0.0702,rocksdb,Add GCC 8 to Travis (#3433) Summary: Avoid `strdup` to use jemalloc on Windows Use `size_t` for consistency Add GCC 8 to Travis Add CMAKE_BUILD_TYPE=Release to Travis Pull Request resolved: Differential Revision: D6837948 Pulled By: sagar0 fbshipit-source-id: b8543c3a4da9cd07ee9a33f9f4623188e233261f/
,,0.0712,rocksdb,port_posix: use posix_memalign() for aligned_alloc Summary: to workaround issue of . and in tcmalloc aligned_alloc and posix_memalign() are basically the same thing. the same applies to GNU glibc. fixes Signed-off-by: Kefu Chai Closes Differential Revision: D8147930 Pulled By: yiwu-arbug fbshipit-source-id: 355afe93c4dd0a96a0d711ef190e8b86fbe8d11d/
,,0.2141,rocksdb,"Fix a crash in WinEnvIO::GetSectorSize (#3975) Summary: Fix a crash in `WinEnvIO::GetSectorSize` that happens on old Windows systems (e.g Windows 7). On old Windows systems that dont support querying StorageAccessAlignmentProperty using IOCTL_STORAGE_QUERY_PROPERTY, the flow calls a different DeviceIoControl with nullptr as lpBytesReturned. When the code reaches this point, we get an access violation. Closes Differential Revision: D8385186 Pulled By: ajkr fbshipit-source-id: fae4c9b4b0a52c8a10182e1b35bcaa30dc393bbb/Catchup with posix features Summary: Catch up with Posix features NewWritableRWFile must fail when file does not exists Implement Env::Truncate() Adjust Env options optimization functions Implement MemoryMappedBuffer on Windows. Closes Differential Revision: D8053610 Pulled By: ajkr fbshipit-source-id: ccd0d46c29648a9f6f496873bc1c9d6c5547487e/"
,,0.1352,rocksdb,Catchup with posix features Summary: Catch up with Posix features NewWritableRWFile must fail when file does not exists Implement Env::Truncate() Adjust Env options optimization functions Implement MemoryMappedBuffer on Windows. Closes Differential Revision: D8053610 Pulled By: ajkr fbshipit-source-id: ccd0d46c29648a9f6f496873bc1c9d6c5547487e/
,,0.413,rocksdb,"run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.1501,rocksdb,Catchup with posix features Summary: Catch up with Posix features NewWritableRWFile must fail when file does not exists Implement Env::Truncate() Adjust Env options optimization functions Implement MemoryMappedBuffer on Windows. Closes Differential Revision: D8053610 Pulled By: ajkr fbshipit-source-id: ccd0d46c29648a9f6f496873bc1c9d6c5547487e/
,,0.4198,rocksdb,"run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/"
,,0.1453,rocksdb,Remove ALIGNAS from StatisticsImpl. (#4061) Summary: Remove over-alignment on `StatisticsImpl` whose benefit is vague and causes UBSAN check to fail due to `std::make_shared` not respecting the over-alignment requirement. Test plan ``` $ make clean && COMPILE_WITH_UBSAN=1 OPT=-g make ubsan_check ``` Closes Differential Revision: D8656506 Pulled By: riversand963 fbshipit-source-id: db355ae9c7bdd2c9e9c5e63cabba13d8d82cc5f9/Align StatisticsImpl / StatisticsData (#4036) Summary: Pinned the alignment of StatisticsData to the cacheline size rather than just extending its size (which could go over two cache lines)if unaligned in allocation. Avoid compile errors in the process as per individual commit messages. strengthen static_assert to CACHELINE rather than the highest common multiple. Closes Differential Revision: D8582844 Pulled By: yiwu-arbug fbshipit-source-id: 363c37029f28e6093e06c60b987bca9aa204bc71/
,,0.0908,rocksdb,Build and tests fixes for Solaris Sparc (#4000) Summary: Here are some fixes for build on Solaris Sparc. It is also fixing CRC test on BigEndian platforms. Closes Differential Revision: D8455394 Pulled By: ajkr fbshipit-source-id: c9289a7b541a5628139c6b77e84368e14dc3d174/
,,0.1843,rocksdb,BlobDB: GetLiveFiles and GetLiveFilesMetadata return relative path (#4326) Summary: `GetLiveFiles` and `GetLiveFilesMetadata` should return path relative to db path. It is a separate issue when `path_relative` is false how can we return relative path. But `DBImpl::GetLiveFiles` dont handle it as well when there are multiple `db_paths`. Pull Request resolved: Differential Revision: D9545904 Pulled By: yiwu-arbug fbshipit-source-id: 6762d879fcb561df2b612e6fdfb4a6b51db03f5d/
,,0.0993,rocksdb,"Sync CURRENT file during checkpoint (#4322) Summary: For the CURRENT file forged during checkpoint, we were forgetting to `fsync` or `fdatasync` it after its creation. This PR fixes it. Differential Revision: D9525939 Pulled By: ajkr fbshipit-source-id: a505483644026ee3f501cfc0dcbe74832165b2e3/"
,,0.1166,rocksdb,"Two code changes to make ""clang analyze"" happy (#4292) Summary: Clang analyze is not happy in two pieces of code, with ""Potential memory leak"". No idea what the problem but slightly changing the code makes clang happy. Pull Request resolved: Differential Revision: D9413555 Pulled By: siying fbshipit-source-id: 9428c9d3664530c72129feefd135ee63d8386137/"
,,0.1961,rocksdb,BlobDB: GetLiveFiles and GetLiveFilesMetadata return relative path (#4326) Summary: `GetLiveFiles` and `GetLiveFilesMetadata` should return path relative to db path. It is a separate issue when `path_relative` is false how can we return relative path. But `DBImpl::GetLiveFiles` dont handle it as well when there are multiple `db_paths`. Pull Request resolved: Differential Revision: D9545904 Pulled By: yiwu-arbug fbshipit-source-id: 6762d879fcb561df2b612e6fdfb4a6b51db03f5d/
,,0.0838,rocksdb,rocksdb: put `#pragma once` before `#ifdef` Summary: Work around upstream bug with modules: Reviewed By: yiwu-arbug Differential Revision: D10209569 fbshipit-source-id: 696853a02a3869e9c33d0e61168ad4b0436fa3c0/
,,0.0913,rocksdb,rocksdb: put `#pragma once` before `#ifdef` Summary: Work around upstream bug with modules: Reviewed By: yiwu-arbug Differential Revision: D10209569 fbshipit-source-id: 696853a02a3869e9c33d0e61168ad4b0436fa3c0/
,,0.106,rocksdb,"WriteBufferManager JNI fixes (#4579) Summary: 1. `WriteBufferManager` should have a reference alive in Java side through `Options`/`DBOptions` otherwise, if its GCed at java side, native side can seg fault. 2. native method `setWriteBufferManager()` in `DBOptions.java` doesnt have its jni method invocation in rocksdbjni which is added in this PR 3. `DBOptionsTest.java` is referencing object of `Options`. Instead it should be testing against `DBOptions`. Seems like a copy paste error. 4. Add a getter for WriteBufferManager. Pull Request resolved: Differential Revision: D10561150 Pulled By: sagar0 fbshipit-source-id: 139a15c7f051a9f77b4200215b88267b48fbc487/"
,,0.1031,rocksdb,"WriteBufferManager JNI fixes (#4579) Summary: 1. `WriteBufferManager` should have a reference alive in Java side through `Options`/`DBOptions` otherwise, if its GCed at java side, native side can seg fault. 2. native method `setWriteBufferManager()` in `DBOptions.java` doesnt have its jni method invocation in rocksdbjni which is added in this PR 3. `DBOptionsTest.java` is referencing object of `Options`. Instead it should be testing against `DBOptions`. Seems like a copy paste error. 4. Add a getter for WriteBufferManager. Pull Request resolved: Differential Revision: D10561150 Pulled By: sagar0 fbshipit-source-id: 139a15c7f051a9f77b4200215b88267b48fbc487/"
,,0.1047,rocksdb,"Sync CURRENT file during checkpoint (#4322) Summary: For the CURRENT file forged during checkpoint, we were forgetting to `fsync` or `fdatasync` it after its creation. This PR fixes it. Differential Revision: D9525939 Pulled By: ajkr fbshipit-source-id: a505483644026ee3f501cfc0dcbe74832165b2e3/"
,,0.1066,rocksdb,"Sync CURRENT file during checkpoint (#4322) Summary: For the CURRENT file forged during checkpoint, we were forgetting to `fsync` or `fdatasync` it after its creation. This PR fixes it. Differential Revision: D9525939 Pulled By: ajkr fbshipit-source-id: a505483644026ee3f501cfc0dcbe74832165b2e3/"
,,0.0895,rocksdb,rocksdb: put `#pragma once` before `#ifdef` Summary: Work around upstream bug with modules: Reviewed By: yiwu-arbug Differential Revision: D10209569 fbshipit-source-id: 696853a02a3869e9c33d0e61168ad4b0436fa3c0/
,,0.08199999999999999,rocksdb,BlobDB: handle IO error on read (#4410) Summary: Fix IO error on read not being handle and crashing the DB. With the fix we properly return the error. Pull Request resolved: Differential Revision: D9979246 Pulled By: yiwu-arbug fbshipit-source-id: 111a85675067a29c03cb60e9a34103f4ff636694/
,,0.261,rocksdb,"BlobDB: handle IO error on write (#4580) Summary: A fix similar to but on the write path. On IO error on `SelectBlobFile()` we didnt return error code properly, but simply a nullptr of `BlobFile`. The `AppendBlob()` method didnt have null check for the pointer and caused crash. The fix make sure we properly return error code in this case. Pull Request resolved: Differential Revision: D10513849 Pulled By: yiwu-arbug fbshipit-source-id: 80bca920d1d7a3541149de981015ad83e0aa14b5/BlobDB: handle IO error on read (#4410) Summary: Fix IO error on read not being handle and crashing the DB. With the fix we properly return the error. Pull Request resolved: Differential Revision: D9979246 Pulled By: yiwu-arbug fbshipit-source-id: 111a85675067a29c03cb60e9a34103f4ff636694/BlobDB: GetLiveFiles and GetLiveFilesMetadata return relative path (#4326) Summary: `GetLiveFiles` and `GetLiveFilesMetadata` should return path relative to db path. It is a separate issue when `path_relative` is false how can we return relative path. But `DBImpl::GetLiveFiles` dont handle it as well when there are multiple `db_paths`. Pull Request resolved: Differential Revision: D9545904 Pulled By: yiwu-arbug fbshipit-source-id: 6762d879fcb561df2b612e6fdfb4a6b51db03f5d/BlobDB: Avoid returning garbage value on key not found (#4321) Summary: When reading an expired key using `Get(..., std::string* value)` API, BlobDB first read the index entry and decode expiration from it. In this case, although BlobDB reset the PinnableSlice, the index entry is stored in user provided string `value`. The value will be returned as a garbage value, despite status being NotFound. Fixing it by use a different PinnableSlice to read the index entry. Pull Request resolved: Differential Revision: D9519042 Pulled By: yiwu-arbug fbshipit-source-id: f054c951a1fa98265228be94f931904ed7056677/BlobDB: Implement DisableFileDeletions (#4314) Summary: `DB::DiableFileDeletions` and `DB::EnableFileDeletions` are used for applications to stop RocksDB background jobs to delete files while they are doing replication. Implement these methods for BlobDB. `DeleteObsolteFiles` now needs to check `disable_file_deletions_` before starting, and will hold `delete_file_mutex_` the whole time while it is running. `DisableFileDeletions` needs to wait on `delete_file_mutex_` for running `DeleteObsolteFiles` job and set `disable_file_deletions_` flag. Pull Request resolved: Differential Revision: D9501373 Pulled By: yiwu-arbug fbshipit-source-id: 81064c1228f1724eff46da22b50ff765b16292cd/BlobDB: Fix expired file not being evicted (#4294) Summary: Fix expired file not being evicted from the DB. We have a background task (previously called `CheckSeqFiles` and I rename it to `EvictExpiredFiles`) to scan and remove expired files, but it only close the files, not marking them as expired. Pull Request resolved: Differential Revision: D9415984 Pulled By: yiwu-arbug fbshipit-source-id: eff7bf0331c52a7ccdb02318602bff7f64f3ef3d/"
,,0.08199999999999999,rocksdb,BlobDB: handle IO error on read (#4410) Summary: Fix IO error on read not being handle and crashing the DB. With the fix we properly return the error. Pull Request resolved: Differential Revision: D9979246 Pulled By: yiwu-arbug fbshipit-source-id: 111a85675067a29c03cb60e9a34103f4ff636694/
,,0.2119,rocksdb,"BlobDB: handle IO error on write (#4580) Summary: A fix similar to but on the write path. On IO error on `SelectBlobFile()` we didnt return error code properly, but simply a nullptr of `BlobFile`. The `AppendBlob()` method didnt have null check for the pointer and caused crash. The fix make sure we properly return error code in this case. Pull Request resolved: Differential Revision: D10513849 Pulled By: yiwu-arbug fbshipit-source-id: 80bca920d1d7a3541149de981015ad83e0aa14b5/BlobDB: handle IO error on read (#4410) Summary: Fix IO error on read not being handle and crashing the DB. With the fix we properly return the error. Pull Request resolved: Differential Revision: D9979246 Pulled By: yiwu-arbug fbshipit-source-id: 111a85675067a29c03cb60e9a34103f4ff636694/BlobDB: Implement DisableFileDeletions (#4314) Summary: `DB::DiableFileDeletions` and `DB::EnableFileDeletions` are used for applications to stop RocksDB background jobs to delete files while they are doing replication. Implement these methods for BlobDB. `DeleteObsolteFiles` now needs to check `disable_file_deletions_` before starting, and will hold `delete_file_mutex_` the whole time while it is running. `DisableFileDeletions` needs to wait on `delete_file_mutex_` for running `DeleteObsolteFiles` job and set `disable_file_deletions_` flag. Pull Request resolved: Differential Revision: D9501373 Pulled By: yiwu-arbug fbshipit-source-id: 81064c1228f1724eff46da22b50ff765b16292cd/BlobDB: Fix expired file not being evicted (#4294) Summary: Fix expired file not being evicted from the DB. We have a background task (previously called `CheckSeqFiles` and I rename it to `EvictExpiredFiles`) to scan and remove expired files, but it only close the files, not marking them as expired. Pull Request resolved: Differential Revision: D9415984 Pulled By: yiwu-arbug fbshipit-source-id: eff7bf0331c52a7ccdb02318602bff7f64f3ef3d/"
,,0.0792,rocksdb,"fix typo in error message, twice (#4457) Summary: Fixes a typo in error messages returned by Iterator::GetProperty(...) Pull Request resolved: Differential Revision: D10281965 Pulled By: sagar0 fbshipit-source-id: 1cd3c665f467ef06cdfd9f482692e6f8568f3d22/"
,,0.1053,rocksdb,"Disable DBIOFailureTest.NoSpaceCompactRange in LITE (#4596) Summary: Since ErrorHandler::RecoverFromNoSpace is no-op in LITE mode, then we should not have this test in LITE mode. If we do keep it, it will cause the test thread to wait on bg_cv_ that will not be signalled. How to reproduce ``` $make clean && git checkout a27fce408e197f68d4d4a613aefc1d84b9a57058 $OPT=""-DROCKSDB_LITE make $./db_io_failure_test ``` Pull Request resolved: Differential Revision: D12818516 Pulled By: riversand963 fbshipit-source-id: bc83524f40fff1e29506979017f7f4c2b70322f3/"
,,0.0943,rocksdb,"Fix CompactFiles support for kDisableCompressionOption (#4438) Summary: Previously `CompactFiles` with `CompressionType::kDisableCompressionOption` caused program to crash on assertion failure. This PR fixes the crash by adding support for that setting. Now, that setting will cause RocksDB to choose compression according to the column familys options. Pull Request resolved: Differential Revision: D10115761 Pulled By: ajkr fbshipit-source-id: a553c6fa76fa5b6f73b0d165d95640da6f454122/"
,,0.1033,rocksdb,"VersionBuilder: optmize SaveTo() to linear time. (#4366) Summary: Because `base_files` and `added_files` both are sorted, using a merge operation to these two sorted arrays is more effective. The complexity is reduced to linear time. optmize the merge complexity. move the `NDEBUG` of sorted `added_files` out of merge process. Signed-off-by: JiYou Pull Request resolved: Differential Revision: D9833592 Pulled By: ajkr fbshipit-source-id: dd32b67ebdca4c20e5e9546ab8082cecefe99fd0/Two code changes to make ""clang analyze"" happy (#4292) Summary: Clang analyze is not happy in two pieces of code, with ""Potential memory leak"". No idea what the problem but slightly changing the code makes clang happy. Pull Request resolved: Differential Revision: D9413555 Pulled By: siying fbshipit-source-id: 9428c9d3664530c72129feefd135ee63d8386137/"
,,0.1667,rocksdb,"Acquire lock on DB LOCK file before starting repair. (#4435) Summary: This commit adds code to acquire lock on the DB LOCK file before starting the repair process. This will prevent multiple processes from performing repair on the same DB simultaneously. Fixes repair_test to work with this change. Pull Request resolved: Differential Revision: D10361499 Pulled By: riversand963 fbshipit-source-id: 3c512c48b7193d383b2279ccecabdb660ac1cf22/Sync CURRENT file during checkpoint (#4322) Summary: For the CURRENT file forged during checkpoint, we were forgetting to `fsync` or `fdatasync` it after its creation. This PR fixes it. Differential Revision: D9525939 Pulled By: ajkr fbshipit-source-id: a505483644026ee3f501cfc0dcbe74832165b2e3/"
,,0.1192,rocksdb,Acquire lock on DB LOCK file before starting repair. (#4435) Summary: This commit adds code to acquire lock on the DB LOCK file before starting the repair process. This will prevent multiple processes from performing repair on the same DB simultaneously. Fixes repair_test to work with this change. Pull Request resolved: Differential Revision: D10361499 Pulled By: riversand963 fbshipit-source-id: 3c512c48b7193d383b2279ccecabdb660ac1cf22/
,,0.0652,rocksdb,fix performance regression introduced by MergeOperator.ShouldMerge (#4266) Summary: This PR addresses issue and implements the following approach to fix it: adds `MergeContext::GetOperandsDirectionForward` and `MergeContext::GetOperandsDirectionBackward` to query merge operands in a specific order `MergeContext::GetOperands` becomes a shortcut for `MergeContext::GetOperandsDirectionForward` pass `MergeContext::GetOperandsDirectionBackward` to `MergeOperator::ShouldMerge` and document the order Pull Request resolved: Differential Revision: D9360750 Pulled By: sagar0 fbshipit-source-id: 20cb73ff017760b062ecdcf4382560767086e092/
,,0.0757,rocksdb,Adjust c test and fix windows compilation issues Summary: Pull Request resolved: Differential Revision: D9844200 Pulled By: sagar0 fbshipit-source-id: 0d9f5f73b28234eaac55d3551ce4e2dc177af138/
,,0.1237,rocksdb,"Add compile time option to work with utf8 filename strings (#4469) Summary: The default behaviour of rocksdb is to use the `*A(` windows API functions. These accept filenames in the currently configured system encoding, be it Latin 1, utf8 or whatever. If the Application intends to completely work with utf8 strings internally, converting these to that codepage properly isnt even always possible. Thus this patch adds a switch to use the `*W(` functions, which accept UTF-16 filenames, and uses C++11 features to translate the UTF8 containing std::string to an UTF16 containing std::wstring. This feature is a compile time options, that can be enabled by setting `WITH_WINDOWS_UTF8_FILENAMES` to true. Pull Request resolved: Differential Revision: D10356011 Pulled By: yiwu-arbug fbshipit-source-id: 27b6ae9171f209085894cdf80069e8a896642044/Fix cross-filesystem checkpoint on Windows (#4365) Summary: Now port/win_env.cc do check error for cross device link creation. Fixes Pull Request resolved: Differential Revision: D9833144 Pulled By: ajkr fbshipit-source-id: be7555e510f4b8d2196d843841606a6cfada7644/Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/Add path to WritableFileWriter. (#4039) Summary: We want to sample the file I/O issued by RocksDB and report the function calls. This requires us to include the file paths otherwise its hard to tell what has been going on. Pull Request resolved: Differential Revision: D8670178 Pulled By: riversand963 fbshipit-source-id: 97ee806d1c583a2983e28e213ee764dc6ac28f7a/"
,,0.2076,rocksdb,"Remove redundant member var and set options (#4631) Summary: In the past, both `DBImpl::atomic_flush_` and `DBImpl::immutable_db_options_.atomic_flush` exist. However, we fail to set `immutable_db_options_.atomic_flush`, but use `DBImpl::atomic_flush_` which is set correctly. This does not lead to incorrect behavior, but is a duplicate of information. Since `immutable_db_options_` is always there and has `atomic_flush`, we should use it as source of truth and remove `DBImpl::atomic_flush_`. Pull Request resolved: Differential Revision: D12928371 Pulled By: riversand963 fbshipit-source-id: f85a811959d3828aad4a3a1b05f71facf19c636d/"
,,0.2384,rocksdb,"Remove redundant member var and set options (#4631) Summary: In the past, both `DBImpl::atomic_flush_` and `DBImpl::immutable_db_options_.atomic_flush` exist. However, we fail to set `immutable_db_options_.atomic_flush`, but use `DBImpl::atomic_flush_` which is set correctly. This does not lead to incorrect behavior, but is a duplicate of information. Since `immutable_db_options_` is always there and has `atomic_flush`, we should use it as source of truth and remove `DBImpl::atomic_flush_`. Pull Request resolved: Differential Revision: D12928371 Pulled By: riversand963 fbshipit-source-id: f85a811959d3828aad4a3a1b05f71facf19c636d/"
,,0.3634,rocksdb,"Concurrent task limiter for compaction thread control (#4332) Summary: The PR is targeting to resolve the issue of: We have a rocksdb created with leveled-compaction with multiple column families (CFs), some of CFs are using HDD to store big and less frequently accessed data and others are using SSD. When there are continuously write traffics going on to all CFs, the compaction thread pool is mostly occupied by those slow HDD compactions, which blocks fully utilize SSD bandwidth. Since atomic write and transaction is needed across CFs, so splitting it to multiple rocksdb instance is not an option for us. With the compaction thread control, we got 30%+ HDD write throughput gain, and also a lot smooth SSD write since less write stall happening. ConcurrentTaskLimiter can be shared with multi-CFs across rocksdb instances, so the feature does not only work for multi-CFs scenarios, but also for multi-rocksdbs scenarios, who need disk IO resource control per tenant. The usage is straight forward: e.g.: // // Enable compaction thread limiter thru ColumnFamilyOptions // std::shared_ptr<ConcurrentTaskLimiter> ctl(NewConcurrentTaskLimiter(""foo_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter ctl; ... // // Compaction thread limiter can be tuned or disabled on-the-fly // ctl->SetMaxOutstandingTask(12); // enlarge to 12 tasks ... ctl->ResetMaxOutstandingTask(); // disable (bypass) thread limiter ctl->SetMaxOutstandingTask(-1); // Same as above ... ctl->SetMaxOutstandingTask(0); // full throttle (0 task) // // Sharing compaction thread limiter among CFs (to resolve multiple storage perf issue) // std::shared_ptr<ConcurrentTaskLimiter> ctl_ssd(NewConcurrentTaskLimiter(""ssd_limiter"", 8)); std::shared_ptr<ConcurrentTaskLimiter> ctl_hdd(NewConcurrentTaskLimiter(""hdd_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt_ssd1(options); ColumnFamilyOptions cf_opt_ssd2(options); ColumnFamilyOptions cf_opt_hdd1(options); ColumnFamilyOptions cf_opt_hdd2(options); ColumnFamilyOptions cf_opt_hdd3(options); // SSD CFs cf_opt_ssd1.compaction_thread_limiter ctl_ssd; cf_opt_ssd2.compaction_thread_limiter ctl_ssd; // HDD CFs cf_opt_hdd1.compaction_thread_limiter ctl_hdd; cf_opt_hdd2.compaction_thread_limiter ctl_hdd; cf_opt_hdd3.compaction_thread_limiter ctl_hdd; ... // // The limiter is disabled by default (or set to nullptr explicitly) // Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter nullptr; Pull Request resolved: Differential Revision: D13226590 Pulled By: siying fbshipit-source-id: 14307aec55b8bd59c8223d04aa6db3c03d1b0c1d/"
,,0.3562,rocksdb,"Concurrent task limiter for compaction thread control (#4332) Summary: The PR is targeting to resolve the issue of: We have a rocksdb created with leveled-compaction with multiple column families (CFs), some of CFs are using HDD to store big and less frequently accessed data and others are using SSD. When there are continuously write traffics going on to all CFs, the compaction thread pool is mostly occupied by those slow HDD compactions, which blocks fully utilize SSD bandwidth. Since atomic write and transaction is needed across CFs, so splitting it to multiple rocksdb instance is not an option for us. With the compaction thread control, we got 30%+ HDD write throughput gain, and also a lot smooth SSD write since less write stall happening. ConcurrentTaskLimiter can be shared with multi-CFs across rocksdb instances, so the feature does not only work for multi-CFs scenarios, but also for multi-rocksdbs scenarios, who need disk IO resource control per tenant. The usage is straight forward: e.g.: // // Enable compaction thread limiter thru ColumnFamilyOptions // std::shared_ptr<ConcurrentTaskLimiter> ctl(NewConcurrentTaskLimiter(""foo_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter ctl; ... // // Compaction thread limiter can be tuned or disabled on-the-fly // ctl->SetMaxOutstandingTask(12); // enlarge to 12 tasks ... ctl->ResetMaxOutstandingTask(); // disable (bypass) thread limiter ctl->SetMaxOutstandingTask(-1); // Same as above ... ctl->SetMaxOutstandingTask(0); // full throttle (0 task) // // Sharing compaction thread limiter among CFs (to resolve multiple storage perf issue) // std::shared_ptr<ConcurrentTaskLimiter> ctl_ssd(NewConcurrentTaskLimiter(""ssd_limiter"", 8)); std::shared_ptr<ConcurrentTaskLimiter> ctl_hdd(NewConcurrentTaskLimiter(""hdd_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt_ssd1(options); ColumnFamilyOptions cf_opt_ssd2(options); ColumnFamilyOptions cf_opt_hdd1(options); ColumnFamilyOptions cf_opt_hdd2(options); ColumnFamilyOptions cf_opt_hdd3(options); // SSD CFs cf_opt_ssd1.compaction_thread_limiter ctl_ssd; cf_opt_ssd2.compaction_thread_limiter ctl_ssd; // HDD CFs cf_opt_hdd1.compaction_thread_limiter ctl_hdd; cf_opt_hdd2.compaction_thread_limiter ctl_hdd; cf_opt_hdd3.compaction_thread_limiter ctl_hdd; ... // // The limiter is disabled by default (or set to nullptr explicitly) // Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter nullptr; Pull Request resolved: Differential Revision: D13226590 Pulled By: siying fbshipit-source-id: 14307aec55b8bd59c8223d04aa6db3c03d1b0c1d/"
,,0.4582,rocksdb,"Concurrent task limiter for compaction thread control (#4332) Summary: The PR is targeting to resolve the issue of: We have a rocksdb created with leveled-compaction with multiple column families (CFs), some of CFs are using HDD to store big and less frequently accessed data and others are using SSD. When there are continuously write traffics going on to all CFs, the compaction thread pool is mostly occupied by those slow HDD compactions, which blocks fully utilize SSD bandwidth. Since atomic write and transaction is needed across CFs, so splitting it to multiple rocksdb instance is not an option for us. With the compaction thread control, we got 30%+ HDD write throughput gain, and also a lot smooth SSD write since less write stall happening. ConcurrentTaskLimiter can be shared with multi-CFs across rocksdb instances, so the feature does not only work for multi-CFs scenarios, but also for multi-rocksdbs scenarios, who need disk IO resource control per tenant. The usage is straight forward: e.g.: // // Enable compaction thread limiter thru ColumnFamilyOptions // std::shared_ptr<ConcurrentTaskLimiter> ctl(NewConcurrentTaskLimiter(""foo_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter ctl; ... // // Compaction thread limiter can be tuned or disabled on-the-fly // ctl->SetMaxOutstandingTask(12); // enlarge to 12 tasks ... ctl->ResetMaxOutstandingTask(); // disable (bypass) thread limiter ctl->SetMaxOutstandingTask(-1); // Same as above ... ctl->SetMaxOutstandingTask(0); // full throttle (0 task) // // Sharing compaction thread limiter among CFs (to resolve multiple storage perf issue) // std::shared_ptr<ConcurrentTaskLimiter> ctl_ssd(NewConcurrentTaskLimiter(""ssd_limiter"", 8)); std::shared_ptr<ConcurrentTaskLimiter> ctl_hdd(NewConcurrentTaskLimiter(""hdd_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt_ssd1(options); ColumnFamilyOptions cf_opt_ssd2(options); ColumnFamilyOptions cf_opt_hdd1(options); ColumnFamilyOptions cf_opt_hdd2(options); ColumnFamilyOptions cf_opt_hdd3(options); // SSD CFs cf_opt_ssd1.compaction_thread_limiter ctl_ssd; cf_opt_ssd2.compaction_thread_limiter ctl_ssd; // HDD CFs cf_opt_hdd1.compaction_thread_limiter ctl_hdd; cf_opt_hdd2.compaction_thread_limiter ctl_hdd; cf_opt_hdd3.compaction_thread_limiter ctl_hdd; ... // // The limiter is disabled by default (or set to nullptr explicitly) // Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter nullptr; Pull Request resolved: Differential Revision: D13226590 Pulled By: siying fbshipit-source-id: 14307aec55b8bd59c8223d04aa6db3c03d1b0c1d/Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.0703,rocksdb,Make compaction_pri kMinOverlappingRatio to be default (#4911) Summary: compaction_pri kMinOverlappingRatio usually provides much better write amplification than the default. fixes one shortcome of this option. Make it default. Pull Request resolved: Differential Revision: D13789262 Pulled By: siying fbshipit-source-id: d90acf8c4dede44f00d183ca4c7a210259378269/
,,0.1893,rocksdb,"Remove unnecessary assersion in AtomicFlushStressTest::TestCheckpoint (#4846) Summary: as titled. We can remove the assersion because we do not perform verification in AtomicFlushStressTest::TestCheckpoint for similar reasons to TestGet, TestPut, etc. Therefore, we override TestCheckpoint in AtomicFlushStressTest so that the assertion `rand_column_families.size() rand_keys.size() is removed, and we do not verify the DB in this function. Pull Request resolved: Differential Revision: D13583377 Pulled By: riversand963 fbshipit-source-id: 03647f3da67e27a397413fd666e3bb43003bf596/fix accounting for range tombstones in TableProperties (#4841) Summary: To be consistent with the accounting of other optypes in `TableProperties`, we should count range tombstones in `TableProperties::num_entries` and `TableProperties::num_deletions`. Updated assertions in stress tests `OnTableFileCreated` handler to accept files with range tombstones only. Pull Request resolved: Differential Revision: D13568424 Pulled By: ajkr fbshipit-source-id: 0139d7806494eda20ece67ec460d2458dbbf6026/Refine db_stress params for atomic flush (#4781) Summary: Separate flag for enabling option from flag for enabling dedicated atomic stress test. I have found setting the former without setting the latter can detect different problems. Pull Request resolved: Differential Revision: D13463211 Pulled By: ajkr fbshipit-source-id: 054f777885b2dc7d5ea99faafa21d6537eee45fd/Improve result report of scan (#4648) Summary: When iterator becomes invalid, there are two possibilities. First, all data in the column family have been scanned and there is nothing more to scan. Second, an underlying error has occurred, causing `status()` to be ok. Therefore, we need to check for both cases when `iter->Valid()`. Pull Request resolved: Differential Revision: D12959601 Pulled By: riversand963 fbshipit-source-id: 49c9382c9ea9e78f2e2b6f3708f0670b822ca8dd/"
,,0.3313,rocksdb,"Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.3353,rocksdb,"Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.3221,rocksdb,"Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.33399999999999996,rocksdb,"Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.1777,rocksdb,"Detect if Jemalloc is linked with the binary (#4844) Summary: Declare Jemalloc non-standard APIs as weak symbols, so that if Jemalloc is linked with the binary, these symbols will be replaced by Jemallocs, otherwise they will be nullptr. This is similar to how folly detect jemalloc, but we assume the main program use jemalloc as long as jemalloc is linked: Pull Request resolved: Differential Revision: D13574934 Pulled By: yiwu-arbug fbshipit-source-id: 7ea871beb1be7d5a1259cc38f9b78078793db2db/JemallocNodumpAllocator: option to limit tcache memory usage (#4736) Summary: Add option to limit tcache usage by allocation size. This is to reduce total tcache size in case there are many user threads accessing the allocator and incur non-trivial memory usage. Pull Request resolved: Differential Revision: D13269305 Pulled By: yiwu-arbug fbshipit-source-id: 95a9b7fc67facd66837c849137e30e137112e19d/JemallocAllocator: thread-local tcache (#4603) Summary: Add option to support thread-local tcache to reduce mutex contention inside Jemalloc arena. Pull Request resolved: Differential Revision: D12830738 Pulled By: yiwu-arbug fbshipit-source-id: 59bd25b165b903f23a6a8531b18d72e140d69f65/"
,,0.1795,rocksdb,"Detect if Jemalloc is linked with the binary (#4844) Summary: Declare Jemalloc non-standard APIs as weak symbols, so that if Jemalloc is linked with the binary, these symbols will be replaced by Jemallocs, otherwise they will be nullptr. This is similar to how folly detect jemalloc, but we assume the main program use jemalloc as long as jemalloc is linked: Pull Request resolved: Differential Revision: D13574934 Pulled By: yiwu-arbug fbshipit-source-id: 7ea871beb1be7d5a1259cc38f9b78078793db2db/fix unused param ""options"" error in jemalloc_nodump_allocator.cc (#4738) Summary: Currently tests are failing on master with the following message: > util/jemalloc_nodump_allocator.cc:132:8: error: unused parameter Ã«optionsÃ­ [-Werror=unused-parameter] Status NewJemallocNodumpAllocator( This PR attempts to fix the issue Pull Request resolved: Differential Revision: D13278804 Pulled By: miasantreble fbshipit-source-id: 64a6204aa685bd85d8b5080655cafef9980fac2f/JemallocNodumpAllocator: option to limit tcache memory usage (#4736) Summary: Add option to limit tcache usage by allocation size. This is to reduce total tcache size in case there are many user threads accessing the allocator and incur non-trivial memory usage. Pull Request resolved: Differential Revision: D13269305 Pulled By: yiwu-arbug fbshipit-source-id: 95a9b7fc67facd66837c849137e30e137112e19d/JemallocAllocator: thread-local tcache (#4603) Summary: Add option to support thread-local tcache to reduce mutex contention inside Jemalloc arena. Pull Request resolved: Differential Revision: D12830738 Pulled By: yiwu-arbug fbshipit-source-id: 59bd25b165b903f23a6a8531b18d72e140d69f65/"
,,0.1119,rocksdb,WriteBufferManger doenst cost to cache if no limit is set (#4695) Summary: WriteBufferManger is not invoked when allocating memory for memtable if the limit is not set even if a cache is passed. It is inconsistent from the comment syas. Fix it. Pull Request resolved: Differential Revision: D13112722 Pulled By: siying fbshipit-source-id: 0b27eef63867f679cd06033ea56907c0569597f4/
,,0.0976,rocksdb,WriteBufferManger doenst cost to cache if no limit is set (#4695) Summary: WriteBufferManger is not invoked when allocating memory for memtable if the limit is not set even if a cache is passed. It is inconsistent from the comment syas. Fix it. Pull Request resolved: Differential Revision: D13112722 Pulled By: siying fbshipit-source-id: 0b27eef63867f679cd06033ea56907c0569597f4/
,,0.1514,rocksdb,JemallocNodumpAllocator: option to limit tcache memory usage (#4736) Summary: Add option to limit tcache usage by allocation size. This is to reduce total tcache size in case there are many user threads accessing the allocator and incur non-trivial memory usage. Pull Request resolved: Differential Revision: D13269305 Pulled By: yiwu-arbug fbshipit-source-id: 95a9b7fc67facd66837c849137e30e137112e19d/
,,0.0733,rocksdb,Reset size_ to 0 in PinnableSlice::Reset (#4962) Summary: It would avoid bugs if the reused PinnableSlice is not actually reassigned and yet the programmer makes conclusions based on the size of the Slice. Pull Request resolved: Differential Revision: D14012710 Pulled By: maysamyabandeh fbshipit-source-id: 23f4e173386b5461fd5650f44cde470805f4e816/
,,0.3678,rocksdb,"JemallocNodumpAllocator: option to limit tcache memory usage (#4736) Summary: Add option to limit tcache usage by allocation size. This is to reduce total tcache size in case there are many user threads accessing the allocator and incur non-trivial memory usage. Pull Request resolved: Differential Revision: D13269305 Pulled By: yiwu-arbug fbshipit-source-id: 95a9b7fc67facd66837c849137e30e137112e19d/Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.0686,rocksdb,Make compaction_pri kMinOverlappingRatio to be default (#4911) Summary: compaction_pri kMinOverlappingRatio usually provides much better write amplification than the default. fixes one shortcome of this option. Make it default. Pull Request resolved: Differential Revision: D13789262 Pulled By: siying fbshipit-source-id: d90acf8c4dede44f00d183ca4c7a210259378269/
,,0.3216,rocksdb,"Concurrent task limiter for compaction thread control (#4332) Summary: The PR is targeting to resolve the issue of: We have a rocksdb created with leveled-compaction with multiple column families (CFs), some of CFs are using HDD to store big and less frequently accessed data and others are using SSD. When there are continuously write traffics going on to all CFs, the compaction thread pool is mostly occupied by those slow HDD compactions, which blocks fully utilize SSD bandwidth. Since atomic write and transaction is needed across CFs, so splitting it to multiple rocksdb instance is not an option for us. With the compaction thread control, we got 30%+ HDD write throughput gain, and also a lot smooth SSD write since less write stall happening. ConcurrentTaskLimiter can be shared with multi-CFs across rocksdb instances, so the feature does not only work for multi-CFs scenarios, but also for multi-rocksdbs scenarios, who need disk IO resource control per tenant. The usage is straight forward: e.g.: // // Enable compaction thread limiter thru ColumnFamilyOptions // std::shared_ptr<ConcurrentTaskLimiter> ctl(NewConcurrentTaskLimiter(""foo_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter ctl; ... // // Compaction thread limiter can be tuned or disabled on-the-fly // ctl->SetMaxOutstandingTask(12); // enlarge to 12 tasks ... ctl->ResetMaxOutstandingTask(); // disable (bypass) thread limiter ctl->SetMaxOutstandingTask(-1); // Same as above ... ctl->SetMaxOutstandingTask(0); // full throttle (0 task) // // Sharing compaction thread limiter among CFs (to resolve multiple storage perf issue) // std::shared_ptr<ConcurrentTaskLimiter> ctl_ssd(NewConcurrentTaskLimiter(""ssd_limiter"", 8)); std::shared_ptr<ConcurrentTaskLimiter> ctl_hdd(NewConcurrentTaskLimiter(""hdd_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt_ssd1(options); ColumnFamilyOptions cf_opt_ssd2(options); ColumnFamilyOptions cf_opt_hdd1(options); ColumnFamilyOptions cf_opt_hdd2(options); ColumnFamilyOptions cf_opt_hdd3(options); // SSD CFs cf_opt_ssd1.compaction_thread_limiter ctl_ssd; cf_opt_ssd2.compaction_thread_limiter ctl_ssd; // HDD CFs cf_opt_hdd1.compaction_thread_limiter ctl_hdd; cf_opt_hdd2.compaction_thread_limiter ctl_hdd; cf_opt_hdd3.compaction_thread_limiter ctl_hdd; ... // // The limiter is disabled by default (or set to nullptr explicitly) // Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter nullptr; Pull Request resolved: Differential Revision: D13226590 Pulled By: siying fbshipit-source-id: 14307aec55b8bd59c8223d04aa6db3c03d1b0c1d/Add the max trace file size limitation option to Tracing (#4610) Summary: If user do not end the trace manually, the tracing will continue which can potential use up all the storage space and cause problem. In this PR, the max trace file size is added to the TraceOptions and user can set the value if they need or the default is 64GB. Pull Request resolved: Differential Revision: D12893400 Pulled By: zhichao-cao fbshipit-source-id: acf4b5a6076bb691778bdfbac4864e1006758953/"
,,0.1048,rocksdb,"Fix Windows broken build error due to non-const override (#4798) Summary: 1) `transaction_base.h` overrides from `transaction.h` with a `const boolean do_validate`. The non-const base declaration, which I cannot see the need for, causes a compilation error on Microsoft Windows. 2) Implicit cast from `double` to `uint64_t` causes a compilation error on Microsoft Windows. Pull Request resolved: Differential Revision: D13519734 Pulled By: sagar0 fbshipit-source-id: 6e8cb80e9a589b1122e1500c21b8e3a3a472b459/Fix inline comments for assumed_tracked (#4762) Summary: Fix the definition of assumed_tracked in Transaction that was introduced in Pull Request resolved: Differential Revision: D13399150 Pulled By: maysamyabandeh fbshipit-source-id: 2a30fe49e3c44adacd7e45cd48eae95023ca9dca/Fix ignoring params in default impl of GetForUpdate (#4679) Summary: The default implementation of GetForUpdate that receives PinnableSlice was mistakenly dropping column_family and exclusive parameters. Pull Request resolved: Differential Revision: D13062531 Pulled By: maysamyabandeh fbshipit-source-id: 7625d0c1ba872a5d894b58ced42147d6c8556a6f/"
,,0.3327,rocksdb,"Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.1352,rocksdb,"Deprecate CompactionFilter::IgnoreSnapshots() false (#4954) Summary: We found that the behavior of CompactionFilter::IgnoreSnapshots() false isnt what we have expected. We thought that snapshot will always be preserved. However, we just realized that, if no snapshot is created while compaction starts, and a snapshot is created after that, the data seen from the snapshot can successfully be dropped by the compaction. This creates a strange behavior to the feature, which is hard to explain. Like what is documented in code comment, this feature is not very useful with snapshot anyway. The decision is to deprecate the feature. We keep the function to avoid to break users code. However, we will fail compactions if false is returned. Pull Request resolved: Differential Revision: D13981900 Pulled By: siying fbshipit-source-id: 2db8c2c3865acd86a28dca625945d1481b1d1e36/"
,,0.0847,rocksdb,"RocksJava must compile on JDK7 (#4768) Summary: Fixes some RocksJava regressions recently introduced, whereby RocksJava would not build on JDK 7. These should have been visible on Travis-CI Pull Request resolved: Differential Revision: D13418173 Pulled By: sagar0 fbshipit-source-id: 57bf223188887f84d9e072031af2e0d2c8a69c30/"
,,0.0816,rocksdb,Expose underlying Read/Write APIs for avoiding unnecessary memory copy (#2303) Summary: adamretter As you already mentioned at . Pull Request resolved: Differential Revision: D10209001 Pulled By: sagar0 fbshipit-source-id: bcbce004112c2edeaff116968d79c6f90aab4b6c/
,,0.0833,rocksdb,Expose underlying Read/Write APIs for avoiding unnecessary memory copy (#2303) Summary: adamretter As you already mentioned at . Pull Request resolved: Differential Revision: D10209001 Pulled By: sagar0 fbshipit-source-id: bcbce004112c2edeaff116968d79c6f90aab4b6c/
,,0.0865,rocksdb,"RocksJava must compile on JDK7 (#4768) Summary: Fixes some RocksJava regressions recently introduced, whereby RocksJava would not build on JDK 7. These should have been visible on Travis-CI Pull Request resolved: Differential Revision: D13418173 Pulled By: sagar0 fbshipit-source-id: 57bf223188887f84d9e072031af2e0d2c8a69c30/"
,,0.1635,rocksdb,Reduce runtime of compact_on_deletion_collector_test (#4779) Summary: It sometimes times out with it is run with TSAN. The patch reduces the iteration from 50 to 30. This reduces the normal runtime from 5.2 to 3.1 seconds and should similarly address the TSAN timeout problem. Pull Request resolved: Differential Revision: D13456862 Pulled By: maysamyabandeh fbshipit-source-id: fdc0ad7d781b1c33b771d2415ff5fa2f1b5e2537/
,,0.1011,rocksdb,"Correct the comment about inlined blob option (#4887) Summary: Corrected a comment asserting that the values ""smaller"" than a min_blob_size will be inlined in the base db. Also fixed the type of ttl_range_secs while dumping blobdb options. Pull Request resolved: Differential Revision: D13680163 Pulled By: sagar0 fbshipit-source-id: 306c8cf2daa52210ffc334a6924ef44ffdedf887/"
,,0.0949,rocksdb,"Correct the comment about inlined blob option (#4887) Summary: Corrected a comment asserting that the values ""smaller"" than a min_blob_size will be inlined in the base db. Also fixed the type of ttl_range_secs while dumping blobdb options. Pull Request resolved: Differential Revision: D13680163 Pulled By: sagar0 fbshipit-source-id: 306c8cf2daa52210ffc334a6924ef44ffdedf887/"
,,0.2601,rocksdb,"fix DeleteRange memory leak for mmap and block cache (#4810) Summary: Previously we were cleaning up range tombstone meta-block by calling `ReleaseCachedEntry`, which wouldnt work if `value nullptr && cache_handle nullptr`. This happened at least in the case with mmap reads and block cache both enabled. I noticed `NewDataBlockIterator` intends to handle all these cases, so migrated to that instead of `NewUnfragmentedRangeTombstoneIterator`. Also changed the table-opening logic to fail on `ReadRangeDelBlock` failure, since that can cause data corruption. Added a test case to verify this behavior. Note the test case does not fail on `TryReopen` because failure to preload table handlers is not considered critical. However, it does fail on any read involving that file since it cannot return correct data. Pull Request resolved: Differential Revision: D13534296 Pulled By: ajkr fbshipit-source-id: 55dde1111717cea6ec4bf38418daab81ccef3599/Fix BlockBasedTable not always using memory allocator if available (#4678) Summary: Fix block based table reader not using memory_allocator when allocating index blocks and compression dictionary blocks. Pull Request resolved: Differential Revision: D13054594 Pulled By: yiwu-arbug fbshipit-source-id: 379f25bcc665395662511c4f873f4b7b55104ce2/"
,,0.1402,rocksdb,Fix BlockBasedTable not always using memory allocator if available (#4678) Summary: Fix block based table reader not using memory_allocator when allocating index blocks and compression dictionary blocks. Pull Request resolved: Differential Revision: D13054594 Pulled By: yiwu-arbug fbshipit-source-id: 379f25bcc665395662511c4f873f4b7b55104ce2/
,,0.1437,rocksdb,Fix BlockBasedTable not always using memory allocator if available (#4678) Summary: Fix block based table reader not using memory_allocator when allocating index blocks and compression dictionary blocks. Pull Request resolved: Differential Revision: D13054594 Pulled By: yiwu-arbug fbshipit-source-id: 379f25bcc665395662511c4f873f4b7b55104ce2/
,,0.3295,rocksdb,"Digest ZSTD compression dictionary once when writing SST file (#4849) Summary: This is essentially a re-submission of with a few improvements: Split `CompressionDict` into two separate classes: `CompressionDict` and `UncompressionDict` Eliminated `Init` functions. Instead do all initialization work in constructors. Added test case for parallel DB open, which is the scenario where failed under TSAN. Pull Request resolved: Differential Revision: D13606039 Pulled By: ajkr fbshipit-source-id: 08c236059798c710db9cbf545fce0f371232d447/fix accounting for range tombstones in TableProperties (#4841) Summary: To be consistent with the accounting of other optypes in `TableProperties`, we should count range tombstones in `TableProperties::num_entries` and `TableProperties::num_deletions`. Updated assertions in stress tests `OnTableFileCreated` handler to accept files with range tombstones only. Pull Request resolved: Differential Revision: D13568424 Pulled By: ajkr fbshipit-source-id: 0139d7806494eda20ece67ec460d2458dbbf6026/Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.1042,rocksdb,"PlainTable should avoid copying Get() results from immortal source. (#4924) Summary: avoids memcopy for Get() results if files are immortable (read-only DB, max_open_files=-1) and the file is ammaped. The same optimization is being applied to PlainTable here. Pull Request resolved: Differential Revision: D13827749 Pulled By: siying fbshipit-source-id: 1f2cbfc530b40ce08ccd53f95f6e78de4d1c2f96/"
,,0.3413,rocksdb,"Digest ZSTD compression dictionary once when writing SST file (#4849) Summary: This is essentially a re-submission of with a few improvements: Split `CompressionDict` into two separate classes: `CompressionDict` and `UncompressionDict` Eliminated `Init` functions. Instead do all initialization work in constructors. Added test case for parallel DB open, which is the scenario where failed under TSAN. Pull Request resolved: Differential Revision: D13606039 Pulled By: ajkr fbshipit-source-id: 08c236059798c710db9cbf545fce0f371232d447/fix DeleteRange memory leak for mmap and block cache (#4810) Summary: Previously we were cleaning up range tombstone meta-block by calling `ReleaseCachedEntry`, which wouldnt work if `value nullptr && cache_handle nullptr`. This happened at least in the case with mmap reads and block cache both enabled. I noticed `NewDataBlockIterator` intends to handle all these cases, so migrated to that instead of `NewUnfragmentedRangeTombstoneIterator`. Also changed the table-opening logic to fail on `ReadRangeDelBlock` failure, since that can cause data corruption. Added a test case to verify this behavior. Note the test case does not fail on `TryReopen` because failure to preload table handlers is not considered critical. However, it does fail on any read involving that file since it cannot return correct data. Pull Request resolved: Differential Revision: D13534296 Pulled By: ajkr fbshipit-source-id: 55dde1111717cea6ec4bf38418daab81ccef3599/Fix BlockBasedTable not always using memory allocator if available (#4678) Summary: Fix block based table reader not using memory_allocator when allocating index blocks and compression dictionary blocks. Pull Request resolved: Differential Revision: D13054594 Pulled By: yiwu-arbug fbshipit-source-id: 379f25bcc665395662511c4f873f4b7b55104ce2/Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.3287,rocksdb,"Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.1402,rocksdb,Fix BlockBasedTable not always using memory allocator if available (#4678) Summary: Fix block based table reader not using memory_allocator when allocating index blocks and compression dictionary blocks. Pull Request resolved: Differential Revision: D13054594 Pulled By: yiwu-arbug fbshipit-source-id: 379f25bcc665395662511c4f873f4b7b55104ce2/
,,0.3221,rocksdb,"Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.1119,rocksdb,"PlainTable should avoid copying Get() results from immortal source. (#4924) Summary: avoids memcopy for Get() results if files are immortable (read-only DB, max_open_files=-1) and the file is ammaped. The same optimization is being applied to PlainTable here. Pull Request resolved: Differential Revision: D13827749 Pulled By: siying fbshipit-source-id: 1f2cbfc530b40ce08ccd53f95f6e78de4d1c2f96/"
,,0.3754,rocksdb,"Fix BlockBasedTable not always using memory allocator if available (#4678) Summary: Fix block based table reader not using memory_allocator when allocating index blocks and compression dictionary blocks. Pull Request resolved: Differential Revision: D13054594 Pulled By: yiwu-arbug fbshipit-source-id: 379f25bcc665395662511c4f873f4b7b55104ce2/Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/"
,,0.115,rocksdb,"PlainTable should avoid copying Get() results from immortal source. (#4924) Summary: avoids memcopy for Get() results if files are immortable (read-only DB, max_open_files=-1) and the file is ammaped. The same optimization is being applied to PlainTable here. Pull Request resolved: Differential Revision: D13827749 Pulled By: siying fbshipit-source-id: 1f2cbfc530b40ce08ccd53f95f6e78de4d1c2f96/"
,,0.1103,rocksdb,Fix only `SyncClosedLogs` for multiple CFs (#4460) Summary: Call `SyncClosedLogs()` only if there are more than one column families. Update several unit tests (in `fault_injection_test` and `db_flush_test`) correspondingly. See for more info. Pull Request resolved: Differential Revision: D12896377 Pulled By: riversand963 fbshipit-source-id: f49afdaec32568f12f001219a3aec1dfde3b32bf/
,,0.2955,rocksdb,"Preload some files even if options.max_open_files (#3340) Summary: Choose to preload some files if options.max_open_files This can slightly narrow the gap of performance between options.max_open_files is and a large number. To avoid a significant regression to DB reopen speed if options.max_open_files Limit the files to preload in DB open time to 16. Pull Request resolved: Differential Revision: D6686945 Pulled By: siying fbshipit-source-id: 8ec11bbdb46e3d0cdee7b6ad5897a09c5a07869f/Fix flaky test DeleteFileRange (#4784) Summary: The test fails sporadically expecting the DB to be empty after DeleteFilesInRange(..., nullptr, nullptr) call which is not. Debugging shows cases where the files are skipped since they are being compacted. The patch fixes the test by waiting for the last CompactRange to finish before calling DeleteFilesInRange. Verified by ``` ~/gtest-parallel/gtest-parallel ./db_compaction_test ``` Pull Request resolved: Differential Revision: D13469402 Pulled By: maysamyabandeh fbshipit-source-id: 3d8f44abe205b82c69f01e7edf27e1f8098248e1/Concurrent task limiter for compaction thread control (#4332) Summary: The PR is targeting to resolve the issue of: We have a rocksdb created with leveled-compaction with multiple column families (CFs), some of CFs are using HDD to store big and less frequently accessed data and others are using SSD. When there are continuously write traffics going on to all CFs, the compaction thread pool is mostly occupied by those slow HDD compactions, which blocks fully utilize SSD bandwidth. Since atomic write and transaction is needed across CFs, so splitting it to multiple rocksdb instance is not an option for us. With the compaction thread control, we got 30%+ HDD write throughput gain, and also a lot smooth SSD write since less write stall happening. ConcurrentTaskLimiter can be shared with multi-CFs across rocksdb instances, so the feature does not only work for multi-CFs scenarios, but also for multi-rocksdbs scenarios, who need disk IO resource control per tenant. The usage is straight forward: e.g.: // // Enable compaction thread limiter thru ColumnFamilyOptions // std::shared_ptr<ConcurrentTaskLimiter> ctl(NewConcurrentTaskLimiter(""foo_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter ctl; ... // // Compaction thread limiter can be tuned or disabled on-the-fly // ctl->SetMaxOutstandingTask(12); // enlarge to 12 tasks ... ctl->ResetMaxOutstandingTask(); // disable (bypass) thread limiter ctl->SetMaxOutstandingTask(-1); // Same as above ... ctl->SetMaxOutstandingTask(0); // full throttle (0 task) // // Sharing compaction thread limiter among CFs (to resolve multiple storage perf issue) // std::shared_ptr<ConcurrentTaskLimiter> ctl_ssd(NewConcurrentTaskLimiter(""ssd_limiter"", 8)); std::shared_ptr<ConcurrentTaskLimiter> ctl_hdd(NewConcurrentTaskLimiter(""hdd_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt_ssd1(options); ColumnFamilyOptions cf_opt_ssd2(options); ColumnFamilyOptions cf_opt_hdd1(options); ColumnFamilyOptions cf_opt_hdd2(options); ColumnFamilyOptions cf_opt_hdd3(options); // SSD CFs cf_opt_ssd1.compaction_thread_limiter ctl_ssd; cf_opt_ssd2.compaction_thread_limiter ctl_ssd; // HDD CFs cf_opt_hdd1.compaction_thread_limiter ctl_hdd; cf_opt_hdd2.compaction_thread_limiter ctl_hdd; cf_opt_hdd3.compaction_thread_limiter ctl_hdd; ... // // The limiter is disabled by default (or set to nullptr explicitly) // Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter nullptr; Pull Request resolved: Differential Revision: D13226590 Pulled By: siying fbshipit-source-id: 14307aec55b8bd59c8223d04aa6db3c03d1b0c1d/Fix flaky test DBCompactionTest::DeleteFileRange (#4776) Summary: The test has been failing sporadically probably because the configured compaction options were actually unused. Verified that by the following: ``` ~/gtest-parallel/gtest-parallel ./db_compaction_test ``` Pull Request resolved: Differential Revision: D13441052 Pulled By: maysamyabandeh fbshipit-source-id: d35075b9e6cef9b9c9d0d571f9cd72ade8eda55d/Allow file-ingest-triggered flush to skip waiting for write-stall clear (#4751) Summary: When write stall has already been triggered due to number of L0 files reaching threshold, file ingestion must proceed with its flush without waiting for the write stall condition to cleared by the compaction because compaction can wait for ingestion to finish (circular wait). In order to avoid this wait, we can set `FlushOptions.allow_write_stall` to be true (default is false). Setting it to false can cause deadlock. This can happen when the number of compaction threads is low. Considere the following ``` Time compaction_thread ingestion_thread | num_running_ingest_file_++ | while(num_running_ingest_file_>0){wait} | flush V ``` Pull Request resolved: Differential Revision: D13343037 Pulled By: riversand963 fbshipit-source-id: d3b95938814af46ec4c463feff0b50c70bd8b23f/use per-level perfcontext for DB::Get calls (#4617) Summary: this PR adds two more per-level perf context counters to track * number of keys returned in Get call, break down by levels * total processing time at each level during Get call Pull Request resolved: Differential Revision: D12898024 Pulled By: miasantreble fbshipit-source-id: 6b84ef1c8097c0d9e97bee1a774958f56ab4a6c4/"
,,0.1398,rocksdb,"Fix spelling errors (#4827) Summary: Hi, Lintian, the Debian package checker complains about spelling error (spelling-error-in-binary). See Pull Request resolved: Differential Revision: D13566362 Pulled By: riversand963 fbshipit-source-id: cd4e9212133c73b0591030de6cdedaa47575968d/"
,,0.1595,rocksdb,Preload some files even if options.max_open_files (#3340) Summary: Choose to preload some files if options.max_open_files This can slightly narrow the gap of performance between options.max_open_files is and a large number. To avoid a significant regression to DB reopen speed if options.max_open_files Limit the files to preload in DB open time to 16. Pull Request resolved: Differential Revision: D6686945 Pulled By: siying fbshipit-source-id: 8ec11bbdb46e3d0cdee7b6ad5897a09c5a07869f/
,,0.2647,rocksdb,"Fix skip WAL for whole write_group when leaders callback fail (#4838) Summary: The original implementation has two problems: 1. If the callback status of leader of the write_group fails, then the whole write_group will not write to WAL, this may cause data loss. 2. The annotation says that Writer.status is the status of memtable inserter, but the original implementation use it for another case which is not consistent with the original design. Looks like we can still reuse Writer.status, but we should modify the annotation, so Writer.status is not only the status of memtable inserter. Pull Request resolved: Differential Revision: D13574070 Pulled By: yiwu-arbug fbshipit-source-id: a2a2aefcfd329c4c6a91652bf090aaf1ce119c4b/Fix spelling errors (#4827) Summary: Hi, Lintian, the Debian package checker complains about spelling error (spelling-error-in-binary). See Pull Request resolved: Differential Revision: D13566362 Pulled By: riversand963 fbshipit-source-id: cd4e9212133c73b0591030de6cdedaa47575968d/Remove redundant member var and set options (#4631) Summary: In the past, both `DBImpl::atomic_flush_` and `DBImpl::immutable_db_options_.atomic_flush` exist. However, we fail to set `immutable_db_options_.atomic_flush`, but use `DBImpl::atomic_flush_` which is set correctly. This does not lead to incorrect behavior, but is a duplicate of information. Since `immutable_db_options_` is always there and has `atomic_flush`, we should use it as source of truth and remove `DBImpl::atomic_flush_`. Pull Request resolved: Differential Revision: D12928371 Pulled By: riversand963 fbshipit-source-id: f85a811959d3828aad4a3a1b05f71facf19c636d/"
,,0.1745,rocksdb,"Use correct FileMeta for atomic flush result install (#4932) Summary: 1. this commit fixes our handling of a combination of two separate edge cases. If a flush job does not pick any memtable to flush (because another flush job has already picked the same memtables), and the column family assigned to the flush job is dropped right before RocksDB calls rocksdb::InstallMemtableAtomicFlushResults, our original code passes a FileMetaData object whose file number is 0, failing the assertion in rocksdb::InstallMemtableAtomicFlushResults (assert(m->GetFileNumber() > 0)). 2. Also piggyback a small change: since we already create a local copy of column familys mutable CF options to eliminate potential race condition with `SetOptions` call, we might as well use the local copy in other function calls in the same scope. Pull Request resolved: Differential Revision: D13901322 Pulled By: riversand963 fbshipit-source-id: b936580af7c127ea0c6c19ea10cd5fcede9fb0f9/"
,,0.163,rocksdb,"Use correct FileMeta for atomic flush result install (#4932) Summary: 1. this commit fixes our handling of a combination of two separate edge cases. If a flush job does not pick any memtable to flush (because another flush job has already picked the same memtables), and the column family assigned to the flush job is dropped right before RocksDB calls rocksdb::InstallMemtableAtomicFlushResults, our original code passes a FileMetaData object whose file number is 0, failing the assertion in rocksdb::InstallMemtableAtomicFlushResults (assert(m->GetFileNumber() > 0)). 2. Also piggyback a small change: since we already create a local copy of column familys mutable CF options to eliminate potential race condition with `SetOptions` call, we might as well use the local copy in other function calls in the same scope. Pull Request resolved: Differential Revision: D13901322 Pulled By: riversand963 fbshipit-source-id: b936580af7c127ea0c6c19ea10cd5fcede9fb0f9/"
,,0.1221,rocksdb,"Fix skip WAL for whole write_group when leaders callback fail (#4838) Summary: The original implementation has two problems: 1. If the callback status of leader of the write_group fails, then the whole write_group will not write to WAL, this may cause data loss. 2. The annotation says that Writer.status is the status of memtable inserter, but the original implementation use it for another case which is not consistent with the original design. Looks like we can still reuse Writer.status, but we should modify the annotation, so Writer.status is not only the status of memtable inserter. Pull Request resolved: Differential Revision: D13574070 Pulled By: yiwu-arbug fbshipit-source-id: a2a2aefcfd329c4c6a91652bf090aaf1ce119c4b/"
,,0.1166,rocksdb,Fix ticker stat for number files closed (#4703) Summary: We havent been populating `NO_FILE_CLOSES` since v1.5.8 even though it was never marked as deprecated. Start populating it again. Conveniently `DeleteTableReader` has an unused `void*` argument that we can use... Blame: 63f216ee0a2a6e28f9dfe24913d134d3a7fa3aca Closes Pull Request resolved: Differential Revision: D13146769 Pulled By: ajkr fbshipit-source-id: ad8d6fb0493e701f60a165a3bca1787d255be008/
,,0.2464,rocksdb,"Fix a flaky test DBFlushTest.SyncFail (#4633) Summary: There is a race condition in DBFlushTest.SyncFail, as illustrated below. ``` time thread1 bg_flush_thread | Flush(wait=false, cfd) | refs_before=cfd->current()->TEST_refs() PickMemtable calls cfd->current()->Ref() V ``` The race condition between thread1 getting the ref count of cfds current version and bg_flush_thread incrementing the cfds current version makes it possible for later assertion on refs_before to fail. Therefore, we add test sync points to enforce the order and assert on the ref count before and after PickMemtable is called in bg_flush_thread. Pull Request resolved: Differential Revision: D12967131 Pulled By: riversand963 fbshipit-source-id: a99d2bacb7869ec5d8d03b24ef2babc0e6ae1a3b/Rollback memtable flush upon atomic flush fail (#4641) Summary: This fixes an assertion. An atomic flush can have multiple flush jobs. Some of them may fail. If any of them fails, we need to rollback all of them. For the flush jobs that do fail, we already call `RollbackMemTableFlush` in `FlushJob::Run`. The tricky part is for flush jobs that have completed successfully. We need to call `RollbackMemTableFlush` for them as well. The newly added DBAtomicFlushTest.AtomicFlushRollbackSomeJobs will SigAbort without the corresponding change in AtomicFlushMemTablesToOutputFiles. Pull Request resolved: Differential Revision: D12943649 Pulled By: riversand963 fbshipit-source-id: c66a4a664a1e0938e938fd41edc5a70c34cdd868/Fix only `SyncClosedLogs` for multiple CFs (#4460) Summary: Call `SyncClosedLogs()` only if there are more than one column families. Update several unit tests (in `fault_injection_test` and `db_flush_test`) correspondingly. See for more info. Pull Request resolved: Differential Revision: D12896377 Pulled By: riversand963 fbshipit-source-id: f49afdaec32568f12f001219a3aec1dfde3b32bf/"
,,0.0918,rocksdb,"Fix Windows broken build error due to non-const override (#4798) Summary: 1) `transaction_base.h` overrides from `transaction.h` with a `const boolean do_validate`. The non-const base declaration, which I cannot see the need for, causes a compilation error on Microsoft Windows. 2) Implicit cast from `double` to `uint64_t` causes a compilation error on Microsoft Windows. Pull Request resolved: Differential Revision: D13519734 Pulled By: sagar0 fbshipit-source-id: 6e8cb80e9a589b1122e1500c21b8e3a3a472b459/"
,,0.1787,rocksdb,"Use correct FileMeta for atomic flush result install (#4932) Summary: 1. this commit fixes our handling of a combination of two separate edge cases. If a flush job does not pick any memtable to flush (because another flush job has already picked the same memtables), and the column family assigned to the flush job is dropped right before RocksDB calls rocksdb::InstallMemtableAtomicFlushResults, our original code passes a FileMetaData object whose file number is 0, failing the assertion in rocksdb::InstallMemtableAtomicFlushResults (assert(m->GetFileNumber() > 0)). 2. Also piggyback a small change: since we already create a local copy of column familys mutable CF options to eliminate potential race condition with `SetOptions` call, we might as well use the local copy in other function calls in the same scope. Pull Request resolved: Differential Revision: D13901322 Pulled By: riversand963 fbshipit-source-id: b936580af7c127ea0c6c19ea10cd5fcede9fb0f9/"
,,0.2881,rocksdb,"Fix potential DB hang while using CompactFiles (#4940) Summary: CompactFiles() may block auto compaction which could cuase DB hang when it reachs level0_stop_writes_trigger. Pull Request resolved: Differential Revision: D13929648 Pulled By: cooldoger fbshipit-source-id: 10842df38df3bebf862cd1a120a88ce961fdd381/Use correct FileMeta for atomic flush result install (#4932) Summary: 1. this commit fixes our handling of a combination of two separate edge cases. If a flush job does not pick any memtable to flush (because another flush job has already picked the same memtables), and the column family assigned to the flush job is dropped right before RocksDB calls rocksdb::InstallMemtableAtomicFlushResults, our original code passes a FileMetaData object whose file number is 0, failing the assertion in rocksdb::InstallMemtableAtomicFlushResults (assert(m->GetFileNumber() > 0)). 2. Also piggyback a small change: since we already create a local copy of column familys mutable CF options to eliminate potential race condition with `SetOptions` call, we might as well use the local copy in other function calls in the same scope. Pull Request resolved: Differential Revision: D13901322 Pulled By: riversand963 fbshipit-source-id: b936580af7c127ea0c6c19ea10cd5fcede9fb0f9/Take snapshots once for all cf flushes (#4934) Summary: FlushMemTablesToOutputFiles calls FlushMemTableToOutputFile for each column family. The patch moves the take-snapshot logic to outside FlushMemTableToOutputFile so that it does it once for all the flushes. This also addresses a deadlock issue for resetting the managed snapshot of job_snapshot in the 2nd call to FlushMemTableToOutputFile. Pull Request resolved: Differential Revision: D13900747 Pulled By: maysamyabandeh fbshipit-source-id: f3cd650c5fff24cf95c1aaf8a10c149d42bf042c/WritePrepared: Fix visible key compacted out by compaction (#4883) Summary: With WritePrepared transaction, flush/compaction can contain uncommitted keys, and those keys can get committed during compaction. If a snapshot is taken before the key is committed, it should not see the key. On the other hand, compaction grab the list of snapshots at its beginning, and only consider those snapshots to dedup keys. Consider the case: ``` seq 1: put ""foo"" ""bar"" seq 2: transaction T: delete ""foo"", prepare seq 3: compaction start seq 4: take snapshot S seq 5: transaction T: commit. ... seq N: compaction iterator reached key ""foo"". ``` When compaction start, the list of snapshot is empty. Compaction doesnt take snapshot S into account. When it reached ""foo"", transaction T is committed. Compaction may think the value ""foo=bar"" is not visible by any snapshot (which is wrong), and compact the value out. The fix is to explicitly take a snapshot before compaction grabbing the list of snapshots. Compaction will then has to keep keys visible to this snapshot. Pull Request resolved: Differential Revision: D13668775 Pulled By: maysamyabandeh fbshipit-source-id: 1cab9615f94b7d3e8522cc3d44c3a14c7d4720e4/Make a copy of MutableCFOptions to avoid race condition (#4876) Summary: If we do not do this, then reading MutableCFOptions may have a race condition with SetOptions which modifies MutableCFOptions. Also reserve space in advance for vectors to avoid reallocation changing the address of its elements. Test plan ``` $make clean && make all check $make clean && COMPILE_WITH_TSAN=1 make all check $make clean && COMPILE_WITH_ASAN=1 make all check ``` Pull Request resolved: Differential Revision: D13644500 Pulled By: riversand963 fbshipit-source-id: 4b8112c5c819d5a2922bb61ad1521b3d2fb2fd47/Avoid switching empty memtable in certain cases (#4792) Summary: in certain cases, we do not perform memtable switching if the active memtable of the column family is empty. Two exceptions: 1. In manual flush, if cached_recoverable_state_empty_ is false, then we need to switch memtable due to requirement of transaction. 2. In switch WAL, we need to switch memtable anyway because we have to seal the memtable if the WAL on which it depends will be closed. This change can potentially delay the occurence of write stalls because number of memtables increase more slowly. Pull Request resolved: Differential Revision: D13499501 Pulled By: riversand963 fbshipit-source-id: 91c9b17ae753578578039f3851667d93610005e1/Concurrent task limiter for compaction thread control (#4332) Summary: The PR is targeting to resolve the issue of: We have a rocksdb created with leveled-compaction with multiple column families (CFs), some of CFs are using HDD to store big and less frequently accessed data and others are using SSD. When there are continuously write traffics going on to all CFs, the compaction thread pool is mostly occupied by those slow HDD compactions, which blocks fully utilize SSD bandwidth. Since atomic write and transaction is needed across CFs, so splitting it to multiple rocksdb instance is not an option for us. With the compaction thread control, we got 30%+ HDD write throughput gain, and also a lot smooth SSD write since less write stall happening. ConcurrentTaskLimiter can be shared with multi-CFs across rocksdb instances, so the feature does not only work for multi-CFs scenarios, but also for multi-rocksdbs scenarios, who need disk IO resource control per tenant. The usage is straight forward: e.g.: // // Enable compaction thread limiter thru ColumnFamilyOptions // std::shared_ptr<ConcurrentTaskLimiter> ctl(NewConcurrentTaskLimiter(""foo_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter ctl; ... // // Compaction thread limiter can be tuned or disabled on-the-fly // ctl->SetMaxOutstandingTask(12); // enlarge to 12 tasks ... ctl->ResetMaxOutstandingTask(); // disable (bypass) thread limiter ctl->SetMaxOutstandingTask(-1); // Same as above ... ctl->SetMaxOutstandingTask(0); // full throttle (0 task) // // Sharing compaction thread limiter among CFs (to resolve multiple storage perf issue) // std::shared_ptr<ConcurrentTaskLimiter> ctl_ssd(NewConcurrentTaskLimiter(""ssd_limiter"", 8)); std::shared_ptr<ConcurrentTaskLimiter> ctl_hdd(NewConcurrentTaskLimiter(""hdd_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt_ssd1(options); ColumnFamilyOptions cf_opt_ssd2(options); ColumnFamilyOptions cf_opt_hdd1(options); ColumnFamilyOptions cf_opt_hdd2(options); ColumnFamilyOptions cf_opt_hdd3(options); // SSD CFs cf_opt_ssd1.compaction_thread_limiter ctl_ssd; cf_opt_ssd2.compaction_thread_limiter ctl_ssd; // HDD CFs cf_opt_hdd1.compaction_thread_limiter ctl_hdd; cf_opt_hdd2.compaction_thread_limiter ctl_hdd; cf_opt_hdd3.compaction_thread_limiter ctl_hdd; ... // // The limiter is disabled by default (or set to nullptr explicitly) // Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter nullptr; Pull Request resolved: Differential Revision: D13226590 Pulled By: siying fbshipit-source-id: 14307aec55b8bd59c8223d04aa6db3c03d1b0c1d/Fix a flaky test DBFlushTest.SyncFail (#4633) Summary: There is a race condition in DBFlushTest.SyncFail, as illustrated below. ``` time thread1 bg_flush_thread | Flush(wait=false, cfd) | refs_before=cfd->current()->TEST_refs() PickMemtable calls cfd->current()->Ref() V ``` The race condition between thread1 getting the ref count of cfds current version and bg_flush_thread incrementing the cfds current version makes it possible for later assertion on refs_before to fail. Therefore, we add test sync points to enforce the order and assert on the ref count before and after PickMemtable is called in bg_flush_thread. Pull Request resolved: Differential Revision: D12967131 Pulled By: riversand963 fbshipit-source-id: a99d2bacb7869ec5d8d03b24ef2babc0e6ae1a3b/Rollback memtable flush upon atomic flush fail (#4641) Summary: This fixes an assertion. An atomic flush can have multiple flush jobs. Some of them may fail. If any of them fails, we need to rollback all of them. For the flush jobs that do fail, we already call `RollbackMemTableFlush` in `FlushJob::Run`. The tricky part is for flush jobs that have completed successfully. We need to call `RollbackMemTableFlush` for them as well. The newly added DBAtomicFlushTest.AtomicFlushRollbackSomeJobs will SigAbort without the corresponding change in AtomicFlushMemTablesToOutputFiles. Pull Request resolved: Differential Revision: D12943649 Pulled By: riversand963 fbshipit-source-id: c66a4a664a1e0938e938fd41edc5a70c34cdd868/Fix only `SyncClosedLogs` for multiple CFs (#4460) Summary: Call `SyncClosedLogs()` only if there are more than one column families. Update several unit tests (in `fault_injection_test` and `db_flush_test`) correspondingly. See for more info. Pull Request resolved: Differential Revision: D12896377 Pulled By: riversand963 fbshipit-source-id: f49afdaec32568f12f001219a3aec1dfde3b32bf/Fix `CompactFiles` bug (#4665) Summary: `CompactFiles` gets `SuperVersion` before `WaitForIngestFile`, while `IngestExternalFile` may add files that overlap with `input_file_names` The timeline of execution flow is as follow: Lets say that level N has two file [1,2] and [5,6] ``` timeline user_thread1 user_thread2 t0 | CompactFiles([1, 2], [5, 6]) begin t1 | GetReferencedSuperVersion() t2 | IngestExternalFile([3,4]) to level N begin t3 | CompactFiles resume V ``` Pull Request resolved: Differential Revision: D13030674 Pulled By: ajkr fbshipit-source-id: 8be19477fd6e505032267a979d32f3097cc3be51/Remove redundant member var and set options (#4631) Summary: In the past, both `DBImpl::atomic_flush_` and `DBImpl::immutable_db_options_.atomic_flush` exist. However, we fail to set `immutable_db_options_.atomic_flush`, but use `DBImpl::atomic_flush_` which is set correctly. This does not lead to incorrect behavior, but is a duplicate of information. Since `immutable_db_options_` is always there and has `atomic_flush`, we should use it as source of truth and remove `DBImpl::atomic_flush_`. Pull Request resolved: Differential Revision: D12928371 Pulled By: riversand963 fbshipit-source-id: f85a811959d3828aad4a3a1b05f71facf19c636d/"
,,0.1467,rocksdb,"Fix spelling errors (#4827) Summary: Hi, Lintian, the Debian package checker complains about spelling error (spelling-error-in-binary). See Pull Request resolved: Differential Revision: D13566362 Pulled By: riversand963 fbshipit-source-id: cd4e9212133c73b0591030de6cdedaa47575968d/"
,,0.266,rocksdb,"Take snapshots once for all cf flushes (#4934) Summary: FlushMemTablesToOutputFiles calls FlushMemTableToOutputFile for each column family. The patch moves the take-snapshot logic to outside FlushMemTableToOutputFile so that it does it once for all the flushes. This also addresses a deadlock issue for resetting the managed snapshot of job_snapshot in the 2nd call to FlushMemTableToOutputFile. Pull Request resolved: Differential Revision: D13900747 Pulled By: maysamyabandeh fbshipit-source-id: f3cd650c5fff24cf95c1aaf8a10c149d42bf042c/WritePrepared: Fix visible key compacted out by compaction (#4883) Summary: With WritePrepared transaction, flush/compaction can contain uncommitted keys, and those keys can get committed during compaction. If a snapshot is taken before the key is committed, it should not see the key. On the other hand, compaction grab the list of snapshots at its beginning, and only consider those snapshots to dedup keys. Consider the case: ``` seq 1: put ""foo"" ""bar"" seq 2: transaction T: delete ""foo"", prepare seq 3: compaction start seq 4: take snapshot S seq 5: transaction T: commit. ... seq N: compaction iterator reached key ""foo"". ``` When compaction start, the list of snapshot is empty. Compaction doesnt take snapshot S into account. When it reached ""foo"", transaction T is committed. Compaction may think the value ""foo=bar"" is not visible by any snapshot (which is wrong), and compact the value out. The fix is to explicitly take a snapshot before compaction grabbing the list of snapshots. Compaction will then has to keep keys visible to this snapshot. Pull Request resolved: Differential Revision: D13668775 Pulled By: maysamyabandeh fbshipit-source-id: 1cab9615f94b7d3e8522cc3d44c3a14c7d4720e4/WritePrepared: snapshot should be larger than max_evicted_seq_ (#4886) Summary: The AdvanceMaxEvictedSeq algorithm assumes that new snapshots always have sequence number larger than the last max_evicted_seq_. To enforce this assumption we make two changes: i) max is not advanced beyond the last published seq, with the exception that the evicted commit entry itself is not published yet, which is quite rare. ii) When obtaining the snapshot if the max_evicted_seq_ is not published yet, commit a dummy entry so that it waits for it to be published and also increased the latest published seq by one above the max. To test these non-realistic corner cases we create a commit cache with size 1 so that every single commit results into eviction. Pull Request resolved: Differential Revision: D13685270 Pulled By: maysamyabandeh fbshipit-source-id: 5461bc09c2a9b75798bfcb9853a256c81cdac0b0/Fix spelling errors (#4827) Summary: Hi, Lintian, the Debian package checker complains about spelling error (spelling-error-in-binary). See Pull Request resolved: Differential Revision: D13566362 Pulled By: riversand963 fbshipit-source-id: cd4e9212133c73b0591030de6cdedaa47575968d/Concurrent task limiter for compaction thread control (#4332) Summary: The PR is targeting to resolve the issue of: We have a rocksdb created with leveled-compaction with multiple column families (CFs), some of CFs are using HDD to store big and less frequently accessed data and others are using SSD. When there are continuously write traffics going on to all CFs, the compaction thread pool is mostly occupied by those slow HDD compactions, which blocks fully utilize SSD bandwidth. Since atomic write and transaction is needed across CFs, so splitting it to multiple rocksdb instance is not an option for us. With the compaction thread control, we got 30%+ HDD write throughput gain, and also a lot smooth SSD write since less write stall happening. ConcurrentTaskLimiter can be shared with multi-CFs across rocksdb instances, so the feature does not only work for multi-CFs scenarios, but also for multi-rocksdbs scenarios, who need disk IO resource control per tenant. The usage is straight forward: e.g.: // // Enable compaction thread limiter thru ColumnFamilyOptions // std::shared_ptr<ConcurrentTaskLimiter> ctl(NewConcurrentTaskLimiter(""foo_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter ctl; ... // // Compaction thread limiter can be tuned or disabled on-the-fly // ctl->SetMaxOutstandingTask(12); // enlarge to 12 tasks ... ctl->ResetMaxOutstandingTask(); // disable (bypass) thread limiter ctl->SetMaxOutstandingTask(-1); // Same as above ... ctl->SetMaxOutstandingTask(0); // full throttle (0 task) // // Sharing compaction thread limiter among CFs (to resolve multiple storage perf issue) // std::shared_ptr<ConcurrentTaskLimiter> ctl_ssd(NewConcurrentTaskLimiter(""ssd_limiter"", 8)); std::shared_ptr<ConcurrentTaskLimiter> ctl_hdd(NewConcurrentTaskLimiter(""hdd_limiter"", 4)); Options options; ColumnFamilyOptions cf_opt_ssd1(options); ColumnFamilyOptions cf_opt_ssd2(options); ColumnFamilyOptions cf_opt_hdd1(options); ColumnFamilyOptions cf_opt_hdd2(options); ColumnFamilyOptions cf_opt_hdd3(options); // SSD CFs cf_opt_ssd1.compaction_thread_limiter ctl_ssd; cf_opt_ssd2.compaction_thread_limiter ctl_ssd; // HDD CFs cf_opt_hdd1.compaction_thread_limiter ctl_hdd; cf_opt_hdd2.compaction_thread_limiter ctl_hdd; cf_opt_hdd3.compaction_thread_limiter ctl_hdd; ... // // The limiter is disabled by default (or set to nullptr explicitly) // Options options; ColumnFamilyOptions cf_opt(options); cf_opt.compaction_thread_limiter nullptr; Pull Request resolved: Differential Revision: D13226590 Pulled By: siying fbshipit-source-id: 14307aec55b8bd59c8223d04aa6db3c03d1b0c1d/Remove redundant member var and set options (#4631) Summary: In the past, both `DBImpl::atomic_flush_` and `DBImpl::immutable_db_options_.atomic_flush` exist. However, we fail to set `immutable_db_options_.atomic_flush`, but use `DBImpl::atomic_flush_` which is set correctly. This does not lead to incorrect behavior, but is a duplicate of information. Since `immutable_db_options_` is always there and has `atomic_flush`, we should use it as source of truth and remove `DBImpl::atomic_flush_`. Pull Request resolved: Differential Revision: D12928371 Pulled By: riversand963 fbshipit-source-id: f85a811959d3828aad4a3a1b05f71facf19c636d/"
,,0.1025,rocksdb,"Change directory where ExternalSSTFileBasicTest runs (#4766) Summary: Change the directory where ExternalSSTFileBasicTest* tests run. **Problem:** Without this change, I spent considerable time chasing around a non-existent issue as ExternalSSTFileTest.* and ExternalSSTFileBasicTest.* create similar directories. Pull Request resolved: Differential Revision: D13409384 Pulled By: sagar0 fbshipit-source-id: c33e1f4d505dfa6efbc788d6c57cdb680053ded3/"
,,0.2077,rocksdb,"fix DeleteRange memory leak for mmap and block cache (#4810) Summary: Previously we were cleaning up range tombstone meta-block by calling `ReleaseCachedEntry`, which wouldnt work if `value nullptr && cache_handle nullptr`. This happened at least in the case with mmap reads and block cache both enabled. I noticed `NewDataBlockIterator` intends to handle all these cases, so migrated to that instead of `NewUnfragmentedRangeTombstoneIterator`. Also changed the table-opening logic to fail on `ReadRangeDelBlock` failure, since that can cause data corruption. Added a test case to verify this behavior. Note the test case does not fail on `TryReopen` because failure to preload table handlers is not considered critical. However, it does fail on any read involving that file since it cannot return correct data. Pull Request resolved: Differential Revision: D13534296 Pulled By: ajkr fbshipit-source-id: 55dde1111717cea6ec4bf38418daab81ccef3599/"
,,0.1595,rocksdb,Preload some files even if options.max_open_files (#3340) Summary: Choose to preload some files if options.max_open_files This can slightly narrow the gap of performance between options.max_open_files is and a large number. To avoid a significant regression to DB reopen speed if options.max_open_files Limit the files to preload in DB open time to 16. Pull Request resolved: Differential Revision: D6686945 Pulled By: siying fbshipit-source-id: 8ec11bbdb46e3d0cdee7b6ad5897a09c5a07869f/
,,0.1611,rocksdb,Preload some files even if options.max_open_files (#3340) Summary: Choose to preload some files if options.max_open_files This can slightly narrow the gap of performance between options.max_open_files is and a large number. To avoid a significant regression to DB reopen speed if options.max_open_files Limit the files to preload in DB open time to 16. Pull Request resolved: Differential Revision: D6686945 Pulled By: siying fbshipit-source-id: 8ec11bbdb46e3d0cdee7b6ad5897a09c5a07869f/
,,0.2465,rocksdb,"fix accounting for range tombstones in TableProperties (#4841) Summary: To be consistent with the accounting of other optypes in `TableProperties`, we should count range tombstones in `TableProperties::num_entries` and `TableProperties::num_deletions`. Updated assertions in stress tests `OnTableFileCreated` handler to accept files with range tombstones only. Pull Request resolved: Differential Revision: D13568424 Pulled By: ajkr fbshipit-source-id: 0139d7806494eda20ece67ec460d2458dbbf6026/Preload some files even if options.max_open_files (#3340) Summary: Choose to preload some files if options.max_open_files This can slightly narrow the gap of performance between options.max_open_files is and a large number. To avoid a significant regression to DB reopen speed if options.max_open_files Limit the files to preload in DB open time to 16. Pull Request resolved: Differential Revision: D6686945 Pulled By: siying fbshipit-source-id: 8ec11bbdb46e3d0cdee7b6ad5897a09c5a07869f/"
,,0.1323,rocksdb,"exclude test CompactFilesShouldTriggerAutoCompaction from ROCKSDB_LITE (#4950) Summary: This will fix the following build error: > db/db_test.cc: In member function Ã«virtual void rocksdb::DBTest_CompactFilesShouldTriggerAutoCompaction_Test::TestBody()Ã­: > db/db_test.cc:5462:8: error: Ã«class rocksdb::DBÃ­ has no member named Ã«GetColumnFamilyMetaDataÃ­ > db_->GetColumnFamilyMetaData(db_->DefaultColumnFamily(), &cf_meta_data); > db/db_test.cc:5490:8: error: Ã«class rocksdb::DBÃ­ has no member named Ã«GetColumnFamilyMetaDataÃ­ > db_->GetColumnFamilyMetaData(db_->DefaultColumnFamily(), &cf_meta_data); > db/db_test.cc:5499:8: error: Ã«class rocksdb::DBÃ­ has no member named Ã«GetColumnFamilyMetaDataÃ­ > db_->GetColumnFamilyMetaData(db_->DefaultColumnFamily(), &cf_meta_data); Pull Request resolved: Differential Revision: D13965378 Pulled By: miasantreble fbshipit-source-id: a975435476fe555b1cd9d5da263ee3da3acdea56/Fix potential DB hang while using CompactFiles (#4940) Summary: CompactFiles() may block auto compaction which could cuase DB hang when it reachs level0_stop_writes_trigger. Pull Request resolved: Differential Revision: D13929648 Pulled By: cooldoger fbshipit-source-id: 10842df38df3bebf862cd1a120a88ce961fdd381/Increase wait time in DBTest.SanitizeNumThreads (#4659) Summary: DBTest.SanitizeNumThreads Sometimes fails. The test waited for 10ms timeout and expect all threads scheduled to be executed. This can be a source of flakiness. Make a check every 1ms and up to 10s. Pull Request resolved: Differential Revision: D13074174 Pulled By: siying fbshipit-source-id: b1d5ff87a326a4fc9eab8d1cc307bbb940dfe70c/"
,,0.217,rocksdb,"Fix point lookup on range tombstone sentinel endpoint (#4829) Summary: Previously for point lookup we decided which file to look into based on user key overlap only. We also did not truncate range tombstones in the point lookup code path. These two ideas did not interact well in cases like this: L1 has range tombstone [a, c)#1 and point key b#2. The data is split between file1 with range [a#1,1, b#72057594037927935,15], and file2 with range [b#2, c#1]. L1s file2 gets compacted to L2. User issues `Get()` for b#3. L1s file1 is opened and the range tombstone [a, c)#1 is found for b, while no point-key for b is found in L1. `Get()` assumes that the range tombstone must cover all data in that range in lower levels, so short circuits and returns `NotFound`. The solution to this problem is to not look into files that only overlap with the point lookup at a range tombstone sentinel endpoint. In the above example, this would mean not opening L1s file1 or its tombstones during the `Get()`. Pull Request resolved: Differential Revision: D13561355 Pulled By: ajkr fbshipit-source-id: a13c21c816870a2f5d32a48af6dbd719a7d9d19f/Preload some files even if options.max_open_files (#3340) Summary: Choose to preload some files if options.max_open_files This can slightly narrow the gap of performance between options.max_open_files is and a large number. To avoid a significant regression to DB reopen speed if options.max_open_files Limit the files to preload in DB open time to 16. Pull Request resolved: Differential Revision: D6686945 Pulled By: siying fbshipit-source-id: 8ec11bbdb46e3d0cdee7b6ad5897a09c5a07869f/Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOptiÃ– (#4697) Summary: Ã–ons (#4676)"" This reverts commit b32d087dbb3b9d9f2c9597caa650d0ca9d2e2d7f. `MemoryAllocator` needs to be with `Cache`, since cache entry can outlive DB and block based table. The cache needs to hold reference to memory allocator when deleting cache entry. Pull Request resolved: Differential Revision: D13133490 Pulled By: yiwu-arbug fbshipit-source-id: 8ef7e8a51263bfd929f892fd062665ff4ce9ce5a/Fix range tombstone covering short-circuit logic (#4698) Summary: Since a range tombstone seen at one level will cover all keys in the range at lower levels, there was a short-circuiting check in Get that reported a key was not found at most one file after the range tombstone was discovered. However, this was incorrect for merge operands, since a deletion might only cover some merge operands, which implies that the key should be found. This PR fixes this logic in the Version portion of Get, and removes the logic from the MemTable portion of Get, since the perforamnce benefit provided there is minimal. Pull Request resolved: Differential Revision: D13142484 Pulled By: abhimadan fbshipit-source-id: cbd74537c806032f2bfa564724d01a80df7c8f10/Fix uninitialized fields in file metadata (#4693) Summary: This is a quick fix for the uninitialized bugs in `LiveFileMetaData` and `SstFileMetaData` that were uncovered in Pull Request resolved: Differential Revision: D13113189 Pulled By: ajkr fbshipit-source-id: 18e798d031d2a59d0b55fc010c135e0126f4042d/Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676) Summary: Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy. It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Pull Request resolved: Differential Revision: D13047662 Pulled By: yiwu-arbug fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd/use per-level perfcontext for DB::Get calls (#4617) Summary: this PR adds two more per-level perf context counters to track * number of keys returned in Get call, break down by levels * total processing time at each level during Get call Pull Request resolved: Differential Revision: D12898024 Pulled By: miasantreble fbshipit-source-id: 6b84ef1c8097c0d9e97bee1a774958f56ab4a6c4/"
,,0.145,rocksdb,"Fix spelling errors (#4827) Summary: Hi, Lintian, the Debian package checker complains about spelling error (spelling-error-in-binary). See Pull Request resolved: Differential Revision: D13566362 Pulled By: riversand963 fbshipit-source-id: cd4e9212133c73b0591030de6cdedaa47575968d/"
,,0.2164,rocksdb,Preload some files even if options.max_open_files (#3340) Summary: Choose to preload some files if options.max_open_files This can slightly narrow the gap of performance between options.max_open_files is and a large number. To avoid a significant regression to DB reopen speed if options.max_open_files Limit the files to preload in DB open time to 16. Pull Request resolved: Differential Revision: D6686945 Pulled By: siying fbshipit-source-id: 8ec11bbdb46e3d0cdee7b6ad5897a09c5a07869f/Fix only `SyncClosedLogs` for multiple CFs (#4460) Summary: Call `SyncClosedLogs()` only if there are more than one column families. Update several unit tests (in `fault_injection_test` and `db_flush_test`) correspondingly. See for more info. Pull Request resolved: Differential Revision: D12896377 Pulled By: riversand963 fbshipit-source-id: f49afdaec32568f12f001219a3aec1dfde3b32bf/
,,0.1745,rocksdb,"Use correct FileMeta for atomic flush result install (#4932) Summary: 1. this commit fixes our handling of a combination of two separate edge cases. If a flush job does not pick any memtable to flush (because another flush job has already picked the same memtables), and the column family assigned to the flush job is dropped right before RocksDB calls rocksdb::InstallMemtableAtomicFlushResults, our original code passes a FileMetaData object whose file number is 0, failing the assertion in rocksdb::InstallMemtableAtomicFlushResults (assert(m->GetFileNumber() > 0)). 2. Also piggyback a small change: since we already create a local copy of column familys mutable CF options to eliminate potential race condition with `SetOptions` call, we might as well use the local copy in other function calls in the same scope. Pull Request resolved: Differential Revision: D13901322 Pulled By: riversand963 fbshipit-source-id: b936580af7c127ea0c6c19ea10cd5fcede9fb0f9/"
,,0.1738,rocksdb,Improve Error Message When wal_dir doesnt exist (#4874) Summary: Right now the error mesage when options.wal_dir doesnt exist is not helpful to users. Be more specific Pull Request resolved: Differential Revision: D13642425 Pulled By: siying fbshipit-source-id: 9a3172ed0f799af233b0f3b2e5e35bc7ce04c7b5/Preload some files even if options.max_open_files (#3340) Summary: Choose to preload some files if options.max_open_files This can slightly narrow the gap of performance between options.max_open_files is and a large number. To avoid a significant regression to DB reopen speed if options.max_open_files Limit the files to preload in DB open time to 16. Pull Request resolved: Differential Revision: D6686945 Pulled By: siying fbshipit-source-id: 8ec11bbdb46e3d0cdee7b6ad5897a09c5a07869f/
,,0.115,rocksdb,Fix ticker stat for number files closed (#4703) Summary: We havent been populating `NO_FILE_CLOSES` since v1.5.8 even though it was never marked as deprecated. Start populating it again. Conveniently `DeleteTableReader` has an unused `void*` argument that we can use... Blame: 63f216ee0a2a6e28f9dfe24913d134d3a7fa3aca Closes Pull Request resolved: Differential Revision: D13146769 Pulled By: ajkr fbshipit-source-id: ad8d6fb0493e701f60a165a3bca1787d255be008/
,,0.1299,rocksdb,"Fix test name typo in PlainTableDBTest Summary: Pull Request resolved: Differential Revision: D13830196 Pulled By: siying fbshipit-source-id: e06bf2a6cd273b5eb18dfd82bdd35ffce197d021/PlainTable should avoid copying Get() results from immortal source. (#4924) Summary: avoids memcopy for Get() results if files are immortable (read-only DB, max_open_files=-1) and the file is ammaped. The same optimization is being applied to PlainTable here. Pull Request resolved: Differential Revision: D13827749 Pulled By: siying fbshipit-source-id: 1f2cbfc530b40ce08ccd53f95f6e78de4d1c2f96/"
,,0.1196,rocksdb,"Change is_range_del_table_empty_ flag to atomic (#4801) Summary: To avoid a race on the flag, make it an atomic_bool. This doesnt seem to significantly affect benchmarks. Pull Request resolved: Differential Revision: D13523845 Pulled By: abhimadan fbshipit-source-id: 3bc29f53c50a4e06cd9f8c6232a4bb221868e055/Fix range tombstone covering short-circuit logic (#4698) Summary: Since a range tombstone seen at one level will cover all keys in the range at lower levels, there was a short-circuiting check in Get that reported a key was not found at most one file after the range tombstone was discovered. However, this was incorrect for merge operands, since a deletion might only cover some merge operands, which implies that the key should be found. This PR fixes this logic in the Version portion of Get, and removes the logic from the MemTable portion of Get, since the perforamnce benefit provided there is minimal. Pull Request resolved: Differential Revision: D13142484 Pulled By: abhimadan fbshipit-source-id: cbd74537c806032f2bfa564724d01a80df7c8f10/WriteBufferManger doenst cost to cache if no limit is set (#4695) Summary: WriteBufferManger is not invoked when allocating memory for memtable if the limit is not set even if a cache is passed. It is inconsistent from the comment syas. Fix it. Pull Request resolved: Differential Revision: D13112722 Pulled By: siying fbshipit-source-id: 0b27eef63867f679cd06033ea56907c0569597f4/"
,,0.2076,rocksdb,"Remove redundant member var and set options (#4631) Summary: In the past, both `DBImpl::atomic_flush_` and `DBImpl::immutable_db_options_.atomic_flush` exist. However, we fail to set `immutable_db_options_.atomic_flush`, but use `DBImpl::atomic_flush_` which is set correctly. This does not lead to incorrect behavior, but is a duplicate of information. Since `immutable_db_options_` is always there and has `atomic_flush`, we should use it as source of truth and remove `DBImpl::atomic_flush_`. Pull Request resolved: Differential Revision: D12928371 Pulled By: riversand963 fbshipit-source-id: f85a811959d3828aad4a3a1b05f71facf19c636d/"
,,0.0954,rocksdb,Move some RocksObject into try-with-resources in Test (#5037) Summary: Fix Pull Request resolved: Differential Revision: D14302474 Pulled By: riversand963 fbshipit-source-id: dcd9dda5d4d6d459315692f355499a39e546d518/
,,0.1279,rocksdb,Move some RocksObject into try-with-resources in Test (#5037) Summary: Fix Pull Request resolved: Differential Revision: D14302474 Pulled By: riversand963 fbshipit-source-id: dcd9dda5d4d6d459315692f355499a39e546d518/Disable getApproximateSizes test (#5035) Summary: Disabling `org.rocksdb.RocksDBTest.getApproximateSizes` test as it is frequently crashing on travis (#5020). It will be re-enabled once the root-cause is found and fixed. Pull Request resolved: Differential Revision: D14294736 Pulled By: sagar0 fbshipit-source-id: e28bff0d143a58ad6c82991fec3d4cf8c0209995/
,,0.0773,rocksdb,"Header logger should call LogHeader() (#4980) Summary: The info log header feature never worked well, because log level Header was not translated to Logger::LogHeader() call. Fix it. Pull Request resolved: Differential Revision: D14087283 Pulled By: siying fbshipit-source-id: 7e7d03ce35fa8d13d4ee549f46f7326f7bc0006d/"
,,0.1216,rocksdb,utilities: Fix build failure with (#5074) Summary: Initialize magic_number to zero to avoid such failure. utilities/blob_db/blob_log_format.cc:91:3: error: magic_number may be used uninitialized in this function [-Werror=maybe-uninitialized] if (magic_number kMagicNumber) { ^~ Signed-off-by: He Zhe Pull Request resolved: Differential Revision: D14505514 Pulled By: miasantreble fbshipit-source-id: 4334462958c2b9c5a7c68c6ab24dadf94ad70902/
,,0.0857,rocksdb,Fix some variable naming in db/transaction_log_impl.* (#5112) Summary: We follow Google C++ Style which indicates variable names should be all underscore: Fix some variable names under db/transaction_log_impl.* Pull Request resolved: Differential Revision: D14631157 Pulled By: siying fbshipit-source-id: 9525c9b0976b843bca377b03897700d87cc60af8/
,,0.0819,rocksdb,Fix some variable naming in db/transaction_log_impl.* (#5112) Summary: We follow Google C++ Style which indicates variable names should be all underscore: Fix some variable names under db/transaction_log_impl.* Pull Request resolved: Differential Revision: D14631157 Pulled By: siying fbshipit-source-id: 9525c9b0976b843bca377b03897700d87cc60af8/
,,0.1269,rocksdb,Improve obsolete_files_test (#5125) Summary: We see a failure of obsolete_files_test but arent able to identify the issue. Improve the test in following way and hope we can debug better next time: 1. Place sync point before automatic compaction runs so race condition will always trigger. 2. Disable sync point before test finishes. 3. ASSERT_OK() instead of ASSERT_TRUE(status.ok()) Pull Request resolved: Differential Revision: D14669456 Pulled By: siying fbshipit-source-id: dccb7648e334501ad651eb212880096eef1f4ab2/
,,0.1592,rocksdb,"LRU Cache to enable mid-point insertion by default (#5508) Summary: Mid-point insertion is a useful feature and is mature now. Make it default. Also changed cache_index_and_filter_blocks_with_high_priority=true as default accordingly, so that we wont evict index and filter blocks easier after the change, to avoid too many surprises to users. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D16021179 fbshipit-source-id: ce8456e8d43b3bfb48df6c304b5290a9d19817eb/simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/Call ValidateOptions from SetOptions (#5368) Summary: Currently we validate options in DB::Open. However the validation step is missing when options are dynamically updated in ::SetOptions. The patch fixes that. Pull Request resolved: Differential Revision: D15540101 Pulled By: maysamyabandeh fbshipit-source-id: d27bbffd8f0252d1b50bcf59e0a70a278ed937f4/"
,,0.185,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/Call ValidateOptions from SetOptions (#5368) Summary: Currently we validate options in DB::Open. However the validation step is missing when options are dynamically updated in ::SetOptions. The patch fixes that. Pull Request resolved: Differential Revision: D15540101 Pulled By: maysamyabandeh fbshipit-source-id: d27bbffd8f0252d1b50bcf59e0a70a278ed937f4/"
,,0.1642,rocksdb,Call ValidateOptions from SetOptions (#5368) Summary: Currently we validate options in DB::Open. However the validation step is missing when options are dynamically updated in ::SetOptions. The patch fixes that. Pull Request resolved: Differential Revision: D15540101 Pulled By: maysamyabandeh fbshipit-source-id: d27bbffd8f0252d1b50bcf59e0a70a278ed937f4/
,,0.1273,rocksdb,"exclude TEST_ENV_URI from rocksdb lite (#5686) Summary: PR added some test coverage for `TEST_ENV_URI`, which unfortunately isnt supported in lite mode, causing some test failures for rocksdb lite. For example, ``` db/db_test_util.cc: In constructor Ã«rocksdb::DBTestBase::DBTestBase(std::__cxx11::string)Ã­: db/db_test_util.cc:57:16: error: Ã«ObjectRegistryÃ­ has not been declared Status s ObjectRegistry::NewInstance()->NewSharedObject(test_env_uri, ^ ``` This PR fixes these errors by excluding the new code from test functions for lite mode. Pull Request resolved: Differential Revision: D16749000 Pulled By: miasantreble fbshipit-source-id: e8b3088c31a78b3dffc5fe7814261909d2c3e369/"
,,0.1728,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/Call ValidateOptions from SetOptions (#5368) Summary: Currently we validate options in DB::Open. However the validation step is missing when options are dynamically updated in ::SetOptions. The patch fixes that. Pull Request resolved: Differential Revision: D15540101 Pulled By: maysamyabandeh fbshipit-source-id: d27bbffd8f0252d1b50bcf59e0a70a278ed937f4/"
,,0.1329,rocksdb,"exclude TEST_ENV_URI from rocksdb lite (#5686) Summary: PR added some test coverage for `TEST_ENV_URI`, which unfortunately isnt supported in lite mode, causing some test failures for rocksdb lite. For example, ``` db/db_test_util.cc: In constructor Ã«rocksdb::DBTestBase::DBTestBase(std::__cxx11::string)Ã­: db/db_test_util.cc:57:16: error: Ã«ObjectRegistryÃ­ has not been declared Status s ObjectRegistry::NewInstance()->NewSharedObject(test_env_uri, ^ ``` This PR fixes these errors by excluding the new code from test functions for lite mode. Pull Request resolved: Differential Revision: D16749000 Pulled By: miasantreble fbshipit-source-id: e8b3088c31a78b3dffc5fe7814261909d2c3e369/"
,,0.10400000000000001,rocksdb,Restrict L0->L0 compaction according to max_compaction_bytes option (#5329) Summary: Modified FindIntraL0Compaction to stop picking more files if total amount of compensated bytes would be larger than max_compaction_bytes option. Pull Request resolved: Differential Revision: D15435728 Pulled By: ThomasFersch fbshipit-source-id: d118a6da88d5df8ee20944422ade37cf6b15d60c/
,,0.1118,rocksdb,"Fix WAL replay by skipping old write batches (#5170) Summary: 1. Fix a bug in WAL replay in which write batches with old sequence numbers are mistakenly inserted into memtables. 2. Add support for benchmarking secondary instance to db_bench_tool. With changes made in this PR, we can start benchmarking secondary instance using two processes. It is also possible to vary the frequency at which the secondary instance tries to catch up with the primary. The info log of the secondary can be found in a directory whose path can be specified with Pull Request resolved: Differential Revision: D15564608 Pulled By: riversand963 fbshipit-source-id: ce97688ed3d33f69d3a0b9266ebbbbf887aa0ec8/"
,,0.0765,rocksdb,Fix env_options_for_read spelling in CompactionJob Summary: Pull Request resolved: Differential Revision: D15563386 Pulled By: sagar0 fbshipit-source-id: 8b26aef47cfc40ff8016daf815582f21cdd40df2/
,,0.1171,rocksdb,"Fix WAL replay by skipping old write batches (#5170) Summary: 1. Fix a bug in WAL replay in which write batches with old sequence numbers are mistakenly inserted into memtables. 2. Add support for benchmarking secondary instance to db_bench_tool. With changes made in this PR, we can start benchmarking secondary instance using two processes. It is also possible to vary the frequency at which the secondary instance tries to catch up with the primary. The info log of the secondary can be found in a directory whose path can be specified with Pull Request resolved: Differential Revision: D15564608 Pulled By: riversand963 fbshipit-source-id: ce97688ed3d33f69d3a0b9266ebbbbf887aa0ec8/"
,,0.2467,rocksdb,"fix rocksdb lite and clang contrun test failures (#5477) Summary: recent commit 671d15cbdd3839acb54cb21a2aa82efca4917155 introduced some test failures: ``` Running stats_history_test [==========] Running 9 tests from 1 test case. [----------] Global test environment set-up. [----------] 9 tests from StatsHistoryTest [ RUN ] StatsHistoryTest.RunStatsDumpPeriodSec monitoring/stats_history_test.cc:63: Failure dbfull()->SetDBOptions({{""stats_dump_period_sec"", ""0""}}) Not implemented: Not supported in ROCKSDB LITE db/db_options_test.cc:28:11: error: unused variable kMicrosInSec [-Werror,-Wunused-const-variable] const int kMicrosInSec 1000000; ``` This PR fixes these failures Pull Request resolved: Differential Revision: D15871814 Pulled By: miasantreble fbshipit-source-id: 0a7023914d2c1784d9d2d3f5bfb47310d4855394/Call ValidateOptions from SetOptions (#5368) Summary: Currently we validate options in DB::Open. However the validation step is missing when options are dynamically updated in ::SetOptions. The patch fixes that. Pull Request resolved: Differential Revision: D15540101 Pulled By: maysamyabandeh fbshipit-source-id: d27bbffd8f0252d1b50bcf59e0a70a278ed937f4/Fix FIFO dynamic options sanitization (#5367) Summary: When dynamically setting options, we check the option type info and skip options that are marked deprecated. However this check is only done at top level, which results in bugs where SetOptions will corrupt option values and cause unexpected system behavior iff a deprecated second level option is set dynamically. For exmaple, the following call: ``` dbfull()->SetOptions( {{""compaction_options_fifo"", ""{allow_compaction=true;max_table_files_size=1024;ttl=731;}""}}); ``` was from pre 6.0 release when `ttl` was part of `compaction_options_fifo`. Now that it got moved out of `compaction_options_fifo`, this call will incorrectly set `compaction_options_fifo.max_table_files_size` to 731 (as `max_table_files_size` is the first one in `OptionsHelper::fifo_compaction_options_type_info` struct) and cause files to gett evicted much faster than expected. This PR adds verification to second level options like `compaction_options_fifo.ttl` or `compaction_options_fifo.max_table_files_size` when set dynamically, and filter out those marked as deprecated. Pull Request resolved: Differential Revision: D15530998 Pulled By: miasantreble fbshipit-source-id: 818258be5c3abe09cd82d62f3c083572d70fecdd/"
,,0.1056,rocksdb,Restrict L0->L0 compaction according to max_compaction_bytes option (#5329) Summary: Modified FindIntraL0Compaction to stop picking more files if total amount of compensated bytes would be larger than max_compaction_bytes option. Pull Request resolved: Differential Revision: D15435728 Pulled By: ThomasFersch fbshipit-source-id: d118a6da88d5df8ee20944422ade37cf6b15d60c/
,,0.10400000000000001,rocksdb,Restrict L0->L0 compaction according to max_compaction_bytes option (#5329) Summary: Modified FindIntraL0Compaction to stop picking more files if total amount of compensated bytes would be larger than max_compaction_bytes option. Pull Request resolved: Differential Revision: D15435728 Pulled By: ThomasFersch fbshipit-source-id: d118a6da88d5df8ee20944422ade37cf6b15d60c/
,,0.1067,rocksdb,"Fix a flaky test with test sync point (#5310) Summary: If DB is opened with `avoid_unnecessary_blocking_io` being true, then `~ColumnFamilyHandleImpl` enqueues a purge request and schedules a background thread to perform the deletion. Without test sync point, whether the SST file is purged or not at a later point in time is not deterministic. If the SST does not exist, it will cause an assertion failure. How to reproduce: ``` $git checkout 6492430eaf1a13730eec81321528558cbf486c96 $make deletefile_test $gtest-parallel 1000 16 ./deletefile_test ``` The test may fail a few times. With changes made in this PR, repeat the above commands, and the test should not fail. Pull Request resolved: Differential Revision: D15361136 Pulled By: riversand963 fbshipit-source-id: c4308d5f8da83472c893bf7f8ceed347fbfa850f/"
,,0.0681,rocksdb,Fix env_options_for_read spelling in CompactionJob Summary: Pull Request resolved: Differential Revision: D15563386 Pulled By: sagar0 fbshipit-source-id: 8b26aef47cfc40ff8016daf815582f21cdd40df2/
,,0.5595,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1958,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/"
,,0.5879,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5879,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5806,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5682,rocksdb,"Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5824,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1904,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/"
,,0.5852,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.4505,rocksdb,"Workload generator (Mixgraph) based on prefix hotness (#5953) Summary: In the previous PR user can use db_bench mix_graph option to generate the workload that is from the social graph. The key is generated based on the key access hotness. In this PR, user can further model the key-range hotness and fit those to two-term-exponential distribution. First, user cuts the whole key space into small key ranges (e.g., key-ranges are the same size and the key-range number is the number of SST files). Then, user calculates the average access count per key of each key-range as the key-range hotness. Next, user fits the key-range hotness to two-term-exponential distribution (f(x) f(x) a*exp(b*x) + c*exp(d*x)) and generate the value of a, b, c, and d. They are the parameters in db_bench: prefix_dist_a, prefix_dist_b, prefix_dist_c, and prefix_dist_d. Finally, user can run db_bench by specify the parameters. For example: `./db_bench Pull Request resolved: Test Plan: run db_bench with different parameters and checked the results. Differential Revision: D18053527 Pulled By: zhichao-cao fbshipit-source-id: 171f8b3142bd76462f1967c58345ad7e4f84bab7/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.2976,rocksdb,"Make the db_stress reopen loop in OperateDb() more robust (#5893) Summary: The loop in OperateDb() is getting quite complicated with the introduction of multiple key operations such as MultiGet and Reseeks. This is resulting in a number of corner cases that hangs db_stress due to synchronization problems during reopen (i.e when option is specified). This PR makes it more robust by ensuring all db_stress threads vote to reopen the DB the exact same number of times. Most of the changes in this diff are due to indentation. Pull Request resolved: Test Plan: Run crash test Differential Revision: D17823827 Pulled By: anand1976 fbshipit-source-id: ec893829f611ac7cac4057c0d3d99f9ffb6a6dd9/Enable partitioned index/filter in stress tests (#5895) Summary: This is the 2nd attempt after the revert of Pull Request resolved: Test Plan: ``` ./tools/db_crashtest.py blackbox ``` Differential Revision: D17822137 Pulled By: maysamyabandeh fbshipit-source-id: 3d148c0d8cc129080410ff859c04b544223c8ea3/Fix reopen voting logic in db_stress to prevent hangs (#5876) Summary: When multiple operations are performed in a db_stress thread in one loop iteration, the reopen voting logic needs to take that into account. It was doing that for MultiGet, but a new option was introduced recently to do multiple iterator seeks per iteration, which broke it again. Fix the logic to be more robust and agnostic of the type of operation performed. Pull Request resolved: Test Plan: Run db_stress Differential Revision: D17733590 Pulled By: anand1976 fbshipit-source-id: 787f01abefa1e83bba43e0b4f4abb26699b2089e/Fix clang analyze warning in db_stress (#5870) Summary: Recent changes trigger clang analyze warning. Fix it. Pull Request resolved: Test Plan: ""USE_CLANG=1 TEST_TMPDIR=/dev/shm/rocksdb OPT=-g make analyze"" and make sure it passes. Differential Revision: D17682533 fbshipit-source-id: 02716f2a24572550a22db4bbe9b54d4872dfae32/Fix three more db_stress bugs (#5867) Summary: Two more bug fixes in db_stress: 1. this is to complete the fix of the regression bug causing overflowing when supporting FLAGS_prefix_size 2. Fix regression bug in compare iterator itself: (1) when creating control iterator, which used the same read option as the normal iterator by mistake; (2) the logic of comparing has some problems. Fix them. (3) disable validation for lower bound now, which generated some wildly different results. Disabling it to make normal tests pass while investigating it. 3. Cleaning up snapshots in verification failure cases. Memory is leaked otherwise. Pull Request resolved: Test Plan: Run ""make crash_test"" for a while and see at least 1 is fixed. Differential Revision: D17671712 fbshipit-source-id: 011f98ea1a72aef23e19ff28656830c78699b402/db_stress: fix run time error when prefix_size (#5862) Summary: When prefix_size stress test crashes with run time error because of overflow. Fix it by not using but 7 in prefix scan mode. Pull Request resolved: Test Plan: Run python tools/db_crashtest.py whitebox \ 888887 and see it doesnt crash. Differential Revision: D17642313 fbshipit-source-id: f029e7651498c905af1b1bee6d310ae50cdcda41/crash_test to do some verification for prefix extractor and iterator bounds. (#5846) Summary: For now, crash_test is not able to report any failure for the logic related to iterator upper, lower bounds or iterators, or reseek. These are features prone to errors. Improve db_stress in several ways: (1) For each iterator run, reseek up to 3 times. (2) For every iterator, create control iterator with upper or lower bound, with total order seek. Compare the results with the iterator. (3) Make simple crash test to avoid prefix size to have more coverage. (4) make prefix_size 0 a valid size and to indicate disabling prefix extractor. Pull Request resolved: Test Plan: Manually hack the code to create wrong results and see they are caught by the tool. Differential Revision: D17631760 fbshipit-source-id: acd460a177bd2124a5ffd7fff490702dba63030b/Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/Support partitioned index and filters in stress/crash tests (#4020) Summary: In `db_stress`, support choosing index type and whether to enable filter partitioning, and randomly set those options in crash test When partitioned filter is enabled by crash test, force partitioned index to also be enabled since its a prerequisite Pull Request resolved: Test Plan: currently this is blocked on fixing the bug that crash test caught: ``` $ TEST_TMPDIR=/data/compaction_bench python ./tools/db_crashtest.py blackbox ... Verification failed for column family 0 key 937501: Value not found: NotFound: Crash-recovery verification failed :( ``` Differential Revision: D8508683 Pulled By: maysamyabandeh fbshipit-source-id: 0337e5d0558bcef26b1f3699f47265a2c1e99629/Extend stress test to cover periodic compaction and compaction TTL (#5741) Summary: Covering periodic compaction and compaction TTL can help us expose potential issues. Add it there. Randomly select value for these two options. Pull Request resolved: Test Plan: Run crash_test and see the perameters generated. Differential Revision: D17059515 fbshipit-source-id: 8213974846a0b6a22fc13be705825c9054d1d097/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/Atomic Flush Crash Test also covers the case that WAL is enabled. (#5729) Summary: AtomicFlushStressTest is a powerful test, but right now we only run it for atomic_flush=true + disable_wal=true. We further extend it to the case where atomic_flush=false + disable_wal false. All the workload generation and validation can stay the same. Atomic flush crash test is also changed to switch between the two test scenarios. It makes the name ""atomic flush crash test"" out of sync from what it really does. We leave it as it is to avoid troubles with continous test set-up. Pull Request resolved: Test Plan: Run ""CRASH_TEST_KILL_ODD=188 TEST_TMPDIR=/dev/shm/ USE_CLANG=1 make whitebox_crash_test_with_atomic_flush"", observe the settings used and see it passed. Differential Revision: D16969791 fbshipit-source-id: 56e37487000ae631e31b0100acd7bdc441c04163/"
,,0.1993,rocksdb,"Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/"
,,0.5852,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1406,rocksdb,Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/
,,0.1439,rocksdb,Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/
,,0.5815,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1985,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/"
,,0.1958,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/"
,,0.1287,rocksdb,"Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/"
,,0.2007,rocksdb,"Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/"
,,0.1951,rocksdb,"Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/"
,,0.5934,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.2053,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/"
,,0.5842,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.2035,rocksdb,"Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/"
,,0.5861,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5861,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5842,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5742,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5464,rocksdb,"use c++17s try_emplace if available (#5696) Summary: This avoids rehashing the key in TrackKey() in case the key is not already in the map of tracked keys, which will happen at least once per key used in a transaction. Additionally fix two typos. Pull Request resolved: Differential Revision: D17210178 Pulled By: lth fbshipit-source-id: 7e2c28e9e505c1d1c1535d435250cf2b191a6fdf/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5079,rocksdb,"Add a unit test to detect infinite loops with reseek optimizations (#5727) Summary: Iterators reseek to the target key after iterating over max_sequential_skip_in_iterations invalid values. The logic is susceptible to an infinite loop bug, which has been present with WritePrepared Transactions up until 6.2 release. Although the bug is not present on master, the patch adds a unit test to prevent it from resurfacing again. Pull Request resolved: Differential Revision: D16952759 Pulled By: maysamyabandeh fbshipit-source-id: d0d973dddc8dfabd5a794931232aa4c862c74f51/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5897,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1322,rocksdb,Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/
,,0.1824,rocksdb,"Fix WriteBatchWithIndex with MergeOperator bug (#5577) Summary: ``` TEST_F(WriteBatchWithIndexTest, TestGetFromBatchAndDBMerge3) { DB* db; Options options; options.create_if_missing true; std::string dbname test::PerThreadDBPath(""write_batch_with_index_test""); options.merge_operator MergeOperators::CreateFromStringId(""stringappend""); DestroyDB(dbname, options); Status s DB::Open(options, dbname, &db); assert(s.ok()); ReadOptions read_options; WriteOptions write_options; FlushOptions flush_options; std::string value; WriteBatchWithIndex batch; ASSERT_OK(db->Put(write_options, ""A"", ""1"")); ASSERT_OK(db->Flush(flush_options, db->DefaultColumnFamily())); ASSERT_OK(batch.Merge(""A"", ""2"")); ASSERT_OK(batch.GetFromBatchAndDB(db, read_options, ""A"", &value)); ASSERT_EQ(value, ""1,2""); delete db; DestroyDB(dbname, options); } ``` Fix ASSERT in batch.GetFromBatchAndDB() Pull Request resolved: Differential Revision: D16379847 fbshipit-source-id: b1320e24ec8e71350c525083cc0a16180a63f752/"
,,0.1862,rocksdb,"Fix WriteBatchWithIndex with MergeOperator bug (#5577) Summary: ``` TEST_F(WriteBatchWithIndexTest, TestGetFromBatchAndDBMerge3) { DB* db; Options options; options.create_if_missing true; std::string dbname test::PerThreadDBPath(""write_batch_with_index_test""); options.merge_operator MergeOperators::CreateFromStringId(""stringappend""); DestroyDB(dbname, options); Status s DB::Open(options, dbname, &db); assert(s.ok()); ReadOptions read_options; WriteOptions write_options; FlushOptions flush_options; std::string value; WriteBatchWithIndex batch; ASSERT_OK(db->Put(write_options, ""A"", ""1"")); ASSERT_OK(db->Flush(flush_options, db->DefaultColumnFamily())); ASSERT_OK(batch.Merge(""A"", ""2"")); ASSERT_OK(batch.GetFromBatchAndDB(db, read_options, ""A"", &value)); ASSERT_EQ(value, ""1,2""); delete db; DestroyDB(dbname, options); } ``` Fix ASSERT in batch.GetFromBatchAndDB() Pull Request resolved: Differential Revision: D16379847 fbshipit-source-id: b1320e24ec8e71350c525083cc0a16180a63f752/"
,,0.0838,rocksdb,Fix comment of function NotifyCollectTableCollectorsOnFinish (#5738) Summary: Signed-off-by: Shafreeck Sea Pull Request resolved: Differential Revision: D17097075 Pulled By: riversand963 fbshipit-source-id: ed01b5f59e8eed262a49abe1f96552842d364af1/
,,0.5870000000000001,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.0961,rocksdb,"Bump up memory order of ref counting of ColumnFamilyData (#5723) Summary: We see this TSAN warning: WARNING: ThreadSanitizer: data race (pid=282806) Write of size 8 at 0x7b6c00000e38 by thread T16 (mutexes: write M1023578822185846136): operator delete(void*) (libtsan.so.0+0x0000000795f8) rocksdb::DBImpl::BackgroundFlush(bool*, rocksdb::JobContext*, rocksdb::LogBuffer*, rocksdb::FlushReason*, rocksdb::Env::Priority) db/db_impl/db_impl_compaction_flush.cc:2202 (db_flush_test+0x00000060b462) rocksdb::DBImpl::BackgroundCallFlush(rocksdb::Env::Priority) db/db_impl/db_impl_compaction_flush.cc:2226 (db_flush_test+0x00000060cbd8) rocksdb::DBImpl::BGWorkFlush(void*) db/db_impl/db_impl_compaction_flush.cc:2073 (db_flush_test+0x00000060d5ac) ...... Previous atomic write of size 4 at 0x7b6c00000e38 by main thread: __tsan_atomic32_fetch_sub (libtsan.so.0+0x00000006d721) std::__atomic_base<int>::fetch_sub(int, std::memory_order) /mnt/gvfs/third-party2/libgcc/c67031f0f739ac61575a061518d6ef5038f99f90/7.x/platform007/5620abc/include/c++/7.3.0/bits/atomic_base.h:524 (db_flush_test+0x0000005f9e38) rocksdb::ColumnFamilyData::Unref() db/column_family.h:286 (db_flush_test+0x0000005f9e38) rocksdb::DBImpl::FlushMemTable(rocksdb::ColumnFamilyData*, rocksdb::FlushOptions const&, rocksdb::FlushReason, bool) db/db_impl/db_impl_compaction_flush.cc:1624 (db_flush_test+0x0000005f9e38) rocksdb::DBImpl::TEST_FlushMemTable(rocksdb::ColumnFamilyData*, rocksdb::FlushOptions const&) db/db_impl/db_impl_debug.cc:127 (db_flush_test+0x00000061ace9) rocksdb::DBFlushTest_CFDropRaceWithWaitForFlushMemTables_Test::TestBody() db/db_flush_test.cc:320 (db_flush_test+0x0000004b44e5) void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) third-party/gtest-1.7.0/fused-src/gtest/gtest-all.cc:3824 (db_flush_test+0x000000be2988) ...... Its still very clear the cause of the warning is because that TSAN treats results from relaxed atomic::fetch_sub() as non-atomic with the operation itself. We can make it more explicit by bumping up the order to CS. Pull Request resolved: Test Plan: Run all existing test. Differential Revision: D16908250 fbshipit-source-id: bf17d39ed19058372bdf97f6440a743f88153021/"
,,0.1188,rocksdb,"Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/"
,,0.6049,rocksdb,"Lower the risk for users to run options.force_consistency_checks true (#5744) Summary: Open-source users recently reported two occurrences of LSM-tree corruption ( is one), which would be caught by options.force_consistency_checks true. options.force_consistency_checks has a usability limitation because it crashes the service once inconsistency is detected. This makes the feature hard to use. Most users serve from multiple RocksDB shards per server and the impacts of crashing the service is higher than it should be. Instead, we just pass the error back to users without killing the service, and ask them to deal with the problem accordingly. Pull Request resolved: Differential Revision: D17096940 Pulled By: pdhandharia fbshipit-source-id: b6780039044e265f26ed2ad03c51f4abbe8b603c/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5824,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.3823,rocksdb,"Fix MultiGet crash when no_block_cache is set (#5991) Summary: This PR fixes In ```BlockBasedTable::RetrieveMultipleBlocks()```, we were calling ```MaybeReadBlocksAndLoadToCache()```, which is a no-op if neither uncompressed nor compressed block cache are configured. Pull Request resolved: Test Plan: 1. Add unit tests that fail with the old code and pass with the new 2. make check and asan_check Cc spetrunia Differential Revision: D18272744 Pulled By: anand1976 fbshipit-source-id: e62fa6090d1a6adf84fcd51dfd6859b03c6aebfe/Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/Fix MultiGet() bug when whole_key_filtering is disabled (#5665) Summary: The batched MultiGet() implementation was not correctly handling bloom filter lookups when whole_key_filtering is disabled. It was incorrectly skipping keys not in the prefix_extractor domain, and not calling transform for keys in domain. This PR fixes both problems by moving the domain check and transformation to the FilterBlockReader. Tests: Unit test (confirmed failed before the fix) make check Pull Request resolved: Differential Revision: D16902380 Pulled By: anand1976 fbshipit-source-id: a6be81ad68a6e37134a65246aec7a2c590eccf00/"
,,0.5797,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.2128,rocksdb,"Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/"
,,0.5213,rocksdb,"Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5861,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1274,rocksdb,"Fix DBFlushTest::FireOnFlushCompletedAfterCommittedResult hang (#6018) Summary: The test would fire two flushes to let them run in parallel. Previously it wait for the first job to be scheduled before firing the second. It is possible the job is not started before the second job being scheduled, making the two job combine into one. Change to wait for the first job being started. Fixes Pull Request resolved: Test Plan: ``` while ./db_flush_test do :; done ``` and let it run for a while. Signed-off-by: Yi Wu Differential Revision: D18405576 Pulled By: riversand963 fbshipit-source-id: 6ebb6262e033d5dc2ef81cb3eb410b314f2de4c9/Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/"
,,0.1691,rocksdb,Refactor deletefile_test.cc (#5822) Summary: Make DeleteFileTest inherit DBTestBase to avoid code duplication. Test Plan (on devserver) ``` $make deletefile_test $./deletefile_test ``` Pull Request resolved: Differential Revision: D17456750 Pulled By: riversand963 fbshipit-source-id: 224e97967da7b98838a98981cd5095d3230a814f/
,,0.5897,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5272,rocksdb,"Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1132,rocksdb,"Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/"
,,0.3777,rocksdb,"Turn compaction asserts to runtime check (#5935) Summary: Compaction iterator has many assert statements that are active only during test runs. Some rare bugs would show up only at runtime could violate the assert condition but go unnoticed since assert statements are not compiled in release mode. Turning the assert statements to runtime check sone pors and cons: Pros: A bug that would result into incorrect data would be detected early before the incorrect data is written to the disk. Cons: Runtime overhead: which should be negligible since compaction cpu is the minority in the overall cpu usage The assert statements might already being violated at runtime, and turning them to runtime failure might result into reliability issues. The patch takes a conservative step in this direction by logging the assert violations at runtime. If we see any violation reported in logs, we investigate. Otherwise, we can go ahead turning them to runtime error. Pull Request resolved: Differential Revision: D18229697 Pulled By: maysamyabandeh fbshipit-source-id: f1890eca80ccd7cca29737f1825badb9aa8038a8/Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/Disable snapshot refresh feature when snap_refresh_nanos is 0 (#5724) Summary: The comments of snap_refresh_nanos advertise that the snapshot refresh feature will be disabled when the option is set to 0. This contract is however not honored in the code: The patch fixes that and also adds an assert to ensure that the feature is not used when the option is zero. Pull Request resolved: Differential Revision: D16918185 Pulled By: maysamyabandeh fbshipit-source-id: fec167287df7d85093e087fc39c0eb243e3bbd7e/"
,,0.2564,rocksdb,"Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/"
,,0.369,rocksdb,"Turn compaction asserts to runtime check (#5935) Summary: Compaction iterator has many assert statements that are active only during test runs. Some rare bugs would show up only at runtime could violate the assert condition but go unnoticed since assert statements are not compiled in release mode. Turning the assert statements to runtime check sone pors and cons: Pros: A bug that would result into incorrect data would be detected early before the incorrect data is written to the disk. Cons: Runtime overhead: which should be negligible since compaction cpu is the minority in the overall cpu usage The assert statements might already being violated at runtime, and turning them to runtime failure might result into reliability issues. The patch takes a conservative step in this direction by logging the assert violations at runtime. If we see any violation reported in logs, we investigate. Otherwise, we can go ahead turning them to runtime error. Pull Request resolved: Differential Revision: D18229697 Pulled By: maysamyabandeh fbshipit-source-id: f1890eca80ccd7cca29737f1825badb9aa8038a8/Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/"
,,0.2553,rocksdb,"Fix clang analyzer error (#5924) Summary: Without this PR, clang analyzer complains. ``` $USE_CLANG=1 make analyze db/compaction/compaction_job_test.cc:161:20: warning: The left operand of is a garbage value if (key.type kTypeBlobIndex) { ~~~~~~~~ ^ 1 warning generated. ``` Test Plan (on devserver) ``` $USE_CLANG=1 make analyze ``` Pull Request resolved: Differential Revision: D17923226 Pulled By: riversand963 fbshipit-source-id: 9d1eb769b5e0de7cb3d89dc90d1cfa895db7fdc8/Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/Disable snapshot refresh feature when snap_refresh_nanos is 0 (#5724) Summary: The comments of snap_refresh_nanos advertise that the snapshot refresh feature will be disabled when the option is set to 0. This contract is however not honored in the code: The patch fixes that and also adds an assert to ensure that the feature is not used when the option is zero. Pull Request resolved: Differential Revision: D16918185 Pulled By: maysamyabandeh fbshipit-source-id: fec167287df7d85093e087fc39c0eb243e3bbd7e/"
,,0.3605,rocksdb,"Turn compaction asserts to runtime check (#5935) Summary: Compaction iterator has many assert statements that are active only during test runs. Some rare bugs would show up only at runtime could violate the assert condition but go unnoticed since assert statements are not compiled in release mode. Turning the assert statements to runtime check sone pors and cons: Pros: A bug that would result into incorrect data would be detected early before the incorrect data is written to the disk. Cons: Runtime overhead: which should be negligible since compaction cpu is the minority in the overall cpu usage The assert statements might already being violated at runtime, and turning them to runtime failure might result into reliability issues. The patch takes a conservative step in this direction by logging the assert violations at runtime. If we see any violation reported in logs, we investigate. Otherwise, we can go ahead turning them to runtime error. Pull Request resolved: Differential Revision: D18229697 Pulled By: maysamyabandeh fbshipit-source-id: f1890eca80ccd7cca29737f1825badb9aa8038a8/Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/"
,,0.1985,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/"
,,0.2829,rocksdb,"Lower the risk for users to run options.force_consistency_checks true (#5744) Summary: Open-source users recently reported two occurrences of LSM-tree corruption ( is one), which would be caught by options.force_consistency_checks true. options.force_consistency_checks has a usability limitation because it crashes the service once inconsistency is detected. This makes the feature hard to use. Most users serve from multiple RocksDB shards per server and the impacts of crashing the service is higher than it should be. Instead, we just pass the error back to users without killing the service, and ask them to deal with the problem accordingly. Pull Request resolved: Differential Revision: D17096940 Pulled By: pdhandharia fbshipit-source-id: b6780039044e265f26ed2ad03c51f4abbe8b603c/"
,,0.3044,rocksdb,"Lower the risk for users to run options.force_consistency_checks true (#5744) Summary: Open-source users recently reported two occurrences of LSM-tree corruption ( is one), which would be caught by options.force_consistency_checks true. options.force_consistency_checks has a usability limitation because it crashes the service once inconsistency is detected. This makes the feature hard to use. Most users serve from multiple RocksDB shards per server and the impacts of crashing the service is higher than it should be. Instead, we just pass the error back to users without killing the service, and ask them to deal with the problem accordingly. Pull Request resolved: Differential Revision: D17096940 Pulled By: pdhandharia fbshipit-source-id: b6780039044e265f26ed2ad03c51f4abbe8b603c/"
,,0.5272,rocksdb,"Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1999,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/"
,,0.3524,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/Lower the risk for users to run options.force_consistency_checks true (#5744) Summary: Open-source users recently reported two occurrences of LSM-tree corruption ( is one), which would be caught by options.force_consistency_checks true. options.force_consistency_checks has a usability limitation because it crashes the service once inconsistency is detected. This makes the feature hard to use. Most users serve from multiple RocksDB shards per server and the impacts of crashing the service is higher than it should be. Instead, we just pass the error back to users without killing the service, and ask them to deal with the problem accordingly. Pull Request resolved: Differential Revision: D17096940 Pulled By: pdhandharia fbshipit-source-id: b6780039044e265f26ed2ad03c51f4abbe8b603c/"
,,0.4277,rocksdb,"Refactor/consolidate legacy Bloom implementation details (#5784) Summary: Refactoring to consolidate implementation details of legacy Bloom filters. This helps to organize and document some related, obscure code. Also added make/cpp var TEST_CACHE_LINE_SIZE so that its easy to compile and run unit tests for non-native cache line size. (Fixed a related test failure in db_properties_test.) Pull Request resolved: Test Plan: make check, including Recently added Bloom schema unit tests (in ./plain_table_db_test && ./bloom_test), and including with TEST_CACHE_LINE_SIZE=128U and TEST_CACHE_LINE_SIZE=256U. Tested the schema tests with temporary fault injection into new implementations. Some performance testing with modified unit tests suggest a small to moderate improvement in speed. Differential Revision: D17381384 Pulled By: pdillinger fbshipit-source-id: ee42586da996798910fc45ac0b6289147f16d8df/Revert changes from PR#5784 accidentally in PR#5780 (#5810) Summary: This will allow us to fix history by having the code changes for PR#5784 properly attributed to it. Pull Request resolved: Differential Revision: D17400231 Pulled By: pdillinger fbshipit-source-id: 2da8b1cdf2533cfedb35b5526eadefb38c291f09/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1917,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/"
,,0.4668,rocksdb,"Add file number/oldest referenced blob file number to {Sst,Live}FileMetaData (#6011) Summary: The patch exposes the file numbers of the SSTs as well as the oldest blob files they contain a reference to through the GetColumnFamilyMetaData/ GetLiveFilesMetaData interface. Pull Request resolved: Test Plan: Fixed and extended the existing unit tests. (The earlier ColumnFamilyMetaDataTest wasnt really testing anything because the generated memtables were never flushed, so the metadata structure was essentially empty.) Differential Revision: D18361697 Pulled By: ltamasi fbshipit-source-id: d5ed1d94ac70858b84393c48711441ddfe1251e9/Fix for lite build (#5971) Summary: Fix for lite build Pull Request resolved: Test Plan: make J=1 LITE=1 all check Differential Revision: D18148306 Pulled By: vjnadimpalli fbshipit-source-id: 5b9a3edc3e73e054fee6b96e6f6e583cecc898f3/Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.4837,rocksdb,"Fix crash when background task fails (#5879) Summary: Fixing crash. Full story in issue: Pull Request resolved: Differential Revision: D17812299 Pulled By: anand1976 fbshipit-source-id: 14e5a4fc502ade974583da9692d0ed6e5014613a/Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/Allow ingesting overlapping files (#5539) Summary: Currently IngestExternalFile() fails when its input files ranges overlap. This condition doesnt need to hold for files that are to be ingested in L0, though. This commit allows overlapping files and forces their target level to L0. Additionally, ingest jobs completion is logged to EventLogger, analogous to flush and compaction jobs. Pull Request resolved: Differential Revision: D17370660 Pulled By: riversand963 fbshipit-source-id: 749a3899b17d1be267a5afd5b0a99d96b38ab2f3/Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/Fix a bug in file ingestion (#5760) Summary: Before this PR, when the number of column families involved in a file ingestion exceeds 2, a bug in the looping logic prevents correct file number being assigned to each ingestion job. Also skip deleting non-existing hard links during cleanup-after-failure. Test plan (devserver) ``` $COMPILE_WITH_ASAN=1 make all $./external_sst_file_test $makke check ``` Pull Request resolved: Differential Revision: D17142982 Pulled By: riversand963 fbshipit-source-id: 06c1847a4e7a402647bcf28d124e70f2a0f9daf6/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.4814,rocksdb,"Move pipeline write waiting logic into WaitForPendingWrites() (#5716) Summary: In pipeline writing mode, memtable switching needs to wait for memtable writing to finish to make sure that when memtables are made immutable, inserts are not going to them. This is currently done in DBImpl::SwitchMemtable(). This is done after flush_scheduler_.TakeNextColumnFamily() is called to fetch the list of column families to switch. The function flush_scheduler_.TakeNextColumnFamily() itself, however, is not thread-safe when being called together with flush_scheduler_.ScheduleFlush(). This change provides a fix, which moves the waiting logic before flush_scheduler_.TakeNextColumnFamily(). WaitForPendingWrites() is a natural place where the logic can happen. Pull Request resolved: Test Plan: Run all tests with ASAN and TSAN. Differential Revision: D18217658 fbshipit-source-id: b9c5e765c9989645bf10afda7c5c726c3f82f6c3/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.2846,rocksdb,"Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/Fix crash when background task fails (#5879) Summary: Fixing crash. Full story in issue: Pull Request resolved: Differential Revision: D17812299 Pulled By: anand1976 fbshipit-source-id: 14e5a4fc502ade974583da9692d0ed6e5014613a/Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/Disable snapshot refresh feature when snap_refresh_nanos is 0 (#5724) Summary: The comments of snap_refresh_nanos advertise that the snapshot refresh feature will be disabled when the option is set to 0. This contract is however not honored in the code: The patch fixes that and also adds an assert to ensure that the feature is not used when the option is zero. Pull Request resolved: Differential Revision: D16918185 Pulled By: maysamyabandeh fbshipit-source-id: fec167287df7d85093e087fc39c0eb243e3bbd7e/"
,,0.4553,rocksdb,"Move pipeline write waiting logic into WaitForPendingWrites() (#5716) Summary: In pipeline writing mode, memtable switching needs to wait for memtable writing to finish to make sure that when memtables are made immutable, inserts are not going to them. This is currently done in DBImpl::SwitchMemtable(). This is done after flush_scheduler_.TakeNextColumnFamily() is called to fetch the list of column families to switch. The function flush_scheduler_.TakeNextColumnFamily() itself, however, is not thread-safe when being called together with flush_scheduler_.ScheduleFlush(). This change provides a fix, which moves the waiting logic before flush_scheduler_.TakeNextColumnFamily(). WaitForPendingWrites() is a natural place where the logic can happen. Pull Request resolved: Test Plan: Run all tests with ASAN and TSAN. Differential Revision: D18217658 fbshipit-source-id: b9c5e765c9989645bf10afda7c5c726c3f82f6c3/Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/Fix crash when background task fails (#5879) Summary: Fixing crash. Full story in issue: Pull Request resolved: Differential Revision: D17812299 Pulled By: anand1976 fbshipit-source-id: 14e5a4fc502ade974583da9692d0ed6e5014613a/Allow users to stop manual compactions (#3971) Summary: Manual compaction may bring in very high load because sometime the amount of data involved in a compaction could be large, which may affect online service. So it would be good if the running compaction making the server busy can be stopped immediately. In this implementation, stopping manual compaction condition is only checked in slow process. We let deletion compaction and trivial move go through. Pull Request resolved: Test Plan: add tests at more spots. Differential Revision: D17369043 fbshipit-source-id: 575a624fb992ce0bb07d9443eb209e547740043c/Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5824,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1406,rocksdb,Apply formatter on recent 45 commits. (#5827) Summary: Some recent commits might not have passed through the formatter. I formatted recent 45 commits. The script hangs for more commits so I stopped there. Pull Request resolved: Test Plan: Run all existing tests. Differential Revision: D17483727 fbshipit-source-id: af23113ee63015d8a43d89a3bc2c1056189afe8f/
,,0.3928,rocksdb,"Update column families log number altogether after flushing during recovery (#5856) Summary: A bug occasionally shows up in crash test, and reproduces it. The bug can surface in the following way. 1. Database has multiple column families. 2. Between one DB restart, the last log file is corrupted in the middle (not the tail) 3. During restart, DB crashes between flushing between two column families. Then DB will fail to be opened again with error ""SST file is ahead of WALs"". Solution is to update the log number associated with each column family altogether after flushing all column families memtables. The version edits should be written to a new MANIFEST. Only after writing to all these version edits succeed does RocksDB (atomically) points the CURRENT file to the new MANIFEST. Test plan (on devserver): ``` $make all && make check ``` Specifically ``` $make db_test2 $./db_test2 ``` Also checked for compatibility as follows. Use this branch, run DBTest2.CrashInRecoveryMultipleCF and preserve the db directory. Then checkout 5.4, build ldb, and dump the MANIFEST. Pull Request resolved: Differential Revision: D17620818 Pulled By: riversand963 fbshipit-source-id: b52ce5969c9a8052cacec2bd805fcfb373589039/Fix crash when background task fails (#5879) Summary: Fixing crash. Full story in issue: Pull Request resolved: Differential Revision: D17812299 Pulled By: anand1976 fbshipit-source-id: 14e5a4fc502ade974583da9692d0ed6e5014613a/Add a unit test to reproduce a corruption bug (#5851) Summary: This is a bug occaionally shows up in crash test, and this unit test is to reproduce it. The bug is following: 1. Database has multiple CFs. 2. Between one DB restart, the last log file is corrupted in the middle (not the tail) 3. During restart, DB crashes between flushes between two CFs. The DB will fail to be opened again with error ""SST file is ahead of WALs"" Pull Request resolved: Test Plan: Run the test itself. Differential Revision: D17614721 fbshipit-source-id: 1b0abce49b203a76a039e38e76bc940429975f20/Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5660000000000001,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5221,rocksdb,"Fix OnFlushCompleted fired before flush result write to MANIFEST (#5908) Summary: When there are concurrent flush job on the same CF, `OnFlushCompleted` can be called before the flush result being install to LSM. Fixing the issue by passing `FlushJobInfo` through `MemTable`, and the thread who commit the flush result can fetch the `FlushJobInfo` and fire `OnFlushCompleted` on behave of the thread actually writing the SST. Fix Pull Request resolved: Test Plan: Add new test. The test will fail without the fix. Differential Revision: D17916144 Pulled By: riversand963 fbshipit-source-id: e18df67d9533b5baee52ae3605026cdeb05cbe10/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5888,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5779,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.1931,rocksdb,"Persistent globally unique DB ID in manifest (#5725) Summary: Each DB has a globally unique ID. A DB can be physically copied around, or backed-up and restored, and the users should be identify the same DB. This unique ID right now is stored as plain text in file IDENTITY under the DB directory. This approach introduces at least two problems: (1) the file is not checksumed; (2) the source of truth of a DB is the manifest file, which can be copied separately from IDENTITY file, causing the DB ID to be wrong. The goal of this PR is solve this problem by moving the DB ID to manifest. To begin with we will write to both identity file and manifest. Write to Manifest is controlled via the flag write_dbid_to_manifest in Options and default is false. Pull Request resolved: Test Plan: Added unit tests. Differential Revision: D16963840 Pulled By: vjnadimpalli fbshipit-source-id: 8a86a4c8c82c716003c40fd6b9d2d758030d92e9/"
,,0.5855,rocksdb,"Refactor deletefile_test.cc (#5822) Summary: Make DeleteFileTest inherit DBTestBase to avoid code duplication. Test Plan (on devserver) ``` $make deletefile_test $./deletefile_test ``` Pull Request resolved: Differential Revision: D17456750 Pulled By: riversand963 fbshipit-source-id: 224e97967da7b98838a98981cd5095d3230a814f/Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.5751,rocksdb,"Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
,,0.0863,rocksdb,Remove key length assertion LRUHandle::CalcTotalCharge (#6115) Summary: Inserting an entry in the block cache with 0 length key is a valid use case. Remove the assertion in ```LRUHandle::CalcTotalCharge```. Pull Request resolved: Differential Revision: D18769693 Pulled By: anand1976 fbshipit-source-id: 34cc159650300dda6d7273480640478f28392cda/
,,0.1386,rocksdb,"Atomic flush rollback once on failure (#6385) Summary: Before this fix, atomic flush codepath may hit an assertion failure on a specific failure case. If all flush jobs within an atomic flush succeed (they do not write to MANIFEST), but batch writing version edits to MANIFEST fails, then `cfd->imm()->RollbackMemTableFlush()` will be called twice, and the second invocation hits assertion failure `assert(m->flush_in_progress_)` since the first invocation resets the variable `flush_in_progress_` to false already. Test plan (dev server): ``` ./db_flush_test make check ``` Both must succeed. Pull Request resolved: Differential Revision: D19782943 Pulled By: riversand963 fbshipit-source-id: 84e1592625e729d1b70fdc8479959387a74cb121/Fix a potential bug scheduling unnecessary threads (#6104) Summary: RocksDB should decrement the counter `unscheduled_flushes_` as soon as the bg thread is scheduled. Before this fix, the counter is decremented only when the bg thread starts and picks an element from the flush queue. This may cause more than necessary bg threads to be scheduled. Not a correctness issue, but may affect flush thread count. Pull Request resolved: Test Plan: ``` make check ``` Differential Revision: D18735584 Pulled By: riversand963 fbshipit-source-id: d36272d4a08a494aeeab6200a3cff7a3d1a2dc10/"
,,0.0957,rocksdb,"Make clang analyze happy with options_test (#6398) Summary: clang analysis shows following warning: options/options_test.cc:1554:24: warning: The left operand of is a garbage value (file_size 1) / readahead_size + 1); ~~~~~~~~~ ^ Explicitly initialize file_size and add an assertion to make clang analysis happy. Pull Request resolved: Test Plan: Run ""make analysis"" and see the warning goes away. Differential Revision: D19819662 fbshipit-source-id: 1589ea91c0c8f78242538f01448e4ad0e5fbc219/"
,,0.0597,rocksdb,"fix some spelling typos (#6464) Summary: Found from Debians ""Lintian"" program Pull Request resolved: Differential Revision: D20162862 Pulled By: zhichao-cao fbshipit-source-id: 06941ee2437b038b2b8045becbe9d2c6fbff3e12/"
,,0.0789,rocksdb,Fix compile error when LZ4 is up to r123 (#6412) Summary: Pull Request resolved: Differential Revision: D19914063 Pulled By: ajkr fbshipit-source-id: 4e401e665d4b449d24c4cdec35a4585eeda95996/
,,0.0639,rocksdb,"fix some spelling typos (#6464) Summary: Found from Debians ""Lintian"" program Pull Request resolved: Differential Revision: D20162862 Pulled By: zhichao-cao fbshipit-source-id: 06941ee2437b038b2b8045becbe9d2c6fbff3e12/"
,,0.0597,rocksdb,"fix some spelling typos (#6464) Summary: Found from Debians ""Lintian"" program Pull Request resolved: Differential Revision: D20162862 Pulled By: zhichao-cao fbshipit-source-id: 06941ee2437b038b2b8045becbe9d2c6fbff3e12/"
