Topic_no,Keywords,Contrib,System,Text
4,"write, add, change, thread, set, make, memtable, support, time, summary, problem, wait, transaction, start, header_file, build, case, win, run, exist",0.2169,conscrypt,AI 143235: am: CL 143073 Bringing the Crypto tests down to zero failures. Original author: jorgp Merged from: //branches/cupcake/... Automated import of CL 143235/AI 143385: am: CL 143235 am: CL 143073 Bringing the Crypto tests down to zero failures. Original author: jorgp Merged from: //branches/cupcake/... Original author: android-build Merged from: //branches/donutburger/... Automated import of CL 143385/AI 143073: Bringing the Crypto tests down to zero failures. BUG=1285921 Automated import of CL 143073/
,,0.2154,conscrypt,AI 143235: am: CL 143073 Bringing the Crypto tests down to zero failures. Original author: jorgp Merged from: //branches/cupcake/... Automated import of CL 143235/AI 143385: am: CL 143235 am: CL 143073 Bringing the Crypto tests down to zero failures. Original author: jorgp Merged from: //branches/cupcake/... Original author: android-build Merged from: //branches/donutburger/... Automated import of CL 143385/AI 143073: Bringing the Crypto tests down to zero failures. BUG=1285921 Automated import of CL 143073/
,,0.2215,conscrypt,AI 143235: am: CL 143073 Bringing the Crypto tests down to zero failures. Original author: jorgp Merged from: //branches/cupcake/... Automated import of CL 143235/AI 143385: am: CL 143235 am: CL 143073 Bringing the Crypto tests down to zero failures. Original author: jorgp Merged from: //branches/cupcake/... Original author: android-build Merged from: //branches/donutburger/... Automated import of CL 143385/AI 143073: Bringing the Crypto tests down to zero failures. BUG=1285921 Automated import of CL 143073/
,,0.2184,conscrypt,AI 143235: am: CL 143073 Bringing the Crypto tests down to zero failures. Original author: jorgp Merged from: //branches/cupcake/... Automated import of CL 143235/AI 143385: am: CL 143235 am: CL 143073 Bringing the Crypto tests down to zero failures. Original author: jorgp Merged from: //branches/cupcake/... Original author: android-build Merged from: //branches/donutburger/... Automated import of CL 143385/AI 143073: Bringing the Crypto tests down to zero failures. BUG=1285921 Automated import of CL 143073/
,,0.2184,conscrypt,AI 143235: am: CL 143073 Bringing the Crypto tests down to zero failures. Original author: jorgp Merged from: //branches/cupcake/... Automated import of CL 143235/AI 143385: am: CL 143235 am: CL 143073 Bringing the Crypto tests down to zero failures. Original author: jorgp Merged from: //branches/cupcake/... Original author: android-build Merged from: //branches/donutburger/... Automated import of CL 143385/AI 143073: Bringing the Crypto tests down to zero failures. BUG=1285921 Automated import of CL 143073/
,,0.4307,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4348,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4348,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4266,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4307,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4317,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4378,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4399,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4307,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4337,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4358,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4358,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4297,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4368,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.44799999999999995,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4246,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4358,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4368,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4368,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4419,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4276,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4276,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4439,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4297,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4337,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.4399,conscrypt,"Resync a load of tests with upstream, make our build faster. I started off with a mission to remove uses of dalvik.annotation.* (stuff like and other useless junk that just makes it harder to stay in sync with upstream). I wrote a script to go through tests showing me the diff between what we have and what upstream has, thinking that in cases where upstream has also added tests, I may as well pull them in at the same time... ...but I didnt realize how close we were to having dx fill its 1.5GiB heap. After trying various alternatives, I decided to bite the bullet and break core-tests up into one .jar per module. This adds parallelism back into this, the slowest part of our build. (I can do even better, but Ill do that in a separate patch, preferably after weve merged recent changes from master.) Only a couple of dependencies were problematic: the worthless TestSuiteFactory which already contained a comment suggesting we get rid of it, and the fact that some tests most notably the concurrent ones also contained main methods that started the JUnit tty-based TestRunner. (In the long run, we want to be running the harmony tests directly from a pristine ""svn co"" of upstream, using DalvikRunner. But this will be a big help in the meantime, and starts the work of getting our current copy of the tests into a state where we can start to extract any meaningful changes/additions weve made.)/"
,,0.0588,conscrypt,Fix benchmark setup for moved TestKeyStore. (#409)/
,,0.2488,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/Add Conscrypt-specific hostname verifier (#636) This allows users to set hostname verifiers either Conscrypt-wide or on an individual trust manager without polluting the default HostnameVerifier of HttpsURLConnection. Fixes"
,,0.2549,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2674,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2535,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2576,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2604,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2535,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2535,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2576,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2064,conscrypt,"Update OID->name mapping code (#670) Add our own class for the standard OIDs. This should eliminate our reliance on the platform data for the vast majority of cases. Catch IllegalAccessError if thrown in OpenJDK, since java.base doesnt export AlgorithmId so under JPMS we might not have access to it. Fixes ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2604,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2604,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2493,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.2576,conscrypt,"Add ServiceTester helper class (#661) We have a lot of tests that follow the same basic pattern of ""for all providers, for some/all algorithms of a given type, check {X, Y, Z}"". This introduces a helper object that handles the common parts (finding supported providers, matching errors to providers, etc) so the tests can focus on the test logic. This change also cleans up a bunch of minor test problems: * Test arrays with assertArraysEqual(), not assertTrue(Arrays.equal()) * Unusual formatting (extraneous blocks, etc.) * Unnecessary installing of BouncyCastleProvider in TestKeyStore/"
,,0.077,frostwire,[android] ANR fix for NotificationUpdateDemon.onTime() isScreenOn() check too expensive for main thread and happening quite a lot/
,,0.066,frostwire,[desktop] removed unused ExceptionUtils/
,,0.0639,frostwire,[android] NPE on ImageWorker. Unused code cleanup/
,,0.1181,frostwire,"[android] ConfigurationManager bootstrapping refactor No NPE should be possible on instance(), if instance is null, it means we need to wait for create to finish creating instance No Future necessary, use simple wait/notify synchronization mechanism Moved called to .create earlier in MainApplication.onCreateSafe/"
,,0.0657,jna,fix Structure equals/hashcode git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Handle last error as an exception if declared git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0873,jna,fix build on w32/fix build on win32/check more thoroughly for unanticipated exceptions/fix crash on win64/fix crash with native mapped callback args/fix MSVC dll callbacks build (mingw64 required)/fix build for MSVC express 10/
,,0.0648,jna,fall back to short names only if long names fail (identified in
,,0.0677,jna,fix build on w32/fix crash on win64/fix MSVC dll callbacks build (mingw64 required)/fix build for MSVC express 10/
,,0.1299,jna,[issue Added new Win32 platform function mappings related to process creation. SECURITY_ATTRIBUTES structure has been moved to WinBase to reflect latest MSDN doco. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0618,jna,correct mappings causing failure on win64/
,,0.0837,jna,note platforms where test fails/attach native thread as daemon to avoid tests hanging on exit/fix build on w32/fix crash on win64/address issue
,,0.0556,jna,fix build on w32/
,,0.0706,jna,down to two failing tests on w32ce-arm/fix some more failing tests/fix load from current module/down to 24 failures (4 crashes) on w32ce-arm/
,,0.0648,jna,fix crash on win64/address issue
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0804,jna,down to 2 failing tests git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0813,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0792,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0813,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0813,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0813,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0833,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0833,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0786,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0887,jna,fix some win64 bugs with structure handling git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.1023,jna,fix some win64 bugs with structure handling git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0926,jna,fix some win64 bugs with structure handling git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0786,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0765,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0786,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0792,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0984,jna,fix some win64 bugs with structure handling git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0786,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0792,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0833,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0772,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0786,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0965,jna,fix some win64 bugs with structure handling git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.1023,jna,fix some win64 bugs with structure handling git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0786,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0744,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0786,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0786,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0744,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0786,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0792,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0849,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.091,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix some win64 bugs with structure handling git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0967,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix some win64 bugs with structure handling git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.1213,jna,fix win64 fp return git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix some win64 bugs with structure handling git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0987,jna,more win64 fixes git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix some win64 bugs with structure handling git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0807,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0828,jna,fix a few win64 tests use ffi_closure_alloc everywhere git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0577,jna,auto-strip profiler prefix set in jna.profiler.prefix/
,,0.0648,jna,Handle last error as an exception if declared git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0898,jna,Merge pull request from lgoldstein/win32-as-last-error Make Win32Exception extend LastErrorException/Make Win32Exception extend LastErrorException/
,,0.0703,jna,"Fix typo for darwin targets/More cygwin fixes/FIX libffi (aix) error. aix_closure.S: Invalid computed offset for register r3, wchen returning from ffi_closure_helper_DARWIN(). ffi_darwin.c: add missing ""break;"" in ""case FFI_AIX:"" return FFI_BAD_ABI when unknown ABI is encountered./"
,,0.1317,jna,[issue Added new Win32 platform function mappings related to process creation. SECURITY_ATTRIBUTES structure has been moved to WinBase to reflect latest MSDN doco. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.1281,jna,[issue Added new Win32 platform function mappings related to process creation. SECURITY_ATTRIBUTES structure has been moved to WinBase to reflect latest MSDN doco. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.1263,jna,[issue Added new Win32 platform function mappings related to process creation. SECURITY_ATTRIBUTES structure has been moved to WinBase to reflect latest MSDN doco. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.1241,jna,fix javadoc warnings git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/[issue Added new Win32 platform function mappings related to process creation. SECURITY_ATTRIBUTES structure has been moved to WinBase to reflect latest MSDN doco. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.1246,jna,[issue Added new Win32 platform function mappings related to process creation. SECURITY_ATTRIBUTES structure has been moved to WinBase to reflect latest MSDN doco. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.1335,jna,[issue Added new Win32 platform function mappings related to process creation. SECURITY_ATTRIBUTES structure has been moved to WinBase to reflect latest MSDN doco. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0639,jna,correct mappings causing failure on win64/
,,0.1192,jna,[issue Added new Win32 platform function mappings related to process creation. SECURITY_ATTRIBUTES structure has been moved to WinBase to reflect latest MSDN doco. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
,,0.0991,jna,Merge pull request from lgoldstein/win32-as-last-error Make Win32Exception extend LastErrorException/Make Win32Exception extend LastErrorException/
,,0.0917,jna,Merge pull request from lgoldstein/win32-as-last-error Make Win32Exception extend LastErrorException/Make Win32Exception extend LastErrorException/
,,0.0597,jna,correct mappings causing failure on win64/
,,0.0657,jna,Merge pull request from matthiasblaesing/flexibleVariantConversion Followup fixes for some regressions/Handle LastErrorException from Kernel32Util#formatMessage in ComUtils#checkRC/
,,0.066,OpenDDS,"Coverity fixes; For Interop, built-in control messages (SEDP) for unregister/dispose also use PID_KEY_HASH/"
,,0.0556,OpenDDS,"Coverity fixes; For Interop, built-in control messages (SEDP) for unregister/dispose also use PID_KEY_HASH/"
,,0.1009,OpenDDS,SharedTransport test: Fix test to wait for correct number of messages to be delivered in cases where command line specified more than one writer/publisher./SharedTransport TestCase: Update wait for data available to a do/while loop in case messages are already all arrived by the time the readcondition wait would be created./
,,0.0597,OpenDDS,Print error message from wait/
,,0.066,OpenDDS,Fixed formatting mask * dds/DCPS/MessageTracker.cpp:/
,,0.0857,OpenDDS,Merge pull request from huangminghuang/tcp_wait_for_acks Fix transport receive buffer allocator synchronization problem/Fix transport receive buffer synchronization problem/
,,0.0806,OpenDDS,Fix unsynchronized access to REQUEST ACK TransportQueueElement./Merge pull request from huangminghuang/tcp_wait_for_acks Fix transport receive buffer allocator synchronization problem/Merge branch jwi-gendirbug/Merge pull request from huangminghuang/tcp_wait_for_acks Fix TCP wait_for_acknowledgements() bug/Fix TCP wait_for_acknowledgements() bug/
,,0.08,OpenDDS,Merge pull request from huangminghuang/tcp_wait_for_acks Fix transport receive buffer allocator synchronization problem/Merge branch jwi-gendirbug/Merge pull request from huangminghuang/tcp_wait_for_acks Fix TCP wait_for_acknowledgements() bug/Fix TCP wait_for_acknowledgements() bug/
,,0.0689,OpenDDS,Merge pull request from huangminghuang/tcp_wait_for_acks Fix transport receive buffer allocator synchronization problem/Merge branch jwi-gendirbug of into jwi-gendirbug/Merge branch master into jwi-gendirbug/
,,0.071,OpenDDS,Merge pull request from huangminghuang/tcp_wait_for_acks Fix transport receive buffer allocator synchronization problem/
,,0.0789,OpenDDS,Merge pull request from jwillemsen/jwi-repoctl-type Fixed typo in repoctl console output/Fixed typo in message * tools/repoctl/repoctl.cpp:/
,,0.0764,OpenDDS,Merge pull request from huangminghuang/master Fix the deadlock issue in TCP reconnect thread/Fix the memory access for tcp reconnect task/
,,0.0694,OpenDDS,Fix tcp deadlock/
,,0.0716,OpenDDS,Fix tcp deadlock/
,,0.0879,OpenDDS,Merge pull request from huangminghuang/master Fix the deadlock issue in TCP reconnect thread/Fix tcp deadlock/
,,0.0879,OpenDDS,Merge pull request from huangminghuang/master Fix the deadlock issue in TCP reconnect thread/Fix tcp deadlock/
,,0.0838,OpenDDS,Merge pull request from huangminghuang/master Fix the deadlock issue in TCP reconnect thread/Fix tcp deadlock/
,,0.0556,OpenDDS,Set IDL4 as Default and Fix IDL Files/
,,0.0618,OpenDDS,Set IDL4 as Default and Fix IDL Files/
,,0.0609,OpenDDS,Merge pull request from simpsont-oci/fix_unregister_with_nil_handle Fix unregister with nil handle/
,,0.0583,OpenDDS,"Incorporated fixes from PR review The beacon message was changed to be a valid RTPS Pad submessage, and the RelayHandler now tests for the value of the first byte as well as the length./"
,,0.0681,OpenDDS,ICE: avoid error from setsockopt on IPv6/
,,0.066,OpenDDS,ICE: avoid error from setsockopt on IPv6/
,,0.0719,OpenDDS,"Improving waiting mechanisms and debugging output in Thrasher test, adding additional test case/"
,,0.0792,OpenDDS,Merge pull request from simpsont-oci/fixes_for_rtps_discovery_with_tcp_transport Fixes for RTPS discovery with TCP transport/
,,0.0751,OpenDDS,Merge pull request from simpsont-oci/fixes_for_rtps_discovery_with_tcp_transport Fixes for RTPS discovery with TCP transport/
,,0.0926,OpenDDS,Merge pull request from jrw972/ice-fixes Fix ICE bugs/Fix ICE bugs * Force loading of default plugins if necessary * Fix deserialization bug * Include ICE Agent Info in SPDP/
,,0.0926,OpenDDS,Merge pull request from jrw972/ice-fixes Fix ICE bugs/Fix ICE bugs * Force loading of default plugins if necessary * Fix deserialization bug * Include ICE Agent Info in SPDP/
,,0.3869,OpenDDS,"Merge pull request from kuznetsovmoci/ParameterListConverter_Return_Types_Fix ParameterListConverter return types fix/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./RTPS 2.4: SPDP Domain ID and BuiltinEndpointQos (#1367) * Add DomainID PID & Add Domain to SPDP Announcement ParameterList. Corectly support new BestEffort Builtin PID / Flags * Updated initialization of ParticipantProxy_t structure with new members * Update tests/security/attributes/run_test.pl Co-Authored-By: Fred Hornsey * Uncommented mistakenly taken out test back. * Added commented out FullMsgSign_PayloadEncrypt_Frag test * Bug fix * Bug fixes/"
,,0.3924,OpenDDS,"Merge pull request from kuznetsovmoci/ParameterListConverter_Return_Types_Fix ParameterListConverter return types fix/Bug fixes for from_param_list, to_param_list return value change/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.0686,OpenDDS,Merge pull request from kuznetsovmoci/ParameterListConverter_Return_Types_Fix ParameterListConverter return types fix/
,,0.336,OpenDDS,"Merge pull request from jonesc-oci/connect-method Connection method Coverity fixes/Changes for Coverity issues./Modify ParticipantLocation to use octet[] for guid. Modify ParticipantLocation test to report a failure if the test exits without checking location. Fix spacing in DiscoveryBase.h./RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3967,OpenDDS,"Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3883,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.387,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3845,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3769,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3857,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3857,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3857,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3883,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3819,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3845,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3782,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.0849,OpenDDS,Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./
,,0.091,OpenDDS,Merge pull request from simpsont-oci/fix_fragment_resend_bug Fix Bug in TransportSendBuffers Fragment Resend Logic/fix edge-case bug in SingleSendBuffers resend_fragments_i/Merge pull request from simpsont-oci/fix_rtps_resend_remove_deadlock Fix deadlock causes by packet removal during reliable resend/
,,0.1539,OpenDDS,Merge pull request from simpsont-oci/fix_rtps_resend_remove_deadlock Fix deadlock causes by packet removal during reliable resend/fixing locking order issue (causes deadlock) between packet removal and reliable packet resend/
,,0.1521,OpenDDS,Merge pull request from simpsont-oci/fix_rtps_resend_remove_deadlock Fix deadlock causes by packet removal during reliable resend/fixing locking order issue (causes deadlock) between packet removal and reliable packet resend/
,,0.3744,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.0899,OpenDDS,Merge pull request from simpsont-oci/fix_rtps_resend_remove_deadlock Fix deadlock causes by packet removal during reliable resend/
,,0.3782,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3458,OpenDDS,"Removed break from active processing side to allow continuance of loop to process candidate links when the first connection link fails to succeed. The loop was kicking out too early. On the passive side of association, for loops that have multiple impls to look at for candidate association was kicking out after the first indication of a successful impl existance, but when the link was null, it did not proceed to the next one in the list to check further, but was simply returning out of the associate method. (cherry picked from commit d6a38fca10a435bb8638d0c6e044a1a6b77f6ae5)/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3819,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.387,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.4228,OpenDDS,"Merge pull request from simpsont-oci/fix_rtps_resend_remove_deadlock Fix deadlock causes by packet removal during reliable resend/fixing locking order issue (causes deadlock) between packet removal and reliable packet resend/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3845,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.1557,OpenDDS,Merge pull request from simpsont-oci/fix_rtps_resend_remove_deadlock Fix deadlock causes by packet removal during reliable resend/fixing locking order issue (causes deadlock) between packet removal and reliable packet resend/
,,0.4016,OpenDDS,"Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3832,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3826,OpenDDS,"Merge pull request from simpsont-oci/fix_rtps_resend_remove_deadlock Fix deadlock causes by packet removal during reliable resend/fixing locking order issue (causes deadlock) between packet removal and reliable packet resend/Merge pull request from mitza-oci/master RtpsUdpDataLink: avoid local classes since they cant be template arguments/RtpsUdpDataLink: avoid local classes since they cant be template arguments/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.4217,OpenDDS,"Merge pull request from simpsont-oci/fix_rtps_resend_remove_deadlock Fix deadlock causes by packet removal during reliable resend/fixing locking order issue (causes deadlock) between packet removal and reliable packet resend/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.4217,OpenDDS,"Merge pull request from simpsont-oci/fix_rtps_resend_remove_deadlock Fix deadlock causes by packet removal during reliable resend/fixing locking order issue (causes deadlock) between packet removal and reliable packet resend/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3605,OpenDDS,"added intersect_sorted_ranges and intersect; modified debug code/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./Fix constants and serialization/"
,,0.3574,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./Fix constants and serialization/"
,,0.4137,OpenDDS,"Merge pull request from jrw972/use-ice-needs-dcps-security OpenDDS crashes when UseIce=1 but DCPSSecurity=0/OpenDDS crashes when UseIce=1 but DCPSSecurity=0 Solution: Check for DCPSSecurity whenever UseIce is active./RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3769,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.2933,OpenDDS,"prevent RtpsWriter WriteDataContainer deadlock issue by releasing locks before calling data_delivered() and data_dropped()/prevent RtpsWriter WriteDataContainer deadlock issue by releasing lock before calling data_delivered()/Merge pull request from simpsont-oci/fix_rtps_resend_remove_deadlock Fix deadlock causes by packet removal during reliable resend/Merge pull request from simpsont-oci/rtps_coverity_issues Fixing Minor RTPS Coverity / Test Issues/fixing locking order issue (causes deadlock) between packet removal and reliable packet resend/fixing coverity issues/Merge pull request from simpsont-oci/attempt_thrasher_corruption_fix Add Thrasher Test for rtps_udp transport, Fix WaitForAck & Durability Issues It Exposed/scalability fixes for these reliability / durability changes based on bench testing/Merge pull request from simpsont-oci/count_overflow_checking RTPS 2.4: checking count values but also allowing overflow/checking count values but also allowing overflow/Merge pull request from mitza-oci/master RtpsUdpDataLink: avoid local classes since they cant be template arguments/RtpsUdpDataLink: avoid local classes since they cant be template arguments/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3883,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3889,OpenDDS,"Merge pull request from objectcomputing/tcp-reconnect-fix Tcp reconnect fix/Added tcp connection timeout for active tcp connection attempts. When unroutable locations were being found in the OS interfaces the connection attempt was hanging for an indefinite amount of time. However, this was not configureable on the active side. Configurable option (active_conn_timeout_period) was added as a TCP-specific option with a default of 5 seconds. (cherry picked from commit 2b568a772ee143b47eff84d3a7b4da30c3a3de63)/Merge pull request from objectcomputing/tcp-reconnect-fix Tcp reconnect fix/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.4076,OpenDDS,"Added tcp connection timeout for active tcp connection attempts. When unroutable locations were being found in the OS interfaces the connection attempt was hanging for an indefinite amount of time. However, this was not configureable on the active side. Configurable option (active_conn_timeout_period) was added as a TCP-specific option with a default of 5 seconds. (cherry picked from commit 2b568a772ee143b47eff84d3a7b4da30c3a3de63)/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.371,OpenDDS,"Merge pull request from objectcomputing/tcp-reconnect-fix Tcp reconnect fix/Merge pull request from objectcomputing/tcp-reconnect-fix Tcp reconnect fix/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.4085,OpenDDS,"Added tcp connection timeout for active tcp connection attempts. When unroutable locations were being found in the OS interfaces the connection attempt was hanging for an indefinite amount of time. However, this was not configureable on the active side. Configurable option (active_conn_timeout_period) was added as a TCP-specific option with a default of 5 seconds. (cherry picked from commit 2b568a772ee143b47eff84d3a7b4da30c3a3de63)/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3807,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3832,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3782,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3782,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3807,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.1274,OpenDDS,Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Style fix/Replaced monitor ptr with smart ptr to fix memory leaks./Merge pull request from jonesc-oci/connect_method Fix ParticipantLocation Test/Add TopicImpl::topic_name() and modify DomainParticipant::is_clean() to avoid unnecessary string copies./
,,0.387,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3895,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.1311,OpenDDS,Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./Merge pull request from jonesc-oci/connect_method Fix ParticipantLocation Test/Add TopicImpl::topic_name() and modify DomainParticipant::is_clean() to avoid unnecessary string copies./
,,0.3706,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3801,OpenDDS,"Whitespace fixes/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.0849,OpenDDS,Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./
,,0.3627,OpenDDS,"Whitespace fixes/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.2938,OpenDDS,"Merge pull request from mitza-oci/master warning fix: removed unused expression/warning fix: removed unused expression/Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./Normalize Enum-like Values to String Functions Normalize these enum-like value to string functions, by making them return `const char *` instead of `OPENDDS_STRING` (Fix for and having them behave the same when an invalid value is passed: `DataSampleHeader.cpp`: `to_string(MessageId)` `to_string(SubMessageId)` `InstanceState.cpp`: `InstanceState::instance_state_string(DDS::InstanceStateKind)` `InstanceState::instance_state_string()` `Sedp.cpp`: `Sedp::Msg::msgTypeToString(MsgType)` `Sedp::Msg::msgTypeToString()` `SafetyProfileStreams.cpp`: `retcode_to_string(DDS::ReturnCode_t)` `WriterInfo.cpp`: `WriterInfo::get_state_str()` `security/CommonUtilities.cpp`: `ctk_to_dds_string(const CryptoTransformKind&)` `transport/tcp/TcpConnection.cpp`: `TcpConnection::reconnect_state_string()` Remove logging single spaces before newlines in `DataWriterImpl.cpp` Try to fix style in `DataWriterImpl_T.h`/"
,,0.3393,OpenDDS,"Merge pull request from kuznetsovmoci/ParameterListConverter_Return_Types_Fix ParameterListConverter return types fix/Bug fix for from_param_list return type change/Bug fixes for from_param_list return type changes/bug fix/ParameterListConverter return types fix/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./RTPS 2.4: SPDP Domain ID and BuiltinEndpointQos (#1367) * Add DomainID PID & Add Domain to SPDP Announcement ParameterList. Corectly support new BestEffort Builtin PID / Flags * Updated initialization of ParticipantProxy_t structure with new members * Update tests/security/attributes/run_test.pl Co-Authored-By: Fred Hornsey * Uncommented mistakenly taken out test back. * Added commented out FullMsgSign_PayloadEncrypt_Frag test * Bug fix * Bug fixes/"
,,0.3077,OpenDDS,"Avoid ACE_SYNCH_MUTEX in the OpenDDS_Rtps library. Use of a preprocessor macro isnt warranted and sync is spelled wrong anyway./RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./Normalize Enum-like Values to String Functions Normalize these enum-like value to string functions, by making them return `const char *` instead of `OPENDDS_STRING` (Fix for and having them behave the same when an invalid value is passed: `DataSampleHeader.cpp`: `to_string(MessageId)` `to_string(SubMessageId)` `InstanceState.cpp`: `InstanceState::instance_state_string(DDS::InstanceStateKind)` `InstanceState::instance_state_string()` `Sedp.cpp`: `Sedp::Msg::msgTypeToString(MsgType)` `Sedp::Msg::msgTypeToString()` `SafetyProfileStreams.cpp`: `retcode_to_string(DDS::ReturnCode_t)` `WriterInfo.cpp`: `WriterInfo::get_state_str()` `security/CommonUtilities.cpp`: `ctk_to_dds_string(const CryptoTransformKind&)` `transport/tcp/TcpConnection.cpp`: `TcpConnection::reconnect_state_string()` Remove logging single spaces before newlines in `DataWriterImpl.cpp` Try to fix style in `DataWriterImpl_T.h`/"
,,0.1502,OpenDDS,Merge pull request from jrw972/use-ice-needs-dcps-security OpenDDS crashes when UseIce=1 but DCPSSecurity=0/OpenDDS crashes when UseIce=1 but DCPSSecurity=0 Solution: Check for DCPSSecurity whenever UseIce is active./
,,0.3483,OpenDDS,"Fix Linux SecurityWithoutFeatures build./Whitespace fixes/Add for ICE functions to fix builds that do not have security enabled./Merge pull request from mitza-oci/master Spdp: avoid local classes since they cant be template arguments/Spdp: avoid local classes since they cant be template arguments/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3777,OpenDDS,"Whitespace fixes/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3739,OpenDDS,"Whitespace fixes/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3845,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3777,OpenDDS,"Add clear_transaction_id() to fix uninitialized memory issue./RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./Fix constants and serialization/"
,,0.2493,OpenDDS,"Merge pull request from mitza-oci/sedp-writer-rch Follow-up to fixed Sedp::Writer reference counting/Follow-up to fixed Sedp::Writer reference counting/Fixes from merge/Merge pull request from simpsont-oci/sedp_transport_call_remove_all_msgs Call remove_all_msgs from SEDP transport client destructors to avoid leaks/call remove_all_msgs from sedp transport destructors to avoid memory leaks/Merge pull request from kuznetsovmoci/ParameterListConverter_Return_Types_Fix ParameterListConverter return types fix/ParameterListConverter return types fix/Merge pull request from simpsont-oci/fix_sedp_win32_access_violaion_error Fixing win32 access violation in SEDP/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./RTPS 2.4: SPDP Domain ID and BuiltinEndpointQos (#1367) * Add DomainID PID & Add Domain to SPDP Announcement ParameterList. Corectly support new BestEffort Builtin PID / Flags * Updated initialization of ParticipantProxy_t structure with new members * Update tests/security/attributes/run_test.pl Co-Authored-By: Fred Hornsey * Uncommented mistakenly taken out test back. * Added commented out FullMsgSign_PayloadEncrypt_Frag test * Bug fix * Bug fixes/Normalize Enum-like Values to String Functions Normalize these enum-like value to string functions, by making them return `const char *` instead of `OPENDDS_STRING` (Fix for and having them behave the same when an invalid value is passed: `DataSampleHeader.cpp`: `to_string(MessageId)` `to_string(SubMessageId)` `InstanceState.cpp`: `InstanceState::instance_state_string(DDS::InstanceStateKind)` `InstanceState::instance_state_string()` `Sedp.cpp`: `Sedp::Msg::msgTypeToString(MsgType)` `Sedp::Msg::msgTypeToString()` `SafetyProfileStreams.cpp`: `retcode_to_string(DDS::ReturnCode_t)` `WriterInfo.cpp`: `WriterInfo::get_state_str()` `security/CommonUtilities.cpp`: `ctk_to_dds_string(const CryptoTransformKind&)` `transport/tcp/TcpConnection.cpp`: `TcpConnection::reconnect_state_string()` Remove logging single spaces before newlines in `DataWriterImpl.cpp` Try to fix style in `DataWriterImpl_T.h`/"
,,0.0652,OpenDDS,Merge pull request from kuznetsovmoci/ParameterListConverter_Return_Types_Fix ParameterListConverter return types fix/ParameterListConverter return types fix/
,,0.2728,OpenDDS,"Merge pull request from jrw972/rtps-relay-compiler-warnings Fix compiler warnings related to RtpsRelay/Fix compiler warnings related to RtpsRelay/Fixes from merge/Merge pull request from kuznetsovmoci/ParameterListConverter_Return_Types_Fix ParameterListConverter return types fix/ParameterListConverter return types fix/Merge pull request from jonesc-oci/connect-method Connection method Coverity fixes/Changes for Coverity issues./Merge pull request from jonesc-oci/connect_method Fix stack use after scope issue./Fix stack use after scope issue./Fix ""ACE_SOCK_Dgram::recv: Operation not supported"" issue in safety-profile builds./Fix Linux SecurityWithoutFeatures build./Whitespace fixes/Add for ICE functions to fix builds that do not have security enabled./Merge pull request from mitza-oci/master Spdp: avoid local classes since they cant be template arguments/Spdp: avoid local classes since they cant be template arguments/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./Spdp Transports WriteFlags work around for bug in older MSVC versions/RTPS 2.4: SPDP Domain ID and BuiltinEndpointQos (#1367) * Add DomainID PID & Add Domain to SPDP Announcement ParameterList. Corectly support new BestEffort Builtin PID / Flags * Updated initialization of ParticipantProxy_t structure with new members * Update tests/security/attributes/run_test.pl Co-Authored-By: Fred Hornsey * Uncommented mistakenly taken out test back. * Added commented out FullMsgSign_PayloadEncrypt_Frag test * Bug fix * Bug fixes/"
,,0.3756,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3819,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.1117,OpenDDS,"Merge pull request from xieshuaix/fix/macro_naming_conflicts Remove macros: DUP and NO_DUP to avoid naming conflicts./Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./Merge pull request from jonesc-oci/connect_method Fix ParticipantLocation Test/Add TopicImpl::topic_name() and modify DomainParticipant::is_clean() to avoid unnecessary string copies./Merge pull request from jonesc-oci/connect_method Fix tests that started failing with memory leak. Remove unnecessary counter./Fix comment./Bug fix + test app for: ""create_new_topic(), called near the end of c (#1355) * Bug fix + test app for: ""create_new_topic(), called near the end of create_topic_i(), can return nullptr. This needs to be checked before attempting to enable() it.""/"
,,0.3383,OpenDDS,"Merge pull request from kuznetsovmoci/Serialization_bug_fix Serialization warning bug fix/Layout fix/Serialization warning bug fix/RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./Serialized Payload Alignment Changes (#1374) * Serialized Payload Alignment Changes * Bug fix * code and output layout fixes/"
,,0.0849,OpenDDS,Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./
,,0.387,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3857,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.0849,OpenDDS,Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./
,,0.3756,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3931,OpenDDS,"Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.3175,OpenDDS,"Merge pull request from kuznetsovmoci/monitor_mem_leaks Fixes for Monitor Test/Bug fixes/Merge pull request from kuznetsovmoci/monitor_mem_leaks Replaced monitor ptr with smart ptr to fix memory leaks./Replaced monitor ptr with smart ptr to fix memory leaks./RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./Normalize Enum-like Values to String Functions Normalize these enum-like value to string functions, by making them return `const char *` instead of `OPENDDS_STRING` (Fix for and having them behave the same when an invalid value is passed: `DataSampleHeader.cpp`: `to_string(MessageId)` `to_string(SubMessageId)` `InstanceState.cpp`: `InstanceState::instance_state_string(DDS::InstanceStateKind)` `InstanceState::instance_state_string()` `Sedp.cpp`: `Sedp::Msg::msgTypeToString(MsgType)` `Sedp::Msg::msgTypeToString()` `SafetyProfileStreams.cpp`: `retcode_to_string(DDS::ReturnCode_t)` `WriterInfo.cpp`: `WriterInfo::get_state_str()` `security/CommonUtilities.cpp`: `ctk_to_dds_string(const CryptoTransformKind&)` `transport/tcp/TcpConnection.cpp`: `TcpConnection::reconnect_state_string()` Remove logging single spaces before newlines in `DataWriterImpl.cpp` Try to fix style in `DataWriterImpl_T.h`/"
,,0.392,OpenDDS,"RTPS discovery and transport cannot handle network changes (#1365) * RTPS discovery and transport cannot handle network changes The locators determined by RTPS discovery and used by the RTPS transport are set at start time. This prevents OpenDDS from be used in situations where the network configuration is expected to change like mobile phones, tablets, etc. To solve this problem, code was added to detect network changes and update the locators announced by SPDP and SEDP. Support was also added for checking for changed locators and updating the RTPS transport to use the new locators. ICE was also updated so that the published AgentInfo is updated when the network configuration changes. Only Linux is supported at this time. Diagnostic logging for these features can be turned on with DCPSDebugLevel > 4./"
,,0.2724,OpenDDS,"addressing compile errors and review comments... renaming a few things and pushing magic numbers into configurable locations/Merge pull request from jrw972/frozen-ice ICE fails to converge promptly/ICE fails to converge promptly Foundations used by connectivity checks that fail were being removed from the set of active foundations. However, this was not being propagated to other checklists that were frozen on that foundation. This change unfreezes checklists after a foundation is removed from the set of active foundations. This change revealed a deadlock which was resolved by queuing STUN sends in SPDP./"
,,0.4614,OpenDDS,"Merge pull request from mitza-oci/ice-testing fixes from ICE testing/fixes from ICE testing/Use reference counting for ICE The time-based behavior of Checklists, ServerReflexiveTasks, and ChangePasswordTasks is governed by the AgentImpl. There was no way to remove a Task from the queue of Tasks waiting on a timer. Consequently, manual memory management was used to 1) prevent a Task from entering the queue twice and 2) to destroy the Task as a scheduled event. The problem is the ServerReflexiveTask and ChangePasswordTask have the same lifetime as their parent object (EndpointManager) but the destruction was solely under the control of the ServerReflexiveTask. Thus, a destroyed ChangePasswordTask may be in the queue of Tasks waiting on a timer. Solution: Change the queue of Tasks to hold weak references./Merge pull request from jrw972/robust-ice ICE fixes related to invalid or stale network addresses/Merge pull request from jrw972/ice-deadlock Fix ICE deadlock from multiple participants in the same process/Fix ICE deadlock from multiple participants in the same process/Merge pull request from jrw972/frozen-ice ICE fails to converge promptly/ICE fails to converge promptly Foundations used by connectivity checks that fail were being removed from the set of active foundations. However, this was not being propagated to other checklists that were frozen on that foundation. This change unfreezes checklists after a foundation is removed from the set of active foundations. This change revealed a deadlock which was resolved by queuing STUN sends in SPDP./"
,,0.3451,OpenDDS,"Use reference counting for ICE The time-based behavior of Checklists, ServerReflexiveTasks, and ChangePasswordTasks is governed by the AgentImpl. There was no way to remove a Task from the queue of Tasks waiting on a timer. Consequently, manual memory management was used to 1) prevent a Task from entering the queue twice and 2) to destroy the Task as a scheduled event. The problem is the ServerReflexiveTask and ChangePasswordTask have the same lifetime as their parent object (EndpointManager) but the destruction was solely under the control of the ServerReflexiveTask. Thus, a destroyed ChangePasswordTask may be in the queue of Tasks waiting on a timer. Solution: Change the queue of Tasks to hold weak references./"
,,0.48100000000000004,OpenDDS,"Use reference counting for ICE The time-based behavior of Checklists, ServerReflexiveTasks, and ChangePasswordTasks is governed by the AgentImpl. There was no way to remove a Task from the queue of Tasks waiting on a timer. Consequently, manual memory management was used to 1) prevent a Task from entering the queue twice and 2) to destroy the Task as a scheduled event. The problem is the ServerReflexiveTask and ChangePasswordTask have the same lifetime as their parent object (EndpointManager) but the destruction was solely under the control of the ServerReflexiveTask. Thus, a destroyed ChangePasswordTask may be in the queue of Tasks waiting on a timer. Solution: Change the queue of Tasks to hold weak references./Merge pull request from jrw972/ice-deadlock Fix ICE deadlock from multiple participants in the same process/Fix ICE deadlock from multiple participants in the same process/Merge pull request from jrw972/frozen-ice ICE fails to converge promptly/ICE fails to converge promptly Foundations used by connectivity checks that fail were being removed from the set of active foundations. However, this was not being propagated to other checklists that were frozen on that foundation. This change unfreezes checklists after a foundation is removed from the set of active foundations. This change revealed a deadlock which was resolved by queuing STUN sends in SPDP./"
,,0.3326,OpenDDS,"Use reference counting for ICE The time-based behavior of Checklists, ServerReflexiveTasks, and ChangePasswordTasks is governed by the AgentImpl. There was no way to remove a Task from the queue of Tasks waiting on a timer. Consequently, manual memory management was used to 1) prevent a Task from entering the queue twice and 2) to destroy the Task as a scheduled event. The problem is the ServerReflexiveTask and ChangePasswordTask have the same lifetime as their parent object (EndpointManager) but the destruction was solely under the control of the ServerReflexiveTask. Thus, a destroyed ChangePasswordTask may be in the queue of Tasks waiting on a timer. Solution: Change the queue of Tasks to hold weak references./"
,,0.2757,OpenDDS,"Merge pull request from jrw972/uninitialized-transaction-ids Fix uninitialized transaction ids in STUN/Fix uninitialized transaction ids in STUN/ICE fails to converge promptly Foundations used by connectivity checks that fail were being removed from the set of active foundations. However, this was not being propagated to other checklists that were frozen on that foundation. This change unfreezes checklists after a foundation is removed from the set of active foundations. This change revealed a deadlock which was resolved by queuing STUN sends in SPDP./"
,,0.4183,OpenDDS,"Use reference counting for ICE The time-based behavior of Checklists, ServerReflexiveTasks, and ChangePasswordTasks is governed by the AgentImpl. There was no way to remove a Task from the queue of Tasks waiting on a timer. Consequently, manual memory management was used to 1) prevent a Task from entering the queue twice and 2) to destroy the Task as a scheduled event. The problem is the ServerReflexiveTask and ChangePasswordTask have the same lifetime as their parent object (EndpointManager) but the destruction was solely under the control of the ServerReflexiveTask. Thus, a destroyed ChangePasswordTask may be in the queue of Tasks waiting on a timer. Solution: Change the queue of Tasks to hold weak references./Merge pull request from jrw972/robust-ice ICE fixes related to invalid or stale network addresses/Add log message for broken platforms concerning ICE/Merge pull request from jrw972/frozen-ice ICE fails to converge promptly/ICE fails to converge promptly Foundations used by connectivity checks that fail were being removed from the set of active foundations. However, this was not being propagated to other checklists that were frozen on that foundation. This change unfreezes checklists after a foundation is removed from the set of active foundations. This change revealed a deadlock which was resolved by queuing STUN sends in SPDP./"
,,0.3424,OpenDDS,"Use reference counting for ICE The time-based behavior of Checklists, ServerReflexiveTasks, and ChangePasswordTasks is governed by the AgentImpl. There was no way to remove a Task from the queue of Tasks waiting on a timer. Consequently, manual memory management was used to 1) prevent a Task from entering the queue twice and 2) to destroy the Task as a scheduled event. The problem is the ServerReflexiveTask and ChangePasswordTask have the same lifetime as their parent object (EndpointManager) but the destruction was solely under the control of the ServerReflexiveTask. Thus, a destroyed ChangePasswordTask may be in the queue of Tasks waiting on a timer. Solution: Change the queue of Tasks to hold weak references./"
,,0.4513,OpenDDS,"Use reference counting for ICE The time-based behavior of Checklists, ServerReflexiveTasks, and ChangePasswordTasks is governed by the AgentImpl. There was no way to remove a Task from the queue of Tasks waiting on a timer. Consequently, manual memory management was used to 1) prevent a Task from entering the queue twice and 2) to destroy the Task as a scheduled event. The problem is the ServerReflexiveTask and ChangePasswordTask have the same lifetime as their parent object (EndpointManager) but the destruction was solely under the control of the ServerReflexiveTask. Thus, a destroyed ChangePasswordTask may be in the queue of Tasks waiting on a timer. Solution: Change the queue of Tasks to hold weak references./Merge pull request from jrw972/frozen-ice ICE fails to converge promptly/ICE fails to converge promptly Foundations used by connectivity checks that fail were being removed from the set of active foundations. However, this was not being propagated to other checklists that were frozen on that foundation. This change unfreezes checklists after a foundation is removed from the set of active foundations. This change revealed a deadlock which was resolved by queuing STUN sends in SPDP./"
,,0.2754,OpenDDS,"ICE fails to converge promptly Foundations used by connectivity checks that fail were being removed from the set of active foundations. However, this was not being propagated to other checklists that were frozen on that foundation. This change unfreezes checklists after a foundation is removed from the set of active foundations. This change revealed a deadlock which was resolved by queuing STUN sends in SPDP./"
,,0.48,OpenDDS,"Use reference counting for ICE The time-based behavior of Checklists, ServerReflexiveTasks, and ChangePasswordTasks is governed by the AgentImpl. There was no way to remove a Task from the queue of Tasks waiting on a timer. Consequently, manual memory management was used to 1) prevent a Task from entering the queue twice and 2) to destroy the Task as a scheduled event. The problem is the ServerReflexiveTask and ChangePasswordTask have the same lifetime as their parent object (EndpointManager) but the destruction was solely under the control of the ServerReflexiveTask. Thus, a destroyed ChangePasswordTask may be in the queue of Tasks waiting on a timer. Solution: Change the queue of Tasks to hold weak references./Merge pull request from jrw972/ice-deadlock Fix ICE deadlock from multiple participants in the same process/Fix ICE deadlock from multiple participants in the same process/Merge pull request from jrw972/frozen-ice ICE fails to converge promptly/ICE fails to converge promptly Foundations used by connectivity checks that fail were being removed from the set of active foundations. However, this was not being propagated to other checklists that were frozen on that foundation. This change unfreezes checklists after a foundation is removed from the set of active foundations. This change revealed a deadlock which was resolved by queuing STUN sends in SPDP./"
,,0.2724,OpenDDS,"Merge pull request from mitza-oci/ice-testing fixes from ICE testing/fixes from ICE testing/addressing compile errors and review comments... renaming a few things and pushing magic numbers into configurable locations/fixing compile error/Spdp.cpp: Fix Some Things I Missed in Rebase/Merge pull request from jrw972/frozen-ice ICE fails to converge promptly/ICE fails to converge promptly Foundations used by connectivity checks that fail were being removed from the set of active foundations. However, this was not being propagated to other checklists that were frozen on that foundation. This change unfreezes checklists after a foundation is removed from the set of active foundations. This change revealed a deadlock which was resolved by queuing STUN sends in SPDP./Merge pull request from kuznetsovmoci/Secure_Participant_Discovery_2 Fixes for Secure Participant Discovery/Fixes for Secure Participant Discovery/Fixes for Secure Participant Discovery/"
,,0.0686,pljava,Bugfix Memory leak plugged./
,,0.1219,realm-java,"SyncCredentials.accessToken + Integration tests (#4018) This PR adds support for SyncCredentials.accessToken() which is required for I also found a number of issues with the integration tests. They have been fixed as well./Merge pull request from realm/merge-8c5f4c-to-master Fix merge from 8c5f4c to master/Waiting longer time for checking auth server (#3913) From log i saw sometimes the ros testing server fails to send the response, maybe 20+50 1s sometimes is too short when the docker host is under heave loading./"
,,0.0726,realm-java,"Enable Kotlin for unit tests and move benchmarks to separate library Kotlin has been added to the unit test suite. In order to not break the dex limit, the benchmarks have been moved to a separate library./"
,,0.2765,rocksdb,"Make provision for db_stress to work with a pre-existing dir Summary: The crash_test depends on db_stress to work with pre-existing dir Test Plan: make db_stress; Run db_stress with destroy_db_initially=0 Reviewers: vamsi, dhruba Reviewed By: dhruba CC: leveldb Differential Revision: should be less than ops_per_thread Summary: For sanity w.r.t. the way we split up the reopens equally among the ops/thread Test Plan: make db_stress; db_stress error Reviewers: vamsi, dhruba Reviewed By: dhruba CC: leveldb Differential Revision: script to periodically run and kill the db_stress test Summary: The script runs and kills the stress test periodically. Default values have been used in the script now. Should I make this a part of the Makefile or automated rocksdb build? The values can be easily changed in the script right now, but should I add some support for variable values or input to the script? I believe the script achieves its objective of unsafe crashes and reopening to expect sanity in the database. Test Plan: python tools/db_crashtest.py Reviewers: dhruba, vamsi, MarkCallaghan Reviewed By: vamsi CC: leveldb Differential Revision: the and in db_stress] Summary: Also added some comments and fixed some bugs in stats reporting. Now the stats seem to match what is expected. Test Plan: /data/users/nponnekanti/rocksdb] ./db_stress LevelDB version : 1.5 Number of threads : 1 Ops per thread : 1000 Read percentage : 10 Delete percentage : 30 Max key : 320 Ratio : 3 Num times DB reopens: 10 Batches/snapshots : 1 Num keys per lock : 4 Compression : snappy No lock creation because test_batches_snapshots set 2013/03/04-15:58:56 Starting database operations 2013/03/04-15:58:56 Reopening database for the 1th time 2013/03/04-15:58:56 Reopening database for the 2th time 2013/03/04-15:58:56 Reopening database for the 3th time 2013/03/04-15:58:56 Reopening database for the 4th time Created bg thread 0x7f4542bff700 2013/03/04-15:58:56 Reopening database for the 5th time 2013/03/04-15:58:56 Reopening database for the 6th time 2013/03/04-15:58:56 Reopening database for the 7th time 2013/03/04-15:58:57 Reopening database for the 8th time 2013/03/04-15:58:57 Reopening database for the 9th time 2013/03/04-15:58:57 Reopening database for the 10th time 2013/03/04-15:58:57 Reopening database for the 11th time 2013/03/04-15:58:57 Limited verification already done during gets Stress Test : 1811.551 micros/op 552 ops/sec : Wrote 0.10 MB (0.05 MB/sec) (598% of 1011 ops) : Wrote 6050 times : Deleted 3050 times : 500/900 gets found the key : Got errors 0 times /data/users/nponnekanti/rocksdb] ./db_stress LevelDB version : 1.5 Number of threads : 1 Ops per thread : 1000 Read percentage : 10 Delete percentage : 30 Max key : 320 Ratio : 3 Num times DB reopens: 10 Batches/snapshots : 0 Num keys per lock : 4 Compression : snappy Creating 80 locks 2013/03/04-15:58:17 Starting database operations 2013/03/04-15:58:17 Reopening database for the 1th time 2013/03/04-15:58:17 Reopening database for the 2th time 2013/03/04-15:58:17 Reopening database for the 3th time 2013/03/04-15:58:17 Reopening database for the 4th time Created bg thread 0x7fc0f5bff700 2013/03/04-15:58:17 Reopening database for the 5th time 2013/03/04-15:58:17 Reopening database for the 6th time 2013/03/04-15:58:18 Reopening database for the 7th time 2013/03/04-15:58:18 Reopening database for the 8th time 2013/03/04-15:58:18 Reopening database for the 9th time 2013/03/04-15:58:18 Reopening database for the 10th time 2013/03/04-15:58:18 Reopening database for the 11th time 2013/03/04-15:58:18 Starting verification Stress Test : 1836.258 micros/op 544 ops/sec : Wrote 0.01 MB (0.01 MB/sec) (59% of 1011 ops) : Wrote 605 times : Deleted 305 times : 50/90 gets found the key : Got errors 0 times 2013/03/04-15:58:18 Verification successful Revert Plan: OK Task ID: Reviewers: emayanke, dhruba Reviewed By: emayanke CC: leveldb Differential Revision: statistics. Remove individual functions like incNumFileOpens Summary: Use only the counter mechanism. Do away with incNumFileOpens, incNumFileClose, incNumFileErrors s/NULL/nullptr/g in db/table_cache.cc Test Plan: make clean check Reviewers: dhruba, heyongqiang, emayanke Reviewed By: dhruba CC: leveldb Differential Revision: a second kind of verification to db_stress Summary: Currently the test tracks all writes in memory and uses it for verification at the end. This has 4 problems: (a) It needs mutex for each write to ensure in-memory update and leveldb update are done atomically. This slows down the benchmark. (b) Verification phase at the end is time consuming as well (c) Does not test batch writes or snapshots (d) We cannot kill the test and restart multiple times in a loop because in-memory state will be lost. I am adding a FLAGS_multi that does MultiGet/MultiPut/MultiDelete instead of get/put/delete to get/put/delete a group of related keys with same values atomically. Every get retrieves the group of keys and checks that their values are same. This does not have the above problems but the downside is that it does less amount of validation than the other approach. Test Plan: This whole this is a test Here is a small run. I am doing larger run now. /data/users/nponnekanti/rocksdb] ./db_stress LevelDB version : 1.5 Number of threads : 32 Ops per thread : 10000 Read percentage : 10 Delete percentage : 30 Max key : 2147483648 Num times DB reopens: 10 Num keys per lock : 4 Compression : snappy Creating 536870912 locks 2013/02/20-16:59:32 Starting database operations Created bg thread 0x7f9ebcfff700 2013/02/20-16:59:37 Reopening database for the 1th time 2013/02/20-16:59:46 Reopening database for the 2th time 2013/02/20-16:59:57 Reopening database for the 3th time 2013/02/20-17:00:11 Reopening database for the 4th time 2013/02/20-17:00:25 Reopening database for the 5th time 2013/02/20-17:00:36 Reopening database for the 6th time 2013/02/20-17:00:47 Reopening database for the 7th time 2013/02/20-17:00:59 Reopening database for the 8th time 2013/02/20-17:01:10 Reopening database for the 9th time 2013/02/20-17:01:20 Reopening database for the 10th time 2013/02/20-17:01:31 Reopening database for the 11th time 2013/02/20-17:01:31 Starting verification Stress Test : 109.125 micros/op 22191 ops/sec : Wrote 0.00 MB (0.23 MB/sec) (59% of 32 ops) : Deleted 10 times 2013/02/20-17:01:31 Verification successful Revert Plan: OK Task ID: Reviewers: dhruba, emayanke Reviewed By: emayanke CC: leveldb Differential Revision: the rocksdb stress test Summary: Fixed a bug in the stress-test where the correct size was not being passed to GenerateValue. This bug was there since the beginning but assertions were switched on in our code-base only recently. Added comments on the top detailing how the stress test works and how to quicken/slow it down after investigation. Test Plan: make all check. ./db_stress Reviewers: dhruba, asad Reviewed By: dhruba CC: vamsi, sheki, heyongqiang, zshao Differential Revision: histogram in statistics.h Summary: * Introduce is histogram in statistics.h * stop watch to measure time. * introduce two timers as a poc. Replaced NULL with nullptr to fight some lint errors Should be useful for google. Test Plan: ran db_bench and check stats. make all check Reviewers: dhruba, heyongqiang Reviewed By: dhruba CC: leveldb Differential Revision:"
,,0.0577,rocksdb,Fix memtable construction in tests/
,,0.1476,rocksdb,"Ignore missing column families Summary: Before this diff, whenever we Write to non-existing column family, Write() would fail. This diff adds an option to not fail a Write() when WriteBatch points to non-existing column family. MongoDB said this would be useful for them, since they might have a transaction updating an index that was dropped by another thread. This way, they dont have to worry about checking if all indexes are alive on every write. They dont care if they lose writes to dropped index. Test Plan: added a small unit test Reviewers: sdong, yhchiang, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision:"
,,0.1416,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1322,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.5198,rocksdb,"Deprecate purge_redundant_kvs_while_flush Summary: This option is guarding the feature implemented 2 and a half years ago: D8991. The feature was enabled by default back then and has been running without issues. There is no reason why any client would turn this feature off. I found no reference in fbcode. Test Plan: none Reviewers: sdong, yhchiang, anthony, dhruba Reviewed By: dhruba Subscribers: dhruba, leveldb Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.1322,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.3883,rocksdb,"Transaction error statuses Summary: Based on feedback from spetrunia, we should better differentiate error statuses for transaction failures. Test Plan: unit tests Reviewers: rven, kradhakrishnan, spetrunia, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Transactions Summary: Initial implementation of Pessimistic Transactions. This diff contains the api changes discussed in D38913. This diff is pretty large, so let me know if people would prefer to meet up to discuss it. MyRocks folks: please take a look at the API in include/rocksdb/utilities/transaction[_db].h and let me know if you have any issues. Also, youll notice a couple of TODOs in the implementation of RollbackToSavePoint(). After chatting with Siying, Im going to send out a separate diff for an alternate implementation of this feature that implements the rollback inside of WriteBatch/WriteBatchWithIndex. We can then decide which route is preferable. Next, Im planning on doing some perf testing and then integrating this diff into MongoRocks for further testing. Test Plan: Unit tests, db_bench parallel testing. Reviewers: igor, rven, sdong, yhchiang, yoshinorim Reviewed By: sdong Subscribers: hermanlee4, maykov, spetrunia, leveldb, dhruba Differential Revision: WriteOptions::timeout_hint_us Summary: In one of our recent meetings, we discussed deprecating features that are not being actively used. One of those features, at least within Facebook, is timeout_hint. The feature is really nicely implemented, but if nobody needs it, we should remove it from our code-base (until we get a valid use-case). Some arguments: * Less code better icache hit rate, smaller builds, simpler code * The motivation for adding timeout_hint_us was to work-around RocksDBs stall issue. However, were currently addressing the stall issue itself (see recent work on stall write_rate), so we should never see sharp lock-ups in the future. * Nobody is using the feature within Facebooks code-base. Googling for `timeout_hint_us` also doesnt yield any users. Test Plan: make check Reviewers: anthony, kradhakrishnan, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, dhruba, leveldb Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.1495,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1322,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1197,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.126,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1307,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1275,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1395,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/Allow EventListener::OnCompactionCompleted to return CompactionJobStats. Summary: Allow EventListener::OnCompactionCompleted to return CompactionJobStats, which contains useful information about a compaction. Example CompactionJobStats returned by OnCompactionCompleted(): smallest_output_key_prefix 05000000 largest_output_key_prefix 06990000 elapsed_time 42419 num_input_records 300 num_input_files 3 num_input_files_at_output_level 2 num_output_records 200 num_output_files 1 actual_bytes_input 167200 actual_bytes_output 110688 total_input_raw_key_bytes 5400 total_input_raw_value_bytes 300000 num_records_replaced 100 is_manual_compaction 1 Test Plan: Developed a mega test in db_test which covers 20 variables in CompactionJobStats. Reviewers: rven, igor, anthony, sdong Reviewed By: sdong Subscribers: tnovak, dhruba, leveldb Differential Revision:"
,,0.599,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.5243,rocksdb,"Deprecate purge_redundant_kvs_while_flush Summary: This option is guarding the feature implemented 2 and a half years ago: D8991. The feature was enabled by default back then and has been running without issues. There is no reason why any client would turn this feature off. I found no reference in fbcode. Test Plan: none Reviewers: sdong, yhchiang, anthony, dhruba Reviewed By: dhruba Subscribers: dhruba, leveldb Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.6047,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.1385,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1322,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1244,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1519,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/"
,,0.1463,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1777,rocksdb,"Fix public API dependency on internal codes and dependency on MAX_INT32 Summary: Public API depends on port/port.h which is wrong. Fix it. Also with gcc 4.8.1 build was broken as MAX_INT32 was not recognized. Fix it by using ::max in linux. Test Plan: Build it and try to build an external project on top of it. Reviewers: anthony, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: yoshinorim, leveldb, dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1322,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.2481,rocksdb,"Parallelize L0-L1 Compaction: Restructure Compaction Job Summary: As of now compactions involving files from Level 0 and Level 1 are single threaded because the files in L0, although sorted, are not range partitioned like the other levels. This means that during L0-L1 compaction each file from L1 needs to be merged with potentially all the files from L0. This attempt to parallelize the L0-L1 compaction assigns a thread and a corresponding iterator to each L1 file that then considers only the key range found in that L1 file and only the L0 files that have those keys (and only the specific portion of those L0 files in which those keys are found). In this way the overlap is minimized and potentially eliminated between different iterators focusing on the same files. The first step is to restructure the compaction logic to break L0-L1 compactions into multiple, smaller, sequential compactions. Eventually each of these smaller jobs will be run simultaneously. Areas to pay extra attention to are Correct aggregation of compaction job statistics across multiple threads Proper opening/closing of output files (make sure each threads is unique) Keys that span multiple L1 files Skewed distributions of keys within L0 files Test Plan: Make and run db_test (newer version has separate compaction tests) and compaction_job_stats_test Reviewers: igor, noetzli, anthony, sdong, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, dhruba, leveldb Differential Revision: purge_redundant_kvs_while_flush Summary: This option is guarding the feature implemented 2 and a half years ago: D8991. The feature was enabled by default back then and has been running without issues. There is no reason why any client would turn this feature off. I found no reference in fbcode. Test Plan: none Reviewers: sdong, yhchiang, anthony, dhruba Reviewed By: dhruba Subscribers: dhruba, leveldb Differential Revision: GCC compilation issues invalid suffix on literal no return statement in function returning non-void CuckooStep::operator= extra qualification rocksdb::spatial::Variant:: dereferencing type-punned pointer will break strict-aliasing rules/Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov DB::Open() when the requested compression is not available Summary: Currently RocksDB silently ignores this issue and doesnt compress the data. Based on discussion, we agree that this is pretty bad because it can cause confusion for our users. This patch fails DB::Open() if we dont support the compression that is specified in the options. Test Plan: make check with LZ4 not present. If Snappy is not present all tests will just fail because Snappy is our default library. We should make Snappy the requirement, since without it our default DB::Open() fails. Reviewers: sdong, MarkCallaghan, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: dump DBOptions for each column family Summary: Currently we dump DBOptions for each column family options we dump. This leads to duplicate lines in our LOG file. This diff fixes that. Test Plan: Check out the LOG Reviewers: sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: IslamAbdelRahman, yoshinorim, dhruba, leveldb Differential Revision: EventListener in stress test. Summary: Include EventListener in stress test. Test Plan: make blackbox_crash_test whitebox_crash_test Reviewers: anthony, igor, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: the log-level of DB summary and options from INFO_LEVEL to WARN_LEVEL Summary: Change the log-level of DB summary and options from INFO_LEVEL to WARN_LEVEL Test Plan: Use db_bench to verify the log level. Sample output: 2015/05/22-00:20:39.778064 7fff75b41300 [WARN] RocksDB version: 3.11.0 2015/05/22-00:20:39.778095 7fff75b41300 [WARN] Git sha rocksdb_build_git_sha:7fee8775a459134c4cb04baae5bd1687e268f2a0 2015/05/22-00:20:39.778099 7fff75b41300 [WARN] Compile date May 22 2015 2015/05/22-00:20:39.778101 7fff75b41300 [WARN] DB SUMMARY 2015/05/22-00:20:39.778145 7fff75b41300 [WARN] SST files in /tmp/rocksdbtest-691931916/dbbench dir, Total Num: 0, files: 2015/05/22-00:20:39.778148 7fff75b41300 [WARN] Write Ahead Log file in /tmp/rocksdbtest-691931916/dbbench: 2015/05/22-00:20:39.778150 7fff75b41300 [WARN] Options.error_if_exists: 0 2015/05/22-00:20:39.778152 7fff75b41300 [WARN] Options.create_if_missing: 1 2015/05/22-00:20:39.778153 7fff75b41300 [WARN] Options.paranoid_checks: 1 Reviewers: MarkCallaghan, igor, kradhakrishnan Reviewed By: igor Subscribers: sdong, dhruba, leveldb Differential Revision: stats_dump_period_sec to 600 by default Summary: Having stats in our LOG more often will help a lot with perf debugging. Test Plan: none Reviewers: sdong, MarkCallaghan Reviewed By: MarkCallaghan Subscribers: dhruba, leveldb Differential Revision:"
,,0.1354,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1291,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.3547,rocksdb,"Transaction error statuses Summary: Based on feedback from spetrunia, we should better differentiate error statuses for transaction failures. Test Plan: unit tests Reviewers: rven, kradhakrishnan, spetrunia, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Transactions Summary: Initial implementation of Pessimistic Transactions. This diff contains the api changes discussed in D38913. This diff is pretty large, so let me know if people would prefer to meet up to discuss it. MyRocks folks: please take a look at the API in include/rocksdb/utilities/transaction[_db].h and let me know if you have any issues. Also, youll notice a couple of TODOs in the implementation of RollbackToSavePoint(). After chatting with Siying, Im going to send out a separate diff for an alternate implementation of this feature that implements the rollback inside of WriteBatch/WriteBatchWithIndex. We can then decide which route is preferable. Next, Im planning on doing some perf testing and then integrating this diff into MongoRocks for further testing. Test Plan: Unit tests, db_bench parallel testing. Reviewers: igor, rven, sdong, yhchiang, yoshinorim Reviewed By: sdong Subscribers: hermanlee4, maykov, spetrunia, leveldb, dhruba Differential Revision: FileExists API Summary: Add new CheckFileExists method. Considered changing the FileExists api but didnt want to break anyones builds. Test Plan: unit tests Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: WriteOptions::timeout_hint_us Summary: In one of our recent meetings, we discussed deprecating features that are not being actively used. One of those features, at least within Facebook, is timeout_hint. The feature is really nicely implemented, but if nobody needs it, we should remove it from our code-base (until we get a valid use-case). Some arguments: * Less code better icache hit rate, smaller builds, simpler code * The motivation for adding timeout_hint_us was to work-around RocksDBs stall issue. However, were currently addressing the stall issue itself (see recent work on stall write_rate), so we should never see sharp lock-ups in the future. * Nobody is using the feature within Facebooks code-base. Googling for `timeout_hint_us` also doesnt yield any users. Test Plan: make check Reviewers: anthony, kradhakrishnan, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, dhruba, leveldb Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.1322,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.2899,rocksdb,"[wal changes 3/3] method in DB to sync WAL without blocking writers Summary: Subj. We really need this feature. Previous diff D40899 has most of the changes to make this possible, this diff just adds the method. Test Plan: `make check`, the new test fails without this diff; ran with ASAN, TSAN and valgrind. Reviewers: igor, rven, IslamAbdelRahman, anthony, kradhakrishnan, tnovak, yhchiang, sdong Reviewed By: sdong Subscribers: MarkCallaghan, maykov, hermanlee4, yoshinorim, tnovak, dhruba Differential Revision: live data size estimate Summary: Fixes T6548822. Added a new function for estimating the size of the live data as proposed in the task. The value can be accessed through the property rocksdb.estimate-live-data-size. Test Plan: There are two unit tests in version_set_test and a simple test in db_test. make version_set_test && ./version_set_test; make db_test && ./db_test gtest_filter=GetProperty Reviewers: rven, igor, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: public API dependency on internal codes and dependency on MAX_INT32 Summary: Public API depends on port/port.h which is wrong. Fix it. Also with gcc 4.8.1 build was broken as MAX_INT32 was not recognized. Fix it by using ::max in linux. Test Plan: Build it and try to build an external project on top of it. Reviewers: anthony, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: yoshinorim, leveldb, dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/Allow GetApproximateSize() to include mem table size if it is skip list memtable Summary: Add an option in GetApproximateSize() so that the result will include estimated sizes in mem tables. To implement it, implement an estimated count from the beginning to a key in skip list. The approach is to count to find the entry, how many Next() is issued from each level, and sum them with a weight that is factor> ^ Test Plan: Add a test case Subscribers: leveldb, dhruba Differential Revision: Bug: CompactRange() doesnt change to correct level caused by using wrong level Summary: In previous change , while renaming parameters, use a wrong parameter, causing CompactRange() to compact not wrong level. Test Plan: Run ""DBTest.MigrateToDynamicLevelMaxBytesBase"" which failed with the patch. Reviewers: rven, yhchiang, kradhakrishnan, igor, anthony Subscribers: leveldb, dhruba Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.1307,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1291,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1369,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1197,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.3457,rocksdb,"[wal changes 3/3] method in DB to sync WAL without blocking writers Summary: Subj. We really need this feature. Previous diff D40899 has most of the changes to make this possible, this diff just adds the method. Test Plan: `make check`, the new test fails without this diff; ran with ASAN, TSAN and valgrind. Reviewers: igor, rven, IslamAbdelRahman, anthony, kradhakrishnan, tnovak, yhchiang, sdong Reviewed By: sdong Subscribers: MarkCallaghan, maykov, hermanlee4, yoshinorim, tnovak, dhruba Differential Revision: L0-L1 Compaction: Restructure Compaction Job Summary: As of now compactions involving files from Level 0 and Level 1 are single threaded because the files in L0, although sorted, are not range partitioned like the other levels. This means that during L0-L1 compaction each file from L1 needs to be merged with potentially all the files from L0. This attempt to parallelize the L0-L1 compaction assigns a thread and a corresponding iterator to each L1 file that then considers only the key range found in that L1 file and only the L0 files that have those keys (and only the specific portion of those L0 files in which those keys are found). In this way the overlap is minimized and potentially eliminated between different iterators focusing on the same files. The first step is to restructure the compaction logic to break L0-L1 compactions into multiple, smaller, sequential compactions. Eventually each of these smaller jobs will be run simultaneously. Areas to pay extra attention to are Correct aggregation of compaction job statistics across multiple threads Proper opening/closing of output files (make sure each threads is unique) Keys that span multiple L1 files Skewed distributions of keys within L0 files Test Plan: Make and run db_test (newer version has separate compaction tests) and compaction_job_stats_test Reviewers: igor, noetzli, anthony, sdong, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, dhruba, leveldb Differential Revision: purge_redundant_kvs_while_flush Summary: This option is guarding the feature implemented 2 and a half years ago: D8991. The feature was enabled by default back then and has been running without issues. There is no reason why any client would turn this feature off. I found no reference in fbcode. Test Plan: none Reviewers: sdong, yhchiang, anthony, dhruba Reviewed By: dhruba Subscribers: dhruba, leveldb Differential Revision: WriteOptions::timeout_hint_us Summary: In one of our recent meetings, we discussed deprecating features that are not being actively used. One of those features, at least within Facebook, is timeout_hint. The feature is really nicely implemented, but if nobody needs it, we should remove it from our code-base (until we get a valid use-case). Some arguments: * Less code better icache hit rate, smaller builds, simpler code * The motivation for adding timeout_hint_us was to work-around RocksDBs stall issue. However, were currently addressing the stall issue itself (see recent work on stall write_rate), so we should never see sharp lock-ups in the future. * Nobody is using the feature within Facebooks code-base. Googling for `timeout_hint_us` also doesnt yield any users. Test Plan: make check Reviewers: anthony, kradhakrishnan, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, dhruba, leveldb Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov DB::Open() when the requested compression is not available Summary: Currently RocksDB silently ignores this issue and doesnt compress the data. Based on discussion, we agree that this is pretty bad because it can cause confusion for our users. This patch fails DB::Open() if we dont support the compression that is specified in the options. Test Plan: make check with LZ4 not present. If Snappy is not present all tests will just fail because Snappy is our default library. We should make Snappy the requirement, since without it our default DB::Open() fails. Reviewers: sdong, MarkCallaghan, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: dump DBOptions for each column family Summary: Currently we dump DBOptions for each column family options we dump. This leads to duplicate lines in our LOG file. This diff fixes that. Test Plan: Check out the LOG Reviewers: sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: IslamAbdelRahman, yoshinorim, dhruba, leveldb Differential Revision: branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/Include EventListener in stress test. Summary: Include EventListener in stress test. Test Plan: make blackbox_crash_test whitebox_crash_test Reviewers: anthony, igor, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: stats_dump_period_sec to 600 by default Summary: Having stats in our LOG more often will help a lot with perf debugging. Test Plan: none Reviewers: sdong, MarkCallaghan Reviewed By: MarkCallaghan Subscribers: dhruba, leveldb Differential Revision:"
,,0.5603,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/Support saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.3192,rocksdb,"[wal changes 3/3] method in DB to sync WAL without blocking writers Summary: Subj. We really need this feature. Previous diff D40899 has most of the changes to make this possible, this diff just adds the method. Test Plan: `make check`, the new test fails without this diff; ran with ASAN, TSAN and valgrind. Reviewers: igor, rven, IslamAbdelRahman, anthony, kradhakrishnan, tnovak, yhchiang, sdong Reviewed By: sdong Subscribers: MarkCallaghan, maykov, hermanlee4, yoshinorim, tnovak, dhruba Differential Revision: branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/Allow GetApproximateSize() to include mem table size if it is skip list memtable Summary: Add an option in GetApproximateSize() so that the result will include estimated sizes in mem tables. To implement it, implement an estimated count from the beginning to a key in skip list. The approach is to count to find the entry, how many Next() is issued from each level, and sum them with a weight that is factor> ^ Test Plan: Add a test case Subscribers: leveldb, dhruba Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.147,rocksdb,"Allow GetApproximateSize() to include mem table size if it is skip list memtable Summary: Add an option in GetApproximateSize() so that the result will include estimated sizes in mem tables. To implement it, implement an estimated count from the beginning to a key in skip list. The approach is to count to find the entry, how many Next() is issued from each level, and sum them with a weight that is factor> ^ Test Plan: Add a test case Subscribers: leveldb, dhruba Differential Revision:"
,,0.3002,rocksdb,"Deprecate WriteOptions::timeout_hint_us Summary: In one of our recent meetings, we discussed deprecating features that are not being actively used. One of those features, at least within Facebook, is timeout_hint. The feature is really nicely implemented, but if nobody needs it, we should remove it from our code-base (until we get a valid use-case). Some arguments: * Less code better icache hit rate, smaller builds, simpler code * The motivation for adding timeout_hint_us was to work-around RocksDBs stall issue. However, were currently addressing the stall issue itself (see recent work on stall write_rate), so we should never see sharp lock-ups in the future. * Nobody is using the feature within Facebooks code-base. Googling for `timeout_hint_us` also doesnt yield any users. Test Plan: make check Reviewers: anthony, kradhakrishnan, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, dhruba, leveldb Differential Revision:"
,,0.1672,rocksdb,"Fix public API dependency on internal codes and dependency on MAX_INT32 Summary: Public API depends on port/port.h which is wrong. Fix it. Also with gcc 4.8.1 build was broken as MAX_INT32 was not recognized. Fix it by using ::max in linux. Test Plan: Build it and try to build an external project on top of it. Reviewers: anthony, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: yoshinorim, leveldb, dhruba Differential Revision: a uncleaned counter in PerfContext::Reset() Summary: new_table_iterator_nanos is not cleaned in PerfContext::Reset() while new_table_block_iter_nanos is cleaned twice. Fix it. Also fix a comment. Test Plan: Build and db_bench with to see the value shown. Reviewers: kradhakrishnan, anthony, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.3148,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.1697,rocksdb,"Tests to avoid to use TMPDIR directly Summary: Directly using TMPDIR can cause problems when running tests using parallel option. Fix them. Test Plan: Run all tests in parallel Reviewers: kradhakrishnan, yhchiang, IslamAbdelRahman, anthony Reviewed By: anthony Subscribers: leveldb, dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1307,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1624,rocksdb,"Tests to avoid to use TMPDIR directly Summary: Directly using TMPDIR can cause problems when running tests using parallel option. Fix them. Test Plan: Run all tests in parallel Reviewers: kradhakrishnan, yhchiang, IslamAbdelRahman, anthony Reviewed By: anthony Subscribers: leveldb, dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.0673,rocksdb,Merge branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/
,,0.1014,rocksdb,"Fix CYGWin release build Summary: Change from one std::to_string() to ToString() for Cygwin build Test Plan: Build it under cygwin Reviewers: rven, anthony, IslamAbdelRahman, igor, kradhakrishnan Reviewed By: igor, kradhakrishnan Subscribers: leveldb, dhruba Differential Revision:"
,,0.212,rocksdb,"Pessimistic Transactions Summary: Initial implementation of Pessimistic Transactions. This diff contains the api changes discussed in D38913. This diff is pretty large, so let me know if people would prefer to meet up to discuss it. MyRocks folks: please take a look at the API in include/rocksdb/utilities/transaction[_db].h and let me know if you have any issues. Also, youll notice a couple of TODOs in the implementation of RollbackToSavePoint(). After chatting with Siying, Im going to send out a separate diff for an alternate implementation of this feature that implements the rollback inside of WriteBatch/WriteBatchWithIndex. We can then decide which route is preferable. Next, Im planning on doing some perf testing and then integrating this diff into MongoRocks for further testing. Test Plan: Unit tests, db_bench parallel testing. Reviewers: igor, rven, sdong, yhchiang, yoshinorim Reviewed By: sdong Subscribers: hermanlee4, maykov, spetrunia, leveldb, dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov WriteEntry on WBWIIterator::Entry() Summary: [This is the resubmit of D39813. Tests were failing, so I reverted the diff. I found the bug and Im now resubmitting] If we dont do this, any calls to Entry() after WBWI mutation will result in undefined behavior. We need to re-fetch the offset from the skip list and regenerate the new pointer (because strings base pointer can change while mutating). Test Plan: COMPILE_WITH_ASAN=1 make write_batch_with_index_test && ./write_batch_with_index_test Reviewers: sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: ""Fix compile"" This reverts commit 51440f83ec82b4b3fa54c1963f58d07a8b2c2810. Revert ""Re-generate WriteEntry on WBWIIterator::Entry()"" This reverts commit 4949ef08db89bdc63028679d1cc11312094e860e./Fix compile Summary: Ooops, sorry about this. Test Plan: compiles Reviewers: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1307,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1788,rocksdb,"Tests to avoid to use TMPDIR directly Summary: Directly using TMPDIR can cause problems when running tests using parallel option. Fix them. Test Plan: Run all tests in parallel Reviewers: kradhakrishnan, yhchiang, IslamAbdelRahman, anthony Reviewed By: anthony Subscribers: leveldb, dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/"
,,0.126,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1354,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1354,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1654,rocksdb,"Fix public API dependency on internal codes and dependency on MAX_INT32 Summary: Public API depends on port/port.h which is wrong. Fix it. Also with gcc 4.8.1 build was broken as MAX_INT32 was not recognized. Fix it by using ::max in linux. Test Plan: Build it and try to build an external project on top of it. Reviewers: anthony, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: yoshinorim, leveldb, dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1322,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1338,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1463,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1432,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1354,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1338,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1307,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1307,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.126,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.2426,rocksdb,"Fail DB::Open() when the requested compression is not available Summary: Currently RocksDB silently ignores this issue and doesnt compress the data. Based on discussion, we agree that this is pretty bad because it can cause confusion for our users. This patch fails DB::Open() if we dont support the compression that is specified in the options. Test Plan: make check with LZ4 not present. If Snappy is not present all tests will just fail because Snappy is our default library. We should make Snappy the requirement, since without it our default DB::Open() fails. Reviewers: sdong, MarkCallaghan, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: EventListener::OnCompactionCompleted to return CompactionJobStats. Summary: Allow EventListener::OnCompactionCompleted to return CompactionJobStats, which contains useful information about a compaction. Example CompactionJobStats returned by OnCompactionCompleted(): smallest_output_key_prefix 05000000 largest_output_key_prefix 06990000 elapsed_time 42419 num_input_records 300 num_input_files 3 num_input_files_at_output_level 2 num_output_records 200 num_output_files 1 actual_bytes_input 167200 actual_bytes_output 110688 total_input_raw_key_bytes 5400 total_input_raw_value_bytes 300000 num_records_replaced 100 is_manual_compaction 1 Test Plan: Developed a mega test in db_test which covers 20 variables in CompactionJobStats. Reviewers: rven, igor, anthony, sdong Reviewed By: sdong Subscribers: tnovak, dhruba, leveldb Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.5553,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/Make ""make all"" work for CYGWIN Summary: Some test and benchmark codes dont build for CYGWIN. Fix it. Test Plan: Build ""make all"" with TARGET_OS=Cygwin on cygwin and make sure it passes. Reviewers: rven, yhchiang, anthony, igor, kradhakrishnan Reviewed By: igor, kradhakrishnan Subscribers: leveldb, dhruba Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.4208,rocksdb,"Pessimistic Transactions Summary: Initial implementation of Pessimistic Transactions. This diff contains the api changes discussed in D38913. This diff is pretty large, so let me know if people would prefer to meet up to discuss it. MyRocks folks: please take a look at the API in include/rocksdb/utilities/transaction[_db].h and let me know if you have any issues. Also, youll notice a couple of TODOs in the implementation of RollbackToSavePoint(). After chatting with Siying, Im going to send out a separate diff for an alternate implementation of this feature that implements the rollback inside of WriteBatch/WriteBatchWithIndex. We can then decide which route is preferable. Next, Im planning on doing some perf testing and then integrating this diff into MongoRocks for further testing. Test Plan: Unit tests, db_bench parallel testing. Reviewers: igor, rven, sdong, yhchiang, yoshinorim Reviewed By: sdong Subscribers: hermanlee4, maykov, spetrunia, leveldb, dhruba Differential Revision: duplicate code in db_bench/db_stress, fixing typos Summary: While working on single delete support for db_bench, I realized that db_bench/db_stress contain a bunch of duplicate code related to copmression and found some typos. This patch removes duplicate code, typos and a redundant in internal_stats.cc. Test Plan: make db_stress && make db_bench && ./db_bench Reviewers: yhchiang, sdong, rven, anthony, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/Make ""make all"" work for CYGWIN Summary: Some test and benchmark codes dont build for CYGWIN. Fix it. Test Plan: Build ""make all"" with TARGET_OS=Cygwin on cygwin and make sure it passes. Reviewers: rven, yhchiang, anthony, igor, kradhakrishnan Reviewed By: igor, kradhakrishnan Subscribers: leveldb, dhruba Differential Revision: compile on darwin Summary: As title Test Plan: make check Reviewers: anthony Subscribers: dhruba, leveldb Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: artificially inflate L0 score Summary: This turns out to be pretty bad because if we prioritize L0->L1 then L1 can grow artificially large, which makes L0->L1 more and more expensive. For example: 256MB L0 + 256MB L1 512MB L1 256MB L0 + 512MB L1 768MB L1 256MB L0 + 768MB L1 1GB L1 .... 256MB L0 + 10GB L1 10.2GB L1 At some point we need to start compacting L1->L2 to speed up L0->L1. Test Plan: The performance improvement is massive for heavy write workload. This is the benchmark I ran: Before this change, the benchmark took 47 minutes to complete. After, the benchmark finished in 2minutes. You can see full results here: Also, we ran this diff on MongoDB on RocksDB on one replicaset. Before the change, our initial sync was so slow that it couldnt keep up with primary writes. After the change, the import finished without any issues Reviewers: dynamike, MarkCallaghan, rven, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: for db_bench and more IO stats Summary: See for the IO stats. I added ""Cumulative compaction:"" and ""Interval compaction:"" lines. The IO rates can be confusing. Rates fro per-level stats lines, Wr(MB/s) & Rd(MB/s), are computed using the duration of the compaction job. If the job reads 10MB, writes 9MB and the job (IO & merging) takes 1 second then the rates are 10MB/s for read and 9MB/s for writes. The IO rates in the Cumulative compaction line uses the total uptime. The IO rates in the Interval compaction line uses the interval uptime. So these Cumalative & Interval compaction IO rates cannot be compared to the per-level IO rates. But both forms of the rates are useful for debugging perf. Task ID: Blame Rev: Test Plan: run db_bench Revert Plan: Database Impact: Memcache Impact: Other Notes: EImportant: begin *PUBLIC* platform impact section Bugzilla: end platform impact Reviewers: igor Reviewed By: igor Subscribers: dhruba Differential Revision:"
,,0.1369,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.608,rocksdb,"Optimistic Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.5752,rocksdb,"Allow GetApproximateSize() to include mem table size if it is skip list memtable Summary: Add an option in GetApproximateSize() so that the result will include estimated sizes in mem tables. To implement it, implement an estimated count from the beginning to a key in skip list. The approach is to count to find the entry, how many Next() is issued from each level, and sum them with a weight that is factor> ^ Test Plan: Add a test case Subscribers: leveldb, dhruba Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.1273,rocksdb,"Allow GetApproximateSize() to include mem table size if it is skip list memtable Summary: Add an option in GetApproximateSize() so that the result will include estimated sizes in mem tables. To implement it, implement an estimated count from the beginning to a key in skip list. The approach is to count to find the entry, how many Next() is issued from each level, and sum them with a weight that is factor> ^ Test Plan: Add a test case Subscribers: leveldb, dhruba Differential Revision:"
,,0.4244,rocksdb,"Deprecate WriteOptions::timeout_hint_us Summary: In one of our recent meetings, we discussed deprecating features that are not being actively used. One of those features, at least within Facebook, is timeout_hint. The feature is really nicely implemented, but if nobody needs it, we should remove it from our code-base (until we get a valid use-case). Some arguments: * Less code better icache hit rate, smaller builds, simpler code * The motivation for adding timeout_hint_us was to work-around RocksDBs stall issue. However, were currently addressing the stall issue itself (see recent work on stall write_rate), so we should never see sharp lock-ups in the future. * Nobody is using the feature within Facebooks code-base. Googling for `timeout_hint_us` also doesnt yield any users. Test Plan: make check Reviewers: anthony, kradhakrishnan, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, dhruba, leveldb Differential Revision: public API dependency on internal codes and dependency on MAX_INT32 Summary: Public API depends on port/port.h which is wrong. Fix it. Also with gcc 4.8.1 build was broken as MAX_INT32 was not recognized. Fix it by using ::max in linux. Test Plan: Build it and try to build an external project on top of it. Reviewers: anthony, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: yoshinorim, leveldb, dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.2334,rocksdb,"Avoid manipulating const char* arrays Summary: We were manipulating `const char*` arrays in CompactionJob to change the sequence number/types of keys. This patch changes UpdateInternalKey() to use string methods to do the manipulation and updates all calls accordingly. Test Plan: Added test case for UpdateInternalKey() in dbformat_test. make && make check Reviewers: sdong, rven, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.4641,rocksdb,"fixed leaking log::Writers Summary: Fixes valgrind errors in column_family_test. Test Plan: `make check`, `make valgrind_check` Reviewers: igor, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: DB::Open() when the requested compression is not available Summary: Currently RocksDB silently ignores this issue and doesnt compress the data. Based on discussion, we agree that this is pretty bad because it can cause confusion for our users. This patch fails DB::Open() if we dont support the compression that is specified in the options. Test Plan: make check with LZ4 not present. If Snappy is not present all tests will just fail because Snappy is our default library. We should make Snappy the requirement, since without it our default DB::Open() fails. Reviewers: sdong, MarkCallaghan, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: dump DBOptions for each column family Summary: Currently we dump DBOptions for each column family options we dump. This leads to duplicate lines in our LOG file. This diff fixes that. Test Plan: Check out the LOG Reviewers: sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: IslamAbdelRahman, yoshinorim, dhruba, leveldb Differential Revision: EventListener::OnCompactionCompleted to return CompactionJobStats. Summary: Allow EventListener::OnCompactionCompleted to return CompactionJobStats, which contains useful information about a compaction. Example CompactionJobStats returned by OnCompactionCompleted(): smallest_output_key_prefix 05000000 largest_output_key_prefix 06990000 elapsed_time 42419 num_input_records 300 num_input_files 3 num_input_files_at_output_level 2 num_output_records 200 num_output_files 1 actual_bytes_input 167200 actual_bytes_output 110688 total_input_raw_key_bytes 5400 total_input_raw_value_bytes 300000 num_records_replaced 100 is_manual_compaction 1 Test Plan: Developed a mega test in db_test which covers 20 variables in CompactionJobStats. Reviewers: rven, igor, anthony, sdong Reviewed By: sdong Subscribers: tnovak, dhruba, leveldb Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.5720000000000001,rocksdb,"Allow GetApproximateSize() to include mem table size if it is skip list memtable Summary: Add an option in GetApproximateSize() so that the result will include estimated sizes in mem tables. To implement it, implement an estimated count from the beginning to a key in skip list. The approach is to count to find the entry, how many Next() is issued from each level, and sum them with a weight that is factor> ^ Test Plan: Add a test case Subscribers: leveldb, dhruba Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.1322,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.3401,rocksdb,"Report live data size estimate Summary: Fixes T6548822. Added a new function for estimating the size of the live data as proposed in the task. The value can be accessed through the property rocksdb.estimate-live-data-size. Test Plan: There are two unit tests in version_set_test and a simple test in db_test. make version_set_test && ./version_set_test; make db_test && ./db_test gtest_filter=GetProperty Reviewers: rven, igor, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/Fixed a bug of CompactionStats in multi-level universal compaction case Summary: Universal compaction can involves in multiple levels. However, the current implementation of bytes_readn and bytes_readnp1 (and some other stats with postfix `n` and `np1`) assumes compaction can only have two levels. This patch fixes this bug and redefines bytes_readn and bytes_readnp1: * bytes_readnp1: the number of bytes read in the compaction output level. * bytes_readn: the total number of bytes read minus bytes_readnp1 Test Plan: Add a test in compaction_job_stats_test Reviewers: igor, sdong, rven, anthony, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: dhruba, leveldb Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: for db_bench and more IO stats Summary: See for the IO stats. I added ""Cumulative compaction:"" and ""Interval compaction:"" lines. The IO rates can be confusing. Rates fro per-level stats lines, Wr(MB/s) & Rd(MB/s), are computed using the duration of the compaction job. If the job reads 10MB, writes 9MB and the job (IO & merging) takes 1 second then the rates are 10MB/s for read and 9MB/s for writes. The IO rates in the Cumulative compaction line uses the total uptime. The IO rates in the Interval compaction line uses the interval uptime. So these Cumalative & Interval compaction IO rates cannot be compared to the per-level IO rates. But both forms of the rates are useful for debugging perf. Task ID: Blame Rev: Test Plan: run db_bench Revert Plan: Database Impact: Memcache Impact: Other Notes: EImportant: begin *PUBLIC* platform impact section Bugzilla: end platform impact Reviewers: igor Reviewed By: igor Subscribers: dhruba Differential Revision:"
,,0.174,rocksdb,"Fix public API dependency on internal codes and dependency on MAX_INT32 Summary: Public API depends on port/port.h which is wrong. Fix it. Also with gcc 4.8.1 build was broken as MAX_INT32 was not recognized. Fix it by using ::max in linux. Test Plan: Build it and try to build an external project on top of it. Reviewers: anthony, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: yoshinorim, leveldb, dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.0673,rocksdb,Merge branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/
,,0.1119,rocksdb,"Make ""make all"" work for CYGWIN Summary: Some test and benchmark codes dont build for CYGWIN. Fix it. Test Plan: Build ""make all"" with TARGET_OS=Cygwin on cygwin and make sure it passes. Reviewers: rven, yhchiang, anthony, igor, kradhakrishnan Reviewed By: igor, kradhakrishnan Subscribers: leveldb, dhruba Differential Revision:"
,,0.1275,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.0673,rocksdb,Merge branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/
,,0.2548,rocksdb,"Replace %llu with format macros in ParsedInternalKey::DebugString()) Test Plan: successfully compiled the code Reviewers: sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.3089,rocksdb,"Allow GetApproximateSize() to include mem table size if it is skip list memtable Summary: Add an option in GetApproximateSize() so that the result will include estimated sizes in mem tables. To implement it, implement an estimated count from the beginning to a key in skip list. The approach is to count to find the entry, how many Next() is issued from each level, and sum them with a weight that is factor> ^ Test Plan: Add a test case Subscribers: leveldb, dhruba Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.0673,rocksdb,Merge branch master of github.com:facebook/rocksdb D40233: Replace %llu with format macros in ParsedInternalKey::DebugString())/
,,0.5989,rocksdb,"Make ""make all"" work for CYGWIN Summary: Some test and benchmark codes dont build for CYGWIN. Fix it. Test Plan: Build ""make all"" with TARGET_OS=Cygwin on cygwin and make sure it passes. Reviewers: rven, yhchiang, anthony, igor, kradhakrishnan Reviewed By: igor, kradhakrishnan Subscribers: leveldb, dhruba Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.3114,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.2828,rocksdb,"Optimistic Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.1369,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.4082,rocksdb,"Deprecate WriteOptions::timeout_hint_us Summary: In one of our recent meetings, we discussed deprecating features that are not being actively used. One of those features, at least within Facebook, is timeout_hint. The feature is really nicely implemented, but if nobody needs it, we should remove it from our code-base (until we get a valid use-case). Some arguments: * Less code better icache hit rate, smaller builds, simpler code * The motivation for adding timeout_hint_us was to work-around RocksDBs stall issue. However, were currently addressing the stall issue itself (see recent work on stall write_rate), so we should never see sharp lock-ups in the future. * Nobody is using the feature within Facebooks code-base. Googling for `timeout_hint_us` also doesnt yield any users. Test Plan: make check Reviewers: anthony, kradhakrishnan, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, dhruba, leveldb Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.3027,rocksdb,"Allow GetApproximateSize() to include mem table size if it is skip list memtable Summary: Add an option in GetApproximateSize() so that the result will include estimated sizes in mem tables. To implement it, implement an estimated count from the beginning to a key in skip list. The approach is to count to find the entry, how many Next() is issued from each level, and sum them with a weight that is factor> ^ Test Plan: Add a test case Subscribers: leveldb, dhruba Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.1275,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1369,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.1322,rocksdb,"Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
,,0.6382,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.6409,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.6402,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.6402,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.6135,rocksdb,"Fix DynamicBloomTest.concurrent_with_perf to pass TSAN Summary: TSAN fails on DynamicBloomTest.concurrent_with_perf. This change fixes it. Not sure why though. Test Plan: Run the test with TSAN and make sure no warning shown. Reviewers: yhchiang, IslamAbdelRahman, anthony, ngbronson, rven Reviewed By: rven Subscribers: rven, leveldb, dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.5671,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision: options.row_cache Summary: options.row_cache should already been initialized as null by default. Still try to set it following current convention, because one valgrind failure reports a failure related to it. Test Plan: Run all unit tests Reviewers: yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
,,0.6355,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.2799,rocksdb,"Running manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: clang build Summary: Fix clang Test Plan: make check Reviewers: sdong, yhchiang, rven Subscribers: dhruba Differential Revision:"
,,0.1909,rocksdb,"Eliminate duplicated property constants Summary: Before this diff, there were duplicated constants to refer to properties (user- facing API had strings and InternalStats had an enum). I noticed these were inconsistent in terms of which constants are provided, names of constants, and documentation of constants. Overall it seemed annoying/error-prone to maintain these duplicated constants. So, this diff gets rid of InternalStatss constants and replaces them with a map keyed on the user-facing constant. The value in that map contains a function pointer to get the property value, so we dont need to do string matching while holding db->mutex_. This approach has a side benefit of making many small handler functions rather than a giant switch-statement. Test Plan: db_properties_test passes, running ""make commit-prereq Reviewers: sdong, yhchiang, kradhakrishnan, IslamAbdelRahman, rven, anthony Reviewed By: anthony Subscribers: dhruba, leveldb Differential Revision: call to install superversion and schedule work in enableautocompactions Summary: This patch fixes There is a recent change in rocksdb to disable auto compactions on startup: However, there is a small timing window where a column family needs to be compacted and schedules a compaction, but the scheduled compaction fails when it checks the disable_auto_compactions setting. The expectation is once the application is ready, it will call EnableAutoCompactions() to allow new compactions to go through. However, if the Column family is stalled because L0 is full, and no writes can go through, it is possible the column family may never have a new compaction request get scheduled. EnableAutoCompaction() should probably schedule an new flush and compaction event when it resets disable_auto_compaction. Using InstallSuperVersionAndScheduleWork, we call SchedulePendingFlush, SchedulePendingCompaction, as well as MaybeScheduleFlushOrcompaction on all the column families to avoid the situation above. This is still a first pass for feedback. Could also just call SchedePendingFlush and SchedulePendingCompaction directly. Test Plan: Run on Asan build cd _build-5.6-ASan/ && ./mysql-test/mtr rocksdb_rpl.rpl_rocksdb_stress_crash Ensure that it no longer hangs during the test. Reviewers: hermanlee4, yhchiang, anthony Reviewed By: anthony Subscribers: leveldb, yhchiang, dhruba Differential Revision: public api to schedule flush/compaction, code to prevent race with db::open Summary: Fixes T8781168. Added a new function EnableAutoCompactions in db.h to be publicly avialable. This allows compaction to be re-enabled after disabling it via SetOptions Refactored code to set the dbptr earlier on in TransactionDB::Open and DB::Open Temporarily disable auto_compaction in TransactionDB::Open until dbptr is set to prevent race condition. Test Plan: Ran make all check verified fix on myrocks side: was able to reproduce the seg fault with ../tools/mysqltest.sh rocksdb.drop_table method was to manually sleep the thread after DB::Open but before TransactionDB ptr was assigned in transaction_db_impl.cc: DB::Open(db_options, dbname, column_families_copy, handles, &db); clock_t goal (60000 * 10) + clock(); while (goal > clock()); ...dbptr(aka rdb) gets assigned below verified my changes fixed the issue. Also added unit test ToggleAutoCompaction in transaction_test.cc Reviewers: hermanlee4, anthony Reviewed By: anthony Subscribers: alex, dhruba Differential Revision:"
,,0.2277,rocksdb,"When slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision:"
,,0.5124,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision: when writing to the last write buffer Summary: Now if inserting to mem table is much faster than writing to files, there is no mechanism users can rely on to avoid stopping for reaching options.max_write_buffer_number. With the commit, if there are more than four maximum write buffers configured, we slow down to the rate of options.delayed_write_rate while we reach the last one. Test Plan: 1. Add a new unit test. 2. Run db_bench with ./db_bench based on hard drive and see stopping is avoided with the commit. Reviewers: yhchiang, IslamAbdelRahman, anthony, rven, kradhakrishnan, igor Reviewed By: igor Subscribers: MarkCallaghan, leveldb, dhruba Differential Revision: manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision: pull request from charsyam/feature/typos fix typos in comments/fix typos in comments/Lint everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.2153,rocksdb,"added public api to schedule flush/compaction, code to prevent race with db::open Summary: Fixes T8781168. Added a new function EnableAutoCompactions in db.h to be publicly avialable. This allows compaction to be re-enabled after disabling it via SetOptions Refactored code to set the dbptr earlier on in TransactionDB::Open and DB::Open Temporarily disable auto_compaction in TransactionDB::Open until dbptr is set to prevent race condition. Test Plan: Ran make all check verified fix on myrocks side: was able to reproduce the seg fault with ../tools/mysqltest.sh rocksdb.drop_table method was to manually sleep the thread after DB::Open but before TransactionDB ptr was assigned in transaction_db_impl.cc: DB::Open(db_options, dbname, column_families_copy, handles, &db); clock_t goal (60000 * 10) + clock(); while (goal > clock()); ...dbptr(aka rdb) gets assigned below verified my changes fixed the issue. Also added unit test ToggleAutoCompaction in transaction_test.cc Reviewers: hermanlee4, anthony Reviewed By: anthony Subscribers: alex, dhruba Differential Revision:"
,,0.6209,rocksdb,"Explictly fail when memtable doesnt support concurrent insert Summary: If users turn on concurrent insert but the memtable doesnt support it, they might see unexcepted crash. Fix it by explicitly fail. Test Plan: Run different setting of stress_test and make sure it fails correctly. Will add a unit test too. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, andrewkr, ngbronson Reviewed By: ngbronson Subscribers: leveldb, dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.5656,rocksdb,"Disable stats about mutex duration by default Summary: Measuring mutex duration will measure time inside DB mutex, which breaks our best practice. Add a stat level in Statistics class. By default, disable to measure the mutex operations. Test Plan: Add a unit test to make sure it is off by default. Reviewers: rven, anthony, IslamAbdelRahman, kradhakrishnan, andrewkr, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, leveldb, dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.10300000000000001,rocksdb,"Merge pull request from charsyam/feature/typos fix typos in comments/fix typos in comments/Lint everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.1066,rocksdb,"Merge pull request from charsyam/feature/typos fix typos in comments/fix typos in comments/Lint everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.0967,rocksdb,"Lint everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.0967,rocksdb,"Lint everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.2971,rocksdb,"Merge pull request from zhipeng-jia/fix_clang_warning Fix clang warnings/Fix clang warnings regarding unnecessary std::move/Running manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision:"
,,0.4258,rocksdb,"Improve perf of Pessimistic Transaction expirations (and optimistic transactions) Summary: copy from task 8196669: 1) Optimistic transactions do not support batching writes from different threads. 2) Pessimistic transactions do not support batching writes if an expiration time is set. In these 2 cases, we currently do not do any write batching in DBImpl::WriteImpl() because there is a WriteCallback that could decide at the last minute to abort the write. But we could support batching write operations with callbacks if we make sure to process the callbacks correctly. To do this, we would first need to modify write_thread.cc to stop preventing writes with callbacks from being batched together. Then we would need to change DBImpl::WriteImpl() to call all WriteCallbacks in a batch, only write the batches that succeed, and correctly set the state of each batchs WriteThread::Writer. Test Plan: Added test WriteWithCallbackTest to write_callback_test.cc which creates multiple client threads and verifies that writes are batched and executed properly. Reviewers: hermanlee4, anthony, ngbronson Subscribers: leveldb, dhruba Differential Revision:"
,,0.2302,rocksdb,"added public api to schedule flush/compaction, code to prevent race with db::open Summary: Fixes T8781168. Added a new function EnableAutoCompactions in db.h to be publicly avialable. This allows compaction to be re-enabled after disabling it via SetOptions Refactored code to set the dbptr earlier on in TransactionDB::Open and DB::Open Temporarily disable auto_compaction in TransactionDB::Open until dbptr is set to prevent race condition. Test Plan: Ran make all check verified fix on myrocks side: was able to reproduce the seg fault with ../tools/mysqltest.sh rocksdb.drop_table method was to manually sleep the thread after DB::Open but before TransactionDB ptr was assigned in transaction_db_impl.cc: DB::Open(db_options, dbname, column_families_copy, handles, &db); clock_t goal (60000 * 10) + clock(); while (goal > clock()); ...dbptr(aka rdb) gets assigned below verified my changes fixed the issue. Also added unit test ToggleAutoCompaction in transaction_test.cc Reviewers: hermanlee4, anthony Reviewed By: anthony Subscribers: alex, dhruba Differential Revision:"
,,0.4296,rocksdb,"Improve perf of Pessimistic Transaction expirations (and optimistic transactions) Summary: copy from task 8196669: 1) Optimistic transactions do not support batching writes from different threads. 2) Pessimistic transactions do not support batching writes if an expiration time is set. In these 2 cases, we currently do not do any write batching in DBImpl::WriteImpl() because there is a WriteCallback that could decide at the last minute to abort the write. But we could support batching write operations with callbacks if we make sure to process the callbacks correctly. To do this, we would first need to modify write_thread.cc to stop preventing writes with callbacks from being batched together. Then we would need to change DBImpl::WriteImpl() to call all WriteCallbacks in a batch, only write the batches that succeed, and correctly set the state of each batchs WriteThread::Writer. Test Plan: Added test WriteWithCallbackTest to write_callback_test.cc which creates multiple client threads and verifies that writes are batched and executed properly. Reviewers: hermanlee4, anthony, ngbronson Subscribers: leveldb, dhruba Differential Revision:"
,,0.0948,rocksdb,"Lint everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.0833,rocksdb,compaction assertion triggering test fix for sequence zeroing assertion trip/
,,0.0948,rocksdb,"Lint everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.2507,rocksdb,"Merge pull request from shuzhang1989/fix_envhdfs_virtual_func fix vfunc inconsistency between env_hdfs and env/fix inconsistency between env_hdfs and env/Merge pull request from zhangyybuaa/fix_hdfs_error Fix build error with hdfs/Running manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: LinkFile() undefined reference error/"
,,0.6004,rocksdb,"Fix CLANG errors introduced by 7d87f02799bd0a8fd36df24fab5baa4968615c86 Summary: Fix some CLANG errors introduced in 7d87f02799bd0a8fd36df24fab5baa4968615c86 Test Plan: Build with both of CLANG and gcc Reviewers: rven, yhchiang, kradhakrishnan, anthony, IslamAbdelRahman, ngbronson Subscribers: leveldb, dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision: manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision:"
,,0.4239,rocksdb,"Improve perf of Pessimistic Transaction expirations (and optimistic transactions) Summary: copy from task 8196669: 1) Optimistic transactions do not support batching writes from different threads. 2) Pessimistic transactions do not support batching writes if an expiration time is set. In these 2 cases, we currently do not do any write batching in DBImpl::WriteImpl() because there is a WriteCallback that could decide at the last minute to abort the write. But we could support batching write operations with callbacks if we make sure to process the callbacks correctly. To do this, we would first need to modify write_thread.cc to stop preventing writes with callbacks from being batched together. Then we would need to change DBImpl::WriteImpl() to call all WriteCallbacks in a batch, only write the batches that succeed, and correctly set the state of each batchs WriteThread::Writer. Test Plan: Added test WriteWithCallbackTest to write_callback_test.cc which creates multiple client threads and verifies that writes are batched and executed properly. Reviewers: hermanlee4, anthony, ngbronson Subscribers: leveldb, dhruba Differential Revision:"
,,0.2506,rocksdb,"When slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision: pull request from charsyam/feature/typos fix typos in comments/fix typos in comments/"
,,0.2531,rocksdb,"Changes for build on solaris Makefile adjust paths for solaris build Makefile enable _GLIBCXX_USE_C99 so that std::to_string is available db_compaction_test.cc Initialise a variable to avoid a compilation error db_impl.cc Include db_test.cc Include Environment.java recognise solaris envrionment options_bulder.cc Make log unambiguous geodb_impl.cc Make log and floor unambiguous/DeleteFilesInRange: Mark files to be deleted as being compacted before applying change Summary: While running the myrocks regression suite, I found that while dropping a table soon after inserting rows into it resulted in an assertion failure in CheckConsistencyForDeletes for not finding a file which was recently added or moved. Marking the files to be deleted as being compacted before calling LogAndApplyChange fixed the assertion failures. Test Plan: DBCompactionTest.DeleteFileRange Reviewers: IslamAbdelRahman, anthony, yhchiang, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba, yoshinorim, leveldb Differential Revision: clang build in db_compaction_test Summary: Fix CLANG build error caused by type mismatch. Changed type to size_t. Test Plan: Clang build and make check Reviewers: sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: pull request from zhipeng-jia/develop Fix clang warning regarding implicit conversion/Fix clang warning regarding implicit conversion/compaction assertion triggering test fix for sequence zeroing assertion trip/Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: valgrind failures in 3 tests in db_compaction_test due to new skiplist changes Summary: Several tests in db_compaction_test are failing with aborts in valgrind. These are LevelCompactionThirdPath, LevelCompactionPathUse and CompressLevelCompaction. We now use the SpecialSkipListFactory to make them more deterministic Test Plan: valgrind Reviewers: anthony, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: incorrect merge in db/db_compaction_test.cc Summary: Fix incorrect merge in db/db_compaction_test.cc Test Plan: db_compaction_test Reviewers: igor, sdong, anthony, IslamAbdelRahman, rven, kradhakrishnan Subscribers: dhruba, leveldb Differential Revision: DBCompactionTestWithParam.CompactionTrigger in non-jemalloc build. Summary: DBCompactionTestWithParam.CompactionTrigger fails in non-jemalloc build, after the skip list memtable change. Fix it by making mem table flush trigger by number of entries. Test Plan: Run the test using both of jemalloc and non-jemalloc build. Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, igor, yhchiang Reviewed By: yhchiang Subscribers: leveldb, dhruba Differential Revision:"
,,0.2055,rocksdb,"Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: forward_iterator allocation of vector. Summary: db_tailing_iter_test was failing on some platforms because of an incorrect allocation and use. This diff fixes the issue. Test Plan: db_tailing_iter_test Run valgrind for db_tailing_iter_test Reviewers: igor, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: pull request from yuslepukhin/fix_forward_iter_outofbounds Fix empty vector write in ForwardIterator/Fix empty vector write in ForwardIterator/"
,,0.6396,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.4759,rocksdb,"db_bench: explicitly clear buffer in compress benchmark Summary: It is reported that in compress benchmark in db_bench, zlib will cause an OOM. The suggestd fix was to clear the buffer. Test Plan: Build and run compress benchmark. Reviewers: IslamAbdelRahman, yhchiang, rven, andrewkr, kradhakrishnan, anthony Reviewed By: anthony Subscribers: leveldb, dhruba Differential Revision: db_bench write rate limit Summary: 1) changes tools/{benchmark,run_flash_bench}.sh to optionally use the write rate limit 2) removes code for and switches the background write rate limit to use Replaces Task ID: Blame Rev: Test Plan: tools/run_flash_bench.sh Revert Plan: Database Impact: Memcache Impact: Other Notes: EImportant: begin *PUBLIC* platform impact section Bugzilla: end platform impact Reviewers: igor Reviewed By: igor Subscribers: dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: use-after free in db_bench Test Plan: valgrind db_bench Reviewers: igor, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: should set options.soft_pending_compaction_bytes_limit Summary: Fix a bug that options.soft_pending_compaction_bytes_limit is not actually set with Test Plan: Run db_bench with this parameter and make sure the parameter is set correctly. Reviewers: anthony, kradhakrishnan, yhchiang, IslamAbdelRahman, igor, rven Reviewed By: rven Subscribers: leveldb, dhruba Differential Revision: minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: another rebase problems./db_bench: in uncompress benchmark, get Snappy size from compressed stream Summary: Now in benchmark ""uncompress"" in db_bench, we get size from compressed stream for all other compression types except Snappy, where we allocate memory based on parameter. Change it to match to behavior of other compression types. Test Plan: Run ./db_bench with snappy and other compression types. Reviewers: yhchiang, kradhakrishnan, anthony, IslamAbdelRahman, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.6429,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.2227,rocksdb,"fix sporadic failure in fault_injection_test Summary: Need to make sure the background task gets scheduled before it goes out of scope. Test Plan: ran test. Will see if sporadic valgrind failures go away. Reviewers: kradhakrishnan Reviewed By: kradhakrishnan Subscribers: dhruba, leveldb Differential Revision: minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.6358,rocksdb,"Improve perf of Pessimistic Transaction expirations (and optimistic transactions) Summary: copy from task 8196669: 1) Optimistic transactions do not support batching writes from different threads. 2) Pessimistic transactions do not support batching writes if an expiration time is set. In these 2 cases, we currently do not do any write batching in DBImpl::WriteImpl() because there is a WriteCallback that could decide at the last minute to abort the write. But we could support batching write operations with callbacks if we make sure to process the callbacks correctly. To do this, we would first need to modify write_thread.cc to stop preventing writes with callbacks from being batched together. Then we would need to change DBImpl::WriteImpl() to call all WriteCallbacks in a batch, only write the batches that succeed, and correctly set the state of each batchs WriteThread::Writer. Test Plan: Added test WriteWithCallbackTest to write_callback_test.cc which creates multiple client threads and verifies that writes are batched and executed properly. Reviewers: hermanlee4, anthony, ngbronson Subscribers: leveldb, dhruba Differential Revision: to closer match MyRocks build Summary: myrocks seems to build rocksdb using (and treats warnings as errors). This diff adds that flag to the rocksdb build, and fixes the compilation failures that result. I have not checked for any other differences in the build flags for rocksdb build as part of myrocks. Test Plan: make check Reviewers: sdong, rven Reviewed By: rven Subscribers: dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.501,rocksdb,"Explictly fail when memtable doesnt support concurrent insert Summary: If users turn on concurrent insert but the memtable doesnt support it, they might see unexcepted crash. Fix it by explicitly fail. Test Plan: Run different setting of stress_test and make sure it fails correctly. Will add a unit test too. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, andrewkr, ngbronson Reviewed By: ngbronson Subscribers: leveldb, dhruba Differential Revision: of writing to the last memtable should not override stopping Summary: Now slowing down for the last mem table takes priority against some stopping conditions. This is logically confusing. Fix it. Test Plan: Run all existing tests. Reviewers: yhchiang, IslamAbdelRahman, kradhakrishnan, andrewkr, anthony Reviewed By: anthony Subscribers: leveldb, dhruba Differential Revision: Visual Studio Warning C4351 Currently Windows build is broken because of Warning C4351. Disable the warning before figuring out the right way to fix it./support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision: when writing to the last write buffer Summary: Now if inserting to mem table is much faster than writing to files, there is no mechanism users can rely on to avoid stopping for reaching options.max_write_buffer_number. With the commit, if there are more than four maximum write buffers configured, we slow down to the rate of options.delayed_write_rate while we reach the last one. Test Plan: 1. Add a new unit test. 2. Run db_bench with ./db_bench based on hard drive and see stopping is avoided with the commit. Reviewers: yhchiang, IslamAbdelRahman, anthony, rven, kradhakrishnan, igor Reviewed By: igor Subscribers: MarkCallaghan, leveldb, dhruba Differential Revision: manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: avoid to form compactions if there is no file Summary: Currently RocksDB may break in lines like this: for (size_t i sorted_runs.size() 1; i >= first_index_after; i--) { if options.level0_file_num_compaction_trigger=0. Fix it by not executing the logic of picking compactions if there is no file (sorted_runs.size() 0). Also internally set options.level0_file_num_compaction_trigger=1 if users give a 0. 0 is a value makes no sense in RocksDB. Test Plan: Run all tests. Will add a unit test too. Reviewers: yhchiang, IslamAbdelRahman, anthony, kradhakrishnan, rven Reviewed By: rven Subscribers: leveldb, dhruba Differential Revision:"
,,0.6787,rocksdb,"Improve perf of Pessimistic Transaction expirations (and optimistic transactions) Summary: copy from task 8196669: 1) Optimistic transactions do not support batching writes from different threads. 2) Pessimistic transactions do not support batching writes if an expiration time is set. In these 2 cases, we currently do not do any write batching in DBImpl::WriteImpl() because there is a WriteCallback that could decide at the last minute to abort the write. But we could support batching write operations with callbacks if we make sure to process the callbacks correctly. To do this, we would first need to modify write_thread.cc to stop preventing writes with callbacks from being batched together. Then we would need to change DBImpl::WriteImpl() to call all WriteCallbacks in a batch, only write the batches that succeed, and correctly set the state of each batchs WriteThread::Writer. Test Plan: Added test WriteWithCallbackTest to write_callback_test.cc which creates multiple client threads and verifies that writes are batched and executed properly. Reviewers: hermanlee4, anthony, ngbronson Subscribers: leveldb, dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.6362,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.5489,rocksdb,"Eliminate duplicated property constants Summary: Before this diff, there were duplicated constants to refer to properties (user- facing API had strings and InternalStats had an enum). I noticed these were inconsistent in terms of which constants are provided, names of constants, and documentation of constants. Overall it seemed annoying/error-prone to maintain these duplicated constants. So, this diff gets rid of InternalStatss constants and replaces them with a map keyed on the user-facing constant. The value in that map contains a function pointer to get the property value, so we dont need to do string matching while holding db->mutex_. This approach has a side benefit of making many small handler functions rather than a giant switch-statement. Test Plan: db_properties_test passes, running ""make commit-prereq Reviewers: sdong, yhchiang, kradhakrishnan, IslamAbdelRahman, rven, anthony Reviewed By: anthony Subscribers: dhruba, leveldb Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: when writing to the last write buffer Summary: Now if inserting to mem table is much faster than writing to files, there is no mechanism users can rely on to avoid stopping for reaching options.max_write_buffer_number. With the commit, if there are more than four maximum write buffers configured, we slow down to the rate of options.delayed_write_rate while we reach the last one. Test Plan: 1. Add a new unit test. 2. Run db_bench with ./db_bench based on hard drive and see stopping is avoided with the commit. Reviewers: yhchiang, IslamAbdelRahman, anthony, rven, kradhakrishnan, igor Reviewed By: igor Subscribers: MarkCallaghan, leveldb, dhruba Differential Revision:"
,,0.2517,rocksdb,"compaction assertion triggering test fix for sequence zeroing assertion trip/Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.2973,rocksdb,"Eliminate duplicated property constants Summary: Before this diff, there were duplicated constants to refer to properties (user- facing API had strings and InternalStats had an enum). I noticed these were inconsistent in terms of which constants are provided, names of constants, and documentation of constants. Overall it seemed annoying/error-prone to maintain these duplicated constants. So, this diff gets rid of InternalStatss constants and replaces them with a map keyed on the user-facing constant. The value in that map contains a function pointer to get the property value, so we dont need to do string matching while holding db->mutex_. This approach has a side benefit of making many small handler functions rather than a giant switch-statement. Test Plan: db_properties_test passes, running ""make commit-prereq Reviewers: sdong, yhchiang, kradhakrishnan, IslamAbdelRahman, rven, anthony Reviewed By: anthony Subscribers: dhruba, leveldb Differential Revision: slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision: manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision: the fix for a race condition in persisting options Summary: This patch fix a race condition in persisting options which will cause a crash when: * Thread A obtain cf options and start to persist options based on that cf options. * Thread B kicks in and finish DropColumnFamily and delete cf_handle. * Thread A wakes up and tries to finish the persisting options and crashes. Test Plan: Add a test in column_family_test that can reproduce the crash Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: marking snapshots for write-conflict checking Take 2 Summary: D51183 was reverted due to breaking the LITE build. This diff is the same as D51183 but with a fix for the LITE BUILD(D51693) Test Plan: run all unit tests Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: ""Fix a race condition in persisting options"" This reverts commit 2fa3ed5180340e485a1caf6fa71cc400ea599278. It breaks RocksDB lite build/Fix a race condition in persisting options Summary: This patch fix a race condition in persisting options which will cause a crash when: * Thread A obtain cf options and start to persist options based on that cf options. * Thread B kicks in and finish DropColumnFamily and delete cf_handle. * Thread A wakes up and tries to finish the persisting options and crashes. Test Plan: Add a test in column_family_test that can reproduce the crash Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: public api to schedule flush/compaction, code to prevent race with db::open Summary: Fixes T8781168. Added a new function EnableAutoCompactions in db.h to be publicly avialable. This allows compaction to be re-enabled after disabling it via SetOptions Refactored code to set the dbptr earlier on in TransactionDB::Open and DB::Open Temporarily disable auto_compaction in TransactionDB::Open until dbptr is set to prevent race condition. Test Plan: Ran make all check verified fix on myrocks side: was able to reproduce the seg fault with ../tools/mysqltest.sh rocksdb.drop_table method was to manually sleep the thread after DB::Open but before TransactionDB ptr was assigned in transaction_db_impl.cc: DB::Open(db_options, dbname, column_families_copy, handles, &db); clock_t goal (60000 * 10) + clock(); while (goal > clock()); ...dbptr(aka rdb) gets assigned below verified my changes fixed the issue. Also added unit test ToggleAutoCompaction in transaction_test.cc Reviewers: hermanlee4, anthony Reviewed By: anthony Subscribers: alex, dhruba Differential Revision: More deterministic and readable Summary: DBTest.DynamicCompactionOptions sometimes fails the assert but I cant repro it locally. Make it more deterministic and readable and see whether the problem is still there. Test Plan: Run tht test and make sure it passes Reviewers: kradhakrishnan, yhchiang, igor, rven, IslamAbdelRahman, anthony Reviewed By: anthony Subscribers: leveldb, dhruba Differential Revision:"
,,0.2482,rocksdb,"Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.2355,rocksdb,"Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: pull request from SherlockNoMad/CounterFix Fix EstimateNumKeys Counter Inaccurate Issue/"
,,0.2301,rocksdb,"When slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision:"
,,0.6381,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: pull request from zhipeng-jia/master Fix typo/Fix typo/"
,,0.0813,rocksdb,compaction assertion triggering test fix for sequence zeroing assertion trip/
,,0.2421,rocksdb,"Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.295,rocksdb,"Running manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision:"
,,0.2409,rocksdb,"Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.3738,rocksdb,"Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision:"
,,0.6905,rocksdb,"Improve perf of Pessimistic Transaction expirations (and optimistic transactions) Summary: copy from task 8196669: 1) Optimistic transactions do not support batching writes from different threads. 2) Pessimistic transactions do not support batching writes if an expiration time is set. In these 2 cases, we currently do not do any write batching in DBImpl::WriteImpl() because there is a WriteCallback that could decide at the last minute to abort the write. But we could support batching write operations with callbacks if we make sure to process the callbacks correctly. To do this, we would first need to modify write_thread.cc to stop preventing writes with callbacks from being batched together. Then we would need to change DBImpl::WriteImpl() to call all WriteCallbacks in a batch, only write the batches that succeed, and correctly set the state of each batchs WriteThread::Writer. Test Plan: Added test WriteWithCallbackTest to write_callback_test.cc which creates multiple client threads and verifies that writes are batched and executed properly. Reviewers: hermanlee4, anthony, ngbronson Subscribers: leveldb, dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.5198,rocksdb,"Disable Visual Studio Warning C4351 Currently Windows build is broken because of Warning C4351. Disable the warning before figuring out the right way to fix it./support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision:"
,,0.6027,rocksdb,"Fix WriteBatchTest.ManyUpdates, WriteBatchTest.LargeKeyValue under clang Summary: Fix current clang failure Test Plan: make sure that both clang and g++ compilation succeed USE_CLANG=1 make check make check Reviewers: anthony, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.6436,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.5735,rocksdb,"Improve perf of Pessimistic Transaction expirations (and optimistic transactions) Summary: copy from task 8196669: 1) Optimistic transactions do not support batching writes from different threads. 2) Pessimistic transactions do not support batching writes if an expiration time is set. In these 2 cases, we currently do not do any write batching in DBImpl::WriteImpl() because there is a WriteCallback that could decide at the last minute to abort the write. But we could support batching write operations with callbacks if we make sure to process the callbacks correctly. To do this, we would first need to modify write_thread.cc to stop preventing writes with callbacks from being batched together. Then we would need to change DBImpl::WriteImpl() to call all WriteCallbacks in a batch, only write the batches that succeed, and correctly set the state of each batchs WriteThread::Writer. Test Plan: Added test WriteWithCallbackTest to write_callback_test.cc which creates multiple client threads and verifies that writes are batched and executed properly. Reviewers: hermanlee4, anthony, ngbronson Subscribers: leveldb, dhruba Differential Revision: to closer match MyRocks build Summary: myrocks seems to build rocksdb using (and treats warnings as errors). This diff adds that flag to the rocksdb build, and fixes the compilation failures that result. I have not checked for any other differences in the build flags for rocksdb build as part of myrocks. Test Plan: make check Reviewers: sdong, rven Reviewed By: rven Subscribers: dhruba Differential Revision: CLANG errors introduced by 7d87f02799bd0a8fd36df24fab5baa4968615c86 Summary: Fix some CLANG errors introduced in 7d87f02799bd0a8fd36df24fab5baa4968615c86 Test Plan: Build with both of CLANG and gcc Reviewers: rven, yhchiang, kradhakrishnan, anthony, IslamAbdelRahman, ngbronson Subscribers: leveldb, dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: the fix for a race condition in persisting options Summary: This patch fix a race condition in persisting options which will cause a crash when: * Thread A obtain cf options and start to persist options based on that cf options. * Thread B kicks in and finish DropColumnFamily and delete cf_handle. * Thread A wakes up and tries to finish the persisting options and crashes. Test Plan: Add a test in column_family_test that can reproduce the crash Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: ""Fix a race condition in persisting options"" This reverts commit 2fa3ed5180340e485a1caf6fa71cc400ea599278. It breaks RocksDB lite build/Fix a race condition in persisting options Summary: This patch fix a race condition in persisting options which will cause a crash when: * Thread A obtain cf options and start to persist options based on that cf options. * Thread B kicks in and finish DropColumnFamily and delete cf_handle. * Thread A wakes up and tries to finish the persisting options and crashes. Test Plan: Add a test in column_family_test that can reproduce the crash Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.6329,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.6449,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.2434,rocksdb,"Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.2482,rocksdb,"Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
,,0.2252,rocksdb,"When slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision:"
,,0.4363,rocksdb,"Improve perf of Pessimistic Transaction expirations (and optimistic transactions) Summary: copy from task 8196669: 1) Optimistic transactions do not support batching writes from different threads. 2) Pessimistic transactions do not support batching writes if an expiration time is set. In these 2 cases, we currently do not do any write batching in DBImpl::WriteImpl() because there is a WriteCallback that could decide at the last minute to abort the write. But we could support batching write operations with callbacks if we make sure to process the callbacks correctly. To do this, we would first need to modify write_thread.cc to stop preventing writes with callbacks from being batched together. Then we would need to change DBImpl::WriteImpl() to call all WriteCallbacks in a batch, only write the batches that succeed, and correctly set the state of each batchs WriteThread::Writer. Test Plan: Added test WriteWithCallbackTest to write_callback_test.cc which creates multiple client threads and verifies that writes are batched and executed properly. Reviewers: hermanlee4, anthony, ngbronson Subscribers: leveldb, dhruba Differential Revision:"
,,0.091,rocksdb,"Lint everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.0948,rocksdb,"Lint everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.6315,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision:"
,,0.6306,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.6291,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: pull request from yuslepukhin/make_vs15_build Fix up VS 15 build./Fix up VS 15 build. Fix warnings Take advantage of native snprintf on VS 15/"
,,0.612,rocksdb,"support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: pull request from charsyam/feature/typos fix typos in comments/fix typos in comments/Lint everything Summary: ``` arc2 lint ``` run the linter on the whole code repo to fix exisitng lint issues Test Plan: make check Reviewers: sdong, rven, anthony, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
,,0.0681,rocksdb,Merge pull request from flyd1005/wip-fix-typo fix typos and remove duplicated words/
,,0.0681,rocksdb,Merge pull request from flyd1005/wip-fix-typo fix typos and remove duplicated words/
,,0.0681,rocksdb,Merge pull request from flyd1005/wip-fix-typo fix typos and remove duplicated words/
,,0.091,rocksdb,"Fix name conflict in delete_shceduler_test and db_sst_test Summary: delete_scheduler_test and db_sst_test share a same directory name, causing possible fails on both tests when running in parallel. Fixed by changing directory name. Test Plan: Run the two tests in parallel: `parallel ./{} ::: delete_scheduler_test db_sst_test` Reviewers: sdong, andrewkr Reviewed By: sdong, andrewkr Subscribers: andrewkr, dhruba Differential Revision:"
,,0.1118,rocksdb,"Fix flaky test `ObsoleteFiles` Summary: The test `ObsoleteFiles` failed occasionally on slow device. This problem appears on Travis CI several times. The reason is that we did not wait until compaction jobs are finished in the test, while in slower device the background jobs take longer time to finish. Test Plan: Pass existing tests. Reviewers: yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.136,rocksdb,"Avoid updating memtable allocated bytes if write_buffer_size is not set Summary: If options.write_buffer_size is not set, nor options.write_buffer_manager, no need to update the bytes allocated counter in MemTableAllocator, which is expensive in parallel memtable insert case. Remove it can improve parallel memtable insert throughput by 10% with write batch size 128. Test Plan: Run benchmarks TEST_TMPDIR=/dev/shm/ ./db_bench The throughput grows 10% with the benchmark. Reviewers: andrewkr, yiwu, IslamAbdelRahman, igor, ngbronson Reviewed By: ngbronson Subscribers: ngbronson, leveldb, andrewkr, dhruba Differential Revision:"
,,0.1028,rocksdb,"Fix deadlock when calling getMergedHistogram Summary: When calling StatisticsImpl::HistogramInfo::getMergedHistogram(), if there is a dying thread, which is calling ThreadLocalPtr::StaticMeta::OnThreadExit() to merge its thread values to HistogramInfo, deadlock will occur. Because the former try to hold merge_lock then ThreadMeta::mutex_, but the later try to hold ThreadMeta::mutex_ then merge_lock. In short, the locking order isnt the same. This patch addressed this issue by releasing merge_lock before folding thread values. Closes Differential Revision: D4211942 Pulled By: ajkr fbshipit-source-id: ef89bcb/"
,,0.1164,rocksdb,Fix build on FreeBSD Summary: ``` CC utilities/column_aware_encoding_exp.o utilities/column_aware_encoding_exp.cc:149:5: error: use of undeclared identifier exit exit(1); ^ utilities/column_aware_encoding_exp.cc:154:5: error: use of undeclared identifier exit exit(1); ^ utilities/column_aware_encoding_exp.cc:158:5: error: use of undeclared identifier exit exit(1); ^ 3 errors generated. ``` Closes Differential Revision: D4399044 Pulled By: IslamAbdelRahman fbshipit-source-id: fbab5a2/
,,0.2075,rocksdb,"Fix 2PC with concurrent memtable insert Summary: If concurrent memtable insert is enabled, and one prepare command and a normal command are grouped into a commit group, the sequence ID will be calculated incorrectly. Closes Differential Revision: D4371081 Pulled By: siying fbshipit-source-id: cd40c6d/Bug: paralle_group status updated in WriteThread::CompleteParallelWorker Summary: Multi-write thread may update the status of the parallel_group in WriteThread::CompleteParallelWorker if the status of Writer is not ok When copy write status to the paralle_group, the write thread just hold the mutex of the the writer processed by itself. it is useless. The thread should held the the leader of the parallel_group instead. Closes Differential Revision: D4252335 Pulled By: siying fbshipit-source-id: 3864cf7/Add WriteOptions.no_slowdown Summary: If the WriteOptions.no_slowdown flag is set AND we need to wait or sleep for the write request, then fail immediately with Status::Incomplete(). Closes Differential Revision: D4191405 Pulled By: maysamyabandeh fbshipit-source-id: 7f3ce3f/"
,,0.4339,rocksdb,"Simplify write thread logic Summary: The concept about early exit in write thread implementation is a confusing one. It means that if early exit is allowed, batch group leader will not responsible to exit the batch group, but the last finished writer do. In case we need to mark log synced, or encounter memtable insert error, early exit is disallowed. This patch remove such a concept by: * In all cases, the last finished writer (not necessary leader) is responsible to exit batch group. * In case of parallel memtable write, leader will also mark log synced after memtable insert and before signal finish (call `CompleteParallelWorker()`). The purpose is to allow mark log synced (which require locking mutex) can run in parallel to memtable insert in other writers. * The last finish writer should handle memtable insert error (update bg_error_) before exiting batch group. Closes Differential Revision: D4869667 Pulled By: yiwu-arbug fbshipit-source-id: aec170847c85b90f4179d6a4608a4fe1361544e3/Refactor WriteImpl (pipeline write part 1) Summary: Refactor WriteImpl() so when I plug-in the pipeline write code (which is an alternative approach for WriteThread), some of the logic can be reuse. I split out the following methods from WriteImpl(): * PreprocessWrite() * HandleWALFull() (previous MaybeFlushColumnFamilies()) * HandleWriteBufferFull() * WriteToWAL() Also adding a constructor to WriteThread::Writer, and move WriteContext into db_impl.h. No real logic change in this patch. Closes Differential Revision: D4781014 Pulled By: yiwu-arbug fbshipit-source-id: d45ca18/"
,,0.2933,rocksdb,"Blob storage pr Summary: The final pull request for Blob Storage. Closes Differential Revision: D5033189 Pulled By: yiwu-arbug fbshipit-source-id: 6356b683ccd58cbf38a1dc55e2ea400feecd5d06/Add bulk create/drop column family API Summary: Adding DB::CreateColumnFamilie() and DB::DropColumnFamilies() to bulk create/drop column families. This is to address the problem creating/dropping 1k column families takes minutes. The bottleneck is we persist options files for every single column family create/drop, and it parses the persisted options file for verification, which take a lot CPU time. The new APIs simply create/drop column families individually, and persist options file once at the end. This improves create 1k column families to within ~0.1s. Further improvement can be merge manifest write to one IO. Closes Differential Revision: D5001578 Pulled By: yiwu-arbug fbshipit-source-id: d4e00bda671451e0b314c13e12ad194b1704aa03/Blob storage helper methods Summary: Split out interfaces needed for blob storage from including * CompactionEventListener and OnFlushBegin listener interfaces. * Blob filename support. Closes Differential Revision: D4905463 Pulled By: yiwu-arbug fbshipit-source-id: 564e73448f1b7a367e5e46216a521e57ea9011b5/Simplify write thread logic Summary: The concept about early exit in write thread implementation is a confusing one. It means that if early exit is allowed, batch group leader will not responsible to exit the batch group, but the last finished writer do. In case we need to mark log synced, or encounter memtable insert error, early exit is disallowed. This patch remove such a concept by: * In all cases, the last finished writer (not necessary leader) is responsible to exit batch group. * In case of parallel memtable write, leader will also mark log synced after memtable insert and before signal finish (call `CompleteParallelWorker()`). The purpose is to allow mark log synced (which require locking mutex) can run in parallel to memtable insert in other writers. * The last finish writer should handle memtable insert error (update bg_error_) before exiting batch group. Closes Differential Revision: D4869667 Pulled By: yiwu-arbug fbshipit-source-id: aec170847c85b90f4179d6a4608a4fe1361544e3/Refactor WriteImpl (pipeline write part 1) Summary: Refactor WriteImpl() so when I plug-in the pipeline write code (which is an alternative approach for WriteThread), some of the logic can be reuse. I split out the following methods from WriteImpl(): * PreprocessWrite() * HandleWALFull() (previous MaybeFlushColumnFamilies()) * HandleWriteBufferFull() * WriteToWAL() Also adding a constructor to WriteThread::Writer, and move WriteContext into db_impl.h. No real logic change in this patch. Closes Differential Revision: D4781014 Pulled By: yiwu-arbug fbshipit-source-id: d45ca18/make total_log_size_ atomic Summary: make total_log_size_ atomic to avoid overflow caused by data race. Closes Differential Revision: D4751391 Pulled By: siying fbshipit-source-id: fac01dd/Make DBImpl::has_unpersisted_data_ atomic Summary: Seems to me `has_unpersisted_data_` is read from read thread and write from write thread concurrently without synchronization. Making it an atomic. I update the logic not because seeing any problem with it, but it just feel confusing. Closes Differential Revision: D4555837 Pulled By: yiwu-arbug fbshipit-source-id: eff2ab8/"
,,0.1873,rocksdb,"Refactor WriteImpl (pipeline write part 1) Summary: Refactor WriteImpl() so when I plug-in the pipeline write code (which is an alternative approach for WriteThread), some of the logic can be reuse. I split out the following methods from WriteImpl(): * PreprocessWrite() * HandleWALFull() (previous MaybeFlushColumnFamilies()) * HandleWriteBufferFull() * WriteToWAL() Also adding a constructor to WriteThread::Writer, and move WriteContext into db_impl.h. No real logic change in this patch. Closes Differential Revision: D4781014 Pulled By: yiwu-arbug fbshipit-source-id: d45ca18/"
,,0.3861,rocksdb,"Simplify write thread logic Summary: The concept about early exit in write thread implementation is a confusing one. It means that if early exit is allowed, batch group leader will not responsible to exit the batch group, but the last finished writer do. In case we need to mark log synced, or encounter memtable insert error, early exit is disallowed. This patch remove such a concept by: * In all cases, the last finished writer (not necessary leader) is responsible to exit batch group. * In case of parallel memtable write, leader will also mark log synced after memtable insert and before signal finish (call `CompleteParallelWorker()`). The purpose is to allow mark log synced (which require locking mutex) can run in parallel to memtable insert in other writers. * The last finish writer should handle memtable insert error (update bg_error_) before exiting batch group. Closes Differential Revision: D4869667 Pulled By: yiwu-arbug fbshipit-source-id: aec170847c85b90f4179d6a4608a4fe1361544e3/Move memtable related files into memtable directory Summary: Move memtable related files into memtable directory. Closes Differential Revision: D4829242 Pulled By: yiwu-arbug fbshipit-source-id: ca70ab6/Refactor WriteImpl (pipeline write part 1) Summary: Refactor WriteImpl() so when I plug-in the pipeline write code (which is an alternative approach for WriteThread), some of the logic can be reuse. I split out the following methods from WriteImpl(): * PreprocessWrite() * HandleWALFull() (previous MaybeFlushColumnFamilies()) * HandleWriteBufferFull() * WriteToWAL() Also adding a constructor to WriteThread::Writer, and move WriteContext into db_impl.h. No real logic change in this patch. Closes Differential Revision: D4781014 Pulled By: yiwu-arbug fbshipit-source-id: d45ca18/"
,,0.2482,rocksdb,"Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/Allow ignoring unknown options when loading options from a file Summary: Added a flag, `ignore_unknown_options`, to skip unknown options when loading an options file (using `LoadLatestOptions`/`LoadOptionsFromFile`) or while verifying options (using `CheckOptionsCompatibility`). This will help in downgrading the db to an older version. Also added `--ignore_unknown_options` flag to ldb **Example Use case:** In MyRocks, if copying from newer version to older version, it is often impossible to start because of new RocksDB options that dont exist in older version, even though data format is compatible. MyRocks uses these load and verify functions in [ha_rocksdb.cc::check_rocksdb_options_compatibility]( **Test Plan:** Updated the unit tests. `make check` ldb: $ ./ldb put a1 b1 OK Now edit /tmp/test_db/<OPTIONS-file> and add an unknown option. Try loading the options now, and it fails: $ ./ldb get a1 Failed: Invalid argument: Unrecognized option DBOptions:: abcd Passes with the new flag $ ./ldb get a1 b1 Closes Differential Revision: D5212091 Pulled By: sagar0 fbshipit-source-id: 2ec17636feb47dc0351b53a77e5f15ef7cbf2ca7/New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
,,0.3737,rocksdb,"New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
,,0.37,rocksdb,"New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
,,0.3789,rocksdb,"New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
,,0.3488,rocksdb,"fixed typo Summary: fixed typo Closes Differential Revision: D5183630 Pulled By: ajkr fbshipit-source-id: 133cfd0445959e70aa2cd1a12151bf3c0c5c3ac5/New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/Fixed some spelling mistakes Summary: Closes Differential Revision: D5079601 Pulled By: sagar0 fbshipit-source-id: ae5696fd735718f544435c64c3179c49b8c04349/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
,,0.1531,rocksdb,"Introduce OnBackgroundError callback Summary: Some users want to prevent rocksdb from entering read-only mode in certain error cases. This diff gives them a callback, `OnBackgroundError`, that they can use to achieve it. call `OnBackgroundError` every time we consider setting `bg_error_`. Use its result to assign `bg_error_` but not to change the functions return status. classified calls using `BackgroundErrorReason` to give the callback some info about where the error happened renamed `ParanoidCheck` to something more specific so we can provide a clear `BackgroundErrorReason` unit tests for the most common cases: flush or compaction errors Closes Differential Revision: D5300190 Pulled By: ajkr fbshipit-source-id: a0ea4564249719b83428e3f4c6ca2c49e366e9b3/"
,,0.3527,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Make ""make analyze"" happy Summary: ""make analyze"" is reporting some errors. Its complicated to look but it seems to me that they are all false positive. Anyway, I think cleaning them up is a good idea. Some of the changes are hacky but I dont know a better way. Closes Differential Revision: D5341710 Pulled By: siying fbshipit-source-id: 6070e430e0e41a080ef441e05e8ec827d45efab6/Fix the reported asan issues Summary: This is to resolve the asan complains. In the meanwhile I am working on clarifying/revisiting the sync rules. Closes Differential Revision: D5338660 Pulled By: yiwu-arbug fbshipit-source-id: ce6f6e0826d43a2c0bfa4328a00c78f73cd6498a/Introduce OnBackgroundError callback Summary: Some users want to prevent rocksdb from entering read-only mode in certain error cases. This diff gives them a callback, `OnBackgroundError`, that they can use to achieve it. call `OnBackgroundError` every time we consider setting `bg_error_`. Use its result to assign `bg_error_` but not to change the functions return status. classified calls using `BackgroundErrorReason` to give the callback some info about where the error happened renamed `ParanoidCheck` to something more specific so we can provide a clear `BackgroundErrorReason` unit tests for the most common cases: flush or compaction errors Closes Differential Revision: D5300190 Pulled By: ajkr fbshipit-source-id: a0ea4564249719b83428e3f4c6ca2c49e366e9b3/Fixing blob db sequence number handling Summary: Blob db rely on base db returning sequence number through write batch after DB::Write(). However after recent changes to the write path, DB::Writ()e no longer return sequence number in some cases. Fixing it by have WriteBatchInternal::InsertInto() always encode sequence number into write batch. Stacking on Closes Differential Revision: D5148358 Pulled By: yiwu-arbug fbshipit-source-id: 8bda0aa07b9334ed03ed381548b39d167dc20c33/New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/"
,,0.47200000000000003,rocksdb,"fix comment Summary: Signed-off-by: tang.jin Closes Differential Revision: D5600861 Pulled By: yiwu-arbug fbshipit-source-id: 9516636cb6e77b09fe0ebef78953adf4b7e88cc8/New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/"
,,0.424,rocksdb,"comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Fixing blob db sequence number handling Summary: Blob db rely on base db returning sequence number through write batch after DB::Write(). However after recent changes to the write path, DB::Writ()e no longer return sequence number in some cases. Fixing it by have WriteBatchInternal::InsertInto() always encode sequence number into write batch. Stacking on Closes Differential Revision: D5148358 Pulled By: yiwu-arbug fbshipit-source-id: 8bda0aa07b9334ed03ed381548b39d167dc20c33/New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/"
,,0.1557,rocksdb,"Introduce OnBackgroundError callback Summary: Some users want to prevent rocksdb from entering read-only mode in certain error cases. This diff gives them a callback, `OnBackgroundError`, that they can use to achieve it. call `OnBackgroundError` every time we consider setting `bg_error_`. Use its result to assign `bg_error_` but not to change the functions return status. classified calls using `BackgroundErrorReason` to give the callback some info about where the error happened renamed `ParanoidCheck` to something more specific so we can provide a clear `BackgroundErrorReason` unit tests for the most common cases: flush or compaction errors Closes Differential Revision: D5300190 Pulled By: ajkr fbshipit-source-id: a0ea4564249719b83428e3f4c6ca2c49e366e9b3/"
,,0.49200000000000005,rocksdb,"New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/"
,,0.332,rocksdb,"add VerifyChecksum() to db.h Summary: We need a tool to check any sst file corruption in the db. It will check all the sst files in current version and read all the blocks (data, meta, index) with checksum verification. If any verification fails, the function will return non-OK status. Closes Differential Revision: D5324269 Pulled By: lightmark fbshipit-source-id: 6f8a272008b722402a772acfc804524c9d1a483b/Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/Introduce OnBackgroundError callback Summary: Some users want to prevent rocksdb from entering read-only mode in certain error cases. This diff gives them a callback, `OnBackgroundError`, that they can use to achieve it. call `OnBackgroundError` every time we consider setting `bg_error_`. Use its result to assign `bg_error_` but not to change the functions return status. classified calls using `BackgroundErrorReason` to give the callback some info about where the error happened renamed `ParanoidCheck` to something more specific so we can provide a clear `BackgroundErrorReason` unit tests for the most common cases: flush or compaction errors Closes Differential Revision: D5300190 Pulled By: ajkr fbshipit-source-id: a0ea4564249719b83428e3f4c6ca2c49e366e9b3/New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/Fixed some spelling mistakes Summary: Closes Differential Revision: D5079601 Pulled By: sagar0 fbshipit-source-id: ae5696fd735718f544435c64c3179c49b8c04349/fixed typo Summary: fixed typo Closes Differential Revision: D5079631 Pulled By: sagar0 fbshipit-source-id: e4c8d1d89b244ee69e9dea1dd013227cc5241026/"
,,0.4507,rocksdb,"Fixing blob db sequence number handling Summary: Blob db rely on base db returning sequence number through write batch after DB::Write(). However after recent changes to the write path, DB::Writ()e no longer return sequence number in some cases. Fixing it by have WriteBatchInternal::InsertInto() always encode sequence number into write batch. Stacking on Closes Differential Revision: D5148358 Pulled By: yiwu-arbug fbshipit-source-id: 8bda0aa07b9334ed03ed381548b39d167dc20c33/New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/"
,,0.3616,rocksdb,"Fix the overflow bug in AwaitState Summary: reports an overflow in AwaitState. nbronson has debugged the issue and presented the fix, which is applied to this patch. Moreover this patch adds more comments to clarify the logic in AwaitState. I tried with both 16 and 64 threads on update benchmark. The fix lowers cpu usage by 1.6 but also lowers the throughput by 1.6 and 2% respectively. Apparently the bug had favored using the spinning more often. Benchmarks: TEST_TMPDIR=/dev/shm/tmpdb time ./db_bench TEST_TMPDIR=/dev/shm/tmpdb time ./db_bench TEST_TMPDIR=/dev/shm/tmpdb time ./db_bench Results $ cat update-16t-bug.txt | tail updaterandom [AVG 3 runs] : 234117 ops/sec; 51.8 MB/sec updaterandom [MEDIAN 3 runs] : 233581 ops/sec; 51.7 MB/sec 3896.42user 1539.12system 6:50.61elapsed 1323%CPU (0avgtext+0avgdata 331308maxresident)k 0inputs+0outputs (0major+1281001minor)pagefaults 0swaps $ cat update-16t-fixed.txt | tail updaterandom [AVG 3 runs] : 230364 ops/sec; 51.0 MB/sec updaterandom [MEDIAN 3 runs] : 226169 ops/sec; 50.0 MB/sec 3865.46user 1568.32system 6:57.63elapsed 1301%CPU (0avgtext+0avgdata 315012maxresident)k 0inputs+0outputs (0major+1342568minor)pagefaults 0swaps $ cat update-64t-bug.txt | tail updaterandom [AVG 3 runs] : 261878 ops/sec; 57.9 MB/sec updaterandom [MEDIAN 3 runs] : 262859 ops/sec; 58.2 MB/sec 926.27user 578.06system 2:27.46elapsed 1020%CPU (0avgtext+0avgdata 475480maxresident)k 0inputs+0outputs (0major+1058728minor)pagefaults 0swaps $ cat update-64t-fixed.txt | tail updaterandom [AVG 3 runs] : 256699 ops/sec; 56.8 MB/sec updaterandom [MEDIAN 3 runs] : 256380 ops/sec; 56.7 MB/sec 933.47user 575.37system 2:30.41elapsed 1003%CPU (0avgtext+0avgdata 482340maxresident)k 0inputs+0outputs (0major+1078557minor)pagefaults 0swaps Closes Differential Revision: D5553732 Pulled By: maysamyabandeh fbshipit-source-id: 98b72dc3a8e0f22ea29d4f7c7790af10c369c5bb/Fix flaky write_callback_test Summary: The test is failing occasionally on the assert: `ASSERT_TRUE(writer->state WriteThread::State::STATE_INIT)`. This is because the test dont make the leader wait for long enough before updating state for its followers. The patch move the update to `threads_waiting` to the end of `WriteThread::JoinBatchGroup:Wait` callback to avoid this happening. Also adding `WriteThread::JoinBatchGroup:Start` and have each thread wait there while another thread is linking to the linked-list. This is to make the check of `is_leader` more deterministic. Also changing two while-loops of `compare_exchange_strong` to plain `fetch_add`, to make it look cleaner. Closes Differential Revision: D5491525 Pulled By: yiwu-arbug fbshipit-source-id: 6e897f122082bd6f98e6d51b31a25e5fd0a3fb82/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Moving static AdaptationContext to outside function Summary: Moving static AdaptationContext to outside function to bypass tsans false report with static initializers. It is because with optimization enabled std::atomic is simplified to as a simple read with no locks. The existing lock produced by static initializer is __cxa_guard_acquire which is apparently not understood by tsan as it is different from normal locks (__gthrw_pthread_mutex_lock). This is a known problem with tsan: A workaround that I tried was to move the static variable outside the function. It is not a good coding practice since it gives global visibility to variable but it is a hackish workaround until g++ tsan is improved. Closes Differential Revision: D5445281 Pulled By: yiwu-arbug fbshipit-source-id: 6142bd934eb5852d8fd7ce027af593ba697ed41d/New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/fixed typo Summary: fixed exisitng existing Closes Differential Revision: D5070169 Pulled By: yiwu-arbug fbshipit-source-id: 8c8450acf50757b767cf78b78314018395738d96/"
,,0.4365,rocksdb,"Fix flaky write_callback_test Summary: The test is failing occasionally on the assert: `ASSERT_TRUE(writer->state WriteThread::State::STATE_INIT)`. This is because the test dont make the leader wait for long enough before updating state for its followers. The patch move the update to `threads_waiting` to the end of `WriteThread::JoinBatchGroup:Wait` callback to avoid this happening. Also adding `WriteThread::JoinBatchGroup:Start` and have each thread wait there while another thread is linking to the linked-list. This is to make the check of `is_leader` more deterministic. Also changing two while-loops of `compare_exchange_strong` to plain `fetch_add`, to make it look cleaner. Closes Differential Revision: D5491525 Pulled By: yiwu-arbug fbshipit-source-id: 6e897f122082bd6f98e6d51b31a25e5fd0a3fb82/comment out unused parameters Summary: This uses `clang-tidy` to comment out unused parameters (in functions, methods and lambdas) in fbcode. Cases that the tool failed to handle are fixed manually. Reviewed By: igorsugak Differential Revision: D5454343 fbshipit-source-id: 5dee339b4334e25e963891b519a5aa81fbf627b2/Fix data races caught by tsan Summary: This fixes the tsan build failures in: write_callback_test persistent_cache_test.* Closes Differential Revision: D5101190 Pulled By: sagar0 fbshipit-source-id: 537e19ed05272b1f34cfbf793aa822b2264a1643/New WriteImpl to pipeline WAL/memtable write Summary: PipelineWriteImpl is an alternative approach to WriteImpl. In WriteImpl, only one thread is allow to write at the same time. This thread will do both WAL and memtable writes for all write threads in the write group. Pending writers wait in queue until the current writer finishes. In the pipeline write approach, two queue is maintained: one WAL writer queue and one memtable writer queue. All writers (regardless of whether they need to write WAL) will still need to first join the WAL writer queue, and after the house keeping work and WAL writing, they will need to join memtable writer queue if needed. The benefit of this approach is that 1. Writers without memtable writes (e.g. the prepare phase of two phase commit) can exit write thread once WAL write is finish. They dont need to wait for memtable writes in case of group commit. 2. Pending writers only need to wait for previous WAL writer finish to be able to join the write thread, instead of wait also for previous memtable writes. Merging and into this PR. Closes Differential Revision: D5054606 Pulled By: yiwu-arbug fbshipit-source-id: ee5b11efd19d3e39d6b7210937b11cefdd4d1c8d/"
,,0.1174,rocksdb,Prevent threads from respawning during joining Summary: Previously the thread pool might be non-empty after joining since concurrent submissions could spawn new threads. This problem didnt affect our background flush/compaction thread pools because the `shutting_down_` flag prevented new jobs from being submitted during/after joining. But I wanted to be able to reuse the `ThreadPool` without such external synchronization. Closes Differential Revision: D5951920 Pulled By: ajkr fbshipit-source-id: 0efec7d0056d36d1338367da75e8b0c089bbc973/
,,0.1808,rocksdb,"BlobDB: Remove the need to get sequence number per write Summary: Previously we store sequence number range of each blob files, and use the sequence number range to check if the file can be possibly visible by a snapshot. But it adds complexity to the code, since the sequence number is only available after a write. (The current implementation get sequence number by calling GetLatestSequenceNumber(), which is wrong.) With the patch, we are not storing sequence number range, and check if snapshot_sequence obsolete_sequence to decide if the file is visible by a snapshot (previously we check if first_sequence snapshot_sequence obsolete_sequence). Closes Differential Revision: D6571497 Pulled By: yiwu-arbug fbshipit-source-id: ca06479dc1fcd8782f6525b62b7762cd47d61909/Fix IOError on WAL write doesnt propagate to write group follower Summary: This is a simpler version of by removing all unrelated changes. Fixing the bug where concurrent writes may get Status::OK while it actually gets IOError on WAL write. This happens when multiple writes form a write batch group, and the leader get an IOError while writing to WAL. The leader failed to pass the error to followers in the group, and the followers end up returning Status::OK() while actually writing nothing. The bug only affect writes in a batch group. Future writes after the batch group will correctly return immediately with the IOError. Closes Differential Revision: D6421644 Pulled By: yiwu-arbug fbshipit-source-id: 1c2a455c5b73f6842423785eb8a9dbfbb191dc0e/"
,,0.2149,rocksdb,"Fix IOError on WAL write doesnt propagate to write group follower Summary: This is a simpler version of by removing all unrelated changes. Fixing the bug where concurrent writes may get Status::OK while it actually gets IOError on WAL write. This happens when multiple writes form a write batch group, and the leader get an IOError while writing to WAL. The leader failed to pass the error to followers in the group, and the followers end up returning Status::OK() while actually writing nothing. The bug only affect writes in a batch group. Future writes after the batch group will correctly return immediately with the IOError. Closes Differential Revision: D6421644 Pulled By: yiwu-arbug fbshipit-source-id: 1c2a455c5b73f6842423785eb8a9dbfbb191dc0e/"
,,0.1141,rocksdb,Fix a race condition in WindowsThread (port::Thread) Summary: Fix a race condition when we create a thread and immediately destroy This case should be supported. What happens is that the thread function needs the Data instance to actually run but has no shared ownership and must rely on the WindowsThread instance to continue existing. To address this we change unique_ptr to shared_ptr and then acquire an additional refcount for the threadproc which destroys it just before the thread exit. We choose to allocate shared_ptr instance on the heap as this allows the original thread to continue w/o waiting for the new thread to start running. Closes Differential Revision: D6511324 Pulled By: yiwu-arbug fbshipit-source-id: 4633ff7996daf4d287a9fe34f60c1dd28cf4ff36/
,,0.1178,rocksdb,Fix a race condition in WindowsThread (port::Thread) Summary: Fix a race condition when we create a thread and immediately destroy This case should be supported. What happens is that the thread function needs the Data instance to actually run but has no shared ownership and must rely on the WindowsThread instance to continue existing. To address this we change unique_ptr to shared_ptr and then acquire an additional refcount for the threadproc which destroys it just before the thread exit. We choose to allocate shared_ptr instance on the heap as this allows the original thread to continue w/o waiting for the new thread to start running. Closes Differential Revision: D6511324 Pulled By: yiwu-arbug fbshipit-source-id: 4633ff7996daf4d287a9fe34f60c1dd28cf4ff36/
,,0.2382,rocksdb,"Improve write time breakdown stats Summary: Theres a group of stats in PerfContext for profiling the write path. They break down the write time into WAL write, memtable insert, throttling, and everything else. We use these stats a lot for figuring out the cause of slow writes. These stats got a bit out of date and are now categorizing some interesting things as ""everything else"", and also do some double counting. This PR fixes it and adds two new stats: time spent waiting for other threads of the batch group, and time spent waiting for scheduling flushes/compactions. Probably these will be enough to explain all the occasional abnormally slow (multiple seconds) writes that were seeing. Closes Differential Revision: D7251562 Pulled By: al13n321 fbshipit-source-id: 0a2d0f5a4fa5677455e1f566da931cb46efe2a0d/"
,,0.2305,rocksdb,"Improve write time breakdown stats Summary: Theres a group of stats in PerfContext for profiling the write path. They break down the write time into WAL write, memtable insert, throttling, and everything else. We use these stats a lot for figuring out the cause of slow writes. These stats got a bit out of date and are now categorizing some interesting things as ""everything else"", and also do some double counting. This PR fixes it and adds two new stats: time spent waiting for other threads of the batch group, and time spent waiting for scheduling flushes/compactions. Probably these will be enough to explain all the occasional abnormally slow (multiple seconds) writes that were seeing. Closes Differential Revision: D7251562 Pulled By: al13n321 fbshipit-source-id: 0a2d0f5a4fa5677455e1f566da931cb46efe2a0d/"
,,0.0911,rocksdb,"Rename function for handling WAL write error Summary: It was misnamed. It actually updates `bg_error_` if `PreprocessWrite()` or `WriteToWAL()` fail, not related to the user callback. Closes Differential Revision: D6955787 Pulled By: ajkr fbshipit-source-id: bd7afc3fdb7a52830c021cbfc25fcbc3ab7d5e10/"
,,0.2437,rocksdb,"Improve write time breakdown stats Summary: Theres a group of stats in PerfContext for profiling the write path. They break down the write time into WAL write, memtable insert, throttling, and everything else. We use these stats a lot for figuring out the cause of slow writes. These stats got a bit out of date and are now categorizing some interesting things as ""everything else"", and also do some double counting. This PR fixes it and adds two new stats: time spent waiting for other threads of the batch group, and time spent waiting for scheduling flushes/compactions. Probably these will be enough to explain all the occasional abnormally slow (multiple seconds) writes that were seeing. Closes Differential Revision: D7251562 Pulled By: al13n321 fbshipit-source-id: 0a2d0f5a4fa5677455e1f566da931cb46efe2a0d/"
,,0.2536,rocksdb,"Improve write time breakdown stats Summary: Theres a group of stats in PerfContext for profiling the write path. They break down the write time into WAL write, memtable insert, throttling, and everything else. We use these stats a lot for figuring out the cause of slow writes. These stats got a bit out of date and are now categorizing some interesting things as ""everything else"", and also do some double counting. This PR fixes it and adds two new stats: time spent waiting for other threads of the batch group, and time spent waiting for scheduling flushes/compactions. Probably these will be enough to explain all the occasional abnormally slow (multiple seconds) writes that were seeing. Closes Differential Revision: D7251562 Pulled By: al13n321 fbshipit-source-id: 0a2d0f5a4fa5677455e1f566da931cb46efe2a0d/"
,,0.3688,rocksdb,"Fix write get stuck when pipelined write is enabled (#4143) Summary: Fix the issue when pipelined write is enabled, writers can get stuck indefinitely and not able to finish the write. It can show with the following example: Assume there are 4 writers W1, W2, W3, W4 (W1 is the first, W4 is the last). T1: all writers pending in WAL writer queue: WAL writer queue: W1, W2, W3, W4 memtable writer queue: empty T2. W1 finish WAL writer and move to memtable writer queue: WAL writer queue: W2, W3, W4, memtable writer queue: W1 T3. W2 and W3 finish WAL write as a batch group. W2 enter ExitAsBatchGroupLeader and move the group to memtable writer queue, but before wake up next leader. WAL writer queue: W4 memtable writer queue: W1, W2, W3 T4. W1, W2, W3 finish memtable write as a batch group. Note that W2 still in the previous ExitAsBatchGroupLeader, although W1 have done memtable write for W2. WAL writer queue: W4 memtable writer queue: empty T5. The thread corresponding to W3 create another writer W3 with the same address as W3. WAL writer queue: W4, W3 memtable writer queue: empty T6. W2 continue with ExitAsBatchGroupLeader. Because the address of W3 is the same as W3, the last writer in its group, it thinks there are no pending writers, so it reset newest_writer_ to null, emptying the queue. W4 and W3 are deleted from the queue and will never be wake up. The issue exists since pipelined write was introduced in 5.5.0. Closes Pull Request resolved: Differential Revision: D8871599 Pulled By: yiwu-arbug fbshipit-source-id: 3502674e51066a954a0660257e24ac588f815e2a/"
,,0.3726,rocksdb,"Fix write get stuck when pipelined write is enabled (#4143) Summary: Fix the issue when pipelined write is enabled, writers can get stuck indefinitely and not able to finish the write. It can show with the following example: Assume there are 4 writers W1, W2, W3, W4 (W1 is the first, W4 is the last). T1: all writers pending in WAL writer queue: WAL writer queue: W1, W2, W3, W4 memtable writer queue: empty T2. W1 finish WAL writer and move to memtable writer queue: WAL writer queue: W2, W3, W4, memtable writer queue: W1 T3. W2 and W3 finish WAL write as a batch group. W2 enter ExitAsBatchGroupLeader and move the group to memtable writer queue, but before wake up next leader. WAL writer queue: W4 memtable writer queue: W1, W2, W3 T4. W1, W2, W3 finish memtable write as a batch group. Note that W2 still in the previous ExitAsBatchGroupLeader, although W1 have done memtable write for W2. WAL writer queue: W4 memtable writer queue: empty T5. The thread corresponding to W3 create another writer W3 with the same address as W3. WAL writer queue: W4, W3 memtable writer queue: empty T6. W2 continue with ExitAsBatchGroupLeader. Because the address of W3 is the same as W3, the last writer in its group, it thinks there are no pending writers, so it reset newest_writer_ to null, emptying the queue. W4 and W3 are deleted from the queue and will never be wake up. The issue exists since pipelined write was introduced in 5.5.0. Closes Pull Request resolved: Differential Revision: D8871599 Pulled By: yiwu-arbug fbshipit-source-id: 3502674e51066a954a0660257e24ac588f815e2a/Avoid sleep in DBTest.GroupCommitTest to fix flakiness Summary: DBTest.GroupCommitTest would often fail when run under valgrind because its sleeps were insufficient to guarantee a group commit had multiple entries. Instead we can use sync point to force a leader to wait until a non-leader thread has enqueued its work, thus guaranteeing a leader can do group commit work for multiple threads. Closes Differential Revision: D8079429 Pulled By: ajkr fbshipit-source-id: 61dc50fad29d2c85547842f681288de60fa29049/"
,,0.2005,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.19399999999999998,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1989,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.2138,rocksdb,"C++17 support (#4482) Summary: Closes Im not sure if youll be happy with `std::random_device{}`, perhaps you would want to use your rand instance instead. I didnt test to see if your rand instance supports the requirements that `std::shuffle` takes. Pull Request resolved: Differential Revision: D10325133 Pulled By: yiwu-arbug fbshipit-source-id: 47b7adaf4bb2b8d64cf090ea6b1b48ef53180581/Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1957,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1679,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/#3865 fix performance regression introduced by MergeOperator.ShouldMerge (#4266) Summary: This PR addresses issue and implements the following approach to fix it: adds `MergeContext::GetOperandsDirectionForward` and `MergeContext::GetOperandsDirectionBackward` to query merge operands in a specific order `MergeContext::GetOperands` becomes a shortcut for `MergeContext::GetOperandsDirectionForward` pass `MergeContext::GetOperandsDirectionBackward` to `MergeOperator::ShouldMerge` and document the order Pull Request resolved: Differential Revision: D9360750 Pulled By: sagar0 fbshipit-source-id: 20cb73ff017760b062ecdcf4382560767086e092/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.19399999999999998,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1957,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.19399999999999998,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1854,rocksdb,"env_librados.h: drop redundant (#4354) Summary: without this change, rocksdb_env_librados_test fails to build. its a regression introduced by 64324e32 Signed-off-by: Kefu Chai Pull Request resolved: Differential Revision: D9702665 Pulled By: riversand963 fbshipit-source-id: 65134eaff0543733210edfc77f89c96709da7a3f/Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1957,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1989,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1839,rocksdb,"Inline doc for format_version 4 (#4350) Summary: Fixes Pull Request resolved: Differential Revision: D9700871 Pulled By: maysamyabandeh fbshipit-source-id: fe1e07803783f34588dc14aba66d51117ca4a180/Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.19399999999999998,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1989,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1957,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.2005,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.2021,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1957,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.2021,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1957,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.19399999999999998,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.2021,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1989,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1815,rocksdb,"Fix typos in comments (#4456) Summary: Fix some typos in the comments Pull Request resolved: Differential Revision: D10209214 Pulled By: miasantreble fbshipit-source-id: dff857ba60396bc95126e635db96d7dc8330d2cb/Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1989,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.2005,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1957,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.2005,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1642,rocksdb,"Add compile time option to work with utf8 filename strings (#4469) Summary: The default behaviour of rocksdb is to use the `*A(` windows API functions. These accept filenames in the currently configured system encoding, be it Latin 1, utf8 or whatever. If the Application intends to completely work with utf8 strings internally, converting these to that codepage properly isnt even always possible. Thus this patch adds a switch to use the `*W(` functions, which accept UTF-16 filenames, and uses C++11 features to translate the UTF8 containing std::string to an UTF16 containing std::wstring. This feature is a compile time options, that can be enabled by setting `WITH_WINDOWS_UTF8_FILENAMES` to true. Pull Request resolved: Differential Revision: D10356011 Pulled By: yiwu-arbug fbshipit-source-id: 27b6ae9171f209085894cdf80069e8a896642044/Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1957,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.19399999999999998,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1957,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1957,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.1973,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.2005,rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339) Summary: As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited. Besides this, try to fix some warnings about loss of data. Pull Request resolved: Differential Revision: D9654990 Pulled By: ajkr fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848/"
,,0.3635,rocksdb,"Update RepeatableThreadTest with MockTimeEnv (#5107) Summary: **This PR updates RepeatableThread::wait, breaking some tests on OS X. The rest of the PR fixes the tests on OS X.** `RepeatableThreadTest.MockEnvTest` uses `MockTimeEnv` and `RepeatableThread`. If `RepeatableThread::wait` calls `TimedWait` with a time smaller than or equal to the current (real) time, `TimedWait` returns immediately on certain platforms, e.g. OS X. addresses this issue by replacing `TimedWait` with `Wait` in test. This fixes the test but makes test/production code diverge, which is not optimal for test coverage. This PR proposes an alternative fix which unifies test and production code path for `RepeatableThread::wait`. We obtain the current (real) time in seconds and add 10 extra seconds to ensure that `RepeatableThread::wait` invokes `TimedWait` with a time greater than (real) current time. This is to prevent the `TimedWait` function from returning immediately without sleeping and releasing the mutex. If `TimedWait` returns immediately, the mutex will not be released, and `RepeatableThread::TEST_WaitForRun` never has a chance to execute the callback which, in this case, updates the result returned by `mock_env->NowMicros()`. Consequently, `RepeatableThread::wait` cannot break out of the loop, causing test to hang. The extra 10 seconds is a best-effort approach because there seems no reliable and deterministic way to provide the aforementioned guarantee. By the time `RepeatableThread::wait` is called, there is no guarantee that the `delay + mock_env->NowMicros()` will be greater than the current real time. However, 10 seconds should be sufficient in most cases. We will keep an eye for possible flakiness of this test. Pull Request resolved: Differential Revision: D14680885 Pulled By: riversand963 fbshipit-source-id: d1ecbe10e1dacd110bd464cd01e188bfee72b89e/"
,,0.3187,rocksdb,"Update RepeatableThreadTest with MockTimeEnv (#5107) Summary: **This PR updates RepeatableThread::wait, breaking some tests on OS X. The rest of the PR fixes the tests on OS X.** `RepeatableThreadTest.MockEnvTest` uses `MockTimeEnv` and `RepeatableThread`. If `RepeatableThread::wait` calls `TimedWait` with a time smaller than or equal to the current (real) time, `TimedWait` returns immediately on certain platforms, e.g. OS X. addresses this issue by replacing `TimedWait` with `Wait` in test. This fixes the test but makes test/production code diverge, which is not optimal for test coverage. This PR proposes an alternative fix which unifies test and production code path for `RepeatableThread::wait`. We obtain the current (real) time in seconds and add 10 extra seconds to ensure that `RepeatableThread::wait` invokes `TimedWait` with a time greater than (real) current time. This is to prevent the `TimedWait` function from returning immediately without sleeping and releasing the mutex. If `TimedWait` returns immediately, the mutex will not be released, and `RepeatableThread::TEST_WaitForRun` never has a chance to execute the callback which, in this case, updates the result returned by `mock_env->NowMicros()`. Consequently, `RepeatableThread::wait` cannot break out of the loop, causing test to hang. The extra 10 seconds is a best-effort approach because there seems no reliable and deterministic way to provide the aforementioned guarantee. By the time `RepeatableThread::wait` is called, there is no guarantee that the `delay + mock_env->NowMicros()` will be greater than the current real time. However, 10 seconds should be sufficient in most cases. We will keep an eye for possible flakiness of this test. Pull Request resolved: Differential Revision: D14680885 Pulled By: riversand963 fbshipit-source-id: d1ecbe10e1dacd110bd464cd01e188bfee72b89e/add GetStatsHistory to retrieve stats snapshots (#4748) Summary: This PR adds public `GetStatsHistory` API to retrieve stats history in the form of an std map. The key of the map is the timestamp in microseconds when the stats snapshot is taken, the value is another std map from stats name to stats value (stored in std string). Two DBOptions are introduced: `stats_persist_period_sec` (default 10 minutes) controls the intervals between two snapshots are taken; `max_stats_history_count` (default 10) controls the max number of history snapshots to keep in memory. RocksDB will stop collecting stats snapshots if `stats_persist_period_sec` is set to 0. (This PR is the in-memory part of Pull Request resolved: Differential Revision: D13961471 Pulled By: miasantreble fbshipit-source-id: ac836d401ecb84ea92216bf9966f969dedf4ad04/"
,,0.3686,rocksdb,"Update RepeatableThreadTest with MockTimeEnv (#5107) Summary: **This PR updates RepeatableThread::wait, breaking some tests on OS X. The rest of the PR fixes the tests on OS X.** `RepeatableThreadTest.MockEnvTest` uses `MockTimeEnv` and `RepeatableThread`. If `RepeatableThread::wait` calls `TimedWait` with a time smaller than or equal to the current (real) time, `TimedWait` returns immediately on certain platforms, e.g. OS X. addresses this issue by replacing `TimedWait` with `Wait` in test. This fixes the test but makes test/production code diverge, which is not optimal for test coverage. This PR proposes an alternative fix which unifies test and production code path for `RepeatableThread::wait`. We obtain the current (real) time in seconds and add 10 extra seconds to ensure that `RepeatableThread::wait` invokes `TimedWait` with a time greater than (real) current time. This is to prevent the `TimedWait` function from returning immediately without sleeping and releasing the mutex. If `TimedWait` returns immediately, the mutex will not be released, and `RepeatableThread::TEST_WaitForRun` never has a chance to execute the callback which, in this case, updates the result returned by `mock_env->NowMicros()`. Consequently, `RepeatableThread::wait` cannot break out of the loop, causing test to hang. The extra 10 seconds is a best-effort approach because there seems no reliable and deterministic way to provide the aforementioned guarantee. By the time `RepeatableThread::wait` is called, there is no guarantee that the `delay + mock_env->NowMicros()` will be greater than the current real time. However, 10 seconds should be sufficient in most cases. We will keep an eye for possible flakiness of this test. Pull Request resolved: Differential Revision: D14680885 Pulled By: riversand963 fbshipit-source-id: d1ecbe10e1dacd110bd464cd01e188bfee72b89e/"
,,0.3151,rocksdb,"Update RepeatableThreadTest with MockTimeEnv (#5107) Summary: **This PR updates RepeatableThread::wait, breaking some tests on OS X. The rest of the PR fixes the tests on OS X.** `RepeatableThreadTest.MockEnvTest` uses `MockTimeEnv` and `RepeatableThread`. If `RepeatableThread::wait` calls `TimedWait` with a time smaller than or equal to the current (real) time, `TimedWait` returns immediately on certain platforms, e.g. OS X. addresses this issue by replacing `TimedWait` with `Wait` in test. This fixes the test but makes test/production code diverge, which is not optimal for test coverage. This PR proposes an alternative fix which unifies test and production code path for `RepeatableThread::wait`. We obtain the current (real) time in seconds and add 10 extra seconds to ensure that `RepeatableThread::wait` invokes `TimedWait` with a time greater than (real) current time. This is to prevent the `TimedWait` function from returning immediately without sleeping and releasing the mutex. If `TimedWait` returns immediately, the mutex will not be released, and `RepeatableThread::TEST_WaitForRun` never has a chance to execute the callback which, in this case, updates the result returned by `mock_env->NowMicros()`. Consequently, `RepeatableThread::wait` cannot break out of the loop, causing test to hang. The extra 10 seconds is a best-effort approach because there seems no reliable and deterministic way to provide the aforementioned guarantee. By the time `RepeatableThread::wait` is called, there is no guarantee that the `delay + mock_env->NowMicros()` will be greater than the current real time. However, 10 seconds should be sufficient in most cases. We will keep an eye for possible flakiness of this test. Pull Request resolved: Differential Revision: D14680885 Pulled By: riversand963 fbshipit-source-id: d1ecbe10e1dacd110bd464cd01e188bfee72b89e/add GetStatsHistory to retrieve stats snapshots (#4748) Summary: This PR adds public `GetStatsHistory` API to retrieve stats history in the form of an std map. The key of the map is the timestamp in microseconds when the stats snapshot is taken, the value is another std map from stats name to stats value (stored in std string). Two DBOptions are introduced: `stats_persist_period_sec` (default 10 minutes) controls the intervals between two snapshots are taken; `max_stats_history_count` (default 10) controls the max number of history snapshots to keep in memory. RocksDB will stop collecting stats snapshots if `stats_persist_period_sec` is set to 0. (This PR is the in-memory part of Pull Request resolved: Differential Revision: D13961471 Pulled By: miasantreble fbshipit-source-id: ac836d401ecb84ea92216bf9966f969dedf4ad04/"
,,0.2987,rocksdb,"Avoid double-compacting data in bottom level in manual compactions (#5138) Summary: Depending on the config, manual compaction (leveled compaction style) does following compactions: L0->L1 L1->L2 ... Ln-1 Ln Ln Ln The final Ln Ln compaction is partly unnecessary as it recompacts all the files that were just generated by the Ln-1 Ln. We should avoid recompacting such files. This rule should be applied to Lmax only. Resolves issue Pull Request resolved: Differential Revision: D14940106 Pulled By: miasantreble fbshipit-source-id: 8d3cf5507a17e76f3333cfd4bac5256d005636e5/Update RepeatableThreadTest with MockTimeEnv (#5107) Summary: **This PR updates RepeatableThread::wait, breaking some tests on OS X. The rest of the PR fixes the tests on OS X.** `RepeatableThreadTest.MockEnvTest` uses `MockTimeEnv` and `RepeatableThread`. If `RepeatableThread::wait` calls `TimedWait` with a time smaller than or equal to the current (real) time, `TimedWait` returns immediately on certain platforms, e.g. OS X. addresses this issue by replacing `TimedWait` with `Wait` in test. This fixes the test but makes test/production code diverge, which is not optimal for test coverage. This PR proposes an alternative fix which unifies test and production code path for `RepeatableThread::wait`. We obtain the current (real) time in seconds and add 10 extra seconds to ensure that `RepeatableThread::wait` invokes `TimedWait` with a time greater than (real) current time. This is to prevent the `TimedWait` function from returning immediately without sleeping and releasing the mutex. If `TimedWait` returns immediately, the mutex will not be released, and `RepeatableThread::TEST_WaitForRun` never has a chance to execute the callback which, in this case, updates the result returned by `mock_env->NowMicros()`. Consequently, `RepeatableThread::wait` cannot break out of the loop, causing test to hang. The extra 10 seconds is a best-effort approach because there seems no reliable and deterministic way to provide the aforementioned guarantee. By the time `RepeatableThread::wait` is called, there is no guarantee that the `delay + mock_env->NowMicros()` will be greater than the current real time. However, 10 seconds should be sufficient in most cases. We will keep an eye for possible flakiness of this test. Pull Request resolved: Differential Revision: D14680885 Pulled By: riversand963 fbshipit-source-id: d1ecbe10e1dacd110bd464cd01e188bfee72b89e/Smooth the deletion of WAL files (#5116) Summary: WAL files are currently not subject to deletion rate limiting by DeleteScheduler. If the size of the WAL files is significant, this can cause a high delete rate on SSDs that may affect other operations. To fix it, force WAL file deletions to go through the SstFileManager. Original PR for this is Pull Request resolved: Differential Revision: D14669437 Pulled By: anand1976 fbshipit-source-id: c5f62d0640cebaa1574de841a1d01e4ce2faadf0/"
,,0.2075,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2092,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2075,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.204,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2075,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.1389,rocksdb,"Allow ldb to open DB as secondary (#5537) Summary: Right now ldb can open running DB through read-only DB. However, it might leave info logs files to the read-only DB directory. Add an option to open the DB as secondary to avoid it. Pull Request resolved: Test Plan: Run ./ldb scan and ./ldb get 0x00000000000000103030303030303030 against a normal db_bench run and observe the output changes. Also observe that no new info logs files are created under /tmp/rocksdbtest-2491/dbbench. Run without and observe that new info logs created under /tmp/rocksdbtest-2491/dbbench. Differential Revision: D16113886 fbshipit-source-id: 4e09dec47c2528f6ca08a9e7a7894ba2d9daebbb/simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.1696,rocksdb,"replace sprintf with its safe version snprintf (#5475) Summary: sprintf is unsafe and has buffer overrun risk. Replace it with the safer version snprintf where buffer size is supplied to avoid overrun. Pull Request resolved: Differential Revision: D15879481 Pulled By: sagar0 fbshipit-source-id: 7ae1958ffc9727fa50261dfbb98ddd74e70a72d8/simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.204,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2109,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2075,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2092,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.204,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2075,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.204,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2109,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2109,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.204,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2075,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2092,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2092,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.1356,rocksdb,"Replace Corruption with TryAgain status when new tail is not visible to TransactionLogIterator (#5474) Summary: When tailing the WAL with TransactionLogIterator, it used to return Corruption status to indicate that the WAL has new tail that is not visible to the iterator, which is a misleading status. The patch replaces it with TryAgain which is more descriptive of a status, indicating that the user needs to create a new iterator to fetch the recent tail. Fixes Pull Request resolved: Differential Revision: D15898953 Pulled By: maysamyabandeh fbshipit-source-id: 40966f6457cb539e1aeb104daeada6b0e46059fc/simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.1575,rocksdb,"Fix bugs in WAL trash file handling (#5520) Summary: 1. Cleanup WAL trash files on open 2. Dont apply deletion rate limit if WAL dir is different from db dir Pull Request resolved: Test Plan: Add new unit tests and make check Differential Revision: D16096750 Pulled By: anand1976 fbshipit-source-id: 6f07858ad864b754b711db416f0389c45ede599b/simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2075,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2092,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.204,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2092,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2075,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2057,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2109,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.1549,rocksdb,"Add missing check before calling PurgeObsoleteFiles in EnableFileDeletions (#5448) Summary: Calling PurgeObsoleteFiles with a JobContext for which HaveSomethingToDelete is false is a precondition violation. This would trigger an assertion in debug builds; however, in release builds with assertions disabled, this can result in the pending_purge_obsolete_files_ counter in DBImpl underflowing, which in turn can lead to the process hanging during database close. Pull Request resolved: Differential Revision: D15792569 Pulled By: ltamasi fbshipit-source-id: 82d92c9b4f6a9efcdc69dbb3d5a52a1ae2dd2472/simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.2075,rocksdb,"simplify include directive involving inttypes (#5402) Summary: When using `PRIu64` type of printf specifier, current code base does the following: ``` __STDC_FORMAT_MACROS __STDC_FORMAT_MACROS ``` However, this can be simplified to ``` ``` as long as flag `-std=c++11` is used. This should solve issues like Pull Request resolved: Differential Revision: D15701195 Pulled By: miasantreble fbshipit-source-id: 6dac0a05f52aadb55e9728038599d3d2e4b59d03/"
,,0.3725,rocksdb,"Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/"
,,0.3686,rocksdb,"Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/"
,,0.223,rocksdb,"Add new persistent 64-bit hash (#5984) Summary: For upcoming new SST filter implementations, we will use a new 64-bit hash function (XXH3 preview, slightly modified). This change updates hash.{h,cc} for that change, adds unit tests, and out-of-lines the implementations to keep hash.h as clean/small as possible. In developing the unit tests, I discovered that the XXH3 preview always returns zero for the empty string. Zero is problematic for some algorithms (including an upcoming SST filter implementation) if it occurs more often than at the ""natural"" rate, so it should not be returned from trivial values using trivial seeds. I modified our fork of XXH3 to return a modest hash of the seed for the empty string. With hash function details out-of-lines in hash.h, it makes sense to enable XXH_INLINE_ALL, so that direct calls to XXH64/XXH32/XXH3p are inlined. To fix array-bounds warnings on some inline calls, I injected some casts to uintptr_t in xxhash.cc. (Issue reported to Yann.) Revised: Reverted using XXH_INLINE_ALL for now. Some Facebook checks are unhappy about on xxhash.cc file. I would fix that by rename to xxhash_cc.h, but to best preserve history I want to do that in a separate commit (PR) from the uintptr casts. Also updated filter_bench for this change, improving the performance predictability of dry run hashing and adding support for 64-bit hash (for upcoming new SST filter implementations, minor dead code in the tool for now). Pull Request resolved: Differential Revision: D18246567 Pulled By: pdillinger fbshipit-source-id: 6162fbf6381d63c8cc611dd7ec70e1ddc883fbb8/Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/"
,,0.3686,rocksdb,"Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/"
,,0.3262,rocksdb,"WritePrepared: Fix flaky test MaxCatchupWithNewSnapshot (#5850) Summary: MaxCatchupWithNewSnapshot tests that the snapshot sequence number will be larger than the max sequence number when the snapshot was taken. However since the test does not have access to the max sequence number when the snapshot was taken, it uses max sequence number after that, which could have advanced the snapshot by then, thus making the test flaky. The fix is to compare with max sequence number before the snapshot was taken, which is a lower bound for the value when the snapshot was taken. Pull Request resolved: Test Plan: ~/gtest-parallel/gtest-parallel ./write_prepared_transaction_test Differential Revision: D17608926 Pulled By: maysamyabandeh fbshipit-source-id: b122ae5a27f982b290bd60da852e28d3c5eb0136/Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/"
,,0.1466,rocksdb,"WriteUnPrepared: Fix bug in savepoints (#5703) Summary: Fix a bug in write unprepared savepoints. When flushing the write batch according to savepoint boundaries, we were forgetting to flush the last write batch after the last savepoint, meaning that some data was not written to DB. Also, add a small optimization where we avoid flushing empty batches. Pull Request resolved: Differential Revision: D16811996 Pulled By: lth fbshipit-source-id: 600c7e0e520ad7a8fad32d77e11d932453e68e3f/WriteUnPrepared: support iterating while writing to transaction (#5699) Summary: In MyRocks, there are cases where we write while iterating through keys. This currently breaks WBWIIterator, because if a write batch flushes during iteration, the delta iterator would point to invalid memory. For now, fix by disallowing flush if there are active iterators. In the future, we will loop through all the iterators on a transaction, and refresh the iterators when a write batch is flushed. Pull Request resolved: Differential Revision: D16794157 Pulled By: lth fbshipit-source-id: 5d5bf70688bd68fe58e8a766475ae88fd1be3190/WriteUnPrepared: Pass snap_released to the callback (#5691) Summary: With changes made in we meant to pass snap_released parameter of ::IsInSnapshot from the read callbacks. Although the variable was defined, passing it to the callback in WritePreparedTxnReadCallback was missing, which is fixed in this PR. Pull Request resolved: Differential Revision: D16767310 Pulled By: maysamyabandeh fbshipit-source-id: 3bf53f5964a2756a66ceef7c8f6b3ac75f102f48/"
,,0.3831,rocksdb,"Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/"
,,0.3657,rocksdb,"Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/"
,,0.3686,rocksdb,"Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/"
