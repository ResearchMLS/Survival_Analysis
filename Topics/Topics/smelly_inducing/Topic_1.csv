,Document_No,Dominant_Topic,Topic_Perc_Contrib,Keywords,Text
0,0,15.0,0.944100022315979,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",AI 144418: am: CL 144381 am: CL 144356 Synchronized code that touches native SSL sessions. Original author: crazybob Merged from: //branches/cupcake/... Original author: android-build Automated import of CL 144418/auto import from
1,1,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",auto import from
2,2,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",auto import from
3,3,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",auto import import from
4,4,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",auto import from
5,5,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",auto import from
6,6,15.0,0.9320999979972839,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",AI 144381: am: CL 144356 Synchronized code that touches native SSL sessions. Original author: crazybob Merged from: //branches/cupcake/... Automated import of CL 144381/auto import from
7,7,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",auto import from
8,8,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",auto import from
9,9,2.0,0.949999988079071,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Each time we start an SSL session, we have to find the trust anchor. This used to be an O(N) operation. If the trust anchor were looking for was close to N, finding it could take a couple seconds. This change makes the operation O(1)./"
10,10,0.0,0.9743000268936157,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Fix OpenSSLSessionImpl.getCreationTime and getLastAccessedTime. This addresses one part of this abandoned change from ursg: Ive also tidied up the native method names to use the harmony ""-Impl"" convention, removed useless methods that just forward to a native method, and removed dead code. Ive canonicalized some of the duplication too, but I want to go through the rest of out OpenSSL code before I really start trying to remove the duplication. When this is submitted, Ill fix the other (unrelated) bug the abandoned change addressed./"
11,11,3.0,0.7623999714851379,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Remove duplication in OpenSSLSocket/OpenSSLServerSocket./
12,12,3.0,0.7623999714851379,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Remove duplication in OpenSSLSocket/OpenSSLServerSocket./
13,13,3.0,0.9970999956130981,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Rewrite JSSE code to use one openssl SSL per SSLSocket an one SSL_CTX per SSLSessionContext Summary: b/1758225: Revisit OpenSSL locking Removed the locking original put in to address b/1678800 which had been causing problems for the HeapWorker thread which was timing out waiting for the lock in the finalizers while other threads were connecting. b/1678800: Reliability tool: Crash in libcrypto Properly fixed the native crash by avoid sharing SSL_SESSION objects between SSL_CTX objects Testing: adb shell run-core-tests tests.xnet.AllTests adb shell run-core-tests javax.net.ssl.AllTests Test app that reloads Details: Each AbstractSessionContext now has an associated SSL_CTX, referenced through the sslCtxNativePointer. SSL_CTX on the native side defines the scope of SSL_SESSION caching, and this brings the Java SSLSessionContext caching into alignment with the native code. OpenSSLSessionImpl now uses AbstractSessionContext instead of SSLSessionContext for access to the underlying SSL_CTX. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Added AbstractSessionContext.putSession so OpenSSLSocketImpl/OpenSSLSessionImpl can directly assign to the current AbstractSessionContext (whether it be a ClientSessionContext or a ServerSessionContext) without casting. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Cleaning up use of SSL_CTX and SSL instances in SSLSocket/SSLServerSocket implementation The major change is that openssl SSL instances are allocated for the life of the matching Java object, replacing the SSL_CTX and the SSL objects that had previously been allocated only starting at handshake time. We should never have been sharing SSL_SESSION instances between SSL_CTX instances, which was the source of the native crashes dating back to cupcake which the OpenSSLSocket.class locking had been preventing. NativeCrypto now has better defined and independant wrappers on openssl functionality. A followon checkin should move the remaining openssl JNI code here with the intent of being able to write and end-to-end test of the openssl code using NativeCrypto without the JSSE implementation classes. The following gives a list of the new native functions with a mapping to the old implementation code. The new code has a more functional style where SSL_CTX and SSL instances are passed and returned as arguments, not extracted from Java instances SSL_CTX_new OpenSSLSocketImpl.nativeinit, OpenSSLServerSocketImpl.nativeinit, SSLParameters.nativeinitsslctx SSL_CTX_get_ciphers_list OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_CTX_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree SSL_new OpenSSLSocketImpl.nativeinit, OpenSSLSocketImpl.init, OpenSSLServerSocketImpl.nativeinit, OpenSSLServerSocketImpl.init SSL_get_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_set_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_get_ciphers OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_set_cipher_list OpenSSLSocketImpl.nativeSetEnabledCipherSuites SSL_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree While the focus in NativeCrypto is on native code, it also contains some helpers/wrappers especially for code that doesnt depend on specific SSL_CTX, SSL instances or that needs to do massaging of data formats between Java and OpenSSL. Some of these had previously been duplicated in the client and server versions of the code. For example: getSupportedCipherSuites OpenSSLSocketImpl.nativegetsupportedciphersuites, OpenSSLServerSocketImpl.nativegetsupportedciphersuites getSupportedProtocols OpenSSLSocketImpl.getSupportedProtocols, OpenSSLServerSocketImpl.getSupportedProtocols getEnabledProtocols OpenSSLSocketImpl.getEnabledProtocols,OpenSSLServerSocketImpl.getEnabledProtocols setEnabledProtocols OpenSSLSocketImpl.setEnabledProtocols setEnabledCipherSuites OpenSSLSocketImpl.setEnabledCipherSuites Moved JNI initialization from OpenSSLSocketImpl to NativeCrypto which is the future home of all the openssl related native code. clinit OpenSSLSocketImpl.nativeinitstatic NativeCrypto.CertificateChainVerifier is a new interface to decouple callbacks from openssl from a specific dependence on a OpenSSLSocketImpl.verify_callback method. Changed to return boolean instead of int. Renamed OpenSSLSocketImpl.ssl to OpenSSLSocketImpl.sslNativePointer for consistency Changed OpenSSLSocketImpl nativeconnect, nativegetsslsession, nativecipherauthenticationmethod, nativeaccept, nativeread, nativewrite, nativeinterrupt, nativeclose, nativefree to take arguments instead of inspect object state in preparation for moving to NativeCrypto other notable NativeCrypto changes included * adding SSL_SESSION_get_peer_cert_chain, SSL_SESSION_get_version, and SSL_get_version (and get_ssl_version) which are ""missing methods"" in openssl * ssl_msg_callback_LOG callback and get_content_type for handshake debugging * removing jfieldIDs for our classes now that we pass in values in arguments * changed aliveAndKicking to be volative since we poll on it to communicate between threads * changed from C style declarations at beginning of block to C++ at first use on methods with major changes * stop freeing SSL instances on error, only SSL_clear it * improved session reuse logging when reproducing b/1678800 * change verify_callback to return verifyCertificateChain result x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketFactoryImpl.java When we accept a server socket, we pass the existing SSL state instance from the server socket to the newly accepted socket via the constructor where it is copied with SSL_dup, instead of through both the constructor and later the accept method. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Cleaned up nativesetclientauth from using SSL_CTX to SSL, passing ssl as argument in preparation for future movement to NativeCrypto. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Removed ssl_op_no cache for rarely used enabled protocol methods so that code could more easily be shared in NativeCrypto between client and server. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed public getId, getCreationTime, getPeerCertificates, getCipherSuite, getProtocol from being instance methods that looked at the OpenSSLSessionImpl object state to be static mthods that take the native pointers as arguments in preparation for moving to NativeCrypto. Rename session sslSessionNativePointer for consistency. Inlined initializeNative, which wasnt really the native code. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Removed lock on OpenSSLSocketImpl.class lock from around OpenSSLSocketImpls use of nativeconnect, nativegetsslsession, and nativecipherauthenticationmethod as well as OpenSSLSessionImpls use of freeImpl, fixing b/1758225: Revisit OpenSSL locking x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Unrelated changes Removed unused ssl_ctx, nativeinitsslctx, getSSLCTX x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java Fix bug in both putSession implementations where we cached sessions with zero length id. Also change indexById to pass in id in client implementation. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Make sure we clone SSLParameters passed to the SSLSocketFactory and SSLServerSocketFactory so that muting the client instance does not change the server instance and vice versa. Explicitly set setUseClientMode(false) on the server SSLParameters. These changes are to bring things more into alignment with the original harmony classes which properly support client/server role switching during handshaking. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketFactoryImpl.java Make locks object fields final x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Moved updateInstanceCount(1) logic and sslParameters assignment to init method x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed getCachedClientSession to respect getUseClientMode x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling of listensers to listeners in javadoc x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling SSLInputStream to SSLOutputStream in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed shutdownInput and shutdownOutput to call to the underlying socket x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Set sslNativePointer to 0 when freeing underlying SSL object x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Removed IOException logging in getSession, which is expected to simply return SSL_NULL_WITH_NULL_NULL when there are problems. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Disabled ""Using factory"" message on successful creation of SocketFactory which was a bit noisy running tests. However, added logging in failure case including the related exception: x-net/src/main/java/javax/net/ssl/SSLSocketFactory.java Disabled logging of OpenSSL session deallocation x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp Register SSLContextImpl as a source of SSL and SSL3 SSLContexts, not just TLS and TLSv1. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/JSSEProvider.java Fix whitespace in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/CertificateRequest.java Change-Id: I99975ae22599c7df0d249fa013ae7ea7c9c08051/"
14,14,3.0,0.9970999956130981,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Rewrite JSSE code to use one openssl SSL per SSLSocket an one SSL_CTX per SSLSessionContext Summary: b/1758225: Revisit OpenSSL locking Removed the locking original put in to address b/1678800 which had been causing problems for the HeapWorker thread which was timing out waiting for the lock in the finalizers while other threads were connecting. b/1678800: Reliability tool: Crash in libcrypto Properly fixed the native crash by avoid sharing SSL_SESSION objects between SSL_CTX objects Testing: adb shell run-core-tests tests.xnet.AllTests adb shell run-core-tests javax.net.ssl.AllTests Test app that reloads Details: Each AbstractSessionContext now has an associated SSL_CTX, referenced through the sslCtxNativePointer. SSL_CTX on the native side defines the scope of SSL_SESSION caching, and this brings the Java SSLSessionContext caching into alignment with the native code. OpenSSLSessionImpl now uses AbstractSessionContext instead of SSLSessionContext for access to the underlying SSL_CTX. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Added AbstractSessionContext.putSession so OpenSSLSocketImpl/OpenSSLSessionImpl can directly assign to the current AbstractSessionContext (whether it be a ClientSessionContext or a ServerSessionContext) without casting. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Cleaning up use of SSL_CTX and SSL instances in SSLSocket/SSLServerSocket implementation The major change is that openssl SSL instances are allocated for the life of the matching Java object, replacing the SSL_CTX and the SSL objects that had previously been allocated only starting at handshake time. We should never have been sharing SSL_SESSION instances between SSL_CTX instances, which was the source of the native crashes dating back to cupcake which the OpenSSLSocket.class locking had been preventing. NativeCrypto now has better defined and independant wrappers on openssl functionality. A followon checkin should move the remaining openssl JNI code here with the intent of being able to write and end-to-end test of the openssl code using NativeCrypto without the JSSE implementation classes. The following gives a list of the new native functions with a mapping to the old implementation code. The new code has a more functional style where SSL_CTX and SSL instances are passed and returned as arguments, not extracted from Java instances SSL_CTX_new OpenSSLSocketImpl.nativeinit, OpenSSLServerSocketImpl.nativeinit, SSLParameters.nativeinitsslctx SSL_CTX_get_ciphers_list OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_CTX_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree SSL_new OpenSSLSocketImpl.nativeinit, OpenSSLSocketImpl.init, OpenSSLServerSocketImpl.nativeinit, OpenSSLServerSocketImpl.init SSL_get_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_set_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_get_ciphers OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_set_cipher_list OpenSSLSocketImpl.nativeSetEnabledCipherSuites SSL_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree While the focus in NativeCrypto is on native code, it also contains some helpers/wrappers especially for code that doesnt depend on specific SSL_CTX, SSL instances or that needs to do massaging of data formats between Java and OpenSSL. Some of these had previously been duplicated in the client and server versions of the code. For example: getSupportedCipherSuites OpenSSLSocketImpl.nativegetsupportedciphersuites, OpenSSLServerSocketImpl.nativegetsupportedciphersuites getSupportedProtocols OpenSSLSocketImpl.getSupportedProtocols, OpenSSLServerSocketImpl.getSupportedProtocols getEnabledProtocols OpenSSLSocketImpl.getEnabledProtocols,OpenSSLServerSocketImpl.getEnabledProtocols setEnabledProtocols OpenSSLSocketImpl.setEnabledProtocols setEnabledCipherSuites OpenSSLSocketImpl.setEnabledCipherSuites Moved JNI initialization from OpenSSLSocketImpl to NativeCrypto which is the future home of all the openssl related native code. clinit OpenSSLSocketImpl.nativeinitstatic NativeCrypto.CertificateChainVerifier is a new interface to decouple callbacks from openssl from a specific dependence on a OpenSSLSocketImpl.verify_callback method. Changed to return boolean instead of int. Renamed OpenSSLSocketImpl.ssl to OpenSSLSocketImpl.sslNativePointer for consistency Changed OpenSSLSocketImpl nativeconnect, nativegetsslsession, nativecipherauthenticationmethod, nativeaccept, nativeread, nativewrite, nativeinterrupt, nativeclose, nativefree to take arguments instead of inspect object state in preparation for moving to NativeCrypto other notable NativeCrypto changes included * adding SSL_SESSION_get_peer_cert_chain, SSL_SESSION_get_version, and SSL_get_version (and get_ssl_version) which are ""missing methods"" in openssl * ssl_msg_callback_LOG callback and get_content_type for handshake debugging * removing jfieldIDs for our classes now that we pass in values in arguments * changed aliveAndKicking to be volative since we poll on it to communicate between threads * changed from C style declarations at beginning of block to C++ at first use on methods with major changes * stop freeing SSL instances on error, only SSL_clear it * improved session reuse logging when reproducing b/1678800 * change verify_callback to return verifyCertificateChain result x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketFactoryImpl.java When we accept a server socket, we pass the existing SSL state instance from the server socket to the newly accepted socket via the constructor where it is copied with SSL_dup, instead of through both the constructor and later the accept method. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Cleaned up nativesetclientauth from using SSL_CTX to SSL, passing ssl as argument in preparation for future movement to NativeCrypto. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Removed ssl_op_no cache for rarely used enabled protocol methods so that code could more easily be shared in NativeCrypto between client and server. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed public getId, getCreationTime, getPeerCertificates, getCipherSuite, getProtocol from being instance methods that looked at the OpenSSLSessionImpl object state to be static mthods that take the native pointers as arguments in preparation for moving to NativeCrypto. Rename session sslSessionNativePointer for consistency. Inlined initializeNative, which wasnt really the native code. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Removed lock on OpenSSLSocketImpl.class lock from around OpenSSLSocketImpls use of nativeconnect, nativegetsslsession, and nativecipherauthenticationmethod as well as OpenSSLSessionImpls use of freeImpl, fixing b/1758225: Revisit OpenSSL locking x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Unrelated changes Removed unused ssl_ctx, nativeinitsslctx, getSSLCTX x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java Fix bug in both putSession implementations where we cached sessions with zero length id. Also change indexById to pass in id in client implementation. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Make sure we clone SSLParameters passed to the SSLSocketFactory and SSLServerSocketFactory so that muting the client instance does not change the server instance and vice versa. Explicitly set setUseClientMode(false) on the server SSLParameters. These changes are to bring things more into alignment with the original harmony classes which properly support client/server role switching during handshaking. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketFactoryImpl.java Make locks object fields final x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Moved updateInstanceCount(1) logic and sslParameters assignment to init method x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed getCachedClientSession to respect getUseClientMode x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling of listensers to listeners in javadoc x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling SSLInputStream to SSLOutputStream in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed shutdownInput and shutdownOutput to call to the underlying socket x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Set sslNativePointer to 0 when freeing underlying SSL object x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Removed IOException logging in getSession, which is expected to simply return SSL_NULL_WITH_NULL_NULL when there are problems. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Disabled ""Using factory"" message on successful creation of SocketFactory which was a bit noisy running tests. However, added logging in failure case including the related exception: x-net/src/main/java/javax/net/ssl/SSLSocketFactory.java Disabled logging of OpenSSL session deallocation x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp Register SSLContextImpl as a source of SSL and SSL3 SSLContexts, not just TLS and TLSv1. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/JSSEProvider.java Fix whitespace in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/CertificateRequest.java Change-Id: I99975ae22599c7df0d249fa013ae7ea7c9c08051/"
15,15,3.0,0.9970999956130981,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Rewrite JSSE code to use one openssl SSL per SSLSocket an one SSL_CTX per SSLSessionContext Summary: b/1758225: Revisit OpenSSL locking Removed the locking original put in to address b/1678800 which had been causing problems for the HeapWorker thread which was timing out waiting for the lock in the finalizers while other threads were connecting. b/1678800: Reliability tool: Crash in libcrypto Properly fixed the native crash by avoid sharing SSL_SESSION objects between SSL_CTX objects Testing: adb shell run-core-tests tests.xnet.AllTests adb shell run-core-tests javax.net.ssl.AllTests Test app that reloads Details: Each AbstractSessionContext now has an associated SSL_CTX, referenced through the sslCtxNativePointer. SSL_CTX on the native side defines the scope of SSL_SESSION caching, and this brings the Java SSLSessionContext caching into alignment with the native code. OpenSSLSessionImpl now uses AbstractSessionContext instead of SSLSessionContext for access to the underlying SSL_CTX. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Added AbstractSessionContext.putSession so OpenSSLSocketImpl/OpenSSLSessionImpl can directly assign to the current AbstractSessionContext (whether it be a ClientSessionContext or a ServerSessionContext) without casting. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Cleaning up use of SSL_CTX and SSL instances in SSLSocket/SSLServerSocket implementation The major change is that openssl SSL instances are allocated for the life of the matching Java object, replacing the SSL_CTX and the SSL objects that had previously been allocated only starting at handshake time. We should never have been sharing SSL_SESSION instances between SSL_CTX instances, which was the source of the native crashes dating back to cupcake which the OpenSSLSocket.class locking had been preventing. NativeCrypto now has better defined and independant wrappers on openssl functionality. A followon checkin should move the remaining openssl JNI code here with the intent of being able to write and end-to-end test of the openssl code using NativeCrypto without the JSSE implementation classes. The following gives a list of the new native functions with a mapping to the old implementation code. The new code has a more functional style where SSL_CTX and SSL instances are passed and returned as arguments, not extracted from Java instances SSL_CTX_new OpenSSLSocketImpl.nativeinit, OpenSSLServerSocketImpl.nativeinit, SSLParameters.nativeinitsslctx SSL_CTX_get_ciphers_list OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_CTX_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree SSL_new OpenSSLSocketImpl.nativeinit, OpenSSLSocketImpl.init, OpenSSLServerSocketImpl.nativeinit, OpenSSLServerSocketImpl.init SSL_get_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_set_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_get_ciphers OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_set_cipher_list OpenSSLSocketImpl.nativeSetEnabledCipherSuites SSL_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree While the focus in NativeCrypto is on native code, it also contains some helpers/wrappers especially for code that doesnt depend on specific SSL_CTX, SSL instances or that needs to do massaging of data formats between Java and OpenSSL. Some of these had previously been duplicated in the client and server versions of the code. For example: getSupportedCipherSuites OpenSSLSocketImpl.nativegetsupportedciphersuites, OpenSSLServerSocketImpl.nativegetsupportedciphersuites getSupportedProtocols OpenSSLSocketImpl.getSupportedProtocols, OpenSSLServerSocketImpl.getSupportedProtocols getEnabledProtocols OpenSSLSocketImpl.getEnabledProtocols,OpenSSLServerSocketImpl.getEnabledProtocols setEnabledProtocols OpenSSLSocketImpl.setEnabledProtocols setEnabledCipherSuites OpenSSLSocketImpl.setEnabledCipherSuites Moved JNI initialization from OpenSSLSocketImpl to NativeCrypto which is the future home of all the openssl related native code. clinit OpenSSLSocketImpl.nativeinitstatic NativeCrypto.CertificateChainVerifier is a new interface to decouple callbacks from openssl from a specific dependence on a OpenSSLSocketImpl.verify_callback method. Changed to return boolean instead of int. Renamed OpenSSLSocketImpl.ssl to OpenSSLSocketImpl.sslNativePointer for consistency Changed OpenSSLSocketImpl nativeconnect, nativegetsslsession, nativecipherauthenticationmethod, nativeaccept, nativeread, nativewrite, nativeinterrupt, nativeclose, nativefree to take arguments instead of inspect object state in preparation for moving to NativeCrypto other notable NativeCrypto changes included * adding SSL_SESSION_get_peer_cert_chain, SSL_SESSION_get_version, and SSL_get_version (and get_ssl_version) which are ""missing methods"" in openssl * ssl_msg_callback_LOG callback and get_content_type for handshake debugging * removing jfieldIDs for our classes now that we pass in values in arguments * changed aliveAndKicking to be volative since we poll on it to communicate between threads * changed from C style declarations at beginning of block to C++ at first use on methods with major changes * stop freeing SSL instances on error, only SSL_clear it * improved session reuse logging when reproducing b/1678800 * change verify_callback to return verifyCertificateChain result x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketFactoryImpl.java When we accept a server socket, we pass the existing SSL state instance from the server socket to the newly accepted socket via the constructor where it is copied with SSL_dup, instead of through both the constructor and later the accept method. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Cleaned up nativesetclientauth from using SSL_CTX to SSL, passing ssl as argument in preparation for future movement to NativeCrypto. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Removed ssl_op_no cache for rarely used enabled protocol methods so that code could more easily be shared in NativeCrypto between client and server. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed public getId, getCreationTime, getPeerCertificates, getCipherSuite, getProtocol from being instance methods that looked at the OpenSSLSessionImpl object state to be static mthods that take the native pointers as arguments in preparation for moving to NativeCrypto. Rename session sslSessionNativePointer for consistency. Inlined initializeNative, which wasnt really the native code. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Removed lock on OpenSSLSocketImpl.class lock from around OpenSSLSocketImpls use of nativeconnect, nativegetsslsession, and nativecipherauthenticationmethod as well as OpenSSLSessionImpls use of freeImpl, fixing b/1758225: Revisit OpenSSL locking x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Unrelated changes Removed unused ssl_ctx, nativeinitsslctx, getSSLCTX x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java Fix bug in both putSession implementations where we cached sessions with zero length id. Also change indexById to pass in id in client implementation. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Make sure we clone SSLParameters passed to the SSLSocketFactory and SSLServerSocketFactory so that muting the client instance does not change the server instance and vice versa. Explicitly set setUseClientMode(false) on the server SSLParameters. These changes are to bring things more into alignment with the original harmony classes which properly support client/server role switching during handshaking. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketFactoryImpl.java Make locks object fields final x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Moved updateInstanceCount(1) logic and sslParameters assignment to init method x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed getCachedClientSession to respect getUseClientMode x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling of listensers to listeners in javadoc x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling SSLInputStream to SSLOutputStream in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed shutdownInput and shutdownOutput to call to the underlying socket x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Set sslNativePointer to 0 when freeing underlying SSL object x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Removed IOException logging in getSession, which is expected to simply return SSL_NULL_WITH_NULL_NULL when there are problems. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Disabled ""Using factory"" message on successful creation of SocketFactory which was a bit noisy running tests. However, added logging in failure case including the related exception: x-net/src/main/java/javax/net/ssl/SSLSocketFactory.java Disabled logging of OpenSSL session deallocation x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp Register SSLContextImpl as a source of SSL and SSL3 SSLContexts, not just TLS and TLSv1. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/JSSEProvider.java Fix whitespace in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/CertificateRequest.java Change-Id: I99975ae22599c7df0d249fa013ae7ea7c9c08051/"
16,16,3.0,0.9970999956130981,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Rewrite JSSE code to use one openssl SSL per SSLSocket an one SSL_CTX per SSLSessionContext Summary: b/1758225: Revisit OpenSSL locking Removed the locking original put in to address b/1678800 which had been causing problems for the HeapWorker thread which was timing out waiting for the lock in the finalizers while other threads were connecting. b/1678800: Reliability tool: Crash in libcrypto Properly fixed the native crash by avoid sharing SSL_SESSION objects between SSL_CTX objects Testing: adb shell run-core-tests tests.xnet.AllTests adb shell run-core-tests javax.net.ssl.AllTests Test app that reloads Details: Each AbstractSessionContext now has an associated SSL_CTX, referenced through the sslCtxNativePointer. SSL_CTX on the native side defines the scope of SSL_SESSION caching, and this brings the Java SSLSessionContext caching into alignment with the native code. OpenSSLSessionImpl now uses AbstractSessionContext instead of SSLSessionContext for access to the underlying SSL_CTX. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Added AbstractSessionContext.putSession so OpenSSLSocketImpl/OpenSSLSessionImpl can directly assign to the current AbstractSessionContext (whether it be a ClientSessionContext or a ServerSessionContext) without casting. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Cleaning up use of SSL_CTX and SSL instances in SSLSocket/SSLServerSocket implementation The major change is that openssl SSL instances are allocated for the life of the matching Java object, replacing the SSL_CTX and the SSL objects that had previously been allocated only starting at handshake time. We should never have been sharing SSL_SESSION instances between SSL_CTX instances, which was the source of the native crashes dating back to cupcake which the OpenSSLSocket.class locking had been preventing. NativeCrypto now has better defined and independant wrappers on openssl functionality. A followon checkin should move the remaining openssl JNI code here with the intent of being able to write and end-to-end test of the openssl code using NativeCrypto without the JSSE implementation classes. The following gives a list of the new native functions with a mapping to the old implementation code. The new code has a more functional style where SSL_CTX and SSL instances are passed and returned as arguments, not extracted from Java instances SSL_CTX_new OpenSSLSocketImpl.nativeinit, OpenSSLServerSocketImpl.nativeinit, SSLParameters.nativeinitsslctx SSL_CTX_get_ciphers_list OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_CTX_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree SSL_new OpenSSLSocketImpl.nativeinit, OpenSSLSocketImpl.init, OpenSSLServerSocketImpl.nativeinit, OpenSSLServerSocketImpl.init SSL_get_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_set_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_get_ciphers OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_set_cipher_list OpenSSLSocketImpl.nativeSetEnabledCipherSuites SSL_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree While the focus in NativeCrypto is on native code, it also contains some helpers/wrappers especially for code that doesnt depend on specific SSL_CTX, SSL instances or that needs to do massaging of data formats between Java and OpenSSL. Some of these had previously been duplicated in the client and server versions of the code. For example: getSupportedCipherSuites OpenSSLSocketImpl.nativegetsupportedciphersuites, OpenSSLServerSocketImpl.nativegetsupportedciphersuites getSupportedProtocols OpenSSLSocketImpl.getSupportedProtocols, OpenSSLServerSocketImpl.getSupportedProtocols getEnabledProtocols OpenSSLSocketImpl.getEnabledProtocols,OpenSSLServerSocketImpl.getEnabledProtocols setEnabledProtocols OpenSSLSocketImpl.setEnabledProtocols setEnabledCipherSuites OpenSSLSocketImpl.setEnabledCipherSuites Moved JNI initialization from OpenSSLSocketImpl to NativeCrypto which is the future home of all the openssl related native code. clinit OpenSSLSocketImpl.nativeinitstatic NativeCrypto.CertificateChainVerifier is a new interface to decouple callbacks from openssl from a specific dependence on a OpenSSLSocketImpl.verify_callback method. Changed to return boolean instead of int. Renamed OpenSSLSocketImpl.ssl to OpenSSLSocketImpl.sslNativePointer for consistency Changed OpenSSLSocketImpl nativeconnect, nativegetsslsession, nativecipherauthenticationmethod, nativeaccept, nativeread, nativewrite, nativeinterrupt, nativeclose, nativefree to take arguments instead of inspect object state in preparation for moving to NativeCrypto other notable NativeCrypto changes included * adding SSL_SESSION_get_peer_cert_chain, SSL_SESSION_get_version, and SSL_get_version (and get_ssl_version) which are ""missing methods"" in openssl * ssl_msg_callback_LOG callback and get_content_type for handshake debugging * removing jfieldIDs for our classes now that we pass in values in arguments * changed aliveAndKicking to be volative since we poll on it to communicate between threads * changed from C style declarations at beginning of block to C++ at first use on methods with major changes * stop freeing SSL instances on error, only SSL_clear it * improved session reuse logging when reproducing b/1678800 * change verify_callback to return verifyCertificateChain result x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketFactoryImpl.java When we accept a server socket, we pass the existing SSL state instance from the server socket to the newly accepted socket via the constructor where it is copied with SSL_dup, instead of through both the constructor and later the accept method. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Cleaned up nativesetclientauth from using SSL_CTX to SSL, passing ssl as argument in preparation for future movement to NativeCrypto. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Removed ssl_op_no cache for rarely used enabled protocol methods so that code could more easily be shared in NativeCrypto between client and server. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed public getId, getCreationTime, getPeerCertificates, getCipherSuite, getProtocol from being instance methods that looked at the OpenSSLSessionImpl object state to be static mthods that take the native pointers as arguments in preparation for moving to NativeCrypto. Rename session sslSessionNativePointer for consistency. Inlined initializeNative, which wasnt really the native code. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Removed lock on OpenSSLSocketImpl.class lock from around OpenSSLSocketImpls use of nativeconnect, nativegetsslsession, and nativecipherauthenticationmethod as well as OpenSSLSessionImpls use of freeImpl, fixing b/1758225: Revisit OpenSSL locking x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Unrelated changes Removed unused ssl_ctx, nativeinitsslctx, getSSLCTX x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java Fix bug in both putSession implementations where we cached sessions with zero length id. Also change indexById to pass in id in client implementation. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Make sure we clone SSLParameters passed to the SSLSocketFactory and SSLServerSocketFactory so that muting the client instance does not change the server instance and vice versa. Explicitly set setUseClientMode(false) on the server SSLParameters. These changes are to bring things more into alignment with the original harmony classes which properly support client/server role switching during handshaking. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketFactoryImpl.java Make locks object fields final x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Moved updateInstanceCount(1) logic and sslParameters assignment to init method x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed getCachedClientSession to respect getUseClientMode x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling of listensers to listeners in javadoc x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling SSLInputStream to SSLOutputStream in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed shutdownInput and shutdownOutput to call to the underlying socket x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Set sslNativePointer to 0 when freeing underlying SSL object x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Removed IOException logging in getSession, which is expected to simply return SSL_NULL_WITH_NULL_NULL when there are problems. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Disabled ""Using factory"" message on successful creation of SocketFactory which was a bit noisy running tests. However, added logging in failure case including the related exception: x-net/src/main/java/javax/net/ssl/SSLSocketFactory.java Disabled logging of OpenSSL session deallocation x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp Register SSLContextImpl as a source of SSL and SSL3 SSLContexts, not just TLS and TLSv1. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/JSSEProvider.java Fix whitespace in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/CertificateRequest.java Change-Id: I99975ae22599c7df0d249fa013ae7ea7c9c08051/"
17,17,3.0,0.9972000122070312,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Rewrite JSSE code to use one openssl SSL per SSLSocket an one SSL_CTX per SSLSessionContext Summary: b/1758225: Revisit OpenSSL locking Removed the locking original put in to address b/1678800 which had been causing problems for the HeapWorker thread which was timing out waiting for the lock in the finalizers while other threads were connecting. b/1678800: Reliability tool: Crash in libcrypto Properly fixed the native crash by avoid sharing SSL_SESSION objects between SSL_CTX objects Testing: adb shell run-core-tests tests.xnet.AllTests adb shell run-core-tests javax.net.ssl.AllTests Test app that reloads Details: Each AbstractSessionContext now has an associated SSL_CTX, referenced through the sslCtxNativePointer. SSL_CTX on the native side defines the scope of SSL_SESSION caching, and this brings the Java SSLSessionContext caching into alignment with the native code. OpenSSLSessionImpl now uses AbstractSessionContext instead of SSLSessionContext for access to the underlying SSL_CTX. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Added AbstractSessionContext.putSession so OpenSSLSocketImpl/OpenSSLSessionImpl can directly assign to the current AbstractSessionContext (whether it be a ClientSessionContext or a ServerSessionContext) without casting. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Cleaning up use of SSL_CTX and SSL instances in SSLSocket/SSLServerSocket implementation The major change is that openssl SSL instances are allocated for the life of the matching Java object, replacing the SSL_CTX and the SSL objects that had previously been allocated only starting at handshake time. We should never have been sharing SSL_SESSION instances between SSL_CTX instances, which was the source of the native crashes dating back to cupcake which the OpenSSLSocket.class locking had been preventing. NativeCrypto now has better defined and independant wrappers on openssl functionality. A followon checkin should move the remaining openssl JNI code here with the intent of being able to write and end-to-end test of the openssl code using NativeCrypto without the JSSE implementation classes. The following gives a list of the new native functions with a mapping to the old implementation code. The new code has a more functional style where SSL_CTX and SSL instances are passed and returned as arguments, not extracted from Java instances SSL_CTX_new OpenSSLSocketImpl.nativeinit, OpenSSLServerSocketImpl.nativeinit, SSLParameters.nativeinitsslctx SSL_CTX_get_ciphers_list OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_CTX_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree SSL_new OpenSSLSocketImpl.nativeinit, OpenSSLSocketImpl.init, OpenSSLServerSocketImpl.nativeinit, OpenSSLServerSocketImpl.init SSL_get_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_set_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_get_ciphers OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_set_cipher_list OpenSSLSocketImpl.nativeSetEnabledCipherSuites SSL_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree While the focus in NativeCrypto is on native code, it also contains some helpers/wrappers especially for code that doesnt depend on specific SSL_CTX, SSL instances or that needs to do massaging of data formats between Java and OpenSSL. Some of these had previously been duplicated in the client and server versions of the code. For example: getSupportedCipherSuites OpenSSLSocketImpl.nativegetsupportedciphersuites, OpenSSLServerSocketImpl.nativegetsupportedciphersuites getSupportedProtocols OpenSSLSocketImpl.getSupportedProtocols, OpenSSLServerSocketImpl.getSupportedProtocols getEnabledProtocols OpenSSLSocketImpl.getEnabledProtocols,OpenSSLServerSocketImpl.getEnabledProtocols setEnabledProtocols OpenSSLSocketImpl.setEnabledProtocols setEnabledCipherSuites OpenSSLSocketImpl.setEnabledCipherSuites Moved JNI initialization from OpenSSLSocketImpl to NativeCrypto which is the future home of all the openssl related native code. clinit OpenSSLSocketImpl.nativeinitstatic NativeCrypto.CertificateChainVerifier is a new interface to decouple callbacks from openssl from a specific dependence on a OpenSSLSocketImpl.verify_callback method. Changed to return boolean instead of int. Renamed OpenSSLSocketImpl.ssl to OpenSSLSocketImpl.sslNativePointer for consistency Changed OpenSSLSocketImpl nativeconnect, nativegetsslsession, nativecipherauthenticationmethod, nativeaccept, nativeread, nativewrite, nativeinterrupt, nativeclose, nativefree to take arguments instead of inspect object state in preparation for moving to NativeCrypto other notable NativeCrypto changes included * adding SSL_SESSION_get_peer_cert_chain, SSL_SESSION_get_version, and SSL_get_version (and get_ssl_version) which are ""missing methods"" in openssl * ssl_msg_callback_LOG callback and get_content_type for handshake debugging * removing jfieldIDs for our classes now that we pass in values in arguments * changed aliveAndKicking to be volative since we poll on it to communicate between threads * changed from C style declarations at beginning of block to C++ at first use on methods with major changes * stop freeing SSL instances on error, only SSL_clear it * improved session reuse logging when reproducing b/1678800 * change verify_callback to return verifyCertificateChain result x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketFactoryImpl.java When we accept a server socket, we pass the existing SSL state instance from the server socket to the newly accepted socket via the constructor where it is copied with SSL_dup, instead of through both the constructor and later the accept method. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Cleaned up nativesetclientauth from using SSL_CTX to SSL, passing ssl as argument in preparation for future movement to NativeCrypto. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Removed ssl_op_no cache for rarely used enabled protocol methods so that code could more easily be shared in NativeCrypto between client and server. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed public getId, getCreationTime, getPeerCertificates, getCipherSuite, getProtocol from being instance methods that looked at the OpenSSLSessionImpl object state to be static mthods that take the native pointers as arguments in preparation for moving to NativeCrypto. Rename session sslSessionNativePointer for consistency. Inlined initializeNative, which wasnt really the native code. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Removed lock on OpenSSLSocketImpl.class lock from around OpenSSLSocketImpls use of nativeconnect, nativegetsslsession, and nativecipherauthenticationmethod as well as OpenSSLSessionImpls use of freeImpl, fixing b/1758225: Revisit OpenSSL locking x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Unrelated changes Removed unused ssl_ctx, nativeinitsslctx, getSSLCTX x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java Fix bug in both putSession implementations where we cached sessions with zero length id. Also change indexById to pass in id in client implementation. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Make sure we clone SSLParameters passed to the SSLSocketFactory and SSLServerSocketFactory so that muting the client instance does not change the server instance and vice versa. Explicitly set setUseClientMode(false) on the server SSLParameters. These changes are to bring things more into alignment with the original harmony classes which properly support client/server role switching during handshaking. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketFactoryImpl.java Make locks object fields final x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Moved updateInstanceCount(1) logic and sslParameters assignment to init method x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed getCachedClientSession to respect getUseClientMode x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling of listensers to listeners in javadoc x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling SSLInputStream to SSLOutputStream in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed shutdownInput and shutdownOutput to call to the underlying socket x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Set sslNativePointer to 0 when freeing underlying SSL object x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Removed IOException logging in getSession, which is expected to simply return SSL_NULL_WITH_NULL_NULL when there are problems. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Disabled ""Using factory"" message on successful creation of SocketFactory which was a bit noisy running tests. However, added logging in failure case including the related exception: x-net/src/main/java/javax/net/ssl/SSLSocketFactory.java Disabled logging of OpenSSL session deallocation x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp Register SSLContextImpl as a source of SSL and SSL3 SSLContexts, not just TLS and TLSv1. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/JSSEProvider.java Fix whitespace in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/CertificateRequest.java Change-Id: I99975ae22599c7df0d249fa013ae7ea7c9c08051/Add a setHandshakeTimeout() to OpenSSLSocketImpl, which sets a read timeout that only applies to the SSL handshake step. Bug: 2362543/"
18,18,3.0,0.9970999956130981,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Rewrite JSSE code to use one openssl SSL per SSLSocket an one SSL_CTX per SSLSessionContext Summary: b/1758225: Revisit OpenSSL locking Removed the locking original put in to address b/1678800 which had been causing problems for the HeapWorker thread which was timing out waiting for the lock in the finalizers while other threads were connecting. b/1678800: Reliability tool: Crash in libcrypto Properly fixed the native crash by avoid sharing SSL_SESSION objects between SSL_CTX objects Testing: adb shell run-core-tests tests.xnet.AllTests adb shell run-core-tests javax.net.ssl.AllTests Test app that reloads Details: Each AbstractSessionContext now has an associated SSL_CTX, referenced through the sslCtxNativePointer. SSL_CTX on the native side defines the scope of SSL_SESSION caching, and this brings the Java SSLSessionContext caching into alignment with the native code. OpenSSLSessionImpl now uses AbstractSessionContext instead of SSLSessionContext for access to the underlying SSL_CTX. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Added AbstractSessionContext.putSession so OpenSSLSocketImpl/OpenSSLSessionImpl can directly assign to the current AbstractSessionContext (whether it be a ClientSessionContext or a ServerSessionContext) without casting. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Cleaning up use of SSL_CTX and SSL instances in SSLSocket/SSLServerSocket implementation The major change is that openssl SSL instances are allocated for the life of the matching Java object, replacing the SSL_CTX and the SSL objects that had previously been allocated only starting at handshake time. We should never have been sharing SSL_SESSION instances between SSL_CTX instances, which was the source of the native crashes dating back to cupcake which the OpenSSLSocket.class locking had been preventing. NativeCrypto now has better defined and independant wrappers on openssl functionality. A followon checkin should move the remaining openssl JNI code here with the intent of being able to write and end-to-end test of the openssl code using NativeCrypto without the JSSE implementation classes. The following gives a list of the new native functions with a mapping to the old implementation code. The new code has a more functional style where SSL_CTX and SSL instances are passed and returned as arguments, not extracted from Java instances SSL_CTX_new OpenSSLSocketImpl.nativeinit, OpenSSLServerSocketImpl.nativeinit, SSLParameters.nativeinitsslctx SSL_CTX_get_ciphers_list OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_CTX_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree SSL_new OpenSSLSocketImpl.nativeinit, OpenSSLSocketImpl.init, OpenSSLServerSocketImpl.nativeinit, OpenSSLServerSocketImpl.init SSL_get_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_set_options OpenSSLSocketImpl.nativesetenabledprotocols SSL_get_ciphers OpenSSLSocketImpl.nativeGetEnabledCipherSuites SSL_set_cipher_list OpenSSLSocketImpl.nativeSetEnabledCipherSuites SSL_free OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree While the focus in NativeCrypto is on native code, it also contains some helpers/wrappers especially for code that doesnt depend on specific SSL_CTX, SSL instances or that needs to do massaging of data formats between Java and OpenSSL. Some of these had previously been duplicated in the client and server versions of the code. For example: getSupportedCipherSuites OpenSSLSocketImpl.nativegetsupportedciphersuites, OpenSSLServerSocketImpl.nativegetsupportedciphersuites getSupportedProtocols OpenSSLSocketImpl.getSupportedProtocols, OpenSSLServerSocketImpl.getSupportedProtocols getEnabledProtocols OpenSSLSocketImpl.getEnabledProtocols,OpenSSLServerSocketImpl.getEnabledProtocols setEnabledProtocols OpenSSLSocketImpl.setEnabledProtocols setEnabledCipherSuites OpenSSLSocketImpl.setEnabledCipherSuites Moved JNI initialization from OpenSSLSocketImpl to NativeCrypto which is the future home of all the openssl related native code. clinit OpenSSLSocketImpl.nativeinitstatic NativeCrypto.CertificateChainVerifier is a new interface to decouple callbacks from openssl from a specific dependence on a OpenSSLSocketImpl.verify_callback method. Changed to return boolean instead of int. Renamed OpenSSLSocketImpl.ssl to OpenSSLSocketImpl.sslNativePointer for consistency Changed OpenSSLSocketImpl nativeconnect, nativegetsslsession, nativecipherauthenticationmethod, nativeaccept, nativeread, nativewrite, nativeinterrupt, nativeclose, nativefree to take arguments instead of inspect object state in preparation for moving to NativeCrypto other notable NativeCrypto changes included * adding SSL_SESSION_get_peer_cert_chain, SSL_SESSION_get_version, and SSL_get_version (and get_ssl_version) which are ""missing methods"" in openssl * ssl_msg_callback_LOG callback and get_content_type for handshake debugging * removing jfieldIDs for our classes now that we pass in values in arguments * changed aliveAndKicking to be volative since we poll on it to communicate between threads * changed from C style declarations at beginning of block to C++ at first use on methods with major changes * stop freeing SSL instances on error, only SSL_clear it * improved session reuse logging when reproducing b/1678800 * change verify_callback to return verifyCertificateChain result x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketFactoryImpl.java When we accept a server socket, we pass the existing SSL state instance from the server socket to the newly accepted socket via the constructor where it is copied with SSL_dup, instead of through both the constructor and later the accept method. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Cleaned up nativesetclientauth from using SSL_CTX to SSL, passing ssl as argument in preparation for future movement to NativeCrypto. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java Removed ssl_op_no cache for rarely used enabled protocol methods so that code could more easily be shared in NativeCrypto between client and server. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed public getId, getCreationTime, getPeerCertificates, getCipherSuite, getProtocol from being instance methods that looked at the OpenSSLSessionImpl object state to be static mthods that take the native pointers as arguments in preparation for moving to NativeCrypto. Rename session sslSessionNativePointer for consistency. Inlined initializeNative, which wasnt really the native code. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Removed lock on OpenSSLSocketImpl.class lock from around OpenSSLSocketImpls use of nativeconnect, nativegetsslsession, and nativecipherauthenticationmethod as well as OpenSSLSessionImpls use of freeImpl, fixing b/1758225: Revisit OpenSSL locking x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Unrelated changes Removed unused ssl_ctx, nativeinitsslctx, getSSLCTX x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/SSLParameters.java Fix bug in both putSession implementations where we cached sessions with zero length id. Also change indexById to pass in id in client implementation. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ClientSessionContext.java x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/ServerSessionContext.java Make sure we clone SSLParameters passed to the SSLSocketFactory and SSLServerSocketFactory so that muting the client instance does not change the server instance and vice versa. Explicitly set setUseClientMode(false) on the server SSLParameters. These changes are to bring things more into alignment with the original harmony classes which properly support client/server role switching during handshaking. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketFactoryImpl.java Make locks object fields final x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Moved updateInstanceCount(1) logic and sslParameters assignment to init method x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed getCachedClientSession to respect getUseClientMode x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling of listensers to listeners in javadoc x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Spelling SSLInputStream to SSLOutputStream in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Changed shutdownInput and shutdownOutput to call to the underlying socket x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Set sslNativePointer to 0 when freeing underlying SSL object x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Removed IOException logging in getSession, which is expected to simply return SSL_NULL_WITH_NULL_NULL when there are problems. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Disabled ""Using factory"" message on successful creation of SocketFactory which was a bit noisy running tests. However, added logging in failure case including the related exception: x-net/src/main/java/javax/net/ssl/SSLSocketFactory.java Disabled logging of OpenSSL session deallocation x-net/src/main/native/org_apache_harmony_xnet_provider_jsse_NativeCrypto.cpp Register SSLContextImpl as a source of SSL and SSL3 SSLContexts, not just TLS and TLSv1. x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/JSSEProvider.java Fix whitespace in comment x-net/src/main/java/org/apache/harmony/xnet/provider/jsse/CertificateRequest.java Change-Id: I99975ae22599c7df0d249fa013ae7ea7c9c08051/"
19,19,9.0,0.864300012588501,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Rename internal SSLParameters to SSLParametersImpl to avoid collision with new javax.net.ssl.SSLParameters Bug: 2672817 Change-Id: Iadf21b848eaf8850fce22721b9ba3739ab2e9fca/
20,20,0.0,0.977400004863739,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Implement OpenSSLMessageDigestJDK.clone and fix OpenSSLMessageDigestJDK.digest DigestInputStream2Test.test_onZ was failing because OpenSSLMessageDigestJDK did not implement Clonable Implementing Clonable required a new NativeCrypto.EVP_MD_CTX_copy method While adding NativeCrypto.EVP_MD_CTX_copy, noticed other methods were not properly named in NativeCrypto.EVP_MD_CTX_* convention. Converted rest of NativeCrypto.cpp to JNI_TRACE logging while debugging DigestOutputStreamTest.test_onZ was failing because OpenSSLMessageDigestJDK.digest did an engineReset Removing the engineReset revealed that digest() could not be called repeatedly on an OpenSSLMessageDigestJDK. Problem was that EVP_DigestFinal can only be called once per digest. Changed engineDigest implementation to use new EVP_MD_CTX_copy to create a temp EVP_MD_CTX which can be used to retreive the digest and then discarded. Bug: 2997405 Change-Id: Ie97c22be245911300d2e729e451a9c4afdb27937/"
21,21,0.0,0.7239999771118164,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Stop allocating empty arrays. Bug: 3166662 Change-Id: I151de373b2bf53786d19824336fa434c02b0b0e8/SSL* AppData should not hold onto JNI global references Summary: NativeCrypto.SSL_do_handshake stored JNI global references in its AppData instance for use in upcalls from OpenSSL that invoke Java callbacks. However, one of the references was to the SSLHandshakeCallbacks which in the common case of OpenSSLSocketImpl is the OpenSSLSocketImpl instance itself. This meant that if code dropped the OpenSSLSocketImpl without closing (such as Apache HTTP Client), the instances would never be collected, and perhaps more importantly, file descriptors would not be closed. The fix is to pass in the objects required during a callback in all downcalls to SSL_* methods that could result in a callback and clear them on return. The existing code already did this for the JNIEnv*, so that code was expanded to handle setting the jobjects as well. Details: In the native code used to extract the FileDescriptor object from a Socket on the call to NativeCrypto.SSL_do_handshake. However, since we need this for every read and write operations, we now do this in Java to avoid the repeated overhead. NativeCrypto.SSL_do_handshake now takes a FileDescriptor, which it extracted from the Socket the convenience function using NativeCrypto.getFileDescriptor(Socket) luni/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java In addition to tracking changes to pass FileDescriptor and SSLHandshakeCallbacks, removed final uses of getFieldId since the code no longer needs to extract FileDescriptors itself luni/src/main/native/NativeCrypto.cpp The Socket field used to be non-null in the wrapper case and null in the non-wrapper case. To simplify things a bit, ""socket this"" in the non-wrapper case. The socket field is now also final and joined by a final FileDescriptor field. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Updated NativeCryptoTest to track FileDescriptor and SSLHandshakeCallbacks by expanding the Hooks.afterHandshake to provide them. Also changed to add a 5 second timeout to many test cases. luni/src/test/java/org/apache/harmony/xnet/provider/jsse/NativeCryptoTest.java Bug: 2989218 Change-Id: Iccef92b59475f3c1929e990893579493ece9d442/Implement OpenSSLMessageDigestJDK.clone and fix OpenSSLMessageDigestJDK.digest DigestInputStream2Test.test_onZ was failing because OpenSSLMessageDigestJDK did not implement Clonable Implementing Clonable required a new NativeCrypto.EVP_MD_CTX_copy method While adding NativeCrypto.EVP_MD_CTX_copy, noticed other methods were not properly named in NativeCrypto.EVP_MD_CTX_* convention. Converted rest of NativeCrypto.cpp to JNI_TRACE logging while debugging DigestOutputStreamTest.test_onZ was failing because OpenSSLMessageDigestJDK.digest did an engineReset Removing the engineReset revealed that digest() could not be called repeatedly on an OpenSSLMessageDigestJDK. Problem was that EVP_DigestFinal can only be called once per digest. Changed engineDigest implementation to use new EVP_MD_CTX_copy to create a temp EVP_MD_CTX which can be used to retreive the digest and then discarded. Bug: 2997405 Change-Id: Ie97c22be245911300d2e729e451a9c4afdb27937/"
22,22,3.0,0.36169999837875366,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Avoid races between OpenSSLSocketImpl I/O and close() The previous change: commit 5f2e6872311240319509aed64d9f58cd5b64719b Author: Brian Carlstrom Date: Mon Aug 23 14:06:51 2010 SSLSocket.read should throw SocketException not NullPointerException added checkOpen() to throw SocketException instead of NullPointerException, but there was still a race between read/write on one thread and close on another that could allow a NullPointerException to escape. This change moves checkOpen() calls to be protected by the existing writeLock/readLock/handshakeLock synchronzied blocks to avoid this case. byte buffer error checking for read/write is also moved into the to lock region to preserve compatability as measured by the test: libcore.javax.net.ssl.SSLSocketTest#test_SSLSocket_close Bug: 3153162 Change-Id: I16299f09dc91871407e88eb718073d21a816f683/CloseGuard: finalizers for closeable objects should log complaints Introducing CloseGuard which warns when resources are implictly cleaned up by finalizers when an explicit termination method, to use the Effective Java ""Issue 7: Avoid finalizers"" terminology, should have been used by the caller. libcore classes that can use CloseGuard now do so. Bug: 3041575 Change-Id: I4a4e3554addaf3075c823feb0a0ff0ad1c1f6196/SSL* AppData should not hold onto JNI global references Summary: NativeCrypto.SSL_do_handshake stored JNI global references in its AppData instance for use in upcalls from OpenSSL that invoke Java callbacks. However, one of the references was to the SSLHandshakeCallbacks which in the common case of OpenSSLSocketImpl is the OpenSSLSocketImpl instance itself. This meant that if code dropped the OpenSSLSocketImpl without closing (such as Apache HTTP Client), the instances would never be collected, and perhaps more importantly, file descriptors would not be closed. The fix is to pass in the objects required during a callback in all downcalls to SSL_* methods that could result in a callback and clear them on return. The existing code already did this for the JNIEnv*, so that code was expanded to handle setting the jobjects as well. Details: In the native code used to extract the FileDescriptor object from a Socket on the call to NativeCrypto.SSL_do_handshake. However, since we need this for every read and write operations, we now do this in Java to avoid the repeated overhead. NativeCrypto.SSL_do_handshake now takes a FileDescriptor, which it extracted from the Socket the convenience function using NativeCrypto.getFileDescriptor(Socket) luni/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java In addition to tracking changes to pass FileDescriptor and SSLHandshakeCallbacks, removed final uses of getFieldId since the code no longer needs to extract FileDescriptors itself luni/src/main/native/NativeCrypto.cpp The Socket field used to be non-null in the wrapper case and null in the non-wrapper case. To simplify things a bit, ""socket this"" in the non-wrapper case. The socket field is now also final and joined by a final FileDescriptor field. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Updated NativeCryptoTest to track FileDescriptor and SSLHandshakeCallbacks by expanding the Hooks.afterHandshake to provide them. Also changed to add a 5 second timeout to many test cases. luni/src/test/java/org/apache/harmony/xnet/provider/jsse/NativeCryptoTest.java Bug: 2989218 Change-Id: Iccef92b59475f3c1929e990893579493ece9d442/OpenSSLSocketImpl should not call NativeCrypto.SSL_set_client_CA_list with an empty array Bug: 3034616 Change-Id: Ib39ebfa737910f0ebce5ac2ad87715579bd7aa3d/SSLSocket should respect timeout of a wrapped Socket Change to using getSoTimeout in OpenSSLSocketImpl instead of directly using the timeout field. This means the proper timeout will be used for instances of the OpenSSLSocketImplWrapper subclass, which is used when an SSLSocket is wrapped around an existing connected non-SSL Socket. The code still maintains the local timeout field, now renamed timeoutMilliseconds, which is now accesed via OpenSSLSocketImpl.getSoTimeout. Doing so prevents a getsockopt syscall that otherwise would be necessary if the super.getSoTimeout() was used. Added two unit tests for testing timeouts with SSLSockets wrapped around Socket. One is simply for getters/setters. The second makes sure the timeout is functioning when set on the underlying socket. Bug: 2973305 Change-Id: Idac52853f5d777fae5060a840eefbfe85d448e4c/Fix HttpsURLConnectionTest failures Focusing on HttpsURLConnectionTest.test_doOutput found a number of unrelated issues, all of which are addressed by this change: {HttpURLConnection,HttpsURLConnection}.connect not ignored on subsequent calls OpenSSLSessionImpl.{getPeerCertificates,getPeerCertificateChain} did not include client certificate OpenSSLSocketImpl.getSession did not skip handshake when SSLSession was already available Fix 3 test issues in HttpsURLConnectionTest Fix 2 test issues in NativeCryptoTest Details: HttpsURLConnectionTest tests (such as test_doOutput) that tried to call URLConnection.connect() at the end of the test were raising exception. The RI URLConnection.connect documentation says calls on connected URLConnections should be ignored. Use ""connected"" instead of ""connection null"" as reason to ignore ""connect"" luni/src/main/java/org/apache/harmony/luni/internal/net/www/protocol/ luni/src/main/java/org/apache/harmony/luni/internal/net/www/protocol/ Converted one caller of getPeerCertificateChain to getPeerCertificates which is the new fast path. Track OpenSSLSessionImpl change to take ""java"" vs ""javax"" certificates. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/AbstractSessionContext.java Move SSL_SESSION_get_peer_cert_chain to be SSL_get_peer_cert_chain (similar to SSL_get_certificate). The problem was that SSL_SESSION_get_peer_cert_chain used SSL_get_peer_cert_chain which in the server case did not include the client cert itself, which required a call to SSL_get_peer_certificate, which needed the SSL instance pointer. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java luni/src/main/native/NativeCrypto.cpp Improved NativeCrypto_SSL_set_verify tracing luni/src/main/native/NativeCrypto.cpp As a side effect of the move to NativeCrypto.SSL_get_peer_certificate, it no longer made sense to lazily create the peer certificate chain since the SSLSession should not depend on a particular SSL instance. The peer chain is now passed in as part of the constructor and the peerCertifcates in the OpenSSLSession can be final (also made localCertificates final). Since peerCertifcates is the newew (java not javax) API and more commonly used, it is what is created from the native code, and peerCertificateChain is not derived from peerCertifcates instead of vice versa. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSessionImpl.java Factored out code to used to create local certificate chain to from array of DER byte arrays into createCertChain so it can be reused to create peer certificate chain. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Fix OpenSSLSocketImpl.getSession to check for existing sslSession to and skip handshake, which was causing an exception if the connection had already been closed. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java Fix test issues: Removed PrintStream wrapper of System.out which was causing vogar to lose output. Added null check in closeSocket, which can happen in timeout case. Removed use of InputStream.available which in OpenSSLSocket case returned 0, causing test to fail incorrectly. luni/src/test/java/org/apache/harmony/luni/tests/internal/net/www/protocol/ Updating to track change to SSL_get_peer_cert_chain. Also fixed some other unrelated test failures caused by IOException on shutdown and false start (aka SSL_MODE_HANDSHAKE_CUTTHROUGH) causing clientCallback.handshakeCompleted to be false. luni/src/test/java/org/apache/harmony/xnet/provider/jsse/NativeCryptoTest.java Bug: b/2981767 Change-Id: Id083beb6496558296c2f74f51ab0970e158b23a9/Use BlockGuard for OpenSSL sockets. StrictMode wasnt catching network usage via SSL. Bug: 2976407 Change-Id: I31fe09861e3aca7b26724b94af88687fb6b9442b/SSLSocket.read should throw SocketException not NullPointerException OpenSSLSocketImpl now uses checkOpen similar to Sockets checkOpenAndCreate to ensure that SocketExceptions are thrown if certain operations are tried after the socket is closed. Also added *_setUseClientMode_afterHandshake tests for SSLSocket and SSLEngine. We properly through IllegalArgument exception in this case, but it wasnt covered by the tests previously. Bug: 2918499 Change-Id: I393ad39bed40a33725d2c0f3f08b9d0b0d3ff85f/"
23,23,3.0,0.7530999779701233,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Stop allocating empty arrays. Bug: 3166662 Change-Id: I151de373b2bf53786d19824336fa434c02b0b0e8/CloseGuard: finalizers for closeable objects should log complaints Introducing CloseGuard which warns when resources are implictly cleaned up by finalizers when an explicit termination method, to use the Effective Java ""Issue 7: Avoid finalizers"" terminology, should have been used by the caller. libcore classes that can use CloseGuard now do so. Bug: 3041575 Change-Id: I4a4e3554addaf3075c823feb0a0ff0ad1c1f6196/Use BufferedInputStream when reading cacerts.bks Change-Id: Ibc20bdcadb5c3bc4bcebfeb96b10c42d9c05e7c8/"
24,24,14.0,0.967199981212616,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Most callers of toLowerCase/toUpperCase should pass Locale.US to avoid problems in Turkey. Some callers should be replaced with equalsIgnoreCase instead. The one exception is StreamTokenizer, where the RI uses the default locale, which is arguably the right thing to do. No-one cares because thats legacy API, but Ive added a test anyway. Ive left HttpCookie and GeneralName for my co-conspirators because the appropriate resolutions arent as obvious there... Bug: 3325637 Change-Id: Ia37a1caaa91b11763ae43e61e445adb45c30f793/"
25,25,15.0,0.800599992275238,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Add support for TLS_EMPTY_RENEGOTIATION_INFO_SCSV cipher suite ""TLS_EMPTY_RENEGOTIATION_INFO_SCSV"" is RFC 5746s renegotiation indication signaling cipher suite value. It is not a real cipher suite. It is just an indication in the default and supported cipher suite lists indicates that the implementation supports secure renegotiation. In the RI, its presence means that the SCSV is sent in the cipher suite list to indicate secure renegotiation support and its absence means to send an empty TLS renegotiation info extension instead. However, OpenSSL doesnt provide an API to give this level of control, instead always sending the SCSV and always including the empty renegotiation info if TLS is used (as opposed to SSL). So we simply allow TLS_EMPTY_RENEGOTIATION_INFO_SCSV to be passed for compatibility as to provide the hint that we support secure renegotiation. Change-Id: I0850bea47568edcfb1f7df99d4e8a747f938406d/Toward EC TLS support Summary: javax.net.ssl tests are now working on the RI KeyManager can now handle EC_EC and EC_RSA OpenSSLSocketImpl.startHandshake now works if KeyManager contains EC certificates Details: Add CipherSuite.getKeyType to provide X509KeyManager key type strings, refactored from OpenSSLServerSocketImpl.checkEnabledCipherSuites. getKeyType is now also used in OpenSSLSocketImpl.startHandshake to avoid calling setCertificate for unnecessary key types. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/CipherSuite.java luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java New CipherSuiteTest to cover new getKeyType as well as existing functionality luni/src/test/java/org/apache/harmony/xnet/provider/jsse/CipherSuiteTest.java Add support to KeyManager implementation for key types of the form EC_EC and EC_RSA. The first part implies the KeyPair algorithm (EC in these new key types) with a potentially different signature algorithm (EC vs RSA in these) luni/src/main/java/org/apache/harmony/xnet/provider/jsse/KeyManagerImpl.java Update NativeCrypto.keyType to support EC_EC and EC_RSA in addition to EC which was added earlier. Change from array of KEY_TYPES to named KEY_TYPE_* constants. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java Overhauled KeyManagerFactoryTest to cover EC, EC_EC, EC_RSA cases luni/src/test/java/libcore/javax/net/ssl/KeyManagerFactoryTest.java support/src/test/java/libcore/java/security/StandardNames.java Changed TestKeyStore.createKeyStore from always using BKS to now use JKS on the RI between BC EC Keys and RI X509 certificates. Because JKS requires a password, we now default ""password"" on the RI. support/src/test/java/libcore/java/security/TestKeyStore.java luni/src/test/java/libcore/javax/net/ssl/SSLContextTest.java support/src/test/java/libcore/java/security/StandardNames.java TestKeyStore.create now accepts key types like EC_RSA. Changed TestKeyStore.createKeys to allow a PrivateKeyEntry to be specified for signing to enable creation of EC_RSA test certificate. Added getRootCertificate/rootCertificate to allow lookup of PrivateKeyEntry for signing. Changed TestKeyStore.getPrivateKey to take explicit signature algorithm to retrieve EC_EC vs EC_RSA entries. support/src/test/java/libcore/java/security/TestKeyStore.java luni/src/test/java/libcore/java/security/KeyStoreTest.java luni/src/test/java/libcore/javax/net/ssl/KeyManagerFactoryTest.java luni/src/test/java/libcore/java/security/cert/PKIXParametersTest.java luni/src/test/java/libcore/javax/net/ssl/TrustManagerFactoryTest.java luni/src/test/java/org/apache/harmony/xnet/provider/jsse/NativeCryptoTest.java support/src/test/java/libcore/java/security/StandardNames.java Added support for EC cipher suites on the RI. Also test with and without new TLS_EMPTY_RENEGOTIATION_INFO_SCSV cipher suite which is used to specify the new TLS secure renegotiation. luni/src/test/java/libcore/javax/net/ssl/SSLEngineTest.java luni/src/test/java/libcore/javax/net/ssl/SSLSocketTest.java support/src/test/java/libcore/java/security/StandardNames.java New TestKeyManager and additional logging in TestTrustManager. Logging in both is disabled by default using DevNullPrintStream. support/src/test/java/libcore/javax/net/ssl/TestKeyManager.java support/src/test/java/libcore/javax/net/ssl/TestTrustManager.java support/src/test/java/libcore/java/io/DevNullPrintStream.java Bug: 3058375 Change-Id: Ia5e2a00a025858e10d1076b900886994b481e05a/"
26,26,15.0,0.7789999842643738,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Toward EC TLS support Summary: javax.net.ssl tests are now working on the RI KeyManager can now handle EC_EC and EC_RSA OpenSSLSocketImpl.startHandshake now works if KeyManager contains EC certificates Details: Add CipherSuite.getKeyType to provide X509KeyManager key type strings, refactored from OpenSSLServerSocketImpl.checkEnabledCipherSuites. getKeyType is now also used in OpenSSLSocketImpl.startHandshake to avoid calling setCertificate for unnecessary key types. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/CipherSuite.java luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java New CipherSuiteTest to cover new getKeyType as well as existing functionality luni/src/test/java/org/apache/harmony/xnet/provider/jsse/CipherSuiteTest.java Add support to KeyManager implementation for key types of the form EC_EC and EC_RSA. The first part implies the KeyPair algorithm (EC in these new key types) with a potentially different signature algorithm (EC vs RSA in these) luni/src/main/java/org/apache/harmony/xnet/provider/jsse/KeyManagerImpl.java Update NativeCrypto.keyType to support EC_EC and EC_RSA in addition to EC which was added earlier. Change from array of KEY_TYPES to named KEY_TYPE_* constants. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java Overhauled KeyManagerFactoryTest to cover EC, EC_EC, EC_RSA cases luni/src/test/java/libcore/javax/net/ssl/KeyManagerFactoryTest.java support/src/test/java/libcore/java/security/StandardNames.java Changed TestKeyStore.createKeyStore from always using BKS to now use JKS on the RI between BC EC Keys and RI X509 certificates. Because JKS requires a password, we now default ""password"" on the RI. support/src/test/java/libcore/java/security/TestKeyStore.java luni/src/test/java/libcore/javax/net/ssl/SSLContextTest.java support/src/test/java/libcore/java/security/StandardNames.java TestKeyStore.create now accepts key types like EC_RSA. Changed TestKeyStore.createKeys to allow a PrivateKeyEntry to be specified for signing to enable creation of EC_RSA test certificate. Added getRootCertificate/rootCertificate to allow lookup of PrivateKeyEntry for signing. Changed TestKeyStore.getPrivateKey to take explicit signature algorithm to retrieve EC_EC vs EC_RSA entries. support/src/test/java/libcore/java/security/TestKeyStore.java luni/src/test/java/libcore/java/security/KeyStoreTest.java luni/src/test/java/libcore/javax/net/ssl/KeyManagerFactoryTest.java luni/src/test/java/libcore/java/security/cert/PKIXParametersTest.java luni/src/test/java/libcore/javax/net/ssl/TrustManagerFactoryTest.java luni/src/test/java/org/apache/harmony/xnet/provider/jsse/NativeCryptoTest.java support/src/test/java/libcore/java/security/StandardNames.java Added support for EC cipher suites on the RI. Also test with and without new TLS_EMPTY_RENEGOTIATION_INFO_SCSV cipher suite which is used to specify the new TLS secure renegotiation. luni/src/test/java/libcore/javax/net/ssl/SSLEngineTest.java luni/src/test/java/libcore/javax/net/ssl/SSLSocketTest.java support/src/test/java/libcore/java/security/StandardNames.java New TestKeyManager and additional logging in TestTrustManager. Logging in both is disabled by default using DevNullPrintStream. support/src/test/java/libcore/javax/net/ssl/TestKeyManager.java support/src/test/java/libcore/javax/net/ssl/TestTrustManager.java support/src/test/java/libcore/java/io/DevNullPrintStream.java Bug: 3058375 Change-Id: Ia5e2a00a025858e10d1076b900886994b481e05a/"
27,27,15.0,0.9157000184059143,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","resolved conflicts for merge of 5fc737eb to master Change-Id: Ifc2a4fd44cef525709a3b9dc0a502b1a0690c6fd/Lots more bounds-checking/exception-throwing consistency. Overflow-safe checks all round, plus better detail messages. This isnt quite everything, but its a large chunk of the work. Most notably, this is all of io and nio. There are numerous changes of exception priority here, and the harmony tests noticed a subset of them in the nio code. Ive modified our checked-out copy of the tests to accept any of the throwable exceptions. Change-Id: Id185f1228fb9a1d5fc9494e78375b5623fb0fe14/Rewrite all backwards comparisons. Strictly, all the ones I could find. This is everything with 0 or null on the left-hand side. Note that this touches several incorrect bounds checks, which I havent fixed: Im going to come back and finish that independent cleanup separately. Change-Id: Ibdb054b53df9aace47c7d2a00ff19122190053e8/Toward EC TLS support Summary: javax.net.ssl tests are now working on the RI KeyManager can now handle EC_EC and EC_RSA OpenSSLSocketImpl.startHandshake now works if KeyManager contains EC certificates Details: Add CipherSuite.getKeyType to provide X509KeyManager key type strings, refactored from OpenSSLServerSocketImpl.checkEnabledCipherSuites. getKeyType is now also used in OpenSSLSocketImpl.startHandshake to avoid calling setCertificate for unnecessary key types. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/CipherSuite.java luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLServerSocketImpl.java luni/src/main/java/org/apache/harmony/xnet/provider/jsse/OpenSSLSocketImpl.java New CipherSuiteTest to cover new getKeyType as well as existing functionality luni/src/test/java/org/apache/harmony/xnet/provider/jsse/CipherSuiteTest.java Add support to KeyManager implementation for key types of the form EC_EC and EC_RSA. The first part implies the KeyPair algorithm (EC in these new key types) with a potentially different signature algorithm (EC vs RSA in these) luni/src/main/java/org/apache/harmony/xnet/provider/jsse/KeyManagerImpl.java Update NativeCrypto.keyType to support EC_EC and EC_RSA in addition to EC which was added earlier. Change from array of KEY_TYPES to named KEY_TYPE_* constants. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/NativeCrypto.java Overhauled KeyManagerFactoryTest to cover EC, EC_EC, EC_RSA cases luni/src/test/java/libcore/javax/net/ssl/KeyManagerFactoryTest.java support/src/test/java/libcore/java/security/StandardNames.java Changed TestKeyStore.createKeyStore from always using BKS to now use JKS on the RI between BC EC Keys and RI X509 certificates. Because JKS requires a password, we now default ""password"" on the RI. support/src/test/java/libcore/java/security/TestKeyStore.java luni/src/test/java/libcore/javax/net/ssl/SSLContextTest.java support/src/test/java/libcore/java/security/StandardNames.java TestKeyStore.create now accepts key types like EC_RSA. Changed TestKeyStore.createKeys to allow a PrivateKeyEntry to be specified for signing to enable creation of EC_RSA test certificate. Added getRootCertificate/rootCertificate to allow lookup of PrivateKeyEntry for signing. Changed TestKeyStore.getPrivateKey to take explicit signature algorithm to retrieve EC_EC vs EC_RSA entries. support/src/test/java/libcore/java/security/TestKeyStore.java luni/src/test/java/libcore/java/security/KeyStoreTest.java luni/src/test/java/libcore/javax/net/ssl/KeyManagerFactoryTest.java luni/src/test/java/libcore/java/security/cert/PKIXParametersTest.java luni/src/test/java/libcore/javax/net/ssl/TrustManagerFactoryTest.java luni/src/test/java/org/apache/harmony/xnet/provider/jsse/NativeCryptoTest.java support/src/test/java/libcore/java/security/StandardNames.java Added support for EC cipher suites on the RI. Also test with and without new TLS_EMPTY_RENEGOTIATION_INFO_SCSV cipher suite which is used to specify the new TLS secure renegotiation. luni/src/test/java/libcore/javax/net/ssl/SSLEngineTest.java luni/src/test/java/libcore/javax/net/ssl/SSLSocketTest.java support/src/test/java/libcore/java/security/StandardNames.java New TestKeyManager and additional logging in TestTrustManager. Logging in both is disabled by default using DevNullPrintStream. support/src/test/java/libcore/javax/net/ssl/TestKeyManager.java support/src/test/java/libcore/javax/net/ssl/TestTrustManager.java support/src/test/java/libcore/java/io/DevNullPrintStream.java Bug: 3058375 Change-Id: Ia5e2a00a025858e10d1076b900886994b481e05a/"
28,28,0.0,0.982699990272522,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Move libcore.base classes to libcore.util and libcore.io. Change-Id: I2340a9dbad3561fa681a8ab47d4f406e72c913e3/Remove useless android-changed comments. Ive changed useful ones to regular comments or TODOs, as appropriate. Ive left ones in code like java.util.concurrent where we really are tracking an upstream source, making the change markers useful. Ive left a handful of others where I intend to actually investigate the implied TODOs before deciding how to resolve them. Change-Id: Iaf71059b818596351cf8ee5a3cf3c85586051fa6/Retire SecurityManager. This change removes all the code that was calling getSecurityManager, and removes all use of AccessController.doPrivileged. It also changes the implementation of AccessController so it doesnt actually do anything; its only there for source-level compatibility. Bug: 2585285 Change-Id: I1f0295a4f12bce0316d8073011d8593fee116f71/"
29,29,18.0,0.6833000183105469,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Fix NativeCrypto FindBugs warnings. Change-Id: I102367575b1257582bb20c659223e3f02650fda4/
30,30,11.0,0.6370999813079834,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Make CertInstaller installed CA certs trusted by applications via default TrustManager (2 of 6) frameworks/base Adding IKeyChainService APIs for CertInstaller and Settings use keystore/java/android/security/IKeyChainService.aidl libcore Improve exceptions to include more information luni/src/main/java/javax/security/auth/x500/X500Principal.java Move guts of RootKeyStoreSpi to TrustedCertificateStore, leaving only KeyStoreSpi methods. Added support for adding user CAs in a separate directory for system. Added support for removing system CAs by placing a copy in a sytem directory luni/src/main/java/org/apache/harmony/xnet/provider/jsse/RootKeyStoreSpi.java luni/src/main/java/org/apache/harmony/xnet/provider/jsse/TrustedCertificateStore.java Formerly static methods on RootKeyStoreSpi are now instance methods on TrustedCertificateStore luni/src/main/java/org/apache/harmony/xnet/provider/jsse/TrustManagerImpl.java Added test for NativeCrypto.X509_NAME_hash_old and X509_NAME_hash to make sure the implementing algorithms doe not change since TrustedCertificateStore depend on X509_NAME_hash_old (OpenSSL changed the algorithm from MD5 to SHA1 when moving from 0.9.8 to 1.0.0) luni/src/test/java/org/apache/harmony/xnet/provider/jsse/NativeCryptoTest.java Extensive test of new TrustedCertificateStore behavior luni/src/test/java/org/apache/harmony/xnet/provider/jsse/TrustedCertificateStoreTest.java TestKeyStore improvements Refactored TestKeyStore to provide simpler createCA method (and internal createCertificate) Cleaned up to remove use of BouncyCastle specific X509Principal in the TestKeyStore API when the public X500Principal would do. Cleaned up TestKeyStore support methods to not throw Exception to remove need for static blocks for catch clauses in tests. support/src/test/java/libcore/java/security/TestKeyStore.java luni/src/test/java/libcore/java/security/KeyStoreTest.java luni/src/test/java/org/apache/harmony/xnet/provider/jsse/NativeCryptoTest.java Added private PKIXParameters contructor for use by IndexedPKIXParameters to avoid wart of having to lookup and pass a TrustAnchor to satisfy the super-class sanity check. luni/src/main/java/org/apache/harmony/xnet/provider/jsse/TrustManagerImpl.java luni/src/main/java/org/apache/harmony/xnet/provider/jsse/IndexedPKIXParameters.java luni/src/main/java/java/security/cert/PKIXParameters.java packages/apps/CertInstaller Change CertInstaller to call IKeyChainService.installCertificate for CA certs to pass them to the KeyChainServiceTest which will make them available to all apps through the TrustedCertificateStore. Change PKCS12 extraction to use AsyncTask. src/com/android/certinstaller/CertInstaller.java Added installCaCertsToKeyChain and hasCaCerts accessor for use by CertInstaller. Use hasUserCertificate() internally. Cleanup coding style. src/com/android/certinstaller/CredentialHelper.java packages/apps/KeyChain Added MANAGE_ACCOUNTS so that IKeyChainService.reset implementation can remove KeyChain accounts. AndroidManifest.xml Implement new IKeyChainService methods: Added IKeyChainService.installCaCertificate to install certs provided by CertInstaller using the TrustedCertificateStore. Added IKeyChainService.reset to allow Settings to remove the KeyChain accounts so that any app granted access to keystore credentials are revoked when the keystore is reset. src/com/android/keychain/KeyChainService.java packages/apps/Settings Changed com.android.credentials.RESET credential reset action to also call IKeyChainService.reset to remove any installed user CAs and remove KeyChain accounts to have AccountManager revoke credential granted to private keys removed during the RESET. src/com/android/settings/CredentialStorage.java Added toast text value for failure case res/values/strings.xml system/core Have init create world readable /data/misc/keychain to allow apps to access user added CA certificates installed by the CertInstaller. rootdir/init.rc Change-Id: Ief57672eea38b3eece23b14c94dedb9ea4713744/Dont cache the underlying Sockets underlying SocketImpls underlying FileDescriptor in OpenSSLSocketImpl. (OpenSSLSocketImpl, of course, being a Socket, not a SocketImpl.) Bug: 4192414 Change-Id: I3c7d0fed70b1b98dc8fcc73f35b3feb0e1eeb2f9/"
31,31,11.0,0.9320999979972839,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Dont trigger a reverse DNS lookup from a log statement. Also nuke a bunch of redundant Javadoc and promote the shutdownInput/shutdownOutput methods that always throw to SSLSocket. Change-Id: I077f7413bb6cba66be6204c68f7911b51a191643
32,32,8.0,0.9049999713897705,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Add OpenSSL KeyPairGenerator and KeyFactory Refactor the way OpenSSL keys are handled so we can generate OpenSSL keys with the KeyPairGenerator and KeyFactory and pass them around without keeping the context in the OpenSSLSignature where it originated. Change-Id: Ib66bd1914e241a240cd97b1ea37e8526998107d9/Add signature generation to OpenSSLSignature Change-Id: I1203516d95a937edb48959146bbec64b338e4f1e/
33,33,18.0,0.9860000014305115,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Tracking openssl-1.0.1 Bug: 6168278 Change-Id: I240d2cbc91f616fd486efc5203e2221c9896d90f/OpenSSL block ciphers, part 1 This implements the NativeCrypto piece necessary to do basic block cipher operations. More work will need to be done to enable useful modes. This gives us the ability to replace BouncyCastles ECB mode that it bases the higher level CBC, CTR, etc modes on. However, calling through JNI to OpenSSL for 16-byte blocks for AES ends up being the same speed as the Java implementation. Further enhancements to use large blocks during the JNI call should show marked improvements in speed. Change-Id: I594a6d13ce5101a1ef2877b84edaa5e5b65e1e71/Support in-memory HTTPS session caching for wrapped sockets. Previously we couldnt reuse sessions with HttpsURLConnection because the host was incorrect (getInetAddress returns null for wrapped sockets) and because the compression method was different (NULL vs. ZLIB). This improves HttpsURLConnection request/response time on localhost from ~275ms to ~145ms (without connection pooling). Change-Id: I97bc343326658690b00589c0c804c2378b91ae61/"
34,34,4.0,0.9620000123977661,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Support in-memory HTTPS session caching for wrapped sockets. Previously we couldnt reuse sessions with HttpsURLConnection because the host was incorrect (getInetAddress returns null for wrapped sockets) and because the compression method was different (NULL vs. ZLIB). This improves HttpsURLConnection request/response time on localhost from ~275ms to ~145ms (without connection pooling). Change-Id: I97bc343326658690b00589c0c804c2378b91ae61/
35,35,13.0,0.5192999839782715,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",NativeCrypto should honor timeout less than one second Bug: Change-Id: I4507a1e9fe37b1c095f7bb4d3e3a55d6d738f7ad/Tracking openssl-1.0.1b Change-Id: I418a5b36670c6cc72e1e6cc29add950409f97f9f/Expose NPN in OpenSSL. This is derived from costins change Ib18da136cb628515d6909c438cd0809452d7058a. It moves the protocols data to the AppDatas callbacks so the memory can be released when the handshake completes. Change-Id: Id61feaa6f28250e393f5c8093688b099e92dce9c/
36,36,13.0,0.920799970626831,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Expose NPN in OpenSSL. This is derived from costins change Ib18da136cb628515d6909c438cd0809452d7058a. It moves the protocols data to the AppDatas callbacks so the memory can be released when the handshake completes. Change-Id: Id61feaa6f28250e393f5c8093688b099e92dce9c/
37,37,16.0,0.9320999979972839,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Fix OpenSSLSocketImpl.close race Move the NativeCrypto.SSL_interrupt call within the close synchronization. Otherwise there can be problems if NativeCrypto_SSL_interrupt tries to use the SSL* and another thread has called NativeCrypto_SSL_free. Bug: 6707288 Change-Id: Id8b0311b10124f2a08f8e0f24595a6ee46805c33/
38,38,17.0,0.9821000099182129,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","NativeCrypto: add OpenSSL X.509 certificate/CRLs Initial implementation of parsing X.509 certificates and certificate revocation lists (CRLs). This lacks support for generating CertPath objects, but that will be added at a later time. This currently isnt the default provider so anything that doesnt explicitly request this provider will not get this implementation. Change-Id: I07ae9f333763087cb6ce47e20e12ceeed750920d/NativeCrypto: output named curves when possible When converting to ASN.1 format from a named curve, try to make sure we can output those named curves whenever possible instead of all the parameters. Also make sure we output in uncompressed point format for compatibility with other implementations. Change-Id: I3f370be694ac709f02e3043a2c1152ad4838ef41/"
39,39,16.0,0.9269000291824341,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Conscrypt: use certificate references in SSL code Instead of marshalling and unmarshalling to ASN.1 DER, just use references to OpenSSL X509 objects everywhere applicable. Change-Id: I1a28ae9232091ee199a9d4c7cd3c7bbd1efa1ca4/"
40,40,16.0,0.9854000210762024,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Conscrypt: use certificate references in SSL code Instead of marshalling and unmarshalling to ASN.1 DER, just use references to OpenSSL X509 objects everywhere applicable. Change-Id: I1a28ae9232091ee199a9d4c7cd3c7bbd1efa1ca4/Conscrypt: remove dependence on stlport This helps with unbundling of Conscrypt by not forcing the app to include a static version of stlport in their program. Change-Id: I5bd17213059b8ae4d8d86921d82b43465253a62f/Properly refcount X509 instances We were leaking X509 references from stacks before so we could get away with reusing references that should have been freed. Since were properly tracking references now, we need to up the reference of things were using. (cherry picked from commit 499f7cd642cc32f89f793fe356afbebeba8bf9c1) Bug: 10610037 Change-Id: I4a4beda9b635881c51194410a6da8274c3c1d429/Fix BIO_OutputStream::write to return the correct length. This was leaving bad OpenSSL error states lying around for later innocent calls to trip over. Also clean up some of the other error reporting/handling. Bug: 9822466 Bug: 10344304 Change-Id: I9e6d6fd9a6c5e466336217b47f45c211aff5555d/Fix libcores NativeCode.mk so we actually compile with Change-Id: Ib665ea7c6f54e43851bc04f0265e65218407c70f/"
41,41,1.0,0.9366999864578247,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Conscrypt: add SHA-224 with tests SHA-224 has made a comeback in the latest StandardNames documentation. This change adds tests for SHA-224 and also Conscrypt providers for things we have code paths to support. Change-Id: I8c200082ff76ee4ae38b6efaa16e6741b33b7f5b/
42,42,16.0,0.9269000291824341,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Conscrypt: use certificate references in SSL code Instead of marshalling and unmarshalling to ASN.1 DER, just use references to OpenSSL X509 objects everywhere applicable. Change-Id: I1a28ae9232091ee199a9d4c7cd3c7bbd1efa1ca4/"
43,43,11.0,0.9682999849319458,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Remove unsupported Cipher modes OpenSSL silently ignores the padding modes when specified for stream ciphers, but apparently Java does not. Change-Id: Icd92122d63b3b8e99d704e8193414dda5057146d/Return IvParameters in OpenSSLCipher#getParameters The getParameters() call was unimplemented in the OpenSSLCipher as an oversight. Add it so code relying on it will continue to work. Additionally add tests for getIV() and getParameters() to make sure they work correctly. (cherry picked from commit 8d59a14a150738b8b3a2a8c31d1a48b8ae0a3d0c) Bug: 10423926 Change-Id: I6bc7fc540509242dff9e5411f66f82be54691cb4/"
44,44,12.0,0.7716000080108643,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Tidy up locking in OpenSSLSocketImpl. We guard all state with a single lock ""stateLock"", which replaces usages of ""this"" and ""handshakeLock"". We do not perform any blocking operations while holding this lock. In particular, startHandshake is no longer synchronized. We use a single integer to keep track of handshake state instead of a pair of booleans. Also fix a bug in getSession, the previous implementation wouldnt work in cut-through mode. This fixes a deadlock in SSLSocketTest_interrupt. Change-Id: I9aef991e0579d4094e287dde8e521d09d6468c51/Conscrypt: use certificate references in SSL code Instead of marshalling and unmarshalling to ASN.1 DER, just use references to OpenSSL X509 objects everywhere applicable. Change-Id: I1a28ae9232091ee199a9d4c7cd3c7bbd1efa1ca4/Some cleanup while investigating test_SSLSocket_interrupt Bug: 10681815 Change-Id: If9a76f4c55b578c6f135befebcc443ab9aef3073/"
45,45,0.0,0.9366000294685364,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",BEAST attack mitigation for OpenSSL-backed SSLSockets. This enables 1/n-1 record splitting for SSLSocket instances backed by OpenSSL. OpenSSL change: Bug: 11514124 Change-Id: I3fef273edd417c51c5723d290656d2e03331d68a/
46,46,1.0,0.6486999988555908,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",AArch64: Use long for pointers in Java sources. Fixing some mistakes in the JNI signatures: some pointers were passed via jint rather than jlong. Change-Id: I6120cc5742c8429a9e0fddda715b5169d820d31a Signed-off-by: Marcus Oakland depending on CipherSuite in OpenSSL-backed sockets. This is in preparation for removing Harmony-backed TLS/SSL implementations. Change-Id: Ic108e16d086fb99b69f0a4e4faeb816dc50a7643/
47,47,11.0,0.6651999950408936,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Remove HarmonyJSSE SSLContext, SSLSocket and SSLServerSocket. HarmonyJSSE SSLEngine implementation is still in use and thus cannot be removed. Change-Id: I3c939e9275ba8f1d00342d1f83c6fdaf110f2317/SSLEngineImpl: fix DHE with client certs If DHE-based key exchanges were selected and there was no matching client certificate selected from X509ExtendedKeyManager, the array would be zero-length and crash. If the client and server certificates did not have DH public keys, the client key exchange would never be created and the server would get a change cipher spec unexpectedly. Change-Id: Ie23b43f4de65e650658c0fb2931e4c1396c136bf/"
48,48,19.0,0.944100022315979,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",Remove unnecessary throws CertificateException from isUserAddedCertificate. Change-Id: If825391c86f7b03fbea42dd6da7700c752d156d7/Support user-installed CA certs for cert pinning. Additionally expose new isUserAddedCertificate() so clients can set policy for user-installed CA certs. Bug: 11257762 Change-Id: If45cd452ab76f393660b34594dcae464af0c0696/
49,49,1.0,0.852400004863739,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Track update to OpenSSL 1.0.1f The constants for handshake cutthrough and CBC record splitting were changed during the upgrade to OpenSSL 1.0.1f. This changes NativeCrypto.java to track them. Change-Id: I9e385c323d5557c5d50cffe3ce797dcf89667ad9/AArch64: Use long for pointers in Java sources. Fixing some mistakes in the JNI signatures: some pointers were passed via jint rather than jlong. Change-Id: I6120cc5742c8429a9e0fddda715b5169d820d31a Signed-off-by: Marcus Oakland prefer Forward Secrecy cipher suites. The documentation for the list of TLS/SSL cipher suites used by default states that cipher suites offering Forward Secrecy are preferred. This CL adjusts the list to conform: FS cipher suites that use RC4_128 bulk encryption algorithm were not preferred over non-FS cipher suites that use AES. Bug: 11220570 Change-Id: Ic9019306898600086920874474764186b710c3ef/Enable AES-GCM cipher suites by default in SSLSocket. AES-GCM is preferred to AES-CBC whose MAC-pad-then-encrypt approach has issues (e.g., Lucky 13 attack). Bug: 11220570 Change-Id: Ib007bc89ccf08358ed3f093f630350fa859e7c35/Enable support for TLSv1.2 cipher suites in SSLSocket. This adds support for AES-GCM and AES-CBC with MACs based on SHA256 and SHA384. Bug: 11220570 Change-Id: I56e7e25c5cd65a4c7662da6d4bbe5720f427e677/Enable TLSv1.1 and TLSv1.2 by default for SSLSocket. TLSv1.1 and TLSv1.2 offer built-in protection against BEAST attack and support for GCM cipher suites. This change causes TLS/SSL handshake failures with a small fraction of servers, load balancers and TLS/SSL accelerators with broken TLS/SSL implementations. Scans demonstrate that the number is around 0.6%. Breaking connectivity (using platform default settings) to a tiny minority of the ecosystem is acceptable because this inconvenience is outweighed by the added safety for the overwheling majority of the ecosystem. App developers affected by this issue should consider asking such servers to be fixed or explicitly disabling TLSv1.1 and TLSv1.2 in their apps. Bug: 11220570 Change-Id: Ice9e8ce550401ba5e3385fd369c40f01c06ac7fd/Stop depending on CipherSuite in OpenSSL-backed sockets. This is in preparation for removing Harmony-backed TLS/SSL implementations. Change-Id: Ic108e16d086fb99b69f0a4e4faeb816dc50a7643/Deprioritize HMAC-MD5 in default TLS/SSL cipher suites. Although HMAC-MD5 is not yet broken, the foundations are now much more shaky that those of HMAC-SHA. See This CL also adds a comment about the key rules governing the preference order of cipher suites used by default. Bug: 11220570 Change-Id: I2a2fe4d427650081637efc14fd7c427a33cbea7e/Prefer Forward Secrecy TLS/SSL cipher suites by default. This modifies the list of TLS/SSL cipher suites used by default to prefer those offering Forward Secrecy (FS) ECDHE and DHE. Bug: 11220570 Change-Id: I20f635d11e937d64de4f4e2fea34e1c5ea7a67ac/Deprioritize RC4-based TLS/SSL cipher suites. Now that BEAST and Lucky13 mitigations are enabled, it is prudent to prefer AES CBC cipher suites over RC4 ones (see Bug: 11220570 Change-Id: I52b9724700fd8eaeebbadcfa518a96823a1410b8/"
50,50,16.0,0.9801999926567078,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","AArch64: Use long for pointers in Java sources. Fixing some mistakes in the JNI signatures: some pointers were passed via jint rather than jlong. Change-Id: I6120cc5742c8429a9e0fddda715b5169d820d31a Signed-off-by: Marcus Oakland depending on CipherSuite in OpenSSL-backed sockets. This is in preparation for removing Harmony-backed TLS/SSL implementations. Change-Id: Ic108e16d086fb99b69f0a4e4faeb816dc50a7643/Use SNI hostname for session caching The session caching wasnt paying attention to the requested SNI hostname when finding cached sessions. This checks the requested SNI hostname in an attempt to get the correct hostname from the cache. Change-Id: If3dbc64f11377a615389de9774c4061d1c92b997/Random cleanups of old code style Add annotation, remove unused imports, and remove unnecessary casts. Also make sure annotations are on a line by themselves. Change-Id: I294b43353d7b1e77fd1c9d031af7b7062f024eee/"
51,51,13.0,0.6345000267028809,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Add OpenSSLEngineImpl Add support for SSLEngine via OpenSSL APIs. Currently this supports just the basic SSLEngine functionality. It can be improved in efficiency and performance, but it appears not to leak anything and be correct according to our test suites. Change-Id: Iea2dc3922e7c30e26daca38361877bd2f88ae668/Refactor OpenSSLSocketImpl Move functionality that will be shared with OpenSSLs SSLEngine implementation out of OpenSSLSocketImpl and into the (soon-to-be) shared SSLParametersImpl. The functionality should stay the same. Change-Id: If8faa3ad2c9c73c0a0cd4b9716639b362b2b26a1/Convert calls to BIO_free to BIO_free_all If we have a chain of BIO, we want to free the entire chain. Otherwise, we might accidentally leave references sitting around. This shouldnt matter for our current use-case, but might help in the future. Change-Id: I586937629e1e4f2e80b5feefe2f49a85e8a31d31/ALPN: change socket calls to SSL_set_alpn_protos Calling SSL_CTX_set_alpn_protos appears to be detrimental to thread safety since the implementation of it resets the values. Its not idempotent to call it multiple times like SSL_CTX_enable_npn. Bug: Change-Id: I09ed9e75d08528300b86201c3e847b26702d4284/Use the new endpointVerificationAlgorithm API Use the new X509ExtendedTrustManager and use the new getEndpointVerificationAlgorithm to check the hostname during the handshake. Bug: 13103812 Change-Id: Id0a74d4ef21a7d7c90357a111f99b09971e535d0/Fix up concurrent use of APIs Code that is incorrectly using MessageDigest, Signature, or Mac in multiple threads simultaneously could cause a SEGV if OpenSSL is clearing out the MD_CTX at the same time another thread is trying to write to it. Make sure we initialize a new MD_CTX after each run to avoid crashing. The program using the instances concurrently is still wrong and will most likely get inconsistent results. Switch to using a context object instance to make sure we can hold a reference to the object during the native call. Bug: 8787753 Change-Id: I2518613a47cf03c811a29d17040804fc708394dd/Throw instead of segfaulting when NULL EVP_PKEY encountered. Change-Id: Idba6702dd43e541b51c990fc3440a17351e6def9/NativeCrypto: Handle 0-byte bignum arrays Some DSA tests were calling with bignum arrays that had the high bit set indicating a negative number. Also an empty array was being passed as another part of the test. This was working, but it was reading one byte past the end of the buffer. Change-Id: Ibd5a0dce61703ea569fd483f8acf66fd149703f8/"
52,52,15.0,0.6265000104904175,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Make AppData creation symmetric AppData was being created in SSL_do_handshake, but freed in SSL_free. Make it symmetric by creating AppData in SSL_new instead. The SSLEngine may call do_handshake multiple times to complete a handshake, but this was creating an AppData each time it entered. Creating in SSL_new avoids the problem of checking whether it was already created on each entry into SSL_do_handshake calls. Bug: 14247219 Change-Id: I825486798250998a4d4141201bda68a4dffe13a4/Add OpenSSLEngineImpl Add support for SSLEngine via OpenSSL APIs. Currently this supports just the basic SSLEngine functionality. It can be improved in efficiency and performance, but it appears not to leak anything and be correct according to our test suites. Change-Id: Iea2dc3922e7c30e26daca38361877bd2f88ae668/ALPN: change socket calls to SSL_set_alpn_protos Calling SSL_CTX_set_alpn_protos appears to be detrimental to thread safety since the implementation of it resets the values. Its not idempotent to call it multiple times like SSL_CTX_enable_npn. Bug: Change-Id: I09ed9e75d08528300b86201c3e847b26702d4284/Add JNI_TRACE_MD to cut down on noise During start-up of vogar, it does thousands of digests on the input class files which makes the output really noisy. Since debugging MD stuff is uncommon, just hide it behind another debug flag. Change-Id: I972a1b61c6ffe2d4cc345b089f0be10751ea32e4/Throw SSLHandshakeException for errors during handshake This is a subclass of SSLHandshake, so its not technically any different, but more sophisticated clients use this to differentiate between a failure during handshake and a general SSL failure. Bug: 13130968 Change-Id: Ifad026c9af6748c1f7cb6a75f8f49aa3e75deea8/Return SSL_TLSEXT_ERR_NOACK with no NPN/ALPN We were returning SSL_TLSEXT_ERR_OK even if we did not select any NPN/ALPN support. Bug: Change-Id: I79ea821512f03f1391247d3bcfc7ac7d042ecb41/Fix up concurrent use of APIs Code that is incorrectly using MessageDigest, Signature, or Mac in multiple threads simultaneously could cause a SEGV if OpenSSL is clearing out the MD_CTX at the same time another thread is trying to write to it. Make sure we initialize a new MD_CTX after each run to avoid crashing. The program using the instances concurrently is still wrong and will most likely get inconsistent results. Switch to using a context object instance to make sure we can hold a reference to the object during the native call. Bug: 8787753 Change-Id: I2518613a47cf03c811a29d17040804fc708394dd/Throw ArrayIndexOutOfBoundsException instead of generic This exception is specifically for arrays which is what were dealing with here. Change-Id: I11be2c75019844701b305240152815d7c610fbef/Harden (EC)DSA signatures against weak nonces. Private key information is leaked by (EC)DSA signatures when nonces are produced by a weak RNG. This CL enables a mitigation provided by OpenSSL: mix in private key and message being signed into randomly generated nonce. Provided private key was generated by strong RNG, this should mitigate the weakness. NOTE: This mitigation is not implemented for signatures which use hardware-backed private keys (AndroidKeyStore). Change-Id: I60dbf57bff3cfcdcbbeb18be5d9dfba523cc6bb8/Throw instead of segfaulting when NULL EVP_PKEY encountered. Change-Id: Idba6702dd43e541b51c990fc3440a17351e6def9/BIGNUM convert to Java BigInteger Java BigInteger is in twos complement, so it needs conversion for negative numbers. We were mishandling it before and the previous change just hacked around it. Actually convert to twos complement instead. Change-Id: I6bfe9577f0936678476193b55433b7d7dbc04400/"
53,53,15.0,0.5647000074386597,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Fix up concurrent use of APIs Code that is incorrectly using MessageDigest, Signature, or Mac in multiple threads simultaneously could cause a SEGV if OpenSSL is clearing out the MD_CTX at the same time another thread is trying to write to it. Make sure we initialize a new MD_CTX after each run to avoid crashing. The program using the instances concurrently is still wrong and will most likely get inconsistent results. Switch to using a context object instance to make sure we can hold a reference to the object during the native call. Bug: 8787753 Change-Id: I2518613a47cf03c811a29d17040804fc708394dd/Harden (EC)DSA signatures against weak nonces. Private key information is leaked by (EC)DSA signatures when nonces are produced by a weak RNG. This CL enables a mitigation provided by OpenSSL: mix in private key and message being signed into randomly generated nonce. Provided private key was generated by strong RNG, this should mitigate the weakness. NOTE: This mitigation is not implemented for signatures which use hardware-backed private keys (AndroidKeyStore). Change-Id: I60dbf57bff3cfcdcbbeb18be5d9dfba523cc6bb8/Late binding: convert OpenSSLSignature to late binding You must be a child of SignatureSpi to do late binding correctly. Also remove useless test. Change-Id: I4190ec919ad0eca9f344a2d7ac4c03216dccab55/OpenSSLSignature: refactor key checking Use OpenSSLKey to do the conversions from different key types. Change-Id: Ie89730bba983cb5f2917fed7194e8b08562f6e16/"
54,54,1.0,0.47940000891685486,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","SSLEngine: fix some behaviors * We were not checking buffer lengths. * wrap/unwrap should start a handshake. Change-Id: I35fbd8bf5eb699923f4712e7590bce7e7e13e529/Add OpenSSLEngineImpl Add support for SSLEngine via OpenSSL APIs. Currently this supports just the basic SSLEngine functionality. It can be improved in efficiency and performance, but it appears not to leak anything and be correct according to our test suites. Change-Id: Iea2dc3922e7c30e26daca38361877bd2f88ae668/Convert calls to BIO_free to BIO_free_all If we have a chain of BIO, we want to free the entire chain. Otherwise, we might accidentally leave references sitting around. This shouldnt matter for our current use-case, but might help in the future. Change-Id: I586937629e1e4f2e80b5feefe2f49a85e8a31d31/ALPN: change socket calls to SSL_set_alpn_protos Calling SSL_CTX_set_alpn_protos appears to be detrimental to thread safety since the implementation of it resets the values. Its not idempotent to call it multiple times like SSL_CTX_enable_npn. Bug: Change-Id: I09ed9e75d08528300b86201c3e847b26702d4284/Use the new endpointVerificationAlgorithm API Use the new X509ExtendedTrustManager and use the new getEndpointVerificationAlgorithm to check the hostname during the handshake. Bug: 13103812 Change-Id: Id0a74d4ef21a7d7c90357a111f99b09971e535d0/Fix up concurrent use of APIs Code that is incorrectly using MessageDigest, Signature, or Mac in multiple threads simultaneously could cause a SEGV if OpenSSL is clearing out the MD_CTX at the same time another thread is trying to write to it. Make sure we initialize a new MD_CTX after each run to avoid crashing. The program using the instances concurrently is still wrong and will most likely get inconsistent results. Switch to using a context object instance to make sure we can hold a reference to the object during the native call. Bug: 8787753 Change-Id: I2518613a47cf03c811a29d17040804fc708394dd/Harden (EC)DSA signatures against weak nonces. Private key information is leaked by (EC)DSA signatures when nonces are produced by a weak RNG. This CL enables a mitigation provided by OpenSSL: mix in private key and message being signed into randomly generated nonce. Provided private key was generated by strong RNG, this should mitigate the weakness. NOTE: This mitigation is not implemented for signatures which use hardware-backed private keys (AndroidKeyStore). Change-Id: I60dbf57bff3cfcdcbbeb18be5d9dfba523cc6bb8/OpenSSLX509Certificate: only catch BadPaddingException We only need to catch BadPaddingException right now. Let the other non-RuntimeException exceptions pass. Change-Id: I5b6878250d428b1ee953092967b7418003ee9216/"
55,55,8.0,0.9886999726295471,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Finish switching to android.system.Os. Looks like I missed one last time... Change-Id: Ib009e87493b36fc815166c44ce3c3a532aa5cd82/Track libcore.os move towards the light. Change-Id: Id41fb809eb764ce60f6d3cecf5715a57af432027/Add back missing sslSession Accidentally removed during refactor. Change-Id: I4295af935b269ec7ea91f1d1d140f32188e15e64/Add OpenSSLEngineImpl Add support for SSLEngine via OpenSSL APIs. Currently this supports just the basic SSLEngine functionality. It can be improved in efficiency and performance, but it appears not to leak anything and be correct according to our test suites. Change-Id: Iea2dc3922e7c30e26daca38361877bd2f88ae668/Refactor OpenSSLSocketImpl Move functionality that will be shared with OpenSSLs SSLEngine implementation out of OpenSSLSocketImpl and into the (soon-to-be) shared SSLParametersImpl. The functionality should stay the same. Change-Id: If8faa3ad2c9c73c0a0cd4b9716639b362b2b26a1/Remove SSLEngineImpl This is replaced by OpenSSL-backed SSLEngineImpl. Change-Id: I7b51f6fa772e431c6283008535bfec90821d0bef/Use the new endpointVerificationAlgorithm API Use the new X509ExtendedTrustManager and use the new getEndpointVerificationAlgorithm to check the hostname during the handshake. Bug: 13103812 Change-Id: Id0a74d4ef21a7d7c90357a111f99b09971e535d0/Support TLS/SSL without X509TrustManager or X509KeyManager. This makes TLS/SSL primitives operate as expected when no X509TrustManager or X509KeyManager is provided. Instead of blowing up with KeyManagementException or NullPointerException (or similar) when X509TrustManager or X509KeyManager is not provided, this CL makes SSLContext.init accept such setup, and makes SSLSocket and SSLEngine reject certificate chains, select no private keys/aliases, and accept no certificate issuers. Bug: 13563574 Change-Id: I8de58377a09025258357dd4da9f6cb1b6f2dab80/"
56,56,8.0,0.8197000026702881,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Assert that the padding extension is enabled by default. Change-Id: I1c8aa589e3274bfd3a5fc66c3e948828903c1966/Expose support for TLS-PSK. TLS-PSK (Pre-Shared Key) is a set of TLS/SSL cipher suites that use symmetric (pre-shared) keys for mutual authentication of peers. These cipher suites are in some scenarios more suitable than those based on public key cryptography and X.509. See RFC 4279 (Pre-Shared Key Ciphersuites for Transport Layer Security (TLS)) for more information. OpenSSL currently supports only the following PSK cipher suites: * TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA256 * TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA384 * TLS_PSK_WITH_3DES_EDE_CBC_SHA * TLS_PSK_WITH_AES_128_CBC_SHA * TLS_PSK_WITH_AES_256_CBC_SHA * TLS_PSK_WITH_RC4_128_SHA The last four cipher suites mutually authenticate the peers and secure the connection using a pre-shared symmetric key. These cipher suites do not provide Forward Secrecy once the pre-shared key is compromised, all previous communications secured with that key can be decrypted. The first two cipher suites combine the pre-shared symmetric key with an ephemeral key obtained from an ECDH key exchange performed during the TLS/SSL handshake, thus providing Forward Secrecy. Users of TLS-PSK are expected to provide an implementation of PSKKeyManager to SSLContext.init and then enable at least one PSK cipher suite in SSLSocket/SSLEngine. Bug: 15073623 Change-Id: I8e59264455f980f23a5e66099c27b5b4d932b9bb/SSL: also allow calls to read/write after cutthrough Also add test to make sure this works. Bug: 14832989 Change-Id: I046111cdcc4086a7104d462696078a767e86b12c/"
57,57,15.0,0.9958999752998352,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","reconcile aosp (e79c25bf33e10da41e489c537823f678e1a1169c) after branching. Please do not merge. Change-Id: I39ab275cd9744ba442fee7db9038107b4603526f/DHKeyPairGenerator: use provided params If the prime is provided in the DHParameterSpec, then use it to generate the key. Bug: 16188130 Change-Id: I42de02c71a58d691ef7ba6e2252367105687b758/Add ability to wrap platform keys This is mostly useful for unbundled Conscrypt currently when working with KeyChain-based keys, but could be good for use with PKCS11-like keys in other JSSE providers. Bug: 15469749 Change-Id: I56bf2eaf3228bdf42d671437f4fffdafb8b47b12/Add more debugging for getting methods When JNI registration fails, we should log it immediately to help with debugging. Otherwise, it will tell you that you called a JNI function with an exception pending. Change-Id: I7cbba4d6639265a79a9d043d120f1a2bf72a85f7/Expose support for TLS-PSK. TLS-PSK (Pre-Shared Key) is a set of TLS/SSL cipher suites that use symmetric (pre-shared) keys for mutual authentication of peers. These cipher suites are in some scenarios more suitable than those based on public key cryptography and X.509. See RFC 4279 (Pre-Shared Key Ciphersuites for Transport Layer Security (TLS)) for more information. OpenSSL currently supports only the following PSK cipher suites: * TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA256 * TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA384 * TLS_PSK_WITH_3DES_EDE_CBC_SHA * TLS_PSK_WITH_AES_128_CBC_SHA * TLS_PSK_WITH_AES_256_CBC_SHA * TLS_PSK_WITH_RC4_128_SHA The last four cipher suites mutually authenticate the peers and secure the connection using a pre-shared symmetric key. These cipher suites do not provide Forward Secrecy once the pre-shared key is compromised, all previous communications secured with that key can be decrypted. The first two cipher suites combine the pre-shared symmetric key with an ephemeral key obtained from an ECDH key exchange performed during the TLS/SSL handshake, thus providing Forward Secrecy. Users of TLS-PSK are expected to provide an implementation of PSKKeyManager to SSLContext.init and then enable at least one PSK cipher suite in SSLSocket/SSLEngine. Bug: 15073623 Change-Id: I8e59264455f980f23a5e66099c27b5b4d932b9bb/Unbundle: hacks to let Conscrypt compile standalone This is the first pass at getting Conscrypt to compile standalone. It works fine in apps currently. There are a few TODOs to fix. Change-Id: I9b43ba12c55e04c8897ccacf38979ca671a55a26/NativeCryptoTest: fix shutdown test These werent actually testing that the exceptions were thrown before. Since we actually throw now, make sure were throwing the expected exception type. Change-Id: I57b11492118dd7c04faa57c58de7b023294b179c/Fix of native crash in the evpUpdate method The org.apache.harmony.security.tests.java.security.MessageDigest1Test CTS test classs testSHAProvider method was causing a SIGSEGV when ""md.update(bytes, 1, was called, as the evpUpdate method was not checking for the inLength parameter being negative. This has been rectified and the test now passes. Bug: 14821275 Change-Id: I94489a518f7a2d4a6e84e58f91d8eee6f0ceb045 Signed-off-by: Marcus Oakland keys: derive public key if not available Also make the params mutex when were inflating from a serial object since it will be null otherwise. Change-Id: I36641725161c0a708ba303500acca368b0511abe/SSL: also allow calls to read/write after cutthrough Also add test to make sure this works. Bug: 14832989 Change-Id: I046111cdcc4086a7104d462696078a767e86b12c/Check for renegotiate_pending for tests Tests call SSL_renegotiate to force a renegotiation, but was relying on AppData being unset in this function. Instead we check that both SSL_is_init_finished is false and SSL_renegotiation_pending is false. Renegotiation is handled by SSL_write implicitly instead of explicitly like the wrapper around SSL_do_handshake does. Change-Id: I7e761afa718503933334cc19fbc696d714eca500/Add DH keys Add the initial steps for DH keys to be generated and handled via OpenSSL. Next steps will be hooking it up via other APIs that use DH keys. Change-Id: Ib159e60db73f82b75e0ba375a1d165c51286edac/SSLSocket: restore previous pre-handshake behavior Before AppData was created in the initial handshake, calling SSL_read or SSL_write would have a NULL appData field. This caused an exception to be thrown. Now we have to check to make sure the handshake completed before we continue on with SSL_read and SSL_write. Change-Id: I969577cf56f61858450a7981a5196f58a6502968/"
58,58,1.0,0.994700014591217,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Enable PSK cipher suites when PSKKeyManager is provided. This enables TLS-PSK cipher suites by default iff SSLContext is initialized with a PSKKeyManager. For consistency, X.509 based cipher suites are no longer enabled by default at all times they are now only enabled by default iff SSLContext is initialized with a X509KeyManager or a X509TrustManager. When both X.509 and PSK cipher suites need to be enabled, PSK cipher suites are given higher priority in the resulting list of cipher suites. This is based on the assumption that in most cases users of TLS/SSL who enable TLS-PSK would prefer TLS-PSK to be used when the peer supports TLS-PSK. Bug: 15073623 Change-Id: I8e2bc3e7a1ea8a986e468973b6bad19dc6b7bc3c/DHKeyPairGenerator: use provided params If the prime is provided in the DHParameterSpec, then use it to generate the key. Bug: 16188130 Change-Id: I42de02c71a58d691ef7ba6e2252367105687b758/Assert that the padding extension is enabled by default. Change-Id: I1c8aa589e3274bfd3a5fc66c3e948828903c1966/Adjust the list of supported ECDHE-PSK cipher suites. The SHA-2 based cipher suites cannot be used with SSLv3 but there is no way to express that in OpenSSLs configuration. This CL thus adjusts the list of supported cipher suites accordingly. Bug: 15073623 Change-Id: I427c99f4c1c72690d95e5a3c63763631c41ddae2/Add ability to wrap platform keys This is mostly useful for unbundled Conscrypt currently when working with KeyChain-based keys, but could be good for use with PKCS11-like keys in other JSSE providers. Bug: 15469749 Change-Id: I56bf2eaf3228bdf42d671437f4fffdafb8b47b12/Expose support for TLS-PSK. TLS-PSK (Pre-Shared Key) is a set of TLS/SSL cipher suites that use symmetric (pre-shared) keys for mutual authentication of peers. These cipher suites are in some scenarios more suitable than those based on public key cryptography and X.509. See RFC 4279 (Pre-Shared Key Ciphersuites for Transport Layer Security (TLS)) for more information. OpenSSL currently supports only the following PSK cipher suites: * TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA256 * TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA384 * TLS_PSK_WITH_3DES_EDE_CBC_SHA * TLS_PSK_WITH_AES_128_CBC_SHA * TLS_PSK_WITH_AES_256_CBC_SHA * TLS_PSK_WITH_RC4_128_SHA The last four cipher suites mutually authenticate the peers and secure the connection using a pre-shared symmetric key. These cipher suites do not provide Forward Secrecy once the pre-shared key is compromised, all previous communications secured with that key can be decrypted. The first two cipher suites combine the pre-shared symmetric key with an ephemeral key obtained from an ECDH key exchange performed during the TLS/SSL handshake, thus providing Forward Secrecy. Users of TLS-PSK are expected to provide an implementation of PSKKeyManager to SSLContext.init and then enable at least one PSK cipher suite in SSLSocket/SSLEngine. Bug: 15073623 Change-Id: I8e59264455f980f23a5e66099c27b5b4d932b9bb/Add DH keys Add the initial steps for DH keys to be generated and handled via OpenSSL. Next steps will be hooking it up via other APIs that use DH keys. Change-Id: Ib159e60db73f82b75e0ba375a1d165c51286edac/"
59,59,4.0,0.9682999849319458,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Keep enough state to completely reset cipher instances OpenSSLs RC4 mutates the given key. AES/CTR mutates the IV. We must store these values locally to enable ""doFinal"" to cause the Cipher instance to be reset to what it was right after ""init"". Note that resetting and encrypting with the same key or IV breaks semantic security. Bug: 16298401 Bug: Change-Id: Ie7e4dcb6cf6cc33ddad31d6b47066dc1b34e6894/"
60,60,1.0,0.8176000118255615,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() Bug: Change-Id: I5bcaf3ee8910ff75e785baed4c4604fee6c5e700/OpenSSLEngineImpl: fix unwrap behavior with array The decrypted bytes should written sequentially into each buffer of the destination array until its full before moving to the next buffer. Change-Id: I2454249c167deafde6c12134d3c8cd658cd7c21b/Log CCS exceptions do not merge. Unlike the previous CL, this uses reflection for android.os.Process and android.util.EventLog throughout. (cherry picked from commit 35b1f354ec2b647966a198ffed932d82eb8eeb5b) Bug: 15452942 Change-Id: I34b9eaedf1f1e450b1f8004887bb0482601d789e/OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. (cherry picked from commit e08f238580e8ee471012bef8240c8d3397c7b780) Bug: 16352665 Change-Id: Idc78204b7077fb367b64e1867c807cd39f596f98/Various fixes in OpenSSLEngineImpl. Fix ""Buffers were not large enough"" exception by directly using the destination buffers. Corrections around bytesProduced and bytesConsumed behavior. Return BUFFER_OVERFLOW if a zero length destination is provided to unwrap. (cherry picked from commit bdfcc189efe41a3f812aeb55ea634bace67d159a) Bug: 16352665 Change-Id: I1f1e9b72cd6968ed4f3c3c0edccbccebc33d6790/Various fixes in OpenSSLEngineImpl. Fix ""Buffers were not large enough"" exception by directly using the destination buffers. Corrections around bytesProduced and bytesConsumed behavior. Return BUFFER_OVERFLOW if a zero length destination is provided to unwrap. Change-Id: I1f1e9b72cd6968ed4f3c3c0edccbccebc33d6790/Log OpenSSL CCS errors Bug: 15452942 Change-Id: I49e7bad6a65c70e113324c02fc23315cff168f5b/Expose support for TLS-PSK. TLS-PSK (Pre-Shared Key) is a set of TLS/SSL cipher suites that use symmetric (pre-shared) keys for mutual authentication of peers. These cipher suites are in some scenarios more suitable than those based on public key cryptography and X.509. See RFC 4279 (Pre-Shared Key Ciphersuites for Transport Layer Security (TLS)) for more information. OpenSSL currently supports only the following PSK cipher suites: * TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA256 * TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA384 * TLS_PSK_WITH_3DES_EDE_CBC_SHA * TLS_PSK_WITH_AES_128_CBC_SHA * TLS_PSK_WITH_AES_256_CBC_SHA * TLS_PSK_WITH_RC4_128_SHA The last four cipher suites mutually authenticate the peers and secure the connection using a pre-shared symmetric key. These cipher suites do not provide Forward Secrecy once the pre-shared key is compromised, all previous communications secured with that key can be decrypted. The first two cipher suites combine the pre-shared symmetric key with an ephemeral key obtained from an ECDH key exchange performed during the TLS/SSL handshake, thus providing Forward Secrecy. Users of TLS-PSK are expected to provide an implementation of PSKKeyManager to SSLContext.init and then enable at least one PSK cipher suite in SSLSocket/SSLEngine. Bug: 15073623 Change-Id: I8e59264455f980f23a5e66099c27b5b4d932b9bb/Unbundle: hacks to let Conscrypt compile standalone This is the first pass at getting Conscrypt to compile standalone. It works fine in apps currently. There are a few TODOs to fix. Change-Id: I9b43ba12c55e04c8897ccacf38979ca671a55a26/SSLEngine: handle EOF for our BIOs If we reache EOF (really the end of our current bytes buffered for read) during writing or reading, dont try to count the returned as part of the read bytes. Change-Id: I76d42b00f14b121f1524e7c035efcf2c99627278/"
61,61,1.0,0.53329998254776,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Log CCS exceptions do not merge. Unlike the previous CL, this uses reflection for android.os.Process and android.util.EventLog throughout. (cherry picked from commit 35b1f354ec2b647966a198ffed932d82eb8eeb5b) Bug: 15452942 Change-Id: I34b9eaedf1f1e450b1f8004887bb0482601d789e/Remove (cherry picked from commit b860016f415dfc5655dcee45f70e8871a2e3edfe) Change-Id: I4302ea4e0200ac80a0b9f3b953d58270b65b3d0c/Remove Change-Id: Iea7c633eb68df576bf72314ff5ce31bc8094d9ce/Log OpenSSL CCS errors Bug: 15452942 Change-Id: I49e7bad6a65c70e113324c02fc23315cff168f5b/SSLParametersImpl is the source of enabled cipher suites and protocols. An instance of SSLParametersImpl is associated with SSLContext and is then cloned into any SSLSocketFactory, SSLServerSocketFactory, SSLSocket, SSLServerSocket, and SSLEngine. This CL ensures that all these primitives obtain their list of enabled cipher suites and protocols from their instance of SSLParametersImpl. Bug: 15073623 Change-Id: I40bf32e8654b299518ec0e77c3218a0790d9c4fd/Expose support for TLS-PSK. TLS-PSK (Pre-Shared Key) is a set of TLS/SSL cipher suites that use symmetric (pre-shared) keys for mutual authentication of peers. These cipher suites are in some scenarios more suitable than those based on public key cryptography and X.509. See RFC 4279 (Pre-Shared Key Ciphersuites for Transport Layer Security (TLS)) for more information. OpenSSL currently supports only the following PSK cipher suites: * TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA256 * TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA384 * TLS_PSK_WITH_3DES_EDE_CBC_SHA * TLS_PSK_WITH_AES_128_CBC_SHA * TLS_PSK_WITH_AES_256_CBC_SHA * TLS_PSK_WITH_RC4_128_SHA The last four cipher suites mutually authenticate the peers and secure the connection using a pre-shared symmetric key. These cipher suites do not provide Forward Secrecy once the pre-shared key is compromised, all previous communications secured with that key can be decrypted. The first two cipher suites combine the pre-shared symmetric key with an ephemeral key obtained from an ECDH key exchange performed during the TLS/SSL handshake, thus providing Forward Secrecy. Users of TLS-PSK are expected to provide an implementation of PSKKeyManager to SSLContext.init and then enable at least one PSK cipher suite in SSLSocket/SSLEngine. Bug: 15073623 Change-Id: I8e59264455f980f23a5e66099c27b5b4d932b9bb/Unbundle: hacks to let Conscrypt compile standalone This is the first pass at getting Conscrypt to compile standalone. It works fine in apps currently. There are a few TODOs to fix. Change-Id: I9b43ba12c55e04c8897ccacf38979ca671a55a26/"
62,62,2.0,0.9269000291824341,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Validate hostname is usable for SNI According to RFC 6066 section 3, the hostname listed in the Server Name Indication (SNI) field is a fully qualified domain name and IP addresses are not permitted. Bug: 16658420 Bug: 17059757 Change-Id: I804e46b6e66599b2770f0f4f0534467987e51208/"
63,63,6.0,0.9269000291824341,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Read property to enable SNI Read the system property ""jsse.enableSNIExtension"" on whether to enable Server Name Indication (SNI) extension. For unbundled builds, this will be enabled by default. For platform builds, this will be disabled by default. Bug: 16658420 Bug: 17059757 Change-Id: I774f5406bf3fe601a42c4ef5e708b31800147eb9/"
64,64,16.0,0.6226999759674072,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","OpenSSLEngineImpl: reduce number of copies needed When the ByteBuffer didnt line up exactly with the backing array, it would allocate a new buffer to write into. Instead, add the ability for OpenSSL to read at an offset in the given array so a copy isnt needed. Change-Id: I149d3f94e4b5cbdc010df80439ae3300cbdc87a5/OpenSSLSocketImpl: Move state checks inside mutex Checking the state of the connection is unreliable if SSL_read and SSL_write are happening in another thread. Move the state checks inside our application mutex so we dont run into another thread mutating the state at the same time. Bug: 15606096 Change-Id: I5ecdeb1551a13098d1b66c5e4009607c9951fa38/Revert ""Revert ""Automatic management of OpenSSL error stack"""" The ""else"" statement in OpenSslError::reset wasnt properly resetting the error state which made a second call into sslRead jump into sslSelect when it should have just returned immediately. Change-Id: I22e8025c0497a04e78daa07cef78191a6ca1a70c/Fix debugging with unbundled conscrypt When JNI_TRACE was enabled, there were missing defines for the debugging code since no platform code is included. Also clang complains about more of the debugging statement formats, so we have to move some things around to get it to be happy. Change-Id: I1a6695c2ef2639cc01cfc3d3a8603f010c659844/Revert ""Automatic management of OpenSSL error stack"" This reverts commit 35666e4cb0fcd063a21d17eebbb571b4e4e822b8. Change-Id: I926d159c4c4b99250caef750732976c1e601e9ef/Automatic management of OpenSSL error stack This removes some complexity in remembering to free the OpenSSL error stack. If you forget, the error will stick around until you make another call. Change-Id: I245a525dcc93077b2bf9909a14a0ef469a2daca4/Fix some JNI_TRACE lines During debugging these would be enabled, but they were copy-pastad to with the wrong args. Change-Id: I23f39ff4807e3fa71f3220912aec3c99db6b9454/"
65,65,16.0,0.9620000123977661,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Add support for TLS_FALLBACK_SCSV (cherry picked from commit 8d7e23e117da591a8d48e6bcda9ed6f58ff1a375) Bug: 17750026 Change-Id: Iaf437ce2bc2b0ae86bb90a67e6e5378b25ae0a81/Add support for TLS_FALLBACK_SCSV Bug: 17750026 Change-Id: I1c2ecbeb914db645f440d58e7f7daa86d880ad6f/OpenSSLEngineImpl: reduce number of copies needed When the ByteBuffer didnt line up exactly with the backing array, it would allocate a new buffer to write into. Instead, add the ability for OpenSSL to read at an offset in the given array so a copy isnt needed. Change-Id: I149d3f94e4b5cbdc010df80439ae3300cbdc87a5/"
66,66,16.0,0.949999988079071,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","OpenSSLEngineImpl: reduce number of copies needed When the ByteBuffer didnt line up exactly with the backing array, it would allocate a new buffer to write into. Instead, add the ability for OpenSSL to read at an offset in the given array so a copy isnt needed. Change-Id: I149d3f94e4b5cbdc010df80439ae3300cbdc87a5/"
67,67,15.0,0.9743000268936157,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Implement write socket timeouts for unbundled apps Change-Id: I4fd604f057ba4288d4f31bf6b3b93307376023d5/Tracking change from AOSP Change-Id: I889af3f7c1de9ef34d9328339e1b421651055ad4/Rename hostname fields and methods to reflect usage The hostname that was supplied when the socket was created is stored as the ""peerHostname"" This is the only one that should be used for Server Name Indication (SNI) purposes. The ""peerHostname"" or the resolved IP address may be used for certificate validation, so keep the use of ""getHostname()"" for cerificate validation. Bug: 16658420 Bug: 17059757 Change-Id: Ifd87dead44fb2f00bbfd5eac7e69fb3fc98e94b4/Use consistent naming for SSLSocket arguments This changes all the host to be hostname and anything that takes an InetAddress will have the name of address to avoid confusing it with a hostname. Bug: 16658420 Bug: 17059757 Change-Id: Iac0628d2d156023dbb80c2e636af6bfe63f46650/"
68,68,13.0,0.9904999732971191,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Switch EVP_CIPHER_CTX to new style Bug: 16656908 Change-Id: Id519c20474a02c70e72d362bc84d26855a74fa33/Convert EVP_PKEY to new style To avoid conflicts in the language spec and how Conscrypt does native calls, we need to wrap all native references in a Java object reference. Calling NativeCryptos static native methods with a raw pointer doesnt guarantee that the calling object wont be finalized during the method running. This pass fixes EVP_PKEY references, but more passes are needed. Bug: 16656908 Change-Id: I5925da40cb37cd328b3a126404944f771732a43e/Allow conscrypt to work with BoringSSL. This is quite a substantial change because of the changes to ENGINEs in BoringSSL. For the most part, are used to allow the code to work with either OpenSSL or BoringSSL. However, in several places, support for things that BoringSSL is dropping have been removed, even when OpenSSL is used. This includes DSA keys and tests for the ENGINE bits that are going away because its unclear how to skip compiling those tests. Change-Id: I941a5ed232391f84b45e070c19d2ffb7ad162b7b/Remove support for DSS TLS/SSL cipher suites. This is in preparation for migration from OpenSSL to BoringSSL. BoringSSL does not support DSS. DSS cipher suites are used by a vanishingly tiny fraction of the Android ecosystem. In all cases, the servers SSL certificate is self-signed (rather than CA issued), making it easy to switch to a new self-signed certificate which is based on RSA or ECDSA. Bug: 17409664 Change-Id: I91067ca9df764edd2b7820e5dec995f24f3910a1/Fix null elements in X509KeyManager.chooseClientAlias keyTypes. This fixes an issue where client certificate types requested by the server from the client, but not known by the client, manifest themselves as null elements in X509KeyManager.chooseClientAlias keyTypes argument. The root cause was that for each element in the CertificateRequest.certificate_types array an element was output into the keyTypes array. For unknown values of certificate_type, a null was output. This CL fixes the issue by ignoring unknown values in certificate_types array. Bug: 18414726 Change-Id: I8565e19a610c0ecfb7cab1b7707c335e0eeb8d89/"
69,69,8.0,0.8810999989509583,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Treat SSL_ERROR_ZERO_RETURN correctly. According to ssl_lib.c, this is returned whenever the socket is being closed (s->shutdown && SSL_RECEIVED_SHUTDOWN && s->s3->warn_alert SSL_AD_CLOSE_NOTIFY). Change-Id: Ied7b3e18f11786351d42a770f4cad11ddae29ff3/Go back to BIO_s_null instead of empty mem buf If you pass NULL to BIO_new_mem_buf, it adds an error to the stack and returns NULL instead of an actual BIO. Go back to BIO_s_null instead. Bug: 18870062 Change-Id: Idba61a90907fbc2ea3528734b8cc9e27eccb1b50/external/conscrypt: sync to latest BoringSSL. This change tweaks one thing because of changes to BoringSSL: RSA methods now have a |supports_digest| member. It also updates several bits of code in order to work with the recently added Change-Id: I1d1ad80b3471fbbe6fcc259e659e425d8129ace5/Clear SSL state safely Since SSL_clear can fail, we should clear the OpenSSL ERR stack if it does fail. However, to aid in spotting bugs, only clear the stack if the SSL_clear itself fails. (cherry picked from commit 86dd832ac26112890b3e815a144ff062ae9b3559) Bug: 18570895 Change-Id: I053d2e2792e64923c1e128b4fcae23b2e660a992/Clear SSL state safely Since SSL_clear can fail, we should clear the OpenSSL ERR stack if it does fail. However, to aid in spotting bugs, only clear the stack if the SSL_clear itself fails. Bug: 18570895 Change-Id: I053d2e2792e64923c1e128b4fcae23b2e660a992/Convert EC_GROUP and EC_POINT to new style Bug: 16656908 Change-Id: Ie912f376f69327ce634cac50763bf86b418049f5/Restore EVP_CIPHER_CTX_set_key_length During the compatibility with BoringSSL change, this appears accidentally removed without removing any of the references from NativeCrypto.java or OpenSSLCipher.java Change-Id: I7fe686b367994f127675b076ab49712767203f49/Convert EVP_PKEY to new style To avoid conflicts in the language spec and how Conscrypt does native calls, we need to wrap all native references in a Java object reference. Calling NativeCryptos static native methods with a raw pointer doesnt guarantee that the calling object wont be finalized during the method running. This pass fixes EVP_PKEY references, but more passes are needed. Bug: 16656908 Change-Id: I5925da40cb37cd328b3a126404944f771732a43e/Fix JNI_TRACE The update to BoringSSL broke some of the tracing messages, so fix their formatting to compile correctly with warning on. Change-Id: I6c7a1e0069b61a787d9e00b929a6c4fa4358a063/Convert EVP_MD_CTX to new style To avoid conflicts in the language spec and how Conscrypt does native calls, we need to wrap all native references in a Java object reference. Calling NativeCryptos static native methods with a raw pointer doesnt guarantee that the calling object wont be finalized during the method running. Bug: 16656908 Change-Id: I165e041a8fe056770d6ce6d6cd064c411575b7c4/Fix mac build. Change-Id: Ib7297bb0631caafed1ff04bcf2d73aea512c01c1/OpenSSLRandom: restore parts for OpenSSL BoringSSL reads /dev/urandom directly, so these calls arent needed. However, OpenSSL needs these calls in some instances to protect against other things going wrong elsewhere. Restore the previous code until BoringSSL is in the tree. Change-Id: I55624e0d98b04e9f5411f69e13a70a78fa0c0d7f/Allow conscrypt to work with BoringSSL. This is quite a substantial change because of the changes to ENGINEs in BoringSSL. For the most part, are used to allow the code to work with either OpenSSL or BoringSSL. However, in several places, support for things that BoringSSL is dropping have been removed, even when OpenSSL is used. This includes DSA keys and tests for the ENGINE bits that are going away because its unclear how to skip compiling those tests. Change-Id: I941a5ed232391f84b45e070c19d2ffb7ad162b7b/Squashed commit of changes from lmp-ub-dev Contains the following changes: commit e31d982cdb0f8e6ef05d1e412576888015e1da17 Merge: eaebc54 b73be72 Author: Neil Fuller Date: Wed Oct 22 10:34:23 2014 +0000 am b73be72e: am 3e21a289: (-s ours) TLS_FALLBACK_SCSV CTS fix for klp-modular-dev * commit b73be72ed97da8f36450d95d52f485cc6f451c61: TLS_FALLBACK_SCSV CTS fix for klp-modular-dev commit eaebc544f3a10c53d7d2f908514122caba569e14 Merge: 223b5da cd50afa Author: Kenny Root Date: Tue Oct 14 17:30:19 2014 +0000 Merge ""Fix SSLEngine to support session resumption."" into lmp-ub-dev commit 223b5da5d70e47b1a497e86474493925b568f6d7 Merge: 8737796 cb7a360 Author: Neil Fuller Date: Thu Oct 9 14:52:00 2014 +0000 am cb7a3605: am ea961ada: Apply conscrypt changes from merge commit * commit cb7a36050f34d3c16be00d532411820761eeb276: Apply conscrypt changes from merge commit commit cd50afad1567b1311e6e979e94a7167b7bf69c94 Author: Doug Steedman Date: Mon Oct 6 13:16:15 2014 Fix SSLEngine to support session resumption. Bug: 17877118 Change-Id: I388b59cde58fdc506ecac9f536e4bbd9161df6ad commit 8737796a646eaec94df32827752a71aee74bd46f Merge: 9564a5f 8d7e23e Author: Kenny Root Date: Mon Oct 6 22:34:20 2014 +0000 am 8d7e23e1: Add support for TLS_FALLBACK_SCSV * commit 8d7e23e117da591a8d48e6bcda9ed6f58ff1a375: Add support for TLS_FALLBACK_SCSV commit 9564a5fb9ed2eecf6299788db35213cb08397212 Merge: 4f58feb 7640613 Author: Kenny Root Date: Fri Sep 12 17:27:23 2014 +0000 am 76406135: am 6dcb23fe: am f427ec90: Fix the ENGINE_finish/ENGINE_free mixup * commit 76406135cf3a3b88afc979fe8e847b9c3d8b93c1: Fix the ENGINE_finish/ENGINE_free mixup commit 4f58feb0ea49dc089a95efba196032ef3c960a39 Merge: ddac5c6 984b7ec Author: Kenny Root Date: Wed Sep 10 07:07:16 2014 +0000 am 984b7ec6: Fix the ENGINE_finish/ENGINE_free mixup * commit 984b7ec6f5aab314117949a48e448ff4f6b65f16: Fix the ENGINE_finish/ENGINE_free mixup commit ddac5c6d7e413b0d68b388fbdf70dbeb3eeae865 Merge: 5a8ca5b 36ba60b Author: Kenny Root Date: Thu Sep 4 22:41:38 2014 +0000 Merge ""Reset lmp-ub-dev to lmp-dev-plus-aosp"" into lmp-ub-dev commit 36ba60b039f1f30ab1ea8f0e2a4da8ae4e3906e5 Author: Kenny Root Date: Wed Aug 27 12:07:07 2014 Reset lmp-ub-dev to lmp-dev-plus-aosp Bug: 17059757 Change-Id: I581963360da47b574e1e2e20c2851485c36fa62c commit 6a4f2ef9e4ea3ebb321d45ca39b30d634ea3b4ad Merge: 9b187af f67d784 Author: Kenny Root Date: Tue Aug 26 04:17:38 2014 +0000 am f67d784a: Add pre-Honeycomb literal IP matching * commit f67d784abe5cef700240be02c68cecd899cd8e6d: Add pre-Honeycomb literal IP matching commit 9b187af33dcd97915a0371d64fe1ee4aba20d0ba Merge: 714ebea 966ae8a Author: Kenny Root Date: Tue Aug 26 04:17:37 2014 +0000 am 966ae8a6: Read property to enable SNI * commit 966ae8a6e12f3235b1cb041e687bda11b41fe4eb: Read property to enable SNI commit 714ebeabcb5e35c6df6a5c21f549cdb6130368c4 Merge: 7724204 54a1ba4 Author: Kenny Root Date: Tue Aug 26 04:06:54 2014 +0000 Merge ""resolved conflicts for merge of 342097db to lmp-dev-plus-aosp"" into lmp-dev-plus-aosp commit 54a1ba421d23bb6d988688c2662715e509172447 Merge: a20d871 342097d Author: Kenny Root Date: Mon Aug 25 21:03:51 2014 resolved conflicts for merge of 342097db to lmp-dev-plus-aosp Change-Id: I853c6b0d3725dafbdc84c4d6d6d1b90529bd949d commit 7724204abf4431d35787c44c4a22cda5489d4e37 Merge: 20f60ac afb3403 Author: Kenny Root Date: Tue Aug 26 00:09:27 2014 +0000 am afb34034: Implement write socket timeouts for unbundled apps * commit afb340348bfc54dbc46964e159fe803f9c93a4dd: Implement write socket timeouts for unbundled apps commit f67d784abe5cef700240be02c68cecd899cd8e6d Author: Kenny Root Date: Wed Aug 20 14:14:26 2014 Add pre-Honeycomb literal IP matching This will allow us to run this code on Gingerbread devices and others that dont have the InetAddress#isNumeric API. Bug: 16658420 Bug: 17059757 Change-Id: I597d539979d58eeaa2677d6f99e911313a550cc1 commit 966ae8a6e12f3235b1cb041e687bda11b41fe4eb Author: Kenny Root Date: Mon Aug 18 10:12:20 2014 Read property to enable SNI Read the system property ""jsse.enableSNIExtension"" on whether to enable Server Name Indication (SNI) extension. For unbundled builds, this will be enabled by default. For platform builds, this will be disabled by default. Bug: 16658420 Bug: 17059757 Change-Id: I774f5406bf3fe601a42c4ef5e708b31800147eb9 commit 342097db97a9b2736531033b2c4b4d8ce4998c67 Author: Kenny Root Date: Wed Aug 20 12:14:52 2014 Validate hostname is usable for SNI According to RFC 6066 section 3, the hostname listed in the Server Name Indication (SNI) field is a fully qualified domain name and IP addresses are not permitted. Bug: 16658420 Bug: 17059757 Change-Id: I804e46b6e66599b2770f0f4f0534467987e51208 commit afb340348bfc54dbc46964e159fe803f9c93a4dd Author: Kenny Root Date: Tue Aug 19 16:33:07 2014 Implement write socket timeouts for unbundled apps Change-Id: I4fd604f057ba4288d4f31bf6b3b93307376023d5 commit 20f60acea153dfdf0c8f75a53d7bd9edb4c7614c Author: Kenny Root Date: Mon Aug 25 11:52:05 2014 Tracking change from AOSP Change-Id: I889af3f7c1de9ef34d9328339e1b421651055ad4 commit 68056b7c9db8a9fb384bbadfc5287730f996896d Merge: 8239dfd cc2ef2e Author: Kenny Root Date: Mon Aug 25 18:03:27 2014 +0000 am cc2ef2e2: Rename hostname fields and methods to reflect usage * commit cc2ef2e2e9ee64f2e0ac2abc7fdf636e2f81fa5e: Rename hostname fields and methods to reflect usage commit 8239dfdcc40a69255d7b2feced960d574ea36321 Merge: e9cf759 076138f Author: Kenny Root Date: Thu Aug 21 16:36:24 2014 +0000 am 076138ff: Use consistent naming for SSLSocket arguments * commit 076138ff29d805ec5a32d6ad96a18ef08c7f1b11: Use consistent naming for SSLSocket arguments commit cc2ef2e2e9ee64f2e0ac2abc7fdf636e2f81fa5e Author: Kenny Root Date: Wed Aug 20 11:26:33 2014 Rename hostname fields and methods to reflect usage The hostname that was supplied when the socket was created is stored as the ""peerHostname"" This is the only one that should be used for Server Name Indication (SNI) purposes. The ""peerHostname"" or the resolved IP address may be used for certificate validation, so keep the use of ""getHostname()"" for cerificate validation. Bug: 16658420 Bug: 17059757 Change-Id: Ifd87dead44fb2f00bbfd5eac7e69fb3fc98e94b4 commit 076138ff29d805ec5a32d6ad96a18ef08c7f1b11 Author: Kenny Root Date: Wed Aug 20 11:24:41 2014 Use consistent naming for SSLSocket arguments This changes all the host to be hostname and anything that takes an InetAddress will have the name of address to avoid confusing it with a hostname. Bug: 16658420 Bug: 17059757 Change-Id: Iac0628d2d156023dbb80c2e636af6bfe63f46650 commit e9cf759ac89fb053c01f1db19931beb14a823618 Merge: ababdd1 7ed0fae Author: Kenny Root Date: Tue Aug 19 19:32:43 2014 +0000 am 7ed0fae1: OpenSSLEngineImpl: reduce number of copies needed * commit 7ed0fae1906061766d0042e69ccba20e4a702bbe: OpenSSLEngineImpl: reduce number of copies needed commit 7ed0fae1906061766d0042e69ccba20e4a702bbe Author: Kenny Root Date: Tue Jul 22 13:03:09 2014 OpenSSLEngineImpl: reduce number of copies needed When the ByteBuffer didnt line up exactly with the backing array, it would allocate a new buffer to write into. Instead, add the ability for OpenSSL to read at an offset in the given array so a copy isnt needed. Change-Id: I149d3f94e4b5cbdc010df80439ae3300cbdc87a5 commit ababdd1ae1272eac174e3a449a413ab35afbc435 Merge: 66c31e0 4b050b6 Author: Kenny Root Date: Fri Aug 15 16:23:14 2014 +0000 am 4b050b6f: OpenSSLSocketImpl: Move state checks inside mutex * commit 4b050b6fb06fbb804557eecc72cc4ff0e0277525: OpenSSLSocketImpl: Move state checks inside mutex commit 66c31e0b613ceefc167a2e1fb226a14c78f84537 Merge: f4b895a 0931d51 Author: Kenny Root Date: Thu Aug 14 20:46:43 2014 +0000 am 0931d51c: OpenSSLSocketImpl: Move state checks inside mutex * commit 0931d51c58b2dc2f612298f99fbf0fa6ed4c3706: OpenSSLSocketImpl: Move state checks inside mutex commit 0931d51c58b2dc2f612298f99fbf0fa6ed4c3706 Author: Kenny Root Date: Tue Aug 5 15:45:32 2014 OpenSSLSocketImpl: Move state checks inside mutex Checking the state of the connection is unreliable if SSL_read and SSL_write are happening in another thread. Move the state checks inside our application mutex so we dont run into another thread mutating the state at the same time. Bug: 15606096 Change-Id: I5ecdeb1551a13098d1b66c5e4009607c9951fa38 commit f4b895ae9c424b5c2d49c744131606adccbc49d7 Merge: a35c400 a260ee6 Author: Kenny Root Date: Wed Aug 13 15:35:28 2014 +0000 am a260ee6d: Revert ""Revert ""Automatic management of OpenSSL error stack"""" * commit a260ee6d0caea43f8010f158a4a35fb712935ae3: Revert ""Revert ""Automatic management of OpenSSL error stack"""" commit a35c40017c8690f821351d6460dfeaa2738b884c Merge: 0edc483 30550a8 Author: Kenny Root Date: Wed Aug 13 15:35:27 2014 +0000 am 30550a8b: Fix debugging with unbundled conscrypt * commit 30550a8b64bbcd6ca537680a17b8726932a29937: Fix debugging with unbundled conscrypt commit a260ee6d0caea43f8010f158a4a35fb712935ae3 Author: Kenny Root Date: Tue Aug 12 15:38:10 2014 Revert ""Revert ""Automatic management of OpenSSL error stack"""" The ""else"" statement in OpenSslError::reset wasnt properly resetting the error state which made a second call into sslRead jump into sslSelect when it should have just returned immediately. Change-Id: I22e8025c0497a04e78daa07cef78191a6ca1a70c commit 30550a8b64bbcd6ca537680a17b8726932a29937 Author: Kenny Root Date: Tue Aug 12 15:13:33 2014 Fix debugging with unbundled conscrypt When JNI_TRACE was enabled, there were missing defines for the debugging code since no platform code is included. Also clang complains about more of the debugging statement formats, so we have to move some things around to get it to be happy. Change-Id: I1a6695c2ef2639cc01cfc3d3a8603f010c659844 commit 0edc4833091846d6cb45961fc9458df842fbbad9 Merge: 107a8fb 2411b8b Author: Kenny Root Date: Tue Aug 12 21:46:12 2014 +0000 am 2411b8bd: Merge ""Revert ""Automatic management of OpenSSL error stack"""" * commit 2411b8bdcde72c956f4150e9a5909b7501f50bad: Revert ""Automatic management of OpenSSL error stack"" commit 2411b8bdcde72c956f4150e9a5909b7501f50bad Merge: 3262a8c b514d72 Author: Kenny Root Date: Tue Aug 12 21:39:32 2014 +0000 Merge ""Revert ""Automatic management of OpenSSL error stack"""" commit b514d72b93c3996d97e38eca6db1ad684965fd9b Author: Kenny Root Date: Tue Aug 12 21:39:17 2014 +0000 Revert ""Automatic management of OpenSSL error stack"" This reverts commit 35666e4cb0fcd063a21d17eebbb571b4e4e822b8. Change-Id: I926d159c4c4b99250caef750732976c1e601e9ef commit 107a8fba8be5be57933f2638b76ac1243b578b9e Merge: 1de007f 3262a8c Author: Kenny Root Date: Tue Aug 12 15:50:14 2014 +0000 am 3262a8c2: Merge ""Automatic management of OpenSSL error stack"" * commit 3262a8c2741b95103149bcdefe2409c24bfddee9: Automatic management of OpenSSL error stack commit 1de007f9f01be8f07a56235dd924c897088a03cb Merge: 94890ae d1bbcd0 Author: Kenny Root Date: Tue Aug 12 15:50:14 2014 +0000 am d1bbcd0e: Relax checks for key vs cert for wrapped keys * commit d1bbcd0ec973e1b8465c204c13b4925fd86e6484: Relax checks for key vs cert for wrapped keys commit 3262a8c2741b95103149bcdefe2409c24bfddee9 Merge: d1bbcd0 35666e4 Author: Kenny Root Date: Tue Aug 12 15:31:02 2014 +0000 Merge ""Automatic management of OpenSSL error stack"" commit d1bbcd0ec973e1b8465c204c13b4925fd86e6484 Author: Kenny Root Date: Mon Aug 11 14:56:58 2014 Relax checks for key vs cert for wrapped keys If a key is a wrapped platform key, we must relax the check. The reason is that we may not have the public values we need to pass the EVP_PKEY_cmp checks that this does. Change-Id: I7ab2be51b0968a9cf771edea01d33fe2367c8185 commit 35666e4cb0fcd063a21d17eebbb571b4e4e822b8 Author: Kenny Root Date: Tue Aug 5 11:05:00 2014 Automatic management of OpenSSL error stack This removes some complexity in remembering to free the OpenSSL error stack. If you forget, the error will stick around until you make another call. Change-Id: I245a525dcc93077b2bf9909a14a0ef469a2daca4 commit 94890aec5735cde2ea5170fb76cd1b847ea66af8 Merge: 8360485 977f087 Author: Kenny Root Date: Tue Aug 5 16:44:42 2014 +0000 am 977f0877: Fix some JNI_TRACE lines * commit 977f08774c628b4640d5454cde050259856965f8: Fix some JNI_TRACE lines commit 977f08774c628b4640d5454cde050259856965f8 Author: Kenny Root Date: Mon Aug 4 12:15:04 2014 Fix some JNI_TRACE lines During debugging these would be enabled, but they were copy-pastad to with the wrong args. Change-Id: I23f39ff4807e3fa71f3220912aec3c99db6b9454 commit 83604854c5160304cafefc9bd40a72c5ee8506eb Merge: 7db3524 1ffe43e Author: Zoltan Szatmary-Ban Date: Thu Jul 31 13:28:57 2014 +0000 am 1ffe43e8: Merge ""Add possibility to get deleted system Certificate Aliases"" into lmp-dev * commit 1ffe43e8277e883c6663c1fb7cfc5e18ba552c40: Add possibility to get deleted system Certificate Aliases commit 7db3524880092126962b7f502af76b4c84da7350 Merge: 5767d63 ad0cd83 Author: Prameet Shah Date: Wed Jul 30 17:04:13 2014 +0000 am ad0cd830: Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() * commit ad0cd83024f38011043d28d70370a8638b88cd72: Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() commit 5767d63d22e87becab387b3bd6597fe41eb34d7e Merge: b389e17 26163c2 Author: Prameet Shah Date: Wed Jul 30 16:31:08 2014 +0000 am 26163c26: Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() * commit 26163c268a6d2625384b87e907afad8ef19f9a47: Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() commit 26163c268a6d2625384b87e907afad8ef19f9a47 Author: Prameet Shah Date: Tue Jul 29 16:45:31 2014 Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() Bug: Change-Id: I5bcaf3ee8910ff75e785baed4c4604fee6c5e700 commit b389e1779651f2c58454a5f98acebd3dd7bc0061 Merge: 5f03b4d e427972 Author: Prameet Shah Date: Thu Jul 24 19:46:28 2014 +0000 am e427972e: OpenSSLEngineImpl: fix unwrap behavior with array * commit e427972eb6141cd67e6d4c9607863a8d990e6be6: OpenSSLEngineImpl: fix unwrap behavior with array commit 5f03b4d63c7632581b032879de791dc82f05ffa0 Merge: 3d935ee 41eb5b6 Author: Prameet Shah Date: Tue Jul 22 19:26:41 2014 +0000 am 41eb5b65: OpenSSLEngineImpl: fix unwrap behavior with array * commit 41eb5b65e524d01e28da474bd37e4349b12fb494: OpenSSLEngineImpl: fix unwrap behavior with array commit 41eb5b65e524d01e28da474bd37e4349b12fb494 Author: Prameet Shah Date: Tue Jul 22 11:50:18 2014 OpenSSLEngineImpl: fix unwrap behavior with array The decrypted bytes should written sequentially into each buffer of the destination array until its full before moving to the next buffer. Change-Id: I2454249c167deafde6c12134d3c8cd658cd7c21b commit 3d935eeca25e00b56cfd8d37a657c7b2986889b3 Merge: 0a36f6c affd45a Author: Alex Klyubin Date: Fri Jul 18 00:32:14 2014 +0000 am affd45a4: Merge ""Improve the Javadoc of PSKKeyManager."" into lmp-dev * commit affd45a413cf844dad797ad4972074efb9de43d8: Improve the Javadoc of PSKKeyManager. commit 0a36f6c1f8b2e195c2dd5aea1a386df090c6d470 Merge: 6492180 af4fa68 Author: rich cannings Date: Thu Jul 17 23:47:33 2014 +0000 am af4fa685: Merge ""Log CCS exceptions do not merge."" into lmp-dev * commit af4fa685f246aaa80c93af62faadbc2fe87dc034: Log CCS exceptions do not merge. commit 6492180ce17a3b5ff822cff1783f00e7a4176491 Merge: aac4168 3b7268c Author: Alex Klyubin Date: Thu Jul 17 18:27:39 2014 +0000 am 3b7268cd: Merge ""Improve the Javadoc of PSKKeyManager."" * commit 3b7268cde4a4fc59591da8a93691927ebf3add57: Improve the Javadoc of PSKKeyManager. commit aac4168d8baef7e12d6fa959c6d6ded9892e9651 Merge: 8573ad0 a749c0d Author: Kenny Root Date: Thu Jul 17 17:07:05 2014 +0000 am a749c0d3: Keep enough state to completely reset cipher instances * commit a749c0d351216be38879600ee8ed01c6793aa256: Keep enough state to completely reset cipher instances commit 8573ad0ddcf7e2f8b2e5ac84c34b7ffab303155c Merge: 4ca5b06 70fdb6d Author: Koushik Dutta Date: Thu Jul 17 17:06:36 2014 +0000 am 70fdb6d2: OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. * commit 70fdb6d2bfa0c313fe389827f0025288f6aeb947: OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. commit 4ca5b0625e3f5a15ae8adf833ab5a69f9d7d517f Merge: 119abfb ded66f5 Author: Koushik Dutta Date: Thu Jul 17 17:06:35 2014 +0000 am ded66f5f: Various fixes in OpenSSLEngineImpl. * commit ded66f5f696994ce7620552e16a4e9124e69e052: Various fixes in OpenSSLEngineImpl. commit 119abfba1fcd9c9cfbd15d0a4ca9ed2188fdfab0 Merge: 5713cdf cbe1f28 Author: Kenny Root Date: Thu Jul 17 15:56:57 2014 +0000 am cbe1f28a: Merge ""Keep enough state to completely reset cipher instances"" * commit cbe1f28adf64396561a3b65bf1452dfa9b6e35ae: Keep enough state to completely reset cipher instances commit cbe1f28adf64396561a3b65bf1452dfa9b6e35ae Merge: e08f238 084e308 Author: Kenny Root Date: Thu Jul 17 15:48:58 2014 +0000 Merge ""Keep enough state to completely reset cipher instances"" commit 3b7268cde4a4fc59591da8a93691927ebf3add57 Merge: cbe1f28 7ac13e0 Author: Alex Klyubin Date: Thu Jul 17 18:20:43 2014 +0000 Merge ""Improve the Javadoc of PSKKeyManager."" commit 5713cdf71c5c6e5179e8369263c702e9512afdd0 Merge: cf55719 e08f238 Author: Koushik Dutta Date: Wed Jul 16 22:05:17 2014 +0000 am e08f2385: OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. * commit e08f238580e8ee471012bef8240c8d3397c7b780: OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. commit cf557195a9b60d7f51a48500afde38481ddbc91c Merge: cbbd7d1 986aeb7 Author: Kenny Root Date: Wed Jul 16 21:41:12 2014 +0000 am 986aeb78: Merge ""Various fixes in OpenSSLEngineImpl."" * commit 986aeb78e533540463daf1753e24840f75b25ce6: Various fixes in OpenSSLEngineImpl. commit e08f238580e8ee471012bef8240c8d3397c7b780 Author: Koushik Dutta Date: Tue Jul 15 22:40:23 2014 OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. Change-Id: Idc78204b7077fb367b64e1867c807cd39f596f98 commit 7ac13e03a79d0c99d181b1a28b1b3699ba3d5739 Author: Alex Klyubin Date: Wed Jul 16 08:33:02 2014 Improve the Javadoc of PSKKeyManager. This clarifies several points and adds sample code. Bug: 15073623 Change-Id: I6e8aadc52277e238a998d6cee36795dab1151d58 commit 986aeb78e533540463daf1753e24840f75b25ce6 Merge: 8f9ac1a bdfcc18 Author: Kenny Root Date: Wed Jul 16 21:15:30 2014 +0000 Merge ""Various fixes in OpenSSLEngineImpl."" commit bdfcc189efe41a3f812aeb55ea634bace67d159a Author: Koushik Dutta Date: Sat Jun 28 19:19:21 2014 Various fixes in OpenSSLEngineImpl. Fix ""Buffers were not large enough"" exception by directly using the destination buffers. Corrections around bytesProduced and bytesConsumed behavior. Return BUFFER_OVERFLOW if a zero length destination is provided to unwrap. Change-Id: I1f1e9b72cd6968ed4f3c3c0edccbccebc33d6790 commit cbbd7d10e8e484c44a78e5b27e8fecda195f1692 Merge: ec7f8e6 fdb7d8c Author: Alex Klyubin Date: Tue Jul 15 18:49:14 2014 +0000 am fdb7d8c5: Enable PSK cipher suites when PSKKeyManager is provided. * commit fdb7d8c53dabac5551e2499d045ba6829bcfc0a0: Enable PSK cipher suites when PSKKeyManager is provided. commit ec7f8e6b27330160f88540f4f2ace7bc2a0720a3 Merge: 5b8ccf1 8f9ac1a Author: Alex Klyubin Date: Tue Jul 15 15:53:46 2014 +0000 am 8f9ac1af: Enable PSK cipher suites when PSKKeyManager is provided. * commit 8f9ac1af0cbdf00e5e47aee32c132522ebc3bd17: Enable PSK cipher suites when PSKKeyManager is provided. commit 5b8ccf1b09df6f35c1709bfc8fd727a291094a5b Merge: 69a2e46 6e2315f Author: Ed Heyl Date: Tue Jul 15 13:34:25 2014 +0000 am 6e2315fd: reconcile aosp (e79c25bf33e10da41e489c537823f678e1a1169c) after branching. Please do not merge. * commit 6e2315fd96c3c4a47450c1a437babacc94bc31a6: reconcile aosp (e79c25bf33e10da41e489c537823f678e1a1169c) after branching. Please do not merge. commit 084e3086be1d7a6b9280b64c7c8cdb7b41a13bea Author: Kenny Root Date: Mon Jul 14 13:25:32 2014 Keep enough state to completely reset cipher instances OpenSSLs RC4 mutates the given key. AES/CTR mutates the IV. We must store these values locally to enable ""doFinal"" to cause the Cipher instance to be reset to what it was right after ""init"". Note that resetting and encrypting with the same key or IV breaks semantic security. Bug: 16298401 Bug: Change-Id: Ie7e4dcb6cf6cc33ddad31d6b47066dc1b34e6894 commit 69a2e460cc0a40e1b951e400589b9932609079ec Merge: 8b7bb32 bca895f Author: David Benjamin Date: Mon Jul 14 18:17:28 2014 +0000 am bca895f8: Pass output buffer length into EVP_DigestSignFinal. * commit bca895f809dd2cef7a0834f0bfeb2a06e42b277d: Pass output buffer length into EVP_DigestSignFinal. commit 8b7bb32af09a01e80442b70dd23e6997a937f103 Merge: a2404c9 e79c25b Author: Kenny Root Date: Mon Jul 14 18:17:28 2014 +0000 am e79c25bf: Merge ""DHKeyPairGenerator: use provided params"" * commit e79c25bf33e10da41e489c537823f678e1a1169c: DHKeyPairGenerator: use provided params commit 8f9ac1af0cbdf00e5e47aee32c132522ebc3bd17 Author: Alex Klyubin Date: Thu Jun 19 13:37:24 2014 Enable PSK cipher suites when PSKKeyManager is"
70,70,13.0,0.9735999703407288,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Allow conscrypt to work with BoringSSL. This is quite a substantial change because of the changes to ENGINEs in BoringSSL. For the most part, are used to allow the code to work with either OpenSSL or BoringSSL. However, in several places, support for things that BoringSSL is dropping have been removed, even when OpenSSL is used. This includes DSA keys and tests for the ENGINE bits that are going away because its unclear how to skip compiling those tests. Change-Id: I941a5ed232391f84b45e070c19d2ffb7ad162b7b/"
71,71,17.0,0.9269000291824341,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Convert EVP_PKEY to new style To avoid conflicts in the language spec and how Conscrypt does native calls, we need to wrap all native references in a Java object reference. Calling NativeCryptos static native methods with a raw pointer doesnt guarantee that the calling object wont be finalized during the method running. This pass fixes EVP_PKEY references, but more passes are needed. Bug: 16656908 Change-Id: I5925da40cb37cd328b3a126404944f771732a43e/"
72,72,17.0,0.9269000291824341,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Convert EVP_PKEY to new style To avoid conflicts in the language spec and how Conscrypt does native calls, we need to wrap all native references in a Java object reference. Calling NativeCryptos static native methods with a raw pointer doesnt guarantee that the calling object wont be finalized during the method running. This pass fixes EVP_PKEY references, but more passes are needed. Bug: 16656908 Change-Id: I5925da40cb37cd328b3a126404944f771732a43e/"
73,73,10.0,0.8203999996185303,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Call EVP_CIPHER_CTX_free instead of EVP_CIPHER_CTX_cleanup. The latter doesnt OpenSSL_free memory allocated by EVP_CIPHER_CTX_new. Its worth noting that EVP_CIPHER_CTX_free doesnt check the return value of EVP_CIPHER_CTX_cleanup so we cant throw if cleanup failed, but we were only ever calling this method from a finalizer anyway. (cherry picked from commit c64652932d8e17ccf7e54c0c76c1b38a86841732) bug: 18617384 Change-Id: Ida65e14ffbed41f56a59e2f5fe77289cac0f5947/Convert EC_GROUP and EC_POINT to new style Bug: 16656908 Change-Id: Ie912f376f69327ce634cac50763bf86b418049f5/Switch EVP_CIPHER_CTX to new style Bug: 16656908 Change-Id: Id519c20474a02c70e72d362bc84d26855a74fa33/Convert EVP_PKEY to new style To avoid conflicts in the language spec and how Conscrypt does native calls, we need to wrap all native references in a Java object reference. Calling NativeCryptos static native methods with a raw pointer doesnt guarantee that the calling object wont be finalized during the method running. This pass fixes EVP_PKEY references, but more passes are needed. Bug: 16656908 Change-Id: I5925da40cb37cd328b3a126404944f771732a43e/Convert EVP_MD_CTX to new style To avoid conflicts in the language spec and how Conscrypt does native calls, we need to wrap all native references in a Java object reference. Calling NativeCryptos static native methods with a raw pointer doesnt guarantee that the calling object wont be finalized during the method running. Bug: 16656908 Change-Id: I165e041a8fe056770d6ce6d6cd064c411575b7c4/Clean up unused variables Change-Id: I9234e649a910408cff9f9d33008642e0c8334276/Track upgrade to OpenSSL 1.0.1j Bug: 18018599 Change-Id: I2b8c62190a9dd5e5fdc6894334cf1d3edfce0a06/Remove SSLv3 from default protocols list for TLS SSLv3 has some systemic problems demonstrated by the POODLE attack. Disable it by default when ""TLS"" is requested since the documentation in Java Standard Names allows us to not support SSL when TLS is requested. Bug: 17136008 Change-Id: Icad1639c7e33b6e495f452a5289b0d20b819d679/"
74,74,9.0,0.8100000023841858,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Switch EVP_CIPHER_CTX to new style Bug: 16656908 Change-Id: Id519c20474a02c70e72d362bc84d26855a74fa33/
75,75,17.0,0.9269000291824341,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Convert EVP_PKEY to new style To avoid conflicts in the language spec and how Conscrypt does native calls, we need to wrap all native references in a Java object reference. Calling NativeCryptos static native methods with a raw pointer doesnt guarantee that the calling object wont be finalized during the method running. This pass fixes EVP_PKEY references, but more passes are needed. Bug: 16656908 Change-Id: I5925da40cb37cd328b3a126404944f771732a43e/"
76,76,14.0,0.983299970626831,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","OpenSSLEngineImpl: return bytes consumed for unwrap During a handshake, unwrap should return the number of bytes consumed by the SSL implementation in addition to changing the source buffer position so that the client can alter its state based on either. Bug: 18921387 Bug: Change-Id: Idf5a3b24c8ad053ef2970bfb66d142a7c2685c02/Return BUFFER_UNDERFLOW if no source bytes were consumed. ... either during the handshake or after. With this change, were backward compatible with older versions of android. Note that newer versions of apache-http rely on this behaviour. bug: 18554122 (cherry picked from commit 6a1b7a85dcdeb19305ad5153579bd11c1eb0bfad) Change-Id: I741d2585548b3d72abae2b696eee2a186e58414c/Return BUFFER_UNDERFLOW if no source bytes were consumed. ... either during the handshake or after. With this change, were backward compatible with older versions of android. Note that newer versions of apache-http rely on this behaviour. bug: 18554122 Change-Id: I574c263e8df4a5f2396ac860608fe85cdbcdbb49/Fix SSLEngine to support session resumption. (cherry picked from commit cd50afad1567b1311e6e979e94a7167b7bf69c94) Bug: 17877118 Change-Id: I388b59cde58fdc506ecac9f536e4bbd9161df6ad/"
77,77,8.0,0.7427999973297119,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Fix OpenSSLSocketImpl.getPort when SNI is used. We were using a non-null hostname as a hint that the socket was constructed with an explicit host and port. This is no longer true because the hostname can be non-null when SNI is used (i.e setHostname is called with a non-null hostname). bug: 18428603 (cherry picked from commit 131640979c0ba3f18581cee9bf5c925ec8a7372b) Change-Id: I5a76a17259e4f50a0b8a29b37a647265a755e326/Squashed commit of changes from lmp-ub-dev Contains the following changes: commit e31d982cdb0f8e6ef05d1e412576888015e1da17 Merge: eaebc54 b73be72 Author: Neil Fuller Date: Wed Oct 22 10:34:23 2014 +0000 am b73be72e: am 3e21a289: (-s ours) TLS_FALLBACK_SCSV CTS fix for klp-modular-dev * commit b73be72ed97da8f36450d95d52f485cc6f451c61: TLS_FALLBACK_SCSV CTS fix for klp-modular-dev commit eaebc544f3a10c53d7d2f908514122caba569e14 Merge: 223b5da cd50afa Author: Kenny Root Date: Tue Oct 14 17:30:19 2014 +0000 Merge ""Fix SSLEngine to support session resumption."" into lmp-ub-dev commit 223b5da5d70e47b1a497e86474493925b568f6d7 Merge: 8737796 cb7a360 Author: Neil Fuller Date: Thu Oct 9 14:52:00 2014 +0000 am cb7a3605: am ea961ada: Apply conscrypt changes from merge commit * commit cb7a36050f34d3c16be00d532411820761eeb276: Apply conscrypt changes from merge commit commit cd50afad1567b1311e6e979e94a7167b7bf69c94 Author: Doug Steedman Date: Mon Oct 6 13:16:15 2014 Fix SSLEngine to support session resumption. Bug: 17877118 Change-Id: I388b59cde58fdc506ecac9f536e4bbd9161df6ad commit 8737796a646eaec94df32827752a71aee74bd46f Merge: 9564a5f 8d7e23e Author: Kenny Root Date: Mon Oct 6 22:34:20 2014 +0000 am 8d7e23e1: Add support for TLS_FALLBACK_SCSV * commit 8d7e23e117da591a8d48e6bcda9ed6f58ff1a375: Add support for TLS_FALLBACK_SCSV commit 9564a5fb9ed2eecf6299788db35213cb08397212 Merge: 4f58feb 7640613 Author: Kenny Root Date: Fri Sep 12 17:27:23 2014 +0000 am 76406135: am 6dcb23fe: am f427ec90: Fix the ENGINE_finish/ENGINE_free mixup * commit 76406135cf3a3b88afc979fe8e847b9c3d8b93c1: Fix the ENGINE_finish/ENGINE_free mixup commit 4f58feb0ea49dc089a95efba196032ef3c960a39 Merge: ddac5c6 984b7ec Author: Kenny Root Date: Wed Sep 10 07:07:16 2014 +0000 am 984b7ec6: Fix the ENGINE_finish/ENGINE_free mixup * commit 984b7ec6f5aab314117949a48e448ff4f6b65f16: Fix the ENGINE_finish/ENGINE_free mixup commit ddac5c6d7e413b0d68b388fbdf70dbeb3eeae865 Merge: 5a8ca5b 36ba60b Author: Kenny Root Date: Thu Sep 4 22:41:38 2014 +0000 Merge ""Reset lmp-ub-dev to lmp-dev-plus-aosp"" into lmp-ub-dev commit 36ba60b039f1f30ab1ea8f0e2a4da8ae4e3906e5 Author: Kenny Root Date: Wed Aug 27 12:07:07 2014 Reset lmp-ub-dev to lmp-dev-plus-aosp Bug: 17059757 Change-Id: I581963360da47b574e1e2e20c2851485c36fa62c commit 6a4f2ef9e4ea3ebb321d45ca39b30d634ea3b4ad Merge: 9b187af f67d784 Author: Kenny Root Date: Tue Aug 26 04:17:38 2014 +0000 am f67d784a: Add pre-Honeycomb literal IP matching * commit f67d784abe5cef700240be02c68cecd899cd8e6d: Add pre-Honeycomb literal IP matching commit 9b187af33dcd97915a0371d64fe1ee4aba20d0ba Merge: 714ebea 966ae8a Author: Kenny Root Date: Tue Aug 26 04:17:37 2014 +0000 am 966ae8a6: Read property to enable SNI * commit 966ae8a6e12f3235b1cb041e687bda11b41fe4eb: Read property to enable SNI commit 714ebeabcb5e35c6df6a5c21f549cdb6130368c4 Merge: 7724204 54a1ba4 Author: Kenny Root Date: Tue Aug 26 04:06:54 2014 +0000 Merge ""resolved conflicts for merge of 342097db to lmp-dev-plus-aosp"" into lmp-dev-plus-aosp commit 54a1ba421d23bb6d988688c2662715e509172447 Merge: a20d871 342097d Author: Kenny Root Date: Mon Aug 25 21:03:51 2014 resolved conflicts for merge of 342097db to lmp-dev-plus-aosp Change-Id: I853c6b0d3725dafbdc84c4d6d6d1b90529bd949d commit 7724204abf4431d35787c44c4a22cda5489d4e37 Merge: 20f60ac afb3403 Author: Kenny Root Date: Tue Aug 26 00:09:27 2014 +0000 am afb34034: Implement write socket timeouts for unbundled apps * commit afb340348bfc54dbc46964e159fe803f9c93a4dd: Implement write socket timeouts for unbundled apps commit f67d784abe5cef700240be02c68cecd899cd8e6d Author: Kenny Root Date: Wed Aug 20 14:14:26 2014 Add pre-Honeycomb literal IP matching This will allow us to run this code on Gingerbread devices and others that dont have the InetAddress#isNumeric API. Bug: 16658420 Bug: 17059757 Change-Id: I597d539979d58eeaa2677d6f99e911313a550cc1 commit 966ae8a6e12f3235b1cb041e687bda11b41fe4eb Author: Kenny Root Date: Mon Aug 18 10:12:20 2014 Read property to enable SNI Read the system property ""jsse.enableSNIExtension"" on whether to enable Server Name Indication (SNI) extension. For unbundled builds, this will be enabled by default. For platform builds, this will be disabled by default. Bug: 16658420 Bug: 17059757 Change-Id: I774f5406bf3fe601a42c4ef5e708b31800147eb9 commit 342097db97a9b2736531033b2c4b4d8ce4998c67 Author: Kenny Root Date: Wed Aug 20 12:14:52 2014 Validate hostname is usable for SNI According to RFC 6066 section 3, the hostname listed in the Server Name Indication (SNI) field is a fully qualified domain name and IP addresses are not permitted. Bug: 16658420 Bug: 17059757 Change-Id: I804e46b6e66599b2770f0f4f0534467987e51208 commit afb340348bfc54dbc46964e159fe803f9c93a4dd Author: Kenny Root Date: Tue Aug 19 16:33:07 2014 Implement write socket timeouts for unbundled apps Change-Id: I4fd604f057ba4288d4f31bf6b3b93307376023d5 commit 20f60acea153dfdf0c8f75a53d7bd9edb4c7614c Author: Kenny Root Date: Mon Aug 25 11:52:05 2014 Tracking change from AOSP Change-Id: I889af3f7c1de9ef34d9328339e1b421651055ad4 commit 68056b7c9db8a9fb384bbadfc5287730f996896d Merge: 8239dfd cc2ef2e Author: Kenny Root Date: Mon Aug 25 18:03:27 2014 +0000 am cc2ef2e2: Rename hostname fields and methods to reflect usage * commit cc2ef2e2e9ee64f2e0ac2abc7fdf636e2f81fa5e: Rename hostname fields and methods to reflect usage commit 8239dfdcc40a69255d7b2feced960d574ea36321 Merge: e9cf759 076138f Author: Kenny Root Date: Thu Aug 21 16:36:24 2014 +0000 am 076138ff: Use consistent naming for SSLSocket arguments * commit 076138ff29d805ec5a32d6ad96a18ef08c7f1b11: Use consistent naming for SSLSocket arguments commit cc2ef2e2e9ee64f2e0ac2abc7fdf636e2f81fa5e Author: Kenny Root Date: Wed Aug 20 11:26:33 2014 Rename hostname fields and methods to reflect usage The hostname that was supplied when the socket was created is stored as the ""peerHostname"" This is the only one that should be used for Server Name Indication (SNI) purposes. The ""peerHostname"" or the resolved IP address may be used for certificate validation, so keep the use of ""getHostname()"" for cerificate validation. Bug: 16658420 Bug: 17059757 Change-Id: Ifd87dead44fb2f00bbfd5eac7e69fb3fc98e94b4 commit 076138ff29d805ec5a32d6ad96a18ef08c7f1b11 Author: Kenny Root Date: Wed Aug 20 11:24:41 2014 Use consistent naming for SSLSocket arguments This changes all the host to be hostname and anything that takes an InetAddress will have the name of address to avoid confusing it with a hostname. Bug: 16658420 Bug: 17059757 Change-Id: Iac0628d2d156023dbb80c2e636af6bfe63f46650 commit e9cf759ac89fb053c01f1db19931beb14a823618 Merge: ababdd1 7ed0fae Author: Kenny Root Date: Tue Aug 19 19:32:43 2014 +0000 am 7ed0fae1: OpenSSLEngineImpl: reduce number of copies needed * commit 7ed0fae1906061766d0042e69ccba20e4a702bbe: OpenSSLEngineImpl: reduce number of copies needed commit 7ed0fae1906061766d0042e69ccba20e4a702bbe Author: Kenny Root Date: Tue Jul 22 13:03:09 2014 OpenSSLEngineImpl: reduce number of copies needed When the ByteBuffer didnt line up exactly with the backing array, it would allocate a new buffer to write into. Instead, add the ability for OpenSSL to read at an offset in the given array so a copy isnt needed. Change-Id: I149d3f94e4b5cbdc010df80439ae3300cbdc87a5 commit ababdd1ae1272eac174e3a449a413ab35afbc435 Merge: 66c31e0 4b050b6 Author: Kenny Root Date: Fri Aug 15 16:23:14 2014 +0000 am 4b050b6f: OpenSSLSocketImpl: Move state checks inside mutex * commit 4b050b6fb06fbb804557eecc72cc4ff0e0277525: OpenSSLSocketImpl: Move state checks inside mutex commit 66c31e0b613ceefc167a2e1fb226a14c78f84537 Merge: f4b895a 0931d51 Author: Kenny Root Date: Thu Aug 14 20:46:43 2014 +0000 am 0931d51c: OpenSSLSocketImpl: Move state checks inside mutex * commit 0931d51c58b2dc2f612298f99fbf0fa6ed4c3706: OpenSSLSocketImpl: Move state checks inside mutex commit 0931d51c58b2dc2f612298f99fbf0fa6ed4c3706 Author: Kenny Root Date: Tue Aug 5 15:45:32 2014 OpenSSLSocketImpl: Move state checks inside mutex Checking the state of the connection is unreliable if SSL_read and SSL_write are happening in another thread. Move the state checks inside our application mutex so we dont run into another thread mutating the state at the same time. Bug: 15606096 Change-Id: I5ecdeb1551a13098d1b66c5e4009607c9951fa38 commit f4b895ae9c424b5c2d49c744131606adccbc49d7 Merge: a35c400 a260ee6 Author: Kenny Root Date: Wed Aug 13 15:35:28 2014 +0000 am a260ee6d: Revert ""Revert ""Automatic management of OpenSSL error stack"""" * commit a260ee6d0caea43f8010f158a4a35fb712935ae3: Revert ""Revert ""Automatic management of OpenSSL error stack"""" commit a35c40017c8690f821351d6460dfeaa2738b884c Merge: 0edc483 30550a8 Author: Kenny Root Date: Wed Aug 13 15:35:27 2014 +0000 am 30550a8b: Fix debugging with unbundled conscrypt * commit 30550a8b64bbcd6ca537680a17b8726932a29937: Fix debugging with unbundled conscrypt commit a260ee6d0caea43f8010f158a4a35fb712935ae3 Author: Kenny Root Date: Tue Aug 12 15:38:10 2014 Revert ""Revert ""Automatic management of OpenSSL error stack"""" The ""else"" statement in OpenSslError::reset wasnt properly resetting the error state which made a second call into sslRead jump into sslSelect when it should have just returned immediately. Change-Id: I22e8025c0497a04e78daa07cef78191a6ca1a70c commit 30550a8b64bbcd6ca537680a17b8726932a29937 Author: Kenny Root Date: Tue Aug 12 15:13:33 2014 Fix debugging with unbundled conscrypt When JNI_TRACE was enabled, there were missing defines for the debugging code since no platform code is included. Also clang complains about more of the debugging statement formats, so we have to move some things around to get it to be happy. Change-Id: I1a6695c2ef2639cc01cfc3d3a8603f010c659844 commit 0edc4833091846d6cb45961fc9458df842fbbad9 Merge: 107a8fb 2411b8b Author: Kenny Root Date: Tue Aug 12 21:46:12 2014 +0000 am 2411b8bd: Merge ""Revert ""Automatic management of OpenSSL error stack"""" * commit 2411b8bdcde72c956f4150e9a5909b7501f50bad: Revert ""Automatic management of OpenSSL error stack"" commit 2411b8bdcde72c956f4150e9a5909b7501f50bad Merge: 3262a8c b514d72 Author: Kenny Root Date: Tue Aug 12 21:39:32 2014 +0000 Merge ""Revert ""Automatic management of OpenSSL error stack"""" commit b514d72b93c3996d97e38eca6db1ad684965fd9b Author: Kenny Root Date: Tue Aug 12 21:39:17 2014 +0000 Revert ""Automatic management of OpenSSL error stack"" This reverts commit 35666e4cb0fcd063a21d17eebbb571b4e4e822b8. Change-Id: I926d159c4c4b99250caef750732976c1e601e9ef commit 107a8fba8be5be57933f2638b76ac1243b578b9e Merge: 1de007f 3262a8c Author: Kenny Root Date: Tue Aug 12 15:50:14 2014 +0000 am 3262a8c2: Merge ""Automatic management of OpenSSL error stack"" * commit 3262a8c2741b95103149bcdefe2409c24bfddee9: Automatic management of OpenSSL error stack commit 1de007f9f01be8f07a56235dd924c897088a03cb Merge: 94890ae d1bbcd0 Author: Kenny Root Date: Tue Aug 12 15:50:14 2014 +0000 am d1bbcd0e: Relax checks for key vs cert for wrapped keys * commit d1bbcd0ec973e1b8465c204c13b4925fd86e6484: Relax checks for key vs cert for wrapped keys commit 3262a8c2741b95103149bcdefe2409c24bfddee9 Merge: d1bbcd0 35666e4 Author: Kenny Root Date: Tue Aug 12 15:31:02 2014 +0000 Merge ""Automatic management of OpenSSL error stack"" commit d1bbcd0ec973e1b8465c204c13b4925fd86e6484 Author: Kenny Root Date: Mon Aug 11 14:56:58 2014 Relax checks for key vs cert for wrapped keys If a key is a wrapped platform key, we must relax the check. The reason is that we may not have the public values we need to pass the EVP_PKEY_cmp checks that this does. Change-Id: I7ab2be51b0968a9cf771edea01d33fe2367c8185 commit 35666e4cb0fcd063a21d17eebbb571b4e4e822b8 Author: Kenny Root Date: Tue Aug 5 11:05:00 2014 Automatic management of OpenSSL error stack This removes some complexity in remembering to free the OpenSSL error stack. If you forget, the error will stick around until you make another call. Change-Id: I245a525dcc93077b2bf9909a14a0ef469a2daca4 commit 94890aec5735cde2ea5170fb76cd1b847ea66af8 Merge: 8360485 977f087 Author: Kenny Root Date: Tue Aug 5 16:44:42 2014 +0000 am 977f0877: Fix some JNI_TRACE lines * commit 977f08774c628b4640d5454cde050259856965f8: Fix some JNI_TRACE lines commit 977f08774c628b4640d5454cde050259856965f8 Author: Kenny Root Date: Mon Aug 4 12:15:04 2014 Fix some JNI_TRACE lines During debugging these would be enabled, but they were copy-pastad to with the wrong args. Change-Id: I23f39ff4807e3fa71f3220912aec3c99db6b9454 commit 83604854c5160304cafefc9bd40a72c5ee8506eb Merge: 7db3524 1ffe43e Author: Zoltan Szatmary-Ban Date: Thu Jul 31 13:28:57 2014 +0000 am 1ffe43e8: Merge ""Add possibility to get deleted system Certificate Aliases"" into lmp-dev * commit 1ffe43e8277e883c6663c1fb7cfc5e18ba552c40: Add possibility to get deleted system Certificate Aliases commit 7db3524880092126962b7f502af76b4c84da7350 Merge: 5767d63 ad0cd83 Author: Prameet Shah Date: Wed Jul 30 17:04:13 2014 +0000 am ad0cd830: Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() * commit ad0cd83024f38011043d28d70370a8638b88cd72: Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() commit 5767d63d22e87becab387b3bd6597fe41eb34d7e Merge: b389e17 26163c2 Author: Prameet Shah Date: Wed Jul 30 16:31:08 2014 +0000 am 26163c26: Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() * commit 26163c268a6d2625384b87e907afad8ef19f9a47: Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() commit 26163c268a6d2625384b87e907afad8ef19f9a47 Author: Prameet Shah Date: Tue Jul 29 16:45:31 2014 Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus() Bug: Change-Id: I5bcaf3ee8910ff75e785baed4c4604fee6c5e700 commit b389e1779651f2c58454a5f98acebd3dd7bc0061 Merge: 5f03b4d e427972 Author: Prameet Shah Date: Thu Jul 24 19:46:28 2014 +0000 am e427972e: OpenSSLEngineImpl: fix unwrap behavior with array * commit e427972eb6141cd67e6d4c9607863a8d990e6be6: OpenSSLEngineImpl: fix unwrap behavior with array commit 5f03b4d63c7632581b032879de791dc82f05ffa0 Merge: 3d935ee 41eb5b6 Author: Prameet Shah Date: Tue Jul 22 19:26:41 2014 +0000 am 41eb5b65: OpenSSLEngineImpl: fix unwrap behavior with array * commit 41eb5b65e524d01e28da474bd37e4349b12fb494: OpenSSLEngineImpl: fix unwrap behavior with array commit 41eb5b65e524d01e28da474bd37e4349b12fb494 Author: Prameet Shah Date: Tue Jul 22 11:50:18 2014 OpenSSLEngineImpl: fix unwrap behavior with array The decrypted bytes should written sequentially into each buffer of the destination array until its full before moving to the next buffer. Change-Id: I2454249c167deafde6c12134d3c8cd658cd7c21b commit 3d935eeca25e00b56cfd8d37a657c7b2986889b3 Merge: 0a36f6c affd45a Author: Alex Klyubin Date: Fri Jul 18 00:32:14 2014 +0000 am affd45a4: Merge ""Improve the Javadoc of PSKKeyManager."" into lmp-dev * commit affd45a413cf844dad797ad4972074efb9de43d8: Improve the Javadoc of PSKKeyManager. commit 0a36f6c1f8b2e195c2dd5aea1a386df090c6d470 Merge: 6492180 af4fa68 Author: rich cannings Date: Thu Jul 17 23:47:33 2014 +0000 am af4fa685: Merge ""Log CCS exceptions do not merge."" into lmp-dev * commit af4fa685f246aaa80c93af62faadbc2fe87dc034: Log CCS exceptions do not merge. commit 6492180ce17a3b5ff822cff1783f00e7a4176491 Merge: aac4168 3b7268c Author: Alex Klyubin Date: Thu Jul 17 18:27:39 2014 +0000 am 3b7268cd: Merge ""Improve the Javadoc of PSKKeyManager."" * commit 3b7268cde4a4fc59591da8a93691927ebf3add57: Improve the Javadoc of PSKKeyManager. commit aac4168d8baef7e12d6fa959c6d6ded9892e9651 Merge: 8573ad0 a749c0d Author: Kenny Root Date: Thu Jul 17 17:07:05 2014 +0000 am a749c0d3: Keep enough state to completely reset cipher instances * commit a749c0d351216be38879600ee8ed01c6793aa256: Keep enough state to completely reset cipher instances commit 8573ad0ddcf7e2f8b2e5ac84c34b7ffab303155c Merge: 4ca5b06 70fdb6d Author: Koushik Dutta Date: Thu Jul 17 17:06:36 2014 +0000 am 70fdb6d2: OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. * commit 70fdb6d2bfa0c313fe389827f0025288f6aeb947: OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. commit 4ca5b0625e3f5a15ae8adf833ab5a69f9d7d517f Merge: 119abfb ded66f5 Author: Koushik Dutta Date: Thu Jul 17 17:06:35 2014 +0000 am ded66f5f: Various fixes in OpenSSLEngineImpl. * commit ded66f5f696994ce7620552e16a4e9124e69e052: Various fixes in OpenSSLEngineImpl. commit 119abfba1fcd9c9cfbd15d0a4ca9ed2188fdfab0 Merge: 5713cdf cbe1f28 Author: Kenny Root Date: Thu Jul 17 15:56:57 2014 +0000 am cbe1f28a: Merge ""Keep enough state to completely reset cipher instances"" * commit cbe1f28adf64396561a3b65bf1452dfa9b6e35ae: Keep enough state to completely reset cipher instances commit cbe1f28adf64396561a3b65bf1452dfa9b6e35ae Merge: e08f238 084e308 Author: Kenny Root Date: Thu Jul 17 15:48:58 2014 +0000 Merge ""Keep enough state to completely reset cipher instances"" commit 3b7268cde4a4fc59591da8a93691927ebf3add57 Merge: cbe1f28 7ac13e0 Author: Alex Klyubin Date: Thu Jul 17 18:20:43 2014 +0000 Merge ""Improve the Javadoc of PSKKeyManager."" commit 5713cdf71c5c6e5179e8369263c702e9512afdd0 Merge: cf55719 e08f238 Author: Koushik Dutta Date: Wed Jul 16 22:05:17 2014 +0000 am e08f2385: OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. * commit e08f238580e8ee471012bef8240c8d3397c7b780: OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. commit cf557195a9b60d7f51a48500afde38481ddbc91c Merge: cbbd7d1 986aeb7 Author: Kenny Root Date: Wed Jul 16 21:41:12 2014 +0000 am 986aeb78: Merge ""Various fixes in OpenSSLEngineImpl."" * commit 986aeb78e533540463daf1753e24840f75b25ce6: Various fixes in OpenSSLEngineImpl. commit e08f238580e8ee471012bef8240c8d3397c7b780 Author: Koushik Dutta Date: Tue Jul 15 22:40:23 2014 OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN. Change-Id: Idc78204b7077fb367b64e1867c807cd39f596f98 commit 7ac13e03a79d0c99d181b1a28b1b3699ba3d5739 Author: Alex Klyubin Date: Wed Jul 16 08:33:02 2014 Improve the Javadoc of PSKKeyManager. This clarifies several points and adds sample code. Bug: 15073623 Change-Id: I6e8aadc52277e238a998d6cee36795dab1151d58 commit 986aeb78e533540463daf1753e24840f75b25ce6 Merge: 8f9ac1a bdfcc18 Author: Kenny Root Date: Wed Jul 16 21:15:30 2014 +0000 Merge ""Various fixes in OpenSSLEngineImpl."" commit bdfcc189efe41a3f812aeb55ea634bace67d159a Author: Koushik Dutta Date: Sat Jun 28 19:19:21 2014 Various fixes in OpenSSLEngineImpl. Fix ""Buffers were not large enough"" exception by directly using the destination buffers. Corrections around bytesProduced and bytesConsumed behavior. Return BUFFER_OVERFLOW if a zero length destination is provided to unwrap. Change-Id: I1f1e9b72cd6968ed4f3c3c0edccbccebc33d6790 commit cbbd7d10e8e484c44a78e5b27e8fecda195f1692 Merge: ec7f8e6 fdb7d8c Author: Alex Klyubin Date: Tue Jul 15 18:49:14 2014 +0000 am fdb7d8c5: Enable PSK cipher suites when PSKKeyManager is provided. * commit fdb7d8c53dabac5551e2499d045ba6829bcfc0a0: Enable PSK cipher suites when PSKKeyManager is provided. commit ec7f8e6b27330160f88540f4f2ace7bc2a0720a3 Merge: 5b8ccf1 8f9ac1a Author: Alex Klyubin Date: Tue Jul 15 15:53:46 2014 +0000 am 8f9ac1af: Enable PSK cipher suites when PSKKeyManager is provided. * commit 8f9ac1af0cbdf00e5e47aee32c132522ebc3bd17: Enable PSK cipher suites when PSKKeyManager is provided. commit 5b8ccf1b09df6f35c1709bfc8fd727a291094a5b Merge: 69a2e46 6e2315f Author: Ed Heyl Date: Tue Jul 15 13:34:25 2014 +0000 am 6e2315fd: reconcile aosp (e79c25bf33e10da41e489c537823f678e1a1169c) after branching. Please do not merge. * commit 6e2315fd96c3c4a47450c1a437babacc94bc31a6: reconcile aosp (e79c25bf33e10da41e489c537823f678e1a1169c) after branching. Please do not merge. commit 084e3086be1d7a6b9280b64c7c8cdb7b41a13bea Author: Kenny Root Date: Mon Jul 14 13:25:32 2014 Keep enough state to completely reset cipher instances OpenSSLs RC4 mutates the given key. AES/CTR mutates the IV. We must store these values locally to enable ""doFinal"" to cause the Cipher instance to be reset to what it was right after ""init"". Note that resetting and encrypting with the same key or IV breaks semantic security. Bug: 16298401 Bug: Change-Id: Ie7e4dcb6cf6cc33ddad31d6b47066dc1b34e6894 commit 69a2e460cc0a40e1b951e400589b9932609079ec Merge: 8b7bb32 bca895f Author: David Benjamin Date: Mon Jul 14 18:17:28 2014 +0000 am bca895f8: Pass output buffer length into EVP_DigestSignFinal. * commit bca895f809dd2cef7a0834f0bfeb2a06e42b277d: Pass output buffer length into EVP_DigestSignFinal. commit 8b7bb32af09a01e80442b70dd23e6997a937f103 Merge: a2404c9 e79c25b Author: Kenny Root Date: Mon Jul 14 18:17:28 2014 +0000 am e79c25bf: Merge ""DHKeyPairGenerator: use provided params"" * commit e79c25bf33e10da41e489c537823f678e1a1169c: DHKeyPairGenerator: use provided params commit 8f9ac1af0cbdf00e5e47aee32c132522ebc3bd17 Author: Alex Klyubin Date: Thu Jun 19 13:37:24 2014 Enable PSK cipher suites when PSKKeyManager is provided. This enables TLS-PSK cipher suites by default iff SSLContext is initialized with a PSKKeyManager. For consistency, X.509 based cipher suites are no longer enabled by default at all times they are now only enabled by default iff SSLContext is initialized with a X509KeyManager or a X509TrustManager. When both X.509 and PSK cipher suites need to be enabled, PSK cipher suites are given higher priority in the resulting list of cipher suites. This is based on the assumption that in most cases users of TLS/SSL who enable TLS-PSK would prefer TLS-PSK to be used when the peer supports TLS-PSK. Bug: 15073623 Change-Id: I8e2bc3e7a1ea8a986e468973b6bad19dc6b7bc3c commit bca895f809dd2cef7a0834f0bfeb2a06e42b277d Author: David Benjamin Date: Thu Jul 10 18:12:08 2014 Pass output buffer length into EVP_DigestSignFinal. EVP_DigestSignFinal expects the input buffer length as *siglen on input. In addition, if sigret is NULL, it returns the buffer size needed. Use this rather than making assumptions about the EVP_PKEY used to initialize the EVP_MD_CTX. commit e79c25bf33e10da41e489c537823f678e1a1169c Merge: a328492 9b226f9 Author: Kenny Root Date: Fri Jul 11 16:46:23 2014 +0000 Merge ""DHKeyPairGenerator: use provided params"" commit 9b226f90a992a4a2267b7a813e3b869851945c4d Author: Kenny Root Date: Thu Jul 10 14:50:48 2014 DHKeyPairGenerator: use provided params If the prime is provided in the DHParameterSpec, then use it to generate the key. Bug: 16188130 Change-Id: I42de02c71a58d691ef7ba6e2252367105687b758 Bug: 18388980 Change-Id: I853b02a32db113a5af3f6166e7d61fab58c3ff73/"
78,78,15.0,0.6861000061035156,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","external/conscrypt: add NativeConstants. NativeConstants.java is generated by a C program and thus the values will automatically be kept in sync with the contents of the OpenSSL headers. Bug: 20521989 Change-Id: Ib5a97bf6ace05988e3eef4a9c8e02d0f707d46ad/conscrypt: throw exception for null references in NativeCrypto Adapted tests to use ""null"" instead of an Object with a null context, as null contexts are now rejected by constructors. bug: 19657430 Change-Id: I47ebfde7170e1818afd64a75a8e4bc1e1d588aea/"
79,79,15.0,0.4733000099658966,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","external/conscrypt: fix WITH_JNI_TRACE in light of BoringSSL update. These values in BoringSSL are now uint32_ts, which upsets the compiler when printing them as longs. This change casts the values to longs so that it continues to work with OpenSSL. Change-Id: I35af51d765d67b3c8c30e55b80eac24dda420a88/OpenSSLCipher: add AEAD cipher This allows us to provide an implementation of AES-GCM using the new EVP_AEAD interface in BoringSSL. It simply buffers up the input until doFinal(...) is called which makes it much safer than any streaming interfaces, because the caller cant use the plaintext until its authenticated by the GHASH (or whatever other AEAD you happen to use). Bug: 20636336 Change-Id: I6e4b063a8137a16102b1f6ac15687a38ddfe1691/external/conscrypt: ask OpenSSL for supported cipher suites. Rather than enumerate the list of supported cipher suites in conscrypt, ask OpenSSL for the list and just maintain a mapping from OpenSSLs names to the standard, external name. (The mapping could also be removed with BoringSSL since it can return the standard name for an SSL_CIPHER*. But in order to keep OpenSSL compat this change doesnt depend on that.) Bug: 20531880 Change-Id: Ib541c9787093e7b900052fdf12dd2a2029b4b020/RI: AttachCurrentThread has different type Change-Id: Ia74b5ecb1af69010d51f963f4f757339deda8b9b/RI: cast to char* for JNI registration The RI has a different type that causing compilation errors if you dont do this cast. Change-Id: I5961d79c88bef6cba2dc0de9c81e310005e4712c/NativeCrypto: not finding a key is not fatal If we dont find a key in the keystore, we should just return null reference. The only time we should throw exceptions is when the key decoding failed or something else like that. Bug: 20488918 (cherry picked from commit 8098cbbc7fbf2d22402da487465a153734f9f9b6) Change-Id: I621b39257bc98d888f7ad390fb8648326c67dfc4/NativeCrypto: not finding a key is not fatal If we dont find a key in the keystore, we should just return null reference. The only time we should throw exceptions is when the key decoding failed or something else like that. Bug: 20488918 Change-Id: I85408615a9c7a63242178908f309f93a2972033c/NativeCrypto: do not discard pending exceptions The switch to native reference objects left some duplicate NullPointerException creation that led to some JNI warnings. Simply get rid of the redundant NullPointerException throws. Bug: 19657430 Change-Id: I7e6bcb74154078cf019bfdea5d2721f6e6cb8524/external/conscrypt: recognise des-ede-cbc as an alias for des-cbc. Bug: 20518919 Change-Id: I2b697529420a5c3fd9f96887a11977d261b3d1aa/OpenSSLEngine: do not try to load ENGINE for BoringSSL Since BoringSSL doesnt use ENGINE instances, we should not fail when the native code returns the equivalent of a NULL instance. This change propagates the knowledge of whether were using BoringSSL or OpenSSL up to the Java layer. Change-Id: Ib8c2224a909564ae6f0c6d5984020c44517f6c29/Add fallthrough intention markers To help identify accidental fallthroughs, clang has an option to warn when one is detected. Add the macro to make it compatible with earlier versions of Clang or GCC. Change-Id: I48add3e3e9c0cbfe9b6d812d3336062a4d971909/BoringSSL PKCS#7 PEM and CRL support. Based on recent additions to BoringSSL itself, this change adds PKCS#7 PEM and CRL support for conscrypt with BoringSSL. Change-Id: Icef9d017dce54c3070b605a70773c60bb1b8cfa2/Add back d2i_PKCS7_bio and PEM_read_bio_PKCS7. For the moment, the BoringSSL version is going to be broken until I get the needed changes into BoringSSL to support this. Change-Id: Id2c3f179c6f9fc4f4385d2274884e69530fabff0/"
80,80,3.0,0.9107000231742859,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","external/conscrypt: support arbitrary ECC groups. The Java provider mechanism doesnt really let us fallback to another provider based on whether certain ECC groups are supported or not. Since I expect that some people will be trying to do Bitcoin on Android, this should keep them happy. Change-Id: I1db48b104e12a6e7dae21df9c31c21bff0d62a9b/OpenSSLCipher: add AEAD cipher This allows us to provide an implementation of AES-GCM using the new EVP_AEAD interface in BoringSSL. It simply buffers up the input until doFinal(...) is called which makes it much safer than any streaming interfaces, because the caller cant use the plaintext until its authenticated by the GHASH (or whatever other AEAD you happen to use). Bug: 20636336 Change-Id: I6e4b063a8137a16102b1f6ac15687a38ddfe1691/Split up JNI library initialization Different platforms require vastly different ways of loading the JNI glue library, so split the loading job into different directories so they can be more easily compiled. Change-Id: I963c2e0d4667cbb655a0788f161eae74d7a2f037/external/conscrypt: ask OpenSSL for supported cipher suites. Rather than enumerate the list of supported cipher suites in conscrypt, ask OpenSSL for the list and just maintain a mapping from OpenSSLs names to the standard, external name. (The mapping could also be removed with BoringSSL since it can return the standard name for an SSL_CIPHER*. But in order to keep OpenSSL compat this change doesnt depend on that.) Bug: 20531880 Change-Id: Ib541c9787093e7b900052fdf12dd2a2029b4b020/external/conscrypt: switch NativeCrypto itself to use NativeConstants. Now that other users of the constants in NativeCrypto have been switched over, those constants can be removed. Bug: 20521989 Change-Id: I276a1c8daeb3501b6924ff68cf9f1e9f6fbd63a9/external/conscrypt: add SSL_CIPHER_get_kx_name This will be used by a future change to avoid needing to know the OpenSSL-internal SSL_aRSA (etc) constants. Bug: 20521989 Change-Id: I99d83005530f81956d102426fe28beeaed058cea/OpenSSLEngine: do not try to load ENGINE for BoringSSL Since BoringSSL doesnt use ENGINE instances, we should not fail when the native code returns the equivalent of a NULL instance. This change propagates the knowledge of whether were using BoringSSL or OpenSSL up to the Java layer. Change-Id: Ib8c2224a909564ae6f0c6d5984020c44517f6c29/Enable any opaque private keys to be used with TLS/SSL stack. Prior to this CL, opaque private keys those that do not expose/export their key material were not supported by Conscrypts SSLSocket, SSLServerSocket and SSLEngine implementations if the keys were backed by other providers. This CL fixes this issue. Conscrypts TLS/SSL stack now works with arbitrary opaque private keys provided that: * for EC private key: an installed implementation of NONEwithECDSA Signature accepts the key for signing; and * for RSA private key: an installed implementation of NONEwithRSA Signature accepts the key for signing and an installed implementation of RSA/ECB/PKCS1Padding Cipher accepts the key for decryption. This normally requires that the JCA Provider which produced the PrivateKey instance expose the above Cipher transformation and Signature algorithms. HOW THIS WORKS The underlying OpenSSL TLS/SSL stack uses the provided private keys only to decrypt and sign. For opaque private keys these requests are delegated (same as before, via CryptoUpcalls) to corresponding Cipher (RSA/ECB/PKCS1Padding) and Signature (NONEwithRSA or NONEwithECDSA) implementations. Even when signing and decryption is outsourced, OpenSSL still needs the modulus (for RSA) and order (for EC), supposedly to estimate output size of signing or decryption operations. This information is not available via the PrivateKey interface. However, an opaque private key may still implement the RSAKey or ECKey interface which provides access to modulus or order but does not provide access to key material. Moreover, in all use cases of private keys with Conscrypts TLS/SSL stack the modulus or order can be obtained and provided to OpenSSL. In the case of private keys used for client or server authentication, the public key of the certificate is used as the source of the information. In the case of TLS Channel ID, the order is currently fixed and known (only NIST P-256 is supported). Bug: 19284418 Change-Id: I8fea2492f9cf48cfc29c3e7d2ee99a68e84e82ec/"
81,81,13.0,0.5514000058174133,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","OpenSSLCipher: add AEAD cipher This allows us to provide an implementation of AES-GCM using the new EVP_AEAD interface in BoringSSL. It simply buffers up the input until doFinal(...) is called which makes it much safer than any streaming interfaces, because the caller cant use the plaintext until its authenticated by the GHASH (or whatever other AEAD you happen to use). Bug: 20636336 Change-Id: I6e4b063a8137a16102b1f6ac15687a38ddfe1691/OpenSSLCipher: refactor in preparation for AEAD BoringSSL uses a different interface for AEAD that is much simplier called EVP_AEAD. Separate out the EVP_CIPHER usage so that we can have another subclass with the EVP_AEAD usage. Bug: 20636336 Change-Id: I661d92bd449f2fcc3c4a6e511155490917ecef0c/OpenSSLCipher: exception when IV not specified If youre decrypting with a mode that requires an IV, init should throw an exception indicating as much. Add the checks to make sure this happens. Bug: 19201819 Change-Id: I2d3481da4f63bffb340dc1197f6b5cb29360fbff/"
82,82,3.0,0.920799970626831,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",external/conscrypt: add NativeConstants. NativeConstants.java is generated by a C program and thus the values will automatically be kept in sync with the contents of the OpenSSL headers. Bug: 20521989 Change-Id: Ib5a97bf6ace05988e3eef4a9c8e02d0f707d46ad/
83,83,3.0,0.9939000010490417,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Rename Arrays to ArrayUtils To avoid conflict with the java.util.Arrays class, rename our own internal compatibility class to ArrayUtils. Change-Id: Iae79a4d37749e16e62712f3bb5038d870b78d999/external/conscrypt: add NativeConstants. NativeConstants.java is generated by a C program and thus the values will automatically be kept in sync with the contents of the OpenSSL headers. Bug: 20521989 Change-Id: Ib5a97bf6ace05988e3eef4a9c8e02d0f707d46ad/Enable any opaque private keys to be used with TLS/SSL stack. Prior to this CL, opaque private keys those that do not expose/export their key material were not supported by Conscrypts SSLSocket, SSLServerSocket and SSLEngine implementations if the keys were backed by other providers. This CL fixes this issue. Conscrypts TLS/SSL stack now works with arbitrary opaque private keys provided that: * for EC private key: an installed implementation of NONEwithECDSA Signature accepts the key for signing; and * for RSA private key: an installed implementation of NONEwithRSA Signature accepts the key for signing and an installed implementation of RSA/ECB/PKCS1Padding Cipher accepts the key for decryption. This normally requires that the JCA Provider which produced the PrivateKey instance expose the above Cipher transformation and Signature algorithms. HOW THIS WORKS The underlying OpenSSL TLS/SSL stack uses the provided private keys only to decrypt and sign. For opaque private keys these requests are delegated (same as before, via CryptoUpcalls) to corresponding Cipher (RSA/ECB/PKCS1Padding) and Signature (NONEwithRSA or NONEwithECDSA) implementations. Even when signing and decryption is outsourced, OpenSSL still needs the modulus (for RSA) and order (for EC), supposedly to estimate output size of signing or decryption operations. This information is not available via the PrivateKey interface. However, an opaque private key may still implement the RSAKey or ECKey interface which provides access to modulus or order but does not provide access to key material. Moreover, in all use cases of private keys with Conscrypts TLS/SSL stack the modulus or order can be obtained and provided to OpenSSL. In the case of private keys used for client or server authentication, the public key of the certificate is used as the source of the information. In the case of TLS Channel ID, the order is currently fixed and known (only NIST P-256 is supported). Bug: 19284418 Change-Id: I8fea2492f9cf48cfc29c3e7d2ee99a68e84e82ec/"
84,84,8.0,0.920799970626831,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","GCM: return the correct AlgorithmParameters Instead of the correct AlgorithmParameters of type ""GCM,"" we were returning the generic ""AES"" version that basically only converts to an IvParameterSpec. Bug: 22319986 Change-Id: Ib42905c3ad31e44b72e8066192bd26981c8351ba/"
85,85,15.0,0.8123000264167786,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Switch OpenSSLMac from EVP_PKEY_HMAC to HMAC_CTX. EVP_PKEY_HMAC is just a wrapper over HMAC_CTX, so this is slightly more efficient. This is also the last consumer of BoringSSLs EVP_PKEY_HMAC, so the API may be removed after this. Change-Id: I545914b429b23631efd3cacaa22c6d2e7d165fab/Use SSL_CTX_set_tmp_ecdh instead of SSL_CTX_set_tmp_ecdh_callback. Conscrypt is the only consumer of SSL_CTX_set_tmp_ecdh_callback for BoringSSL. The callback variant is also bizarre. The key length parameter is legacy and pointless. When used with SSL_OP_SINGLE_ECDH (which BoringSSL always enables), theres no point in configuring the callback over a static group. The callback also does not participate in supported_curves negotiation. Change-Id: Ie588532a559f13d2b69b7278f9b8d4d41e31828d/Consistently use ARRAY_OFFSET_*INVALID macros. Not all the ad-hoc ones check for integer overflow correctly. Consistently use the same check everywhere. Change-Id: I913b7de792406d9819a6830cc21ec500ddceff6e/Fix error conditions in certificate/PKCS#7 reading When an error condition is encountered in BoringSSL, sometimes it deliberately does not put something on the ERR stack to prevent abuse of that knowledge. Instead we need to throw an exception explicitly when no error is pushed onto the stack. Bug: 21034231 Change-Id: Ia06347c5653672c982ecff2c26be9b091d03009f/Fix up JNI_TRACE for AEAD Bug: 21762837 Change-Id: I11042be8fe1e046ac96759b4554ce9229e1cf6f3/NativeCrypto: special case for empty cipher list For the Java language, setting an empty cipher list is not an error but its an error in OpenSSL. However, the underlying API actually updates the cipher list to an empty string as intended. So we need to handle this special case by clearing the error stack and making sure that our expectation is satisfied. Bug: 21195269 Change-Id: Id21792215513f4e0d6e051160f69e5f830d39015/external/conscrypt: tweaks for next BoringSSL import. Upstream BoringSSL has dropped |SSL_ST_BEFORE| (which appears to have been unused) and all the |*_LOCK_*| symbols. The latter are replaced with |*_up_ref|, with so that it continues to work with OpenSSL. Change-Id: Ib609c83d428b7624e24e3b96c93afc2e482e6a6d/NativeCrypto: return of 0 is error for EVP_Sign/VerifyFinal We need to check the ERR stack on a return code of 0. Previously there was a comment indicating the weird behavior about DSA keys throwing after a check for a return value of but this API is never supposed to return anything other than 1 for success or 0 for failure. (cherry picked from commit 49854878b83114e3e15c7ad3ca030352b786b5df) Bug: 18869265 Change-Id: Ic871c63b6d65949053819950ed8053f47501bd60/NativeCrypto: return of 0 is error for EVP_Sign/VerifyFinal We need to check the ERR stack on a return code of 0. Previously there was a comment indicating the weird behavior about DSA keys throwing after a check for a return value of but this API is never supposed to return anything other than 1 for success or 0 for failure. Bug: 18869265 Change-Id: Ic871c63b6d65949053819950ed8053f47501bd60/"
86,86,1.0,0.9810000061988831,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","GCM: set default tag size to 12 bytes According to RFC 5084, the default value of the GCM tag should be 12 octets (bytes). Change the default tag length from 0 to 12 to honor this. Bug: 22855843 Change-Id: I1ed16df24d0cfa9fff2593a3402c97faf913e05e/GCM: return the correct AlgorithmParameters Instead of the correct AlgorithmParameters of type ""GCM,"" we were returning the generic ""AES"" version that basically only converts to an IvParameterSpec. Bug: 22319986 Change-Id: Ib42905c3ad31e44b72e8066192bd26981c8351ba/OpenSSLCipher: adjust expected length with padding in decrypt mode Consider the |final| buffer when computing the expected length Should not expect an extra block when using padding in decrypting mode Bug: 19186852 Change-Id: I206442d45c4cf68363201738ba9d0b035f19c436/Revert ""OpenSSLCipher: adjust expected length with padding in decrypt mode"" This reverts commit eb3a7e31c78231a19cb76ce2a5974b03a0187b96. Change-Id: I822a51cfb7c8a2a3785d8694f5ab9f9fef552111/"
87,87,4.0,0.9524999856948853,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",NativeCrypto: Add TLS SCT extension support. Change-Id: I438b23ecb86340f837f62359b342637966b81512/NativeCrypto: support OCSP stapling This only provides access to the OCSP data. It does not use it to verify the certificate. Change-Id: Ib448cbf52a5c824655585afc62d1580404a44f2c/NativeCrypto: add method to extract extensions from an OCSP response. This will be useful to extract signed timestamps to perform Certificate Transparency verification. Change-Id: I44db4435ce47d9c5562323c18d475be24b00bca7/
88,88,3.0,0.9472000002861023,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Move BlockGuard and CloseGuard to Platform This was causing issues on Gingerbread devices since CloseGuard was not in that release yet. Move them out to Platform so we can filter on release when we decide whether to instantiate or not. Bug: 24607028 Change-Id: Iba0bbb0b878076319ace40f848aa5e307e2c3ad8/
89,89,1.0,0.5246999859809875,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Clear BoringSSL error queue in NativeCrypto.EVP_DigestVerifyFinal. This fixes a bug introduced in b4345a619c1f34e2390210d11476a8619cebd695 where NativeCrypto.EVP_DigestVerifyFinal left the BAD_SIGNATURE error in the BoringSSL error queue when a signature did not verify. Some of the following NativeCrypto operations would then fail because they assumed that it was their BoringSSL calls that generated the BAD_SIGNATURE error. The fix is to unconditionally clear the BoringSSL error queue at the end of NativeCrypto.EVP_DigestVerifyFinal, same as its predecessor NativeCrypto.EVP_VerifyFinal did. Change-Id: I0d092b1b39afa3c6d19a785cbf7dd311ffcd4c04/Switch from EVP_[Sign|Verify] to EVP_Digest[Sign|Verify]. This switches Conscrypts Signature implementations from the older EVP_Sign/EVP_Verify API to the newer EVP_DigestSign/EVP_DigestVerify API. The main factor driving this switch is to expose RSASSA-PSS which does not work via the old API. In particular, this change: * adds EVP_DigestSign* and EVP_DigestVerify* to NativeCrypto. Some of these NativeCrypto functions were already there but werent used. This made it easier to adjust their signatures to best results. * switches Signature implementation from EVP_Sign/EVP_Verify to EVP_DigestSign/EVP_DigestVerify. * removes EVP_Sign* and EVP_Verify* from NativeCrypto because they are no longer used. * inlines NativeCryptos evpInit into its EVP_DigestInit_ex because the latter became the only user of evpInit after the cleanup. Change-Id: Id29ea4fc2bc5b1cd81daaee8b475fd147616de51/Adjust names of digest-related NativeCrypto methods. This adjusts the names of digest-related NativeCrypto methods to match the names of underlying BoringSSL functions. This makes it easier to reason about the functionality being invoked via NativeCrypto. Change-Id: I04e2148ba818ae3e9ad60871b046052fcfffec4d/Zero-copy digesting for direct ByteBuffer input. Prior to this change, Conscrypts MessageDigest.update(ByteBuffer) invoked for a direct ByteBuffer resulted in the creation of a new byte[] of size ByteBuffer.remaining() and the copying of the ByteBuffers contents into that array. This change implements an optimization which avoids the allocation and copying, by making BoringSSL EVP_DigestUpdate read directly from the memory region represented by the direct ByteBuffer. Change-Id: I112d318128402d1d78e226df9dfe54af55955953/NativeCrypto: Add TLS SCT extension support. Change-Id: I438b23ecb86340f837f62359b342637966b81512/NativeCrypto: support OCSP stapling This only provides access to the OCSP data. It does not use it to verify the certificate. Change-Id: Ib448cbf52a5c824655585afc62d1580404a44f2c/NativeCrypto: add method to extract extensions from an OCSP response. This will be useful to extract signed timestamps to perform Certificate Transparency verification. Change-Id: I44db4435ce47d9c5562323c18d475be24b00bca7/Add method to delete extension from a certificate The OpenSSLX509Certificate is still immutable. Instead a modified copy is returned. The use case for this is recreating the TBS component of a Precertificate as described by RFC6962 section 3.2. Change-Id: I2a9305ae7464642910decaf5ab46121a6f15d722/Remove references to OpenSSLs |wbuf|. The |wbuf| member is an internal field that disappears in the latest BoringSSL revision. Also, it doesnt appear to be neccessary: SSL_write wont report that bytes were written until the record has hit the transport, so theres no need to be sensitive to an implementation detail. (See also cl/100529082.) Change-Id: I036bb7ebf69649025967a2af467313d7676e62ca/"
90,90,1.0,0.98580002784729,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Switch from EVP_[Sign|Verify] to EVP_Digest[Sign|Verify]. This switches Conscrypts Signature implementations from the older EVP_Sign/EVP_Verify API to the newer EVP_DigestSign/EVP_DigestVerify API. The main factor driving this switch is to expose RSASSA-PSS which does not work via the old API. In particular, this change: * adds EVP_DigestSign* and EVP_DigestVerify* to NativeCrypto. Some of these NativeCrypto functions were already there but werent used. This made it easier to adjust their signatures to best results. * switches Signature implementation from EVP_Sign/EVP_Verify to EVP_DigestSign/EVP_DigestVerify. * removes EVP_Sign* and EVP_Verify* from NativeCrypto because they are no longer used. * inlines NativeCryptos evpInit into its EVP_DigestInit_ex because the latter became the only user of evpInit after the cleanup. Change-Id: Id29ea4fc2bc5b1cd81daaee8b475fd147616de51/Zero-copy HMAC and signing/verification for direct ByteBuffer. Prior to this change, Conscrypts Mac and Signature implementations copied the contents of direct ByteBuffer inputs. This change implements an optimization which avoids the allocation and copying of contents of direct ByteBuffer inputs. Bug: 24674857 Change-Id: I1436839182483fd42318d4b0af4d633283e3453d/"
91,91,3.0,0.970300018787384,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Verify certificate transparency on domain for which it is enabled. For the time being, CT is enabled by using Java security properties. This makes it possible to deploy it without updating Androids frameworks through GMSCore. Change-Id: Iff9646b7d80f1386965b2b4f4f46a9d80c780a58/Move BlockGuard and CloseGuard to Platform This was causing issues on Gingerbread devices since CloseGuard was not in that release yet. Move them out to Platform so we can filter on release when we decide whether to instantiate or not. Bug: 24607028 Change-Id: Iba0bbb0b878076319ace40f848aa5e307e2c3ad8/"
92,92,10.0,0.9648000001907349,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add ExtendedSSLSession, et al. In order to support SNI certificate selection of the server-side and enhanced certificate verification on the client side, we add ExtendedSSLSession and the getHandshakeSession support. This is just to set up for future implementations of SNI and ExtendedX509TrustManager and doesnt actually implement the logic needed to fully support the new features. Change-Id: I300d3134d8ab9c184d6473183612dc53658a8221/Fix for OpenJdk SocketImpl. OpenJdk sockets start their life with a null FileDescriptor. b/25805791 tracks fixing the SocketImpl Change-Id: Ia14afda04aa0a109f944c549719ad50bb3aeadab/"
93,93,2.0,0.9366000294685364,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Remove java.lang.IntegralToString usage. java.lang.IntegralToString is going away, replaced its usage by small helper class, Hex. + Fixes the ""Illegal class access"" exception from TrustedCertificateStoreTest & TrustManagerImplTest. (cherry-picked from 61e984f441b9194f0ae907e6fc28502858df6852 + 61e984f441b9194f0ae907e6fc28502858df6852) Bug: 24932279 (cherry picked from commit e279a9854d15d20a0b3807fe96f0805b43cd4dae) Change-Id: Id48cd9c2dfade328f01c669afa20fe2e7a630fc2/"
94,94,15.0,0.9546999931335449,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Track False Start change in tests In BoringSSL, the SSL_MODE_ENABLE_FALSE_START (aka SSL_MODE_HANDSHAKE_CUTTHROUGH) is unconditionally enabled because BoringSSL does the appropriate checks internally. Make sure our tests also reflect this fact by testing the appropriate settings. Bug: 26139262 Bug: 26139500 Change-Id: I125aa440cdb76d2efbfee2be7387b47d22446950/"
95,95,9.0,0.9269000291824341,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Add handshake session and ExtendedX509TrustManager support This enables the new API to specify when a host should be verified by hostname. Before there was no public API that was capable of indicating to the TrustManager which DNS hostname you were intending to connect with. Change-Id: Ic5845d1e93f02b54d971673a280d0a3571739fbf/
96,96,10.0,0.9648000001907349,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add ExtendedSSLSession, et al. In order to support SNI certificate selection of the server-side and enhanced certificate verification on the client side, we add ExtendedSSLSession and the getHandshakeSession support. This is just to set up for future implementations of SNI and ExtendedX509TrustManager and doesnt actually implement the logic needed to fully support the new features. Change-Id: I300d3134d8ab9c184d6473183612dc53658a8221/Fix for OpenJdk SocketImpl. OpenJdk sockets start their life with a null FileDescriptor. b/25805791 tracks fixing the SocketImpl Change-Id: Ia14afda04aa0a109f944c549719ad50bb3aeadab/"
97,97,1.0,0.9711999893188477,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Fix compilation with JNI_TRACE_** Change-Id: I8417daea4b10f8c02642fe6c9be170312461139c/Use some C++11 concepts Run clang-modernizer over the native code and clang-format for the changed lines. Change-Id: I02211de90214567a128c4e3ca88aad26541a7629/Basic implementation of RSASSA-PSS Signature. This makes Conscrypt provide RSASSA-PSS Signature implementations. These implementations currently do not support changing their parameters (e.g., via Signature.setParameter(PSSParameterSpec)) and returning their current parameters (e.g., via Signature.getParameters()). This will be added in a follow-up change. Bug: 25794302 Change-Id: I1488e0e9592f92a9e15365131c76ce2902ad4607/"
98,98,0.0,0.6632999777793884,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Support for PSS Signature configuration via PSSParameterSpec. This adds support for configuring the PSS Signature implementation via java.security.spec.PSSParameterSpec. This also makes the signature implementation return its current configuration as AlgorithmParameters of algorithm ""PSS"" from which a PSSParameterSpec can be obtained. Bug: 25794302 Change-Id: Ib7e087cdc75a6b02898afafdfc4308802d6eb5d5/Basic implementation of RSASSA-PSS Signature. This makes Conscrypt provide RSASSA-PSS Signature implementations. These implementations currently do not support changing their parameters (e.g., via Signature.setParameter(PSSParameterSpec)) and returning their current parameters (e.g., via Signature.getParameters()). This will be added in a follow-up change. Bug: 25794302 Change-Id: I1488e0e9592f92a9e15365131c76ce2902ad4607/"
99,99,1.0,0.9743000268936157,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Add support for SNI API This adds support for retrieving SNI name as a server and setting SNI name as a client. It currently doesnt implement use of the SNIMatcher API. Change-Id: I4f76fcbd96bd7c3398532f3858bbdd0d06103082/Add ChaCha20-Poly1305 as an enabled cipher suite Change-Id: Idc143d37c63cf3436ccdddc22abcb11802fc6615/external/conscrypt: sort list of cipher suite strings. This change sorts the list using sort(1). Change-Id: Ief0c407969c92405464b9b2e9ebc694f98260263/Basic implementation of RSASSA-PSS Signature. This makes Conscrypt provide RSASSA-PSS Signature implementations. These implementations currently do not support changing their parameters (e.g., via Signature.setParameter(PSSParameterSpec)) and returning their current parameters (e.g., via Signature.getParameters()). This will be added in a follow-up change. Bug: 25794302 Change-Id: I1488e0e9592f92a9e15365131c76ce2902ad4607/"
100,100,14.0,0.9789000153541565,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Add support for SNI API This adds support for retrieving SNI name as a server and setting SNI name as a client. It currently doesnt implement use of the SNIMatcher API. Change-Id: I4f76fcbd96bd7c3398532f3858bbdd0d06103082/Add handshake session and ExtendedX509TrustManager support This enables the new API to specify when a host should be verified by hostname. Before there was no public API that was capable of indicating to the TrustManager which DNS hostname you were intending to connect with. Change-Id: Ic5845d1e93f02b54d971673a280d0a3571739fbf/Revert ""Revert ""Add ExtendedSSLSession, et al."""" This reverts commit 132c311de656e7396b78b388c6351be8a84a159c. Some stubs were neded to allow building on unbundled builds. Change-Id: I713d00923eecac7e323d53e561cf509794cc4fd4/Add ExtendedSSLSession, et al. In order to support SNI certificate selection of the server-side and enhanced certificate verification on the client side, we add ExtendedSSLSession and the getHandshakeSession support. This is just to set up for future implementations of SNI and ExtendedX509TrustManager and doesnt actually implement the logic needed to fully support the new features. Change-Id: I300d3134d8ab9c184d6473183612dc53658a8221/"
101,101,3.0,0.9567999839782715,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Add compat methods for using SNI and other new features Newer revisions of Android have SSLParameters with SNI support, endpoint identification algorithm support, and honor cipher suite order preference support. Add these to the /compat/ subtree so we can use them if available in unbundled releases. Change-Id: Iab3a3e6863b025c64790b08952a8b43cf087e920/"
102,102,4.0,0.9405999779701233,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","OpenSSLSocketImpl: Dont accidentally create a SocketImpl. We dont call super.close() when were wrapping a socket, so well have to be careful not to call any superclass methods that might end up creating a SocketImpl. bug: 27250522 Change-Id: Ie98127d002cc3b3dd3dd419b62adcfec47817479/"
103,103,7.0,0.949999988079071,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Update d2i_SSL_SESSION test expectations Update d2i_SSL_SESSION to only throw IOException and change tests to expect that to happen. Since IOException is declared as a thrown exception, non-test code should already be expecting this. Bug: 27526112 Change-Id: Ic8c1a47debce9cb76221150d050be86d010c6ec3/"
104,104,9.0,0.970300018787384,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Improve path building This CL changes certificate path building from building the first possible chain only to building all possible chains until a valid chain is found or all potential chains are exhausted. This will allow us to more gracefully handle CA and intermediate changes. This CL does _not_ change the verification step in any way, all chains generated are still verified the same as they were before. (cherry-picked from commit 381c900af12815e6f0c01519d8ebdd57297303e9) Change-Id: Ia8c4cd4131eb6ddf299da144b963a24cd1b64605/"
105,105,16.0,0.9926999807357788,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","UniqueMutex for explicit ordering with ScopedSslBio The MUTEX_LOCK / MUTEX_UNLOCK semantics work if you also explicitly clear out resources that were supposed to be cleared before the lock is released. However, with wrapper classes that do it automatically, you cant get the correct ordering. Instead of converting these all to manual acquire and release, convert the mutex handling to use automatic release via UniqueMutex so that ordering is correct with resources that should be protected by the mutex. Thanks to Zhen Song for finding these issues. Bug: 28473706 Change-Id: I4b63ce674e0fc343fe156936df7e8f6e3130722f/Revert ""Switch Conscrypt to EC_GROUP_new_arbitrary."" This reverts commit f695b9fa2d3b67f95cc40fb485db5ee73da60f25. Having a different API for this case than upstream is more trouble than is worth it. A separate ""incomplete EC_GROUP"" state is a nuisance, but not much more of a nuisance for future hopes than having separate ""static EC_GROUP"" and ""arbitrary EC_GROUP"" buckets. (BoringSSL will keep both APIs around until this is cycled everywhere so we wont need more multi-sided changes.) Change-Id: Iad4604a04d75b29b9aac9dfde0f9ae18964017e8/OpenSSLSessionImpl: add better errors when converting Frequently an old SSLSession cache from a different version of OpenSSL or BoringSSL will cause the de-serialization of the SSLSession information to fail. This will spam the logs and happens Frequently when GmsCores ProviderInstaller is used. For now try to extract a bit more useful information from the error thrown by native code and dont bother to print the stack trace since its not fatal. (cherry picked from commit de8236f4bb9d70fa4e6a52679b4bf40b04c44f9b) Bug: 25328662 Change-Id: I0a396a52418e7911b98133b45bbfafcc6651e863/OpenSSLSessionImpl: add better errors when converting Frequently an old SSLSession cache from a different version of OpenSSL or BoringSSL will cause the de-serialization of the SSLSession information to fail. This will spam the logs and happens Frequently when GmsCores ProviderInstaller is used. For now try to extract a bit more useful information from the error thrown by native code and dont bother to print the stack trace since its not fatal. Bug: 25328662 Change-Id: I0a396a52418e7911b98133b45bbfafcc6651e863/"
106,106,18.0,0.9269000291824341,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Fix static analysis findings Add annotations where we intentionally left out and a brief explanation. Add synchronized keyword where needed by overriding methods so they match the parent class. Change-Id: I55591a5902530f1c2fb8cc89260c3df09648ec8e/
107,107,10.0,0.989300012588501,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fix static analysis findings Add annotations where we intentionally left out and a brief explanation. Add synchronized keyword where needed by overriding methods so they match the parent class. Change-Id: I55591a5902530f1c2fb8cc89260c3df09648ec8e/Allow SSLSession to return IP address In an effort to not use reverse DNS, we no longer return hostnames from sockets created via IP addresses. However, this also made the SSLSession return null when a Socket is created to an IP address instead of an FQDN. While being careful not to trigger another DNS lookup, simply return a textual representation of the IP address connected when the SSLSocket has no knowledge of what the actual FQDN is supposed to be. (cherry picked from commit ee1a154153a1b20d55fc4b0dd9752277f0cd6451) Bug: 27123298 Change-Id: Ie37e214f91e4f005f90da0d4a2aba1cd604d60b7/Try to get peer hostname from SocketAddress Java 7 added a new method to InetSocketAddress called getHostString() which returns the unresolved host for a given address. This should be suitable for use with SNI as long as it isnt an IP address. This also helps with testing because we can use serialization tricks to rewrite the ""hostname"" field of an already-serialized loopback address. Bug: 27271561 Change-Id: I9845e57d505712cdfee87d18246a1a3b021deea3/OpenSSLSocketImpl: Dont accidentally create a SocketImpl. We dont call super.close() when were wrapping a socket, so well have to be careful not to call any superclass methods that might end up creating a SocketImpl. bug: 27250522 Change-Id: Ie98127d002cc3b3dd3dd419b62adcfec47817479/"
108,108,6.0,0.9603999853134155,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Use OCSP-stapled responses in TrustManagerImpl Test: vogar host \ out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack \ out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack \ libcore/luni/src/test/java/libcore/javax/net/ssl/*Test.java Change-Id: I45762ab463be7aebea848387677d0fa8f92424bd/Move CertBlacklist to conscrypt CertBlacklist is mostly unchanged from bouncycastle except removing the bouncycastle Digest and Hex dependencies in isPublicKeyBlackListed. (cherry picked from commit ce5bdd0391d93d9a4b1fe7005041271341eb69b2) Bug: 29397721 Change-Id: Icccdcc0e108e8b0c60c47522114749518247a598/
109,109,18.0,0.492000013589859,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Expose CT through libcore NetworkSecurityPolicy Bug: 28746284 Change-Id: I6549a997823b38dc256911a66ac558c90bf6f762/
110,110,15.0,0.977400004863739,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Use built-in key debugging mechanism When debugging a network flow its useful to log the negotiated keys to see what is happening inside the session. Previously this was implemented in Conscrypt, but BoringSSL has this capability built-in now. Documentation at Test: vogar host \ out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack \ out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack \ libcore/luni/src/test/java/libcore/javax/net/ssl/*Test.java Change-Id: I50a5b315d302492667a28926161836e34b9dd357/JNI_TRACE_KEYS for non-RSA connections Wireshark uses a different format for connections that are non-RSA such as those negotiated with ECDHE or DHE. This change prints out the keys in the format that Wireshark expects. Change-Id: If81e172091c29fe7e65068091be382677f23e425/"
111,111,0.0,0.9366999864578247,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Add implementation of OpenSSLSocket that uses OpenSSLEngineImpl Use above class of wrapped socket does not have a file descriptor OpenSSLEngine now exposes ALPN state Enable use of PrivilegedAction to load .so Created by MOE: MOE_MIGRATED_REVID=123899267/
112,112,5.0,0.9869999885559082,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Improve performance of OpenSSLEngine Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Test: cts-tradefed run cts CtsLibcoreOkHttpTestCases arm64-v8a Change-Id: I947bd3701e90bd65104f8f5c07ba218c4e051944/Use fewer deprecated BoringSSL APIs. ""Handshake cutthrough"" was renamed to False Start at some point. Use the newer APIs which match what everyone refers to it as. SSL_set_reject_peer_renegotiations needed to be generalized at some point to take an enum. In doing so, it no longer has this weird double-negative spelling. The non-_long variants of SSL_alert_type_string and SSL_alert_desc_string dont do anything useful. We thought the one- and two-character codes were nuts, so they just return and now. Change-Id: I83d1fa26b0ea05284b0d73f1e2a58df07887aefe Test: mma in external/conscrypt/Convert NativeCryptoTest to JUnit4 This is basically a regex substitution change with minimnal renames just to convert to JUnit4. Further JUnit4-isms will come in subsequent changes. Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Change-Id: Icb6aedc3acee31d62750132bbe8eeaf9150bd3c0/Remove NPN support NPN is deprecated and clients should use ALPN instead for now. In the interest of removing support for it in BoringSSL, we will remove NPN support from Conscrypt as well. For now keep around the NPN setters and getters in OpenSSLSocketImpl and OpenSSLEngineImpl to help with backward compatibility for users that may be using reflection. Test: make && make cts && cts-tradefed run cts CtsLibcoreTestCases Change-Id: Ia4edb21412d9c4b2440291ae0a8a97d2217bf5b5/Drop RC4 cipher suite support from TLS Bug: 30977793 Test: libcore/run-libcore-tests libcore/luni/src/test/java/libcore/javax/net/ssl/* and running NativeCryptoTest. Change-Id: I04b91a6d3bf75a757d2c74bd1a39aea2709a9199/"
113,113,10.0,0.692799985408783,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Fix missing CTPolicy argument to TrustManagerImpl() Incorrect merge resolution on my part dropped the constructor argument. Test: run OpenSSLSocketImplTest Change-Id: I9b71782050a59558d49223d398c3ad22847e89a5/ct: Allow configurable policy. The configuration will be hooked up in a following CL Test: ran OpenSSLSocketImplTest Change-Id: Ia9af1564cbe7b37746e2cbfe8c5e9c08eab55c76/
114,114,15.0,0.7545999884605408,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Fix some leaks on sk_push error. sk_push only takes ownership of the pointer on success, so the pattern needs to be slightly different. Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Change-Id: Ic1b10b9aae5addf20bf770c334ada9bc461c97b8/Fix reference counting. SSL_use_certificate and friends were leaking and client_cert_cb was failing to give refcounts for objects it returns. The two cancelled each other out. Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Test: cts-tradefed run cts CtsLibcoreOkHttpTestCases arm64-v8a Change-Id: I9e9e75902054f59be12f68fb14cf9f3f75a7a46e/Restrict TLS signing to non-RSA-PSS algorithms A recent change in BoringSSL allowed connections to use RSA-PSS as the signing algorithm for TLSv1.2 connections. However, the CryptoUpcalls interface is not ready for this and it cannot currently make the upcall correctly to have these signed. Temporarily disable RSA-PSS signatures with TLS by explicitly setting the list of signature algorithms. Test:cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Test:cts-tradefed run cts CtsLibcoreOkHttpTestCases arm64-v8a Bug: 31714503 Change-Id: Ie1c23b7231b5673816946a6c06030e1e25752415/OpenSSLCipherRSA: add support for OAEP labels The OAEP label L can be specified by using a custom PSource.PSpecified with OAEPParameterSpec. This is not commonly used, but is an option thats easily supported. Test: vogar host out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack libcore/luni/src/test/java/libcore/javax/crypto/CipherTest.java Change-Id: Ide7a087b5bc4dde31d55e38a8efdd16e3dd44ba5/NativeCrypto: add debugging format checking when debug off Before if you enabled WITH_JNI_TRACE you might get some formatting errors because format is not checked when debugging is enabled. Switch to constexpr to enable debugging and rely on Dead Code Elimination pass in the compiler to remove all the debug code when its not in use. This allows the compiler to properly check printf-style formatting for debug statements instead of the preprocessor removing the code. Test: compile with kWithJniTrace true and run vogar tests Change-Id: Ief3fe1c099a38d802db32deb7ffa91e4c8d4a572/No need to call ERR_remove_thread_state. In BoringSSL, error data is maintained in thread-locals and automatically released via thread-local destructors. ERR_remove_thread_state just calls ERR_clear_error now anyway. Change-Id: Ie4b54ec0573f58076eba3102079f773425debcdc Test: mma/Convert EVP_AEAD_CTX to stack-only The EVP_AEAD_CTX object is only valid for the period of time in which it is being used, so there is no need to have separate calls to EVP_AEAD_CTX_init and EVP_AEAD_CTX_cleanup. Sweep the arguments to these into the EVP_AEAD_CTX_seal and EVP_AEAD_CTX_open calls. Test: make build-art-host vogar && vogar host out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack libcore/luni/src/test/java/libcore/javax/crypto/CipherTest.java Change-Id: I02b2c5759efdd8f29ec354ee9c10053dd1f9c53c/Switch to BoringSSL scopers The latest version of BoringSSL includes scopers for various types. Switch to those to reduce redundancy. There are a few that are missing and EVP_AEAD_CTX can be rewritten to not need anything beyond the supplied ScopedEVP_AEAD_CTX in BoringSSL. Test: make && vogar host out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/mockito-api-hostdex_intermediates/classes.jack libcore/luni/src/test/java/libcore/javax/net/ssl/*Test.java libcore/luni/src/test/java/libcore/javax/crypto/*Test.java Change-Id: I814f10936e6d7eb479da99391723fc9bce4e6096/Remove NPN support NPN is deprecated and clients should use ALPN instead for now. In the interest of removing support for it in BoringSSL, we will remove NPN support from Conscrypt as well. For now keep around the NPN setters and getters in OpenSSLSocketImpl and OpenSSLEngineImpl to help with backward compatibility for users that may be using reflection. Test: make && make cts && cts-tradefed run cts CtsLibcoreTestCases Change-Id: Ia4edb21412d9c4b2440291ae0a8a97d2217bf5b5/Substitute NULL for nullptr Since we dont actually rely on a STL, we dont have to get NULL from, but since were compiling C++11 we get nullptr for free. This also fixes builds against MacOS SDK since it doesnt have available when you explicitly opt out of an STL in the Android.mk module. Test: mmma external/conscrypt; make PRODUCT-sdk_phone_armv7-sdk Change-Id: I54929c7e5c05ec271925f5f3d1896df1661e9b59/"
115,115,6.0,0.9821000099182129,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Move MGF1 algorithm name and OID to EvpMdRef Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Change-Id: Ia3bbceb2a6022ebfbbd7ce1b4c2bb8d8c5ca956b/Move JCA names and OIDs to constants Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Change-Id: I2fb67d6e9aa812b3b2ea26e14d18fbe752c70fc3/Use EvpMdRef for size calculation Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Change-Id: I9c909e401d6224f626b69dae3ac21e16a7a9b03c/Consolidate EVP_MD references to one place There were several places where EVP_get_digestbyname was being called for the same data. Consolidate these all down to one place so there is no need to call it several times in the same program. Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Change-Id: Ib3f8b678c775e74eb5edaabde42f042d7b4eac95/OpenSSLSignature: always throw on setting context Most of the EVP_PKEY_CTX_ctrl error codes are not handled by Conscrypt so make sure we have a default exception in case something goes awry during setup. Test: vogar host out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/mockito-api-hostdex_intermediates/classes.jack libcore/luni/src/test/java/libcore/java/security/SignatureTest.java Change-Id: I1f5a753242b6bc31cca9feb96486bbc86ad8af54/
116,116,6.0,0.5123999714851379,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Improve performance of OpenSSLEngine Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Test: cts-tradefed run cts CtsLibcoreOkHttpTestCases arm64-v8a Change-Id: I947bd3701e90bd65104f8f5c07ba218c4e051944/OpenSSLCipherRSA: add support for OAEP labels The OAEP label L can be specified by using a custom PSource.PSpecified with OAEPParameterSpec. This is not commonly used, but is an option thats easily supported. Test: vogar host out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack libcore/luni/src/test/java/libcore/javax/crypto/CipherTest.java Change-Id: Ide7a087b5bc4dde31d55e38a8efdd16e3dd44ba5/Strip out SSLv3 from enabled protocols for app compat HttpsURLConnection on Android before Marshmallow tried to setEnabledProtocols with just ""SSLv3"" without checking if it was a supported protocol. Instead of throwing IllegalArgumentException when the unsupported protocol is encountered, strip it out and later throw an SSLHandshakeException if no protocols are enabled. Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Test: cts-tradefed run cts CtsLibcoreOkHttpTestCases arm64-v8a Bug: 32053327 Bug: 30977793 Change-Id: I2f2008d85fcc5b5fbdc71722a3d6e0a9c22bfbc2/Add NativeCrypto.EVP_PKEY_CTX_set_rsa_oaep_md This will allow us to set the OAEP message digest function later on in addition to the MGF1 message digest function. Test: mmma external/conscrypt && vogar host out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/conscrypt-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/conscrypt-tests-hostdex_intermediates/classes.jack com.android.org.conscrypt.NativeCryptoTest Change-Id: Iee45d973e253f3b5c60919d70571abb96d97bb08/Add EVP_PKEY_{encrypt,decrypt} calls This adds the structure and tests for the EVP_PKEY_encrypt and EVP_PKEY_decrypt functions to be called from Java. Test: mmma external/conscrypt && vogar host out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/conscrypt-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/conscrypt-tests-hostdex_intermediates/classes.jack com.android.org.conscrypt.NativeCryptoTest Change-Id: I36dfbd6c6f76c5cbda8490375e8579ca32a4babb/Use fewer deprecated BoringSSL APIs. ""Handshake cutthrough"" was renamed to False Start at some point. Use the newer APIs which match what everyone refers to it as. SSL_set_reject_peer_renegotiations needed to be generalized at some point to take an enum. In doing so, it no longer has this weird double-negative spelling. The non-_long variants of SSL_alert_type_string and SSL_alert_desc_string dont do anything useful. We thought the one- and two-character codes were nuts, so they just return and now. Change-Id: I83d1fa26b0ea05284b0d73f1e2a58df07887aefe Test: mma in external/conscrypt/Blacklisting TLS 1.3 ciphersuites from Android TLS 1.3 adds a new set of AEAD-only ciphers, which will be exposed by BoringSSLs draft TLS 1.3 implementation. Were not ready to ship TLS 1.3 in Conscrypt yet, but get_cipher_names returns the new ciphers by default (cipher/version filtering happens much later). Suppress those ciphers for now. Test: cts-tradefed run cts CtsLibcoreTestCases CtsLibcoreOkHttpTestCases arm64-v8a Change-Id: I14421aec8dceb4b0eb7347b8ebf88a87a10ba856/Drop SSLv3 support Bug: 30977793 Test: libcore/run-libcore-tests libcore/luni/src/test/java/libcore/javax/net/ssl/* Change-Id: Ic88ff61bb16017e213a017ecdb16a1ac5b9baa48/Convert EVP_AEAD_CTX to stack-only The EVP_AEAD_CTX object is only valid for the period of time in which it is being used, so there is no need to have separate calls to EVP_AEAD_CTX_init and EVP_AEAD_CTX_cleanup. Sweep the arguments to these into the EVP_AEAD_CTX_seal and EVP_AEAD_CTX_open calls. Test: make build-art-host vogar && vogar host out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack libcore/luni/src/test/java/libcore/javax/crypto/CipherTest.java Change-Id: I02b2c5759efdd8f29ec354ee9c10053dd1f9c53c/OpenSSLSignature: always throw on setting context Most of the EVP_PKEY_CTX_ctrl error codes are not handled by Conscrypt so make sure we have a default exception in case something goes awry during setup. Test: vogar host out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/mockito-api-hostdex_intermediates/classes.jack libcore/luni/src/test/java/libcore/java/security/SignatureTest.java Change-Id: I1f5a753242b6bc31cca9feb96486bbc86ad8af54/Drop RC4 cipher suite support from TLS Bug: 30977793 Test: libcore/run-libcore-tests libcore/luni/src/test/java/libcore/javax/net/ssl/* and running NativeCryptoTest. Change-Id: I04b91a6d3bf75a757d2c74bd1a39aea2709a9199/"
117,117,10.0,0.9929999709129333,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Improve performance of OpenSSLEngine Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Test: cts-tradefed run cts CtsLibcoreOkHttpTestCases arm64-v8a Change-Id: I947bd3701e90bd65104f8f5c07ba218c4e051944/Move CT to platform/ This moves the CT code from main to platform and the checking from OpenSSLSocketImpl to TrustManagerImpl. Theres still some plumbing that needs to be done in main to store the TLS extension data so TrustManagerImpl can get it. Test: Run OpenSSLSocketImplTest, verified network connections still work Change-Id: I643db4668cbec2d1bb221156c5844667ae8701c8/OpenSSLEngineImpl: throw ISE if client/server mode not set According to SSLEngine documentation, IllegalStateException will be thrown if or is called before setting the client/server mode. When OpenSSLEngineImpl was written, it was written against the existing tests which did not fail in this scenario due to a missing fail() call in the try/catch. The existing test calls with a 10 byte buffer which immediatly hits the BUFFER_OVERFLOW condition. To avoid this the ByteBuffer check was moved below the state check which means calling with a too-small buffer without starting the handshake first will fail on the buffer size check only after the first call. This should not affect callers as they have to handle this condition during the normal operation of the SSLEngine anyway. Test: vogar host out/target/common/obj/JAVA_LIBRARIES/bouncycastle_intermediates/classes.jack out/target/common/obj/JAVA_LIBRARIES/bouncycastle-ocsp_intermediates/classes.jack out/target/common/obj/JAVA_LIBRARIES/bouncycastle-bcpkix_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack libcore/harmony-tests/src/test/java/org/apache/harmony/tests/javax/net/ssl/SSLEngineTest.java Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Bug: 31301555 Change-Id: I6f4c36d5abcc71e020ce40c7f61df2ed01b1a53e/Remove NPN support NPN is deprecated and clients should use ALPN instead for now. In the interest of removing support for it in BoringSSL, we will remove NPN support from Conscrypt as well. For now keep around the NPN setters and getters in OpenSSLSocketImpl and OpenSSLEngineImpl to help with backward compatibility for users that may be using reflection. Test: make && make cts && cts-tradefed run cts CtsLibcoreTestCases Change-Id: Ia4edb21412d9c4b2440291ae0a8a97d2217bf5b5/"
118,118,5.0,0.4510999917984009,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Improve performance of OpenSSLEngine Test: cts-tradefed run cts CtsLibcoreTestCases arm64-v8a Test: cts-tradefed run cts CtsLibcoreOkHttpTestCases arm64-v8a Change-Id: I947bd3701e90bd65104f8f5c07ba218c4e051944/Use fewer deprecated BoringSSL APIs. ""Handshake cutthrough"" was renamed to False Start at some point. Use the newer APIs which match what everyone refers to it as. SSL_set_reject_peer_renegotiations needed to be generalized at some point to take an enum. In doing so, it no longer has this weird double-negative spelling. The non-_long variants of SSL_alert_type_string and SSL_alert_desc_string dont do anything useful. We thought the one- and two-character codes were nuts, so they just return and now. Change-Id: I83d1fa26b0ea05284b0d73f1e2a58df07887aefe Test: mma in external/conscrypt/Move CT to platform/ This moves the CT code from main to platform and the checking from OpenSSLSocketImpl to TrustManagerImpl. Theres still some plumbing that needs to be done in main to store the TLS extension data so TrustManagerImpl can get it. Test: Run OpenSSLSocketImplTest, verified network connections still work Change-Id: I643db4668cbec2d1bb221156c5844667ae8701c8/Partial revert of ""Fix static analysis findings"" This reverts commit ee3b1694a206350e87a70cc83dd35a1ec2053fd6. This causes deadlocks for wrapped sockets. Bug: 31449904 Test: make build-art-host vogar && vogar host out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack out/host/common/obj/JAVA_LIBRARIES/core-tests-hostdex_intermediates/classes.jack libcore/luni/src/test/java/libcore/javax/net/ssl/SSLSocketTest.java Change-Id: Iedf2244ae4ed8bd79dae996406c7668ebadc832e/Remove NPN support NPN is deprecated and clients should use ALPN instead for now. In the interest of removing support for it in BoringSSL, we will remove NPN support from Conscrypt as well. For now keep around the NPN setters and getters in OpenSSLSocketImpl and OpenSSLEngineImpl to help with backward compatibility for users that may be using reflection. Test: make && make cts && cts-tradefed run cts CtsLibcoreTestCases Change-Id: Ia4edb21412d9c4b2440291ae0a8a97d2217bf5b5/Fix imports globally Ran script to fix imports globally on all .java files. Committing results. Test: mmma external/conscrypt Change-Id: I6cd19c0f3dc66c8dcaec3c92a6019b4c0154e9e4/"
119,119,12.0,0.972000002861023,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Locking down public APIs (#157) Tried to be as aggressive as I could, so this probably deserves a fairly thorough review. I left most of OpenSSLSocketImpl public, because I think its needed by a few external projects. I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated. Fixes Java 8 style SNI hostname to OpenSSLEngineImpl (#155) The SNIHostName, et al., support was lacking from OpenSSLEngineImpl causing endpoint protocol identification to fail in Netty tests./"
120,120,12.0,0.949999988079071,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Benchmark fixes and various cleanup. (#188)/Make openjdk target support Java 7/Adding platform to the build (#158)/Add Java 8 style SNI hostname to OpenSSLEngineImpl (#155) The SNIHostName, et al., support was lacking from OpenSSLEngineImpl causing endpoint protocol identification to fail in Netty tests./"
121,121,12.0,0.8944000005722046,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
122,122,12.0,0.5489000082015991,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Adding all factory methods for engine socket. (#192) Also properly throwing SSLHandshakeException in some cases. Fixes OpenSSLSocketImplTest to cover both socket types (#182)/Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./Add handshake listener to engine. (#136) Fixes
123,123,15.0,0.7203999757766724,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Importing more Android integ tests. (#184)/Locking down public APIs (#157) Tried to be as aggressive as I could, so this probably deserves a fairly thorough review. I left most of OpenSSLSocketImpl public, because I think its needed by a few external projects. I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated. Fixes OCSP and SCTs on the SSL, not SSL_CTX. As Conscrypt is currently set up, one SSL_CTX (owned, ultimately, by the SSLContext) may correspond to multiple SSLParameters which, in the Java API, are configured on the SSLSocket or SSLEngine directly. Thus we should use the SSL versions of the APIs which now exist. This avoids mutating an SSL_CTX which may be shared by multiple SSLs with different configurations. Change-Id: I19485c316087004c6050d85520b0169f2ca0d493/Exposing SSL_max_seal_overhead (#135) Also adding a method to calculate the maximum buffer size required for a wrap operation./Remove DHE/"
124,124,2.0,0.9728000164031982,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Locking down public APIs (#157) Tried to be as aggressive as I could, so this probably deserves a fairly thorough review. I left most of OpenSSLSocketImpl public, because I think its needed by a few external projects. I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated. Fixes AEADBadTagException throwing to method This is not available on all Android versions, so we access it via reflection here. Extract it to its own method so we can suppress the Error-Prone warning more precisely./"
125,125,12.0,0.8198000192642212,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Adding all factory methods for engine socket. (#192) Also properly throwing SSLHandshakeException in some cases. Fixes down public APIs (#157) Tried to be as aggressive as I could, so this probably deserves a fairly thorough review. I left most of OpenSSLSocketImpl public, because I think its needed by a few external projects. I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated. Fixes top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./Suppress Error-Prone warnings These warnings are not useful here, so suppress them./"
126,126,12.0,0.8944000005722046,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
127,127,12.0,0.8944000005722046,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
128,128,12.0,0.6960999965667725,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./Allow handshakeListener to be set when engineState is MODE_SET (#137)/
129,129,12.0,0.5367000102996826,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Refactor OpenSSLSocketImplTest to cover both socket types (#182)/Importing Android integration tests (#178)/Locking down public APIs (#157) Tried to be as aggressive as I could, so this probably deserves a fairly thorough review. I left most of OpenSSLSocketImpl public, because I think its needed by a few external projects. I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated. Fixes top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./Configure OCSP and SCTs on the SSL, not SSL_CTX. As Conscrypt is currently set up, one SSL_CTX (owned, ultimately, by the SSLContext) may correspond to multiple SSLParameters which, in the Java API, are configured on the SSLSocket or SSLEngine directly. Thus we should use the SSL versions of the APIs which now exist. This avoids mutating an SSL_CTX which may be shared by multiple SSLs with different configurations. Change-Id: I19485c316087004c6050d85520b0169f2ca0d493/Adding conversion utility ALPN protocols (#140) Exposing additional set methods in OpenSSLEngineImpl and OpenSSLSocketImpl to allow the caller to set the ALPN protocols without having to manually encode. Also simplifying the exposure of the maxSealOverhead value./"
130,130,12.0,0.9049999713897705,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Add Java 8 style SNI hostname to OpenSSLEngineImpl (#155) The SNIHostName, et al., support was lacking from OpenSSLEngineImpl causing endpoint protocol identification to fail in Netty tests./"
131,131,12.0,0.9660999774932861,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Refactoring session management (#172) This change breaks session management into two distinct types: SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession. ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers. Fixes"
132,132,12.0,0.982699990272522,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Use InetAddress originalHostName (if present) for getHostnameOrIP (#303) * Use InetAddress originalHostName (is present) for getHostnameOrIP Test: CtsLibcoreTestCases Bug: 35942385 Bug: 31028374 Change-Id: Ie1acc2dd23fadbbc48de1a5845146e9a5953e9cf * Fix lint problems and remove class introduced in API 19./Support Java 6 Runtime (#299) Various fixes to support Java 6, 7, and 8. Separating out utility classes (for openjdk) to be explicit as to which methods are supported by particular Java version. Adding the ability to specify the test JVM to use on the command-line. For example, the following will build with the default Java installation, but will run the openjdk and integ-tests with Java 6: ./gradlew build Fixes session management (#172) This change breaks session management into two distinct types: SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession. ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers. Fixes fixes to the Conscrypt engine. (#201)/"
133,133,12.0,0.6162999868392944,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Support Java 6 Runtime (#299) Various fixes to support Java 6, 7, and 8. Separating out utility classes (for openjdk) to be explicit as to which methods are supported by particular Java version. Adding the ability to specify the test JVM to use on the command-line. For example, the following will build with the default Java installation, but will run the openjdk and integ-tests with Java 6: ./gradlew build Fixes to SSL_get0_peer_certificates. This works towards issue So the exception can be routed out properly, this moves the SSL_get0_peer_certificates call to after doHandshake completes in ConscryptFileDescriptorSocket. Although, due to False Start (the ""cut-through"" logic in that class), the handshake may not be fully complete at the time, BoringSSLs API is such that the certificates and other properties will be available once SSL_do_handshake first completes./Pass encoded local certs to BoringSSL (#253) Current code was encoding and then decoding the certs before finally passing them to the native code. We were also separately setting and then verifying the private key. All of this can be replaced with a single JNI call SSL_set_chain_and_key, which accepts the encoded certs (we dont have to decode them again). See This shows a perf bump for the handshake (from ~750 to 800 ops/sec)./Cleaning up JNI exceptions (#252) There were a bunch of exceptions that are being thrown from JNI methods that arent currently declared. Also removed a few unused JNI methods and duplicate constants, preferring those from NativeConstants./Remove Java OpenSSL name mapping. (#227) As of [0], BoringSSL supports the standard cipher suite names. The Java names are the same, with the exception of TLS_RSA_WITH_3DES_EDE_CBC_SHA/SSL_RSA_WITH_3DES_EDE_CBC_SHA for historical reasons. Add code to map between that exception but otherwise rely on the native support. [0] session management (#172) This change breaks session management into two distinct types: SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession. ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers. Fixes"
134,134,12.0,0.5127999782562256,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Use InetAddress originalHostName (if present) for getHostnameOrIP (#303) * Use InetAddress originalHostName (is present) for getHostnameOrIP Test: CtsLibcoreTestCases Bug: 35942385 Bug: 31028374 Change-Id: Ie1acc2dd23fadbbc48de1a5845146e9a5953e9cf * Fix lint problems and remove class introduced in API 19./User-friendly errors when native library fails to load (#295) Currently, we throw in static initialization of NativeCryptoJni, which leads to ClassNotFoundException whenever we try to access the native code at runtime. The user has no way of knowing that the library failed to load, or why. This PR attempts to make it more clear as to what went wrong, by doing the following: Log each load error, in the order attempted. Select the ""best"" load error to be thrown Dont throw from the NativeCrypto static initializer. Rather, save the best error and throw when attempting to access top-level conscrypt classes./Support Java 6 Runtime (#299) Various fixes to support Java 6, 7, and 8. Separating out utility classes (for openjdk) to be explicit as to which methods are supported by particular Java version. Adding the ability to specify the test JVM to use on the command-line. For example, the following will build with the default Java installation, but will run the openjdk and integ-tests with Java 6: ./gradlew build Fixes certs in verify callback (#248) This code was copied from Netty/netty-tcnative and seems to significantly increases performance of the verify callback. Before we call back to Java, we first encode all of the certs and then decode them in Java into X509Certificate instances. Previous code was calling into JNI for each method in the certificate. This helps in addressing session management (#172) This change breaks session management into two distinct types: SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession. ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers. Fixes fixes to the Conscrypt engine. (#201)/"
135,135,12.0,0.9660999774932861,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Refactoring session management (#172) This change breaks session management into two distinct types: SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession. ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers. Fixes"
136,136,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Various fixes to the Conscrypt engine. (#201)/
137,137,5.0,0.6266000270843506,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Switch to SSL_get0_peer_certificates. This works towards issue So the exception can be routed out properly, this moves the SSL_get0_peer_certificates call to after doHandshake completes in ConscryptFileDescriptorSocket. Although, due to False Start (the ""cut-through"" logic in that class), the handshake may not be fully complete at the time, BoringSSLs API is such that the certificates and other properties will be available once SSL_do_handshake first completes./Pass encoded local certs to BoringSSL (#253) Current code was encoding and then decoding the certs before finally passing them to the native code. We were also separately setting and then verifying the private key. All of this can be replaced with a single JNI call SSL_set_chain_and_key, which accepts the encoded certs (we dont have to decode them again). See This shows a perf bump for the handshake (from ~750 to 800 ops/sec)./Cleaning up JNI exceptions (#252) There were a bunch of exceptions that are being thrown from JNI methods that arent currently declared. Also removed a few unused JNI methods and duplicate constants, preferring those from NativeConstants./Remove Java OpenSSL name mapping. (#227) As of [0], BoringSSL supports the standard cipher suite names. The Java names are the same, with the exception of TLS_RSA_WITH_3DES_EDE_CBC_SHA/SSL_RSA_WITH_3DES_EDE_CBC_SHA for historical reasons. Add code to map between that exception but otherwise rely on the native support. [0] AlgorithmParameters.GCM in Conscrypt. (#217) * Implement AlgorithmParameters.GCM in Conscrypt. In order to handle the ASN.1 encoding, exposes a subset of the ASN.1 encoding API from BoringSSL in NativeCrypto. * Rename {write,read}_integer to {write,read}_uint64. Add a UniquePtr to ensure exceptions dont cause a memory leak./Some parsing and serializing fixes. (#219) This fixes a memory leak in NativeCrypto_i2d_PKCS7. It never frees derBytes. Also removing a dependency on the legacy ASN.1 stack./Add availability checks (#216) Fixes session management (#172) This change breaks session management into two distinct types: SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession. ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers. Fixes fixes to the Conscrypt engine. (#201)/"
138,138,0.0,0.977400004863739,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Conformance fixes for the engine-based socket. (#202) This allows the SSLSocketTest to pass with the engine-based socket enabled. Also restructuring the inheritance hierarchy so that the FD and engine sockets both behave the same way (both either wrappers or not). The restructure involves the following: AbstractConscryptSocket: New base class for both sockets. It handles the wrap/no-wrap logic. OpenSSLSocketImplWrapper: deleted and replaced by AbstractConscryptSocket. OpenSSLSocketImpl: reduced to a public shim class between AbstractConscryptSocket and the implementations. For backward-compat only. ConscryptFileDescriptorSocket: Renamed from OpenSSLSocketImpl. The old FD socket./Various fixes to the Conscrypt engine. (#201)/
139,139,10.0,0.4959999918937683,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Move closer to the old test version The original test never called shutdownInput() since the Javadoc indicates it will make the socket behave in a different manner than expected in this test. Also assert that were not using the type of socket that we expect to return if we get a SocketException. This will indicate when this difference is fixed and the test can be changed to reflect the expectation./Conformance fixes for the engine-based socket. (#202) This allows the SSLSocketTest to pass with the engine-based socket enabled. Also restructuring the inheritance hierarchy so that the FD and engine sockets both behave the same way (both either wrappers or not). The restructure involves the following: AbstractConscryptSocket: New base class for both sockets. It handles the wrap/no-wrap logic. OpenSSLSocketImplWrapper: deleted and replaced by AbstractConscryptSocket. OpenSSLSocketImpl: reduced to a public shim class between AbstractConscryptSocket and the implementations. For backward-compat only. ConscryptFileDescriptorSocket: Renamed from OpenSSLSocketImpl. The old FD socket./Updates to SSLSocketTest to support engine-based socket. (#199)/
140,140,12.0,0.9797999858856201,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Support Java 6 Runtime (#299) Various fixes to support Java 6, 7, and 8. Separating out utility classes (for openjdk) to be explicit as to which methods are supported by particular Java version. Adding the ability to specify the test JVM to use on the command-line. For example, the following will build with the default Java installation, but will run the openjdk and integ-tests with Java 6: ./gradlew build Fixes session management (#172) This change breaks session management into two distinct types: SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession. ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers. Fixes"
141,141,17.0,0.9405999779701233,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Add utility method for identifying a Conscrypt Provider. (#313) Also flattening the Conscrypt class. The current structure with subclasses helps to organize, but it makes API stability harder. Partially addresses"
142,142,8.0,0.9049999713897705,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Adding support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes
143,143,8.0,0.9269000291824341,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Add missing Java 9 methods for ALPN (#339) Fixes support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes
144,144,8.0,0.9049999713897705,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Adding support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes
145,145,8.0,0.9049999713897705,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Adding support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes
146,146,10.0,0.9603999853134155,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Add missing Java 9 methods for ALPN (#339) Fixes app_data.h field init ordering. (#337) The ordering needs to be in the same order as the fields to comply with strict warnings./Adding support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes
147,147,10.0,0.987500011920929,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Implement unadorned ChaCha20. (#367) ChaCha20 is a stream cipher in its own right, and it was pointed out that it was weird that requesting ""ChaCha20"" alone from Conscrypt returned an implementation of ChaCha20+Poly1305. This implements plain ChaCha20 and makes it the default implementation, so to access ChaCha20+Poly1305 the caller must ask for ChaCha20/Poly1305/NoPadding explicitly. We havent made a release with ChaCha20 in it yet, so it should be fine to change the meaning of requesting ""ChaCha20""./Implement ChaCha20 support. (#356) Adds Cipher, KeyGenerator, and AlgorithmParameters for ChaCha20. Main changes are in OpenSSLCipher$EVP_AEAD. Refactored so the GCM- specific portions are all in the GCM subclass and the generic AEAD portions (such as not allowing reuse of key/IV combinations) are in the superclass. Also updates so that calling Cipher.init() on an instance of AES/GCM/NoPadding with a GCM AlgorithmParameters will pick up the values from the AlgorithmParameters (previously, it would only work with GCMParameterSpec or something convertable to IvParameterSpec)./Use public API to get session ID. (#348)/Add missing Java 9 methods for ALPN (#339) Fixes support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes"
148,148,8.0,0.9842000007629395,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Switch from wrapping to subclassing for Java 9 methods. (#359) Older Android apps rely on OpenSSLSocketImpl being in the superclass chain of sockets, which means wrapping them can cause problems. Instead, use a subclass to add the appropriate methods. This also makes the subclass implementation quite clean, but it pollutes the Platform classes with a bunch of methods and means we have to remember not to use the constructors directly./Provide the issuers when calling chooseClientAlias. (#344) I cant see any reason why the issuers shouldnt be provided, and this method hasnt been changed in at least 4 years, so I expect it was just an oversight. Fixes missing Java 9 methods for ALPN (#339) Fixes support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes"
149,149,8.0,0.9049999713897705,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Adding support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes
150,150,13.0,0.7791000008583069,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Adding support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes renegotiation with sockets (#321) Fixes Fixes send and receive close_notify alerts (#325) As part: * Dont call SSL_clear() when shutting down an SSL, because we need to be able to hand it close_notify messages still. Since both SSLEngine and SSLSocket arent reusable, this is safe. * Return SSL_ERROR_ZERO_RETURN explicitly when it happens, so that we can properly adjust our state in response to a close_notify being received. * Check if there are pending bytes to be sent when wrap() is called, even if the SSLEngine is closed./"
151,151,8.0,0.9269000291824341,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Add missing Java 9 methods for ALPN (#339) Fixes support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes
152,152,10.0,0.9848999977111816,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Cleaning up various warnings. (#365)/Allow modes (ECB and NONE) and padding (NOPADDING) for ARC4. (#361) This allows callers to request ARC4/NONE/NOPADDING as an equivalent name to requesting just ARC4. Both NONE and ECB are supported as modes for RSA and for other providers implementations of stream ciphers like ARC4 that dont use modes, so we accept both here as well./Implement ChaCha20 support. (#356) Adds Cipher, KeyGenerator, and AlgorithmParameters for ChaCha20. Main changes are in OpenSSLCipher$EVP_AEAD. Refactored so the GCM- specific portions are all in the GCM subclass and the generic AEAD portions (such as not allowing reuse of key/IV combinations) are in the superclass. Also updates so that calling Cipher.init() on an instance of AES/GCM/NoPadding with a GCM AlgorithmParameters will pick up the values from the AlgorithmParameters (previously, it would only work with GCMParameterSpec or something convertable to IvParameterSpec)./"
153,153,8.0,0.9861999750137329,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Throw SocketException if the FD socket has had its SSL freed (#343) Under some circumstances (which are repeatable, but I havent been able to lock down), the HTTP connection reuse system in the JDK will attempt to reuse a finalized ConscryptFileDescriptorSocket. When this happens, the socket throws a NullPointerException from the native code when methods are called, because the native SSL object has been freed, which causes a lot of problems. Instead, have the socket throw SocketException instead, which everything understands and responds properly to. As best as I can tell, this is happening because the finalizer of some object thats not ours but has a strong reference to our socket adds a new strong reference, but our finalizer has already been enqueued and thus is run even though the object then can later be reused. I havent been able to find this finalizer, but everything tolerates the code as written and responds properly to the SocketException, so it seems like a good solution until we get rid of the FD socket entirely. Fixes support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes"
154,154,10.0,0.977400004863739,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Implement ChaCha20 support. (#356) Adds Cipher, KeyGenerator, and AlgorithmParameters for ChaCha20. Main changes are in OpenSSLCipher$EVP_AEAD. Refactored so the GCM- specific portions are all in the GCM subclass and the generic AEAD portions (such as not allowing reuse of key/IV combinations) are in the superclass. Also updates so that calling Cipher.init() on an instance of AES/GCM/NoPadding with a GCM AlgorithmParameters will pick up the values from the AlgorithmParameters (previously, it would only work with GCMParameterSpec or something convertable to IvParameterSpec)./Adding support for Java 9 server-side ALPN protocol selection. (#319) Java 9 adds a setHandshakeApplicationProtocolSelector method to both SSLEngine and SSLSocket that allows the application to provide a BiFunction to choose the protocol. This PR attempts to provide support for this method while still maintaining backward compatibility with ealier versions of Java. Fixes"
155,155,9.0,0.9843999743461609,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Refactoring externalization of SSLSessions (#383) This is an implementation to This change attempts to provide more consistency to the session that is returned to the caller by `ConscryptEngine`/`ConscryptFileDescriptorSocket`. Main changes: New interface ConscryptSession adds a few methods currently only defined by ActiveSession New interface SessionDecorator that defines getDelegate() New class ProvidedSessionDecorator delegates to an external provider of the ""current"" session. The provider implementations are in ConscryptEngine and ConscryptFileDescriptorSocket. New class SessionSnapshot that takes a snapshot of any ConscryptSession. Changed ActiveSession and SSLNullSession to implement ConscryptSession. Updated ConscryptEngine/ConscryptFileDescriptorSocket to create a SessionSnapshot when closing. Additional cleanup: Split out Java7SessionWrapper into two classes: Java7ExtendedSSLSession and Java8ExtendedSSLSession. The Java 8 version no longer requires reflection and is more consistent with platform-specific code elsewhere. Both classes implement SessionDecorator. Renamed SslWrapper->NativeSsl and SslSessionWrapper->NativeSslSession for clarity, since the term ""wrapper"" was being overloaded. Fixes"
156,156,16.0,0.5569000244140625,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Pass NativeSsl references to NativeCrypto (#408) * Pass NativeSsl references to NativeCrypto The existing implementation of passing raw addresses to NativeCrypto can cause issues where the native code may still be executing when the finalizer runs and frees the underlying native resources. A call to NativeSsl.read(), for instance, is not enough to keep the NativeSsl or its owning socket alive, so if its waiting for input the finalizer can run. Switching to passing the Java object to native code keeps the Java object alive for GC purposes, preventing its finalizer from running. As part of this, also move the freeing of NativeSsl instances into a finalizer on NativeSsl instead of on the sockets. The sockets can still become garbage even if the NativeSsl is kept alive, so we only want to free it when the NativeSsl itself is garbage. We will also want to do this for other native objects, but SSL* instances are by far the most-used native objects and the most likely to be used in a long-running I/O operation, so starting here gives us a lot of benefit. * Reliably close objects in tests. * Pass both pointer and Java reference. This allows us to access the SSL* pointer without having to indirect through the Java objects fields, but still prevents the NativeSsl from being GCed while the method is being run. * Explain unsafe finalization fix in NativeCrypto Javadoc./"
157,157,9.0,0.9843999743461609,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Refactoring externalization of SSLSessions (#383) This is an implementation to This change attempts to provide more consistency to the session that is returned to the caller by `ConscryptEngine`/`ConscryptFileDescriptorSocket`. Main changes: New interface ConscryptSession adds a few methods currently only defined by ActiveSession New interface SessionDecorator that defines getDelegate() New class ProvidedSessionDecorator delegates to an external provider of the ""current"" session. The provider implementations are in ConscryptEngine and ConscryptFileDescriptorSocket. New class SessionSnapshot that takes a snapshot of any ConscryptSession. Changed ActiveSession and SSLNullSession to implement ConscryptSession. Updated ConscryptEngine/ConscryptFileDescriptorSocket to create a SessionSnapshot when closing. Additional cleanup: Split out Java7SessionWrapper into two classes: Java7ExtendedSSLSession and Java8ExtendedSSLSession. The Java 8 version no longer requires reflection and is more consistent with platform-specific code elsewhere. Both classes implement SessionDecorator. Renamed SslWrapper->NativeSsl and SslSessionWrapper->NativeSslSession for clarity, since the term ""wrapper"" was being overloaded. Fixes"
158,158,13.0,0.8568000197410583,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Add compile option for checking error queue. (#416) We need to ensure that the BoringSSL error stack is clear when returning from native functions, otherwise a later function might inspect the error stack and interpret it incorrectly. Adds the compile option CONSCRYPT_CHECK_ERROR_QUEUE which enables the macro CHECK_ERROR_QUEUE_ON_RETURN. That macro, when enabled, creates a class that checks the error queue is empty in its destructor. The macro has been added to almost every native method called from Java, enforcing that the error queue is empty when we return from native code back to Java code. Adds the gradle property checkErrorQueue to enable the code and adds it to the Travis config. Also fixes a couple places found by this checking that we were failing to clear the error queue after handling errors./Clean up exception throwing in native code. (#417) Change throwExceptionIfNecessary to throwExceptionFromBoringSSLError. The definition changes from throwing an exception if theres an error on the stack to having a precondition of having an error on the stack. This makes its behavior and the expected usage more clear (it always results in an exception pending). This also should let us know if were encountering return-failure-but-dont-stack-an-error situations that we didnt know about. Normalize function names to throwFooException. Ensure that we always return immediately after throwing an exception. Some call sites allowed the exception-throwing branch to fall through to the return statement from a non-throwing branch, which is unclear, since that return statement is useless, and runs the risk of additional code being inserted after the exception. Fixes"
159,159,13.0,0.9835000038146973,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Throw SocketException on ERR_SSL_SYSCALL. (#430) SSLSocketTest#test_SSLSocket_interrupt_readWrapperAndCloseUnderlying is failing periodically on our internal continuous builds, and it appears to be happening due to a race condition. The test is testing what happens when an SSLSocket thats wrapping an underlying Socket is blocked on a read and then underlying socket is closed by another thread. There appears to be a race condition between the OS waking up the reading thread and the write of to java.io.FileDescriptors private field. If the reading thread wakes up and proceeds past the check of the file descriptors validity before the field write is visible, then it will attempt to call SSL_read() and get ERR_SSL_SYSCALL, and it responds by returning whereas the test expects SocketException to be thrown (which it does if the file descriptor is invalid). This changes the code to always throw SocketException when ERR_SSL_SYSCALL is reported with a return value of 0, which the BoringSSL docs say happens ""if the transport returned EOF"", which should mean the file descriptor is closed./Finalization safety for SSL_CTX objects. (#427)/Add compile option for checking error queue. (#416) We need to ensure that the BoringSSL error stack is clear when returning from native functions, otherwise a later function might inspect the error stack and interpret it incorrectly. Adds the compile option CONSCRYPT_CHECK_ERROR_QUEUE which enables the macro CHECK_ERROR_QUEUE_ON_RETURN. That macro, when enabled, creates a class that checks the error queue is empty in its destructor. The macro has been added to almost every native method called from Java, enforcing that the error queue is empty when we return from native code back to Java code. Adds the gradle property checkErrorQueue to enable the code and adds it to the Travis config. Also fixes a couple places found by this checking that we were failing to clear the error queue after handling errors./Clean up exception throwing in native code. (#417) Change throwExceptionIfNecessary to throwExceptionFromBoringSSLError. The definition changes from throwing an exception if theres an error on the stack to having a precondition of having an error on the stack. This makes its behavior and the expected usage more clear (it always results in an exception pending). This also should let us know if were encountering return-failure-but-dont-stack-an-error situations that we didnt know about. Normalize function names to throwFooException. Ensure that we always return immediately after throwing an exception. Some call sites allowed the exception-throwing branch to fall through to the return statement from a non-throwing branch, which is unclear, since that return statement is useless, and runs the risk of additional code being inserted after the exception. Fixes unused parameters in native_crypto.cc (#412) Our Android build rules generate errors for unused parameters. We cant enable the warnings in the external build rules because BoringSSL has many unused parameters and we build the two together in the external build./More finalization safety. (#410) This updates OpenSSLX509Certificate and OpenSSLX509CRL in the same way as NativeSsl was done previously./Pass NativeSsl references to NativeCrypto (#408) * Pass NativeSsl references to NativeCrypto The existing implementation of passing raw addresses to NativeCrypto can cause issues where the native code may still be executing when the finalizer runs and frees the underlying native resources. A call to NativeSsl.read(), for instance, is not enough to keep the NativeSsl or its owning socket alive, so if its waiting for input the finalizer can run. Switching to passing the Java object to native code keeps the Java object alive for GC purposes, preventing its finalizer from running. As part of this, also move the freeing of NativeSsl instances into a finalizer on NativeSsl instead of on the sockets. The sockets can still become garbage even if the NativeSsl is kept alive, so we only want to free it when the NativeSsl itself is garbage. We will also want to do this for other native objects, but SSL* instances are by far the most-used native objects and the most likely to be used in a long-running I/O operation, so starting here gives us a lot of benefit. * Reliably close objects in tests. * Pass both pointer and Java reference. This allows us to access the SSL* pointer without having to indirect through the Java objects fields, but still prevents the NativeSsl from being GCed while the method is being run. * Explain unsafe finalization fix in NativeCrypto Javadoc./Fix error detection in RSA_generate_key_ex. (#398) RSA_generate_key_ex returns 1 on success and 0 on failure, so we could never detect failures that happened. Also update an allocation failure to throw OutOfMemoryError instead of RuntimeException./Add support for accessing tls-unique channel binding value. (#388)/Check an X509-like structure submember for nullness. (#380) Weve seen very sporadic crashes due to null pointer dereferencing somewhere inside X509_get_ext_by_critical, and this is the only way I can see that that can happen. X509_get_ext_by_critical passes x->cert_info->extensions to X509v3_get_ext_by_critical, and thats the only pointer that isnt explicitly checked for nullness. These crashes are incredibly rare, so its not out of the realm of possibility for them to be memory corruption or something, but better safe than sorry./"
160,160,13.0,0.9861999750137329,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Clean up exception throwing in native code. (#417) Change throwExceptionIfNecessary to throwExceptionFromBoringSSLError. The definition changes from throwing an exception if theres an error on the stack to having a precondition of having an error on the stack. This makes its behavior and the expected usage more clear (it always results in an exception pending). This also should let us know if were encountering return-failure-but-dont-stack-an-error situations that we didnt know about. Normalize function names to throwFooException. Ensure that we always return immediately after throwing an exception. Some call sites allowed the exception-throwing branch to fall through to the return statement from a non-throwing branch, which is unclear, since that return statement is useless, and runs the risk of additional code being inserted after the exception. Fixes if an exception is pending before throwing another one. (#386) This can occur if a BoringSSL call results in a socket read from a Java socket which throws an exception. Since throwing an exception when another exception is pending causes the process to crash, just let the other exception propagate out of the function./"
161,161,9.0,0.9843999743461609,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Refactoring externalization of SSLSessions (#383) This is an implementation to This change attempts to provide more consistency to the session that is returned to the caller by `ConscryptEngine`/`ConscryptFileDescriptorSocket`. Main changes: New interface ConscryptSession adds a few methods currently only defined by ActiveSession New interface SessionDecorator that defines getDelegate() New class ProvidedSessionDecorator delegates to an external provider of the ""current"" session. The provider implementations are in ConscryptEngine and ConscryptFileDescriptorSocket. New class SessionSnapshot that takes a snapshot of any ConscryptSession. Changed ActiveSession and SSLNullSession to implement ConscryptSession. Updated ConscryptEngine/ConscryptFileDescriptorSocket to create a SessionSnapshot when closing. Additional cleanup: Split out Java7SessionWrapper into two classes: Java7ExtendedSSLSession and Java8ExtendedSSLSession. The Java 8 version no longer requires reflection and is more consistent with platform-specific code elsewhere. Both classes implement SessionDecorator. Renamed SslWrapper->NativeSsl and SslSessionWrapper->NativeSslSession for clarity, since the term ""wrapper"" was being overloaded. Fixes"
162,162,9.0,0.9843999743461609,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Refactoring externalization of SSLSessions (#383) This is an implementation to This change attempts to provide more consistency to the session that is returned to the caller by `ConscryptEngine`/`ConscryptFileDescriptorSocket`. Main changes: New interface ConscryptSession adds a few methods currently only defined by ActiveSession New interface SessionDecorator that defines getDelegate() New class ProvidedSessionDecorator delegates to an external provider of the ""current"" session. The provider implementations are in ConscryptEngine and ConscryptFileDescriptorSocket. New class SessionSnapshot that takes a snapshot of any ConscryptSession. Changed ActiveSession and SSLNullSession to implement ConscryptSession. Updated ConscryptEngine/ConscryptFileDescriptorSocket to create a SessionSnapshot when closing. Additional cleanup: Split out Java7SessionWrapper into two classes: Java7ExtendedSSLSession and Java8ExtendedSSLSession. The Java 8 version no longer requires reflection and is more consistent with platform-specific code elsewhere. Both classes implement SessionDecorator. Renamed SslWrapper->NativeSsl and SslSessionWrapper->NativeSslSession for clarity, since the term ""wrapper"" was being overloaded. Fixes"
163,163,13.0,0.6065999865531921,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Pass NativeSsl references to NativeCrypto (#408) * Pass NativeSsl references to NativeCrypto The existing implementation of passing raw addresses to NativeCrypto can cause issues where the native code may still be executing when the finalizer runs and frees the underlying native resources. A call to NativeSsl.read(), for instance, is not enough to keep the NativeSsl or its owning socket alive, so if its waiting for input the finalizer can run. Switching to passing the Java object to native code keeps the Java object alive for GC purposes, preventing its finalizer from running. As part of this, also move the freeing of NativeSsl instances into a finalizer on NativeSsl instead of on the sockets. The sockets can still become garbage even if the NativeSsl is kept alive, so we only want to free it when the NativeSsl itself is garbage. We will also want to do this for other native objects, but SSL* instances are by far the most-used native objects and the most likely to be used in a long-running I/O operation, so starting here gives us a lot of benefit. * Reliably close objects in tests. * Pass both pointer and Java reference. This allows us to access the SSL* pointer without having to indirect through the Java objects fields, but still prevents the NativeSsl from being GCed while the method is being run. * Explain unsafe finalization fix in NativeCrypto Javadoc./Add support for accessing tls-unique channel binding value. (#388)/"
164,164,9.0,0.9843999743461609,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Refactoring externalization of SSLSessions (#383) This is an implementation to This change attempts to provide more consistency to the session that is returned to the caller by `ConscryptEngine`/`ConscryptFileDescriptorSocket`. Main changes: New interface ConscryptSession adds a few methods currently only defined by ActiveSession New interface SessionDecorator that defines getDelegate() New class ProvidedSessionDecorator delegates to an external provider of the ""current"" session. The provider implementations are in ConscryptEngine and ConscryptFileDescriptorSocket. New class SessionSnapshot that takes a snapshot of any ConscryptSession. Changed ActiveSession and SSLNullSession to implement ConscryptSession. Updated ConscryptEngine/ConscryptFileDescriptorSocket to create a SessionSnapshot when closing. Additional cleanup: Split out Java7SessionWrapper into two classes: Java7ExtendedSSLSession and Java8ExtendedSSLSession. The Java 8 version no longer requires reflection and is more consistent with platform-specific code elsewhere. Both classes implement SessionDecorator. Renamed SslWrapper->NativeSsl and SslSessionWrapper->NativeSslSession for clarity, since the term ""wrapper"" was being overloaded. Fixes"
165,165,14.0,0.9945999979972839,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Add a function to force an engine read. (#453) The SSLEngine implementation sometimes need to prompt BoringSSL to process any incoming TLS data in order to determine whether there is enough data to produce plaintext. We previously were reading into a zero-byte buffer to do this, but that causes ambiguity in return values because a return of 0 from SSL_read() could mean a failure (generally EOF) or could mean a successful read of 0 bytes (which would be expected when reading into a zero-byte array). Instead, use SSL_peek. Fixes struct timevals tv_usec to print as long. (#474) Apparently some Darwin environments have 32-bit suseconds_t, which means we get a warning trying to print it via %ld. Just force whatever we have to be a long, so it always gets printed properly./Add logging macros that work on all platforms. (#462) This adds CONSCRYPT_LOG_X macros that redirect to either ALOG on Android or fprintf(stderr) on non-Android. In the future, we could use these to allow users to register a logging callback and send the logs to a destination of their choice (via java.util.Logger or log4j or what have you), but for now well keep it simple. Fixes support for token binding and EKM. (#445) Token binding allows higher-level protocols to bind their higher-level authentication tokens to their TLS sessions, making it more difficult for attackers to present those higher-level tokens in a future session. At the TLS level, all that needs to occur is a parameter negotiation, the rest of the token binding protocol is left up to the caller. Parameter options are specified as integers, and I decided not to supply constants for the currently-defined values. This is a niche enough use case that any user of it should be able to decide what values they want to support (and will want to share constants with whatever higher-level protocol theyre using with token binding). BoringSSL also doesnt supply constants for these values, so were in good company there. Keying material exporting (aka EKM, for exported keying material) is specified in RFC 5705, and is necessary for implementing token binding as well as other protocols./Parse ASN1_TIME structures in constructors. (#446) The legacy OpenSSL APIs in BoringSSL dont parse ASN1_TIME values until theyre used, which means that the existing code could explode in the middle of X509Certificate.getNotAfter() and similar Date-returning calls, and those calls arent declared to throw anything. Instead, read and cache the values in the constructor, where we can throw a relevant exception if necessary. We have to clone the Date values when returning them because Date is mutable./"
166,166,6.0,0.9761999845504761,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Disallow invalid SNI hostnames in setHostname(). (#470) The code that sets the SNI value on the connection checks for impl.getUseSni() && AddressUtils.isValidSniHostname(hostname) which is good for hostnames that were supplied as the hostname to connect to, since they may or may not be valid for SNI, but means that if you set an invalid hostname with setHostname() it will just silently be omitted from the connection and no SNI extension will be included in the handshake. Better to reject the hostname immediately. Also disallow hostnames with trailing dots, which arent legal SNI hostnames per RFC 6066. Also disallow null bytes./"
167,167,6.0,0.9761999845504761,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Disallow invalid SNI hostnames in setHostname(). (#470) The code that sets the SNI value on the connection checks for impl.getUseSni() && AddressUtils.isValidSniHostname(hostname) which is good for hostnames that were supplied as the hostname to connect to, since they may or may not be valid for SNI, but means that if you set an invalid hostname with setHostname() it will just silently be omitted from the connection and no SNI extension will be included in the handshake. Better to reject the hostname immediately. Also disallow hostnames with trailing dots, which arent legal SNI hostnames per RFC 6066. Also disallow null bytes./"
168,168,2.0,0.5896999835968018,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Update short buffer handling. (#440) This fixes a number of problems in Conscrypts ciphers when they are given an output buffer that is too small for the output. We didnt specifically handle CIPHER_R_BUFFER_TOO_SMALL errors from BoringSSL, so we threw RuntimeException on encountering them. We should be throwing ShortBufferException. Raw ChaCha20 didnt check for a short buffer at all, so it just passed the arrays down to native code, which could cause crashes or other weird behavior. EVP_AEAD ciphers used update() to only record data that needed to be encrypted and then did the actual encrypting in doFinal(). This combined with our implementation that implements doFinal() as a combination of updateInternal() + doFinalInternal() led to a situation where if the buffer passed to doFinal() was too small, the data would get added to the buffer to be encrypted in updateInternal() and then doFinalInternal() call would fail, which would mean a future call to doFinal() with the same data would end up encrypting that data twice. This call pattern is used by some internal CipherSpi methods from OpenJDK, so you could see it in practice even if the caller did nothing wrong. Also add tests around short buffer handling./Calculate output size ourselves for AES/GCM and ChaCha20. (#438) Since both AES/GCM and ChaCha20 dont have any variance in the length of their AEAD tags, we can just calculate the output length directly rather than making a JNI call. This gives better bounds for the necessary buffer size; in particular, we previously werent factoring in the removal of the tag during decryption, so we always demanded an unnecessarily-long buffer./"
169,169,14.0,0.618399977684021,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Disallow invalid SNI hostnames in setHostname(). (#470) The code that sets the SNI value on the connection checks for impl.getUseSni() && AddressUtils.isValidSniHostname(hostname) which is good for hostnames that were supplied as the hostname to connect to, since they may or may not be valid for SNI, but means that if you set an invalid hostname with setHostname() it will just silently be omitted from the connection and no SNI extension will be included in the handshake. Better to reject the hostname immediately. Also disallow hostnames with trailing dots, which arent legal SNI hostnames per RFC 6066. Also disallow null bytes./Add support for token binding and EKM. (#445) Token binding allows higher-level protocols to bind their higher-level authentication tokens to their TLS sessions, making it more difficult for attackers to present those higher-level tokens in a future session. At the TLS level, all that needs to occur is a parameter negotiation, the rest of the token binding protocol is left up to the caller. Parameter options are specified as integers, and I decided not to supply constants for the currently-defined values. This is a niche enough use case that any user of it should be able to decide what values they want to support (and will want to share constants with whatever higher-level protocol theyre using with token binding). BoringSSL also doesnt supply constants for these values, so were in good company there. Keying material exporting (aka EKM, for exported keying material) is specified in RFC 5705, and is necessary for implementing token binding as well as other protocols./"
170,170,3.0,0.9916999936103821,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Support TLS 1.3 sessions. (#515) TLS 1.3 sessions are designed to only be used once, preventing correlation across connections. This implements support for those sessions by removing such sessions from the cache whenever theyre retrieved from the session cache and not sending them to the persistent (filesystem-based) cache, which we currently cannot delete individual items from. Also switch our per-host-port caches to support a list of cached sessions rather than a single one, so that if we get multiple sessions from the peer we can cache them all and use them when establishing multiple connections in parallel or such things./Obey supported_signature_algorithms when present (#521) In TLS 1.2, servers can supply a list of supported signature algorithms, and we should base our key choice on what is supported by the server. In TLS 1.3, this is the only mechanism for supplying the servers requirements, so this is necessary for TLS 1.3 support./Support opaque keys with RSA-PSS signatures. (#513) TLS 1.3 only uses RSA-PSS signatures (rather than RSA-PKCS#1), so we need to support these. Implement it by switching to Cipher.RSA/ECB/NoPadding instead of using Signature.RSA. Fix the opaque key tests so they actually work properly./Stop using set_options to set protocol support. (#500) The *_set_min_protocol_version and *_set_max_protocol_version functions were introduced to be explicit about setting supported versions. Use those functions instead of SSL_set_options with disabling constants. There arent any higher-level tests because this is already covered by tests like SSLSocket.test_SSLSocket_setEnabledProtocols, SSLSocket.test_SSLSocket_noncontiguousProtocols_useLower, etc./"
171,171,7.0,0.9567999839782715,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Merge ConscryptSocketBase and AbstractConscryptSocket (#529) We used to need this separation because we had delegating sockets that inherited from AbstractConscryptSocket but not ConscryptSocketBase, but those are gone now, so we no longer need to have two classes here. While the remaining file is named AbstractConscryptSocket (to parallel AbstractConscryptEngine), what really happened is the abstract methods from AbstractConscryptSocket got merged into ConscryptSocketBase, and then the resulting class was renamed./"
172,172,12.0,0.4830999970436096,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","fix ConscryptEngine.closeInbound should free resources (#511) If closeOutbound is invoked prior to closeInbound, resources should be freed./"
173,173,12.0,0.9805999994277954,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Dont init cipher suites if native code didnt load (#517) The try/catch and setting of loadError prevents the first static initializer in NativeCrypto from rendering the class unloadable, but the second static initializer still does, which means that users tend to get NoClassDefFoundErrors when they try to use Conscrypt instead of a more useful UnsatisfiedLinkError that points to the root cause./Stop using set_options to set protocol support. (#500) The *_set_min_protocol_version and *_set_max_protocol_version functions were introduced to be explicit about setting supported versions. Use those functions instead of SSL_set_options with disabling constants. There arent any higher-level tests because this is already covered by tests like SSLSocket.test_SSLSocket_setEnabledProtocols, SSLSocket.test_SSLSocket_noncontiguousProtocols_useLower, etc./"
174,174,6.0,0.6327000260353088,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Provide TrustManagerFactory (#516) This is necessary for users who want to enable TLS 1.3, since the TrustManagerFactory implementation shipped with OpenJDK throws an exception if it encounters an SSLSocket or SSLEngine thats negotiated TLS 1.3. Use our TrustManager in tests. Also adds a HostnameVerifier in the tests that does the simplest thing, because our TrustManager verifies hostnames by default, whereas the OpenJDK one doesnt, and the bundled HostnameVerifier on OpenJDK just fails to verify anything./Support opaque keys with RSA-PSS signatures. (#513) TLS 1.3 only uses RSA-PSS signatures (rather than RSA-PKCS#1), so we need to support these. Implement it by switching to Cipher.RSA/ECB/NoPadding instead of using Signature.RSA. Fix the opaque key tests so they actually work properly./"
175,175,11.0,0.98580002784729,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Support TLS 1.3 (#524) Enables support for negotiating TLS 1.3. TLS 1.3 is not enabled unless SSLContext.TLSv1.3 is requested or setEnabledProtocols() is called with a set of values that includes TLSv1.3. Detailed changes: Adds protocol constants for TLS 1.3, and provides SSLContext.TLSv1.3, which has TLS 1.3 enabled by default. Adjusts cipher suite code for TLS 1.3 suites. When enabled, all TLS 1.3 cipher suites are always returned from supportedCipherSuites() and enabledCipherSuites(). Attempts to customize TLS 1.3 cipher suites in setEnabledCipherSuites() are ignored. Splits {SSLEngine,SSLSocket}Test into version-dependent and version-independent tests. The latter remain in {SSLEngine,SSLSocket}Test and the former move into new files {SSLEngine,SSLSocket}VersionCompatibilityTest, which are parameterized to test all combinations of client and server on TLS 1.2 and TLS 1.3. Remove a pile of RI-specific TLS-related expectation declarations from StandardNames. We dont actually verify the behavior of the RI at any point, so it was just making the code more confusing. Fixes"
176,176,14.0,0.967199981212616,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Allow d2i_X509_bio and friends to not set an error (#552) BoringSSL recently changed so that d2i_X509_bio doesnt set an error in the error queue when it receives certain garbage inputs. We dont actually care what error is set in the queue (we just end up catching whatever is thrown and throwing CertificateException), so change to tolerate having no error in the queue./"
177,177,12.0,0.9472000002861023,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Add explicit no-arg constructors (#563) Were somewhat inconsistent on whether we include these or not, but we now want these to be present so we can add annotations to them to support Android features. This also makes it more obvious that these classes are publicly constructible./"
178,178,11.0,0.98580002784729,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Support TLS 1.3 (#524) Enables support for negotiating TLS 1.3. TLS 1.3 is not enabled unless SSLContext.TLSv1.3 is requested or setEnabledProtocols() is called with a set of values that includes TLSv1.3. Detailed changes: Adds protocol constants for TLS 1.3, and provides SSLContext.TLSv1.3, which has TLS 1.3 enabled by default. Adjusts cipher suite code for TLS 1.3 suites. When enabled, all TLS 1.3 cipher suites are always returned from supportedCipherSuites() and enabledCipherSuites(). Attempts to customize TLS 1.3 cipher suites in setEnabledCipherSuites() are ignored. Splits {SSLEngine,SSLSocket}Test into version-dependent and version-independent tests. The latter remain in {SSLEngine,SSLSocket}Test and the former move into new files {SSLEngine,SSLSocket}VersionCompatibilityTest, which are parameterized to test all combinations of client and server on TLS 1.2 and TLS 1.3. Remove a pile of RI-specific TLS-related expectation declarations from StandardNames. We dont actually verify the behavior of the RI at any point, so it was just making the code more confusing. Fixes"
179,179,1.0,0.8098999857902527,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Switch preferred SSLContext to TLSv13/
180,180,10.0,0.9634000062942505,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Update Provider installation in tests (#629) Change CertPinManagerTest and CTVerifierTest to only install the provider if its not installed already and check if the provider was installed before uninstalling. Otherwise, when running on the Android platform they will uninstall the platform copy of Conscrypt and break every following test./Drop support for Java 6 (#606)/"
181,181,10.0,0.5249999761581421,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Support updateAAD(ByteBuffer) (#647)/
182,182,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Add server certificate SNI support (#712)/
183,183,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Add server certificate SNI support (#712)/
184,184,10.0,0.6833000183105469,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Run tests on Java 11 as well as 7 and 8 (#668)/
185,185,3.0,0.5249999761581421,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",[dekstop] Using Mp4Demuxer in yt download/
186,186,1.0,0.6833000183105469,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",[desktop] Not using mp4parser for tagging/
187,187,6.0,0.6833000183105469,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[desktop] Updated to jlibtorrent 1.1.0.22/
188,188,4.0,0.6832000017166138,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",[desktop] share from library in a single click./
189,189,6.0,0.6833000183105469,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[desktop] Updated to jlibtorrent 1.1.0.22/
190,190,3.0,0.6610000133514404,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","[desktop] Not using rxjava in search, using old plain listener./"
191,191,6.0,0.8944000005722046,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[alll] Update to jlibtorrent 1.1.0.25/[common] Adding jlibtorrent listener as early as possible/
192,192,6.0,0.6833000183105469,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[all] Update to jlibtorrent 1.1.0.23/
193,193,17.0,0.9405999779701233,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[common] Bullet proofed EZTV search. Now works on android, supports both .torrents and magnets./[common] Fixes EZTV search (Issue"
194,194,1.0,0.8812000155448914,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",[common] username fix for YT/[common] Fixed YT dash extraction/[common] Improved regex for date and player id/
195,195,13.0,0.9049999713897705,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[common] Added parsing of secondary content in YT/[common] Avoid channels in YT search performer/
196,196,17.0,0.9136000275611877,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[common] Bullet proofed EZTV search. Now works on android, supports both .torrents and magnets./"
197,197,6.0,0.7222999930381775,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[android] Updated to jlibtorrent 1.1.0.21 and fixed issues with common changes/[common] Added static API to sc performer to allow for better DRY code reuse/
198,198,19.0,0.7764999866485596,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",[common] Should fix Monova search. Issue (tested only on desktop)/[common] DaysOld calculation does not belong to (abstract)SearchResult since its a used for consideration in particular cases/
199,199,13.0,0.8416000008583069,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[common][fmp4] Refactor for better design and performance/
200,200,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] more player code cleanup./
201,201,8.0,0.7623999714851379,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] more fragment refactors. more player code cleanup./
202,202,17.0,0.9049999713897705,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] this should close (adding album songs to empty playlist) (more cases pending)/
203,203,17.0,0.9366999864578247,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] fixed scrolling issue on profile adapters./[android] feedback when creating new playlist with list of tracks./[android] more player code cleanup./
204,204,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] more player code cleanup./
205,205,17.0,0.9900000095367432,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android/apollo] Bug fix. Now a playlist can be created out of More by artist songs./[android] build 242 Fixes refresh issues adding/removing playlists. Refactors, cleanup./[android] dont call refresh() inside restartLoader. minor optimizations. hunting for bug on favoritefragment./[android] multiple fixes and cleanup. Back arrow on top bar and back button behave the same goBack(). If user is inside an album view, it goes back to the artist profile, otherwise it finishes the task. When an album is removed list of albums is refreshed and songs are removed from recent./[android] fixes issue where last added favorite wasnt displaying. other refresh issues remain./[android] fixes on playlist handling./[android] fix display of recents, refactor on albumLoader./[android] fixes crash on long pressing special playlists./[android] almost ready to merge, cleaning up, fixes./[android] mistery solved, several fixes, lots of logging to find it./[android] more refactors. brought Ref.class to apollo to be able to submit this work to cyanogens project later on./[android] more fragment refactors. player refactor. next batch of fragment refactors and fixes./[android] more player code cleanup./"
206,206,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] more player code cleanup./
207,207,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] more player code cleanup./
208,208,17.0,0.864300012588501,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] player refactor. next batch of fragment refactors and fixes./[android] more player code cleanup./
209,209,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] more fragment refactors. player refactor.
210,210,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] more player code cleanup./
211,211,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] more player code cleanup./
212,212,17.0,0.864300012588501,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] almost ready to merge, cleaning up, fixes./"
213,213,17.0,0.920799970626831,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] this should close (adding album songs to empty playlist) (more cases pending)/[android] more player code cleanup./
214,214,17.0,0.84170001745224,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] typo refactor./[android] fixes on playlist handling./
215,215,17.0,0.944100022315979,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] fix backgrounds of MyMusic search. cleanup, refactors./[android] more refactors. brought Ref.class to apollo to be able to submit this work to cyanogens project later on./[android] more player code cleanup./"
216,216,14.0,0.8416000008583069,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] fix display of recents, refactor on albumLoader./"
217,217,15.0,0.864300012588501,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","[android] mistery solved, several fixes, lots of logging to find it./"
218,218,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] refactors, more fragments working./"
219,219,8.0,0.8416000008583069,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] updated My Music delete dialog/
220,220,17.0,0.8944000005722046,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] wip BasePlaylistDialog/[android] build 242 Fixes refresh issues adding/removing playlists. Refactors, cleanup./"
221,221,8.0,0.977400004863739,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] Fixed show cover on lockscreen is stopped not show is paused from notification show is close notification not show is paused from foreground not show is kill notification and paused- not show/[android] Avoiding NPE related to apollo/[android] Dont crash if we cant get the MODIFY_PHONE_STATE permission. This is only used for remote control from the lock screen./[android] merge from master/[android] player release refactor./[android] refactor. less aggresive locking./[android] NPEs are possible even after checking for non null. Fix./
222,222,17.0,0.8812000155448914,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] fix display of recents, refactor on albumLoader./[android] Loaders refactor./"
223,223,17.0,0.920799970626831,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] mistery solved, several fixes, lots of logging to find it./[android] next batch of fragment refactors and fixes./"
224,224,17.0,0.864300012588501,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] fixed scrolling issue on profile adapters./
225,225,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] more player code cleanup./
226,226,2.0,0.7222999930381775,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","[android] style set up for confirm list dialog/[android] simplification ConfirmListDialog./[android] confirm list dialog not dependent on SearchResult. Refactors adds label that shows how many items have been checked./[android] adding selected checkbox counter and event handlers. wip/Making more fail safe getting the last selected item from results lists/Single selection Mode Already working on dialog menu/[android] work on ConfirmListDialog (checkboxes, radiobuttons)/[android] simplification ConfirmListDialog./[android] confirm list dialog not dependent on SearchResult. Refactors adds label that shows how many items have been checked./[android] adding selected checkbox counter and event handlers. wip/Making more fail safe getting the last selected item from results lists/Single selection Mode Already working on dialog menu/[android] work on ConfirmListDialog (checkboxes, radiobuttons)/[android] Updated to jlibtorrent 1.1.0.21 and fixed issues with common changes/"
227,227,8.0,0.9682999849319458,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] pluggin HandpickedTorrentDialog on more use cases, got it to crash./[android] trying to plug handpick dialog without adding any UI thread code inside TransferManager. Attempting to do it with a torrent fetcher listener object that creates the handpick dialog as soon as it knows about the .torrent byte array./[android] Improved BitTorrent Status display on transfers./"
228,228,3.0,0.49790000915527344,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","[android] style set up for the custom default dialog/[android] pluggin HandpickedTorrentDialog on more use cases, got it to crash./[android] Not using rxjava in search, using old plain listener./"
229,229,14.0,0.9789000153541565,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] Pull-to-refresh the android way/[android] refresh when files removed/added from transfers./[android] Fixes issue update My files headers on file deletion. Updates ""My Files"" header file count when: File is removed from ""My Files"" File is/are removed from ""My Music"", context menu actions. File is removed from Music Player screen during playback. File is/are removed from a finished transfer./[android] Updates file count after files are deleted. Fixes"
230,230,3.0,0.8416000008583069,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",[android] Not using apache FileUtils for scan files/
231,231,3.0,0.7623999714851379,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",[common] Removed DirectoryUtils/
232,232,4.0,0.8812000155448914,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",[android] Making sure temp is created in http downloads/
233,233,3.0,0.8944000005722046,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","[android] Not using rxjava in search, using old plain listener./"
234,234,8.0,0.9735999703407288,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] adding seed action to finished non torrent transfers in transfer list adapter. (untested)/[android] Seed one or more selected files in My Files/[android] Shows descriptive error on Internet dropped (Issue Clears errored cloud transfers./[android] Changed presentation of demuxig progress/[android] DRY refactor/
235,235,12.0,0.9136000275611877,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","[android] brought back size parsing for audio, previews both audio and video results./No british here/"
236,236,8.0,0.5249999761581421,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] updated TermsUseDialog added new dialog_default_scroll xml/
237,237,8.0,0.4968000054359436,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] copyright fixes. DRY refactor with dlg.setStyle()/[android] updated ui for the onLastDialog and onShutdownDialog/
238,238,8.0,0.9682999849319458,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] updated TermsUseDialog added new dialog_default_scroll xml/[android] updated ui for the onLastDialog and onShutdownDialog/[android] search and other torrent downloads now support partial download dialog. edge cases to fix./[android] trying to plug handpick dialog without adding any UI thread code inside TransferManager. Attempting to do it with a torrent fetcher listener object that creates the handpick dialog as soon as it knows about the .torrent byte array./
239,239,15.0,0.9567999839782715,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] support for single selection mode./[android] confirm list dialog not dependent on SearchResult. Refactors selection Mode Already working on dialog menu/[android] confirm list dialog not dependent on SearchResult. Refactors
240,240,7.0,0.6439999938011169,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[desktop] only show transfers tab after user has started partial download dialog. Still need to work on some kind of temporary feedback as torrent fetching might take a few seconds, specially in the case of magnet search results./"
241,241,15.0,0.9136000275611877,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[desktop] update filters when cloud transfers are seeded. added focus listener to text filter. updated changelog to include update about translations./
242,242,15.0,0.9797999858856201,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","[desktop] When transfers are started we make sure ALL transfers are shown. I attempted to show transfers on their downloading or seeding state depending on what I thought they should be, but this automatic picking of the filter is a terrible experience as transfers can change state rapidly and then the user may think they didnt donwload or theyre gone. Instead, whenever we depend on an automatic switch to the transfers tab its better to leave the experience as it used to be, show all. If the user wants to filter transfers this must be a voluntary action. The experience is much nicer./"
243,243,15.0,0.9801999926567078,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","[desktop] cleanup/[desktop] When transfers are started we make sure ALL transfers are shown. I attempted to show transfers on their downloading or seeding state depending on what I thought they should be, but this automatic picking of the filter is a terrible experience as transfers can change state rapidly and then the user may think they didnt donwload or theyre gone. Instead, whenever we depend on an automatic switch to the transfers tab its better to leave the experience as it used to be, show all. If the user wants to filter transfers this must be a voluntary action. The experience is much nicer./"
244,244,19.0,0.6761999726295471,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","[common] Another NPE fix due to bad timing, in need of refactor/[common] Fixed NPE but the main reason is bad timing in loading (needs refactor)/"
245,245,6.0,0.8416000008583069,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[common] No more populateDynamicTrashChecker/[common] further improvements on eztv parsing/
246,246,14.0,0.920799970626831,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[common] Fixes YT search issues/[common] fix some yt regex/[common] YT preliminary html offset finder was broken after updates./
247,247,17.0,0.762499988079071,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[common] Fixes Torlock Search/
248,248,2.0,0.949999988079071,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Everyting was ""ok"" in the View. The problem lay with no listener that would change the adapter (and data in system). Problem with the adapter is that it uses a copy of the data other than the one in ArryAdaper (why? 0.o) so that copy also has to be updated./"
249,249,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] apollo cleanup/
250,250,8.0,0.977400004863739,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] fixed missing remote control update logic path/[android] no need of duplicated Ref class/[android] avoid NPE/[android] reverted ""true"" argument in stop playback, notification was not working properly/[android] NPE fix reported in console/[android] try to stop and dismiss music faster. avoid issue of music that stopped coming back on next session./[android] Using last service id for stopSelf in MusicPlaybackService/"
251,251,18.0,0.6216999888420105,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",[android] default waterfall shortcodes/[android] added missing support frostwire logic for plus/
252,252,1.0,0.8416000008583069,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",[android] Added retry logic to ImageLoader. Using it in FileListAdapter/
253,253,5.0,0.8812000155448914,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",[android] Added swipe detector on search. TO-DO: Add it on panel that shows while no results are in./
254,254,8.0,0.9269000291824341,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] changelog/[android] turned off ui-debug flag./[android] refactored Software Update dialog to extend AbstractDialog./[android] SoftwareUpdater dialog bullets working./
255,255,16.0,0.8812000155448914,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",[android] pass magnet url if you have it to add it to torrent_params so that the download after the magnet download can start faster./
256,256,9.0,0.8416000008583069,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","[android] Now, starting from Lollipop, everyone uses the same media scanning method/"
257,257,10.0,0.5163999795913696,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","[android] SeedAction class refactor/[android] crash reported in console/[android] pass magnet url if you have it to add it to torrent_params so that the download after the magnet download can start faster./[android] re-implemented ShowNoWifiInformationDialog by extending AbstractDialog./[android] updated NoWifi dialog, added ok button for easy dismissal/[android] temporarily disable Seed action for single files if save path is SD Card./[android] ResumeDownloadMenuAction no longer doubles up for Seeding. Now Seeding action also supports BittorrentDownloads in constructor and has a new dialog to turn on BitTorrent in case its turned off./"
258,258,17.0,0.5468999743461609,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] use .newInstance()/[android] Rename File action dialog refactor./
259,259,4.0,0.864300012588501,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",[android] refactored Delete files menu action dialog./
260,260,15.0,0.4903999865055084,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] refactored Cancel Menu Ation dialogs to extend AbstractDialog./[android] Removed redundant DownloadTransfer/[android] Refactor to use Transfer from common/
261,261,17.0,0.9049999713897705,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] Playlist creation dialog brought up to speed./[android] set up an onClickListener class for the CreateNewPlaylistMenuAction Dialog/
262,262,15.0,0.9847000241279602,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] Closes more descriptive error on Connection timed out for cloud transfers./[android] UI refresh optimization Minor formatting/[android] ResumeDownloadMenuAction no longer doubles up for Seeding. Now Seeding action also supports BittorrentDownloads in constructor and has a new dialog to turn on BitTorrent in case its turned off./[android] Coded UI scanning status/[android] Using new ui YouTubedDownload/[common] HttpDownload DRY refactor/[android] Added new transfer states/[android] Refactor preparing to remove ui BittorrentDownload/[android] Refactor to remove payment options from ui BittorrentDownload/[android] Moved formatting out of model/[android] Removed UI isPausable method/[android] Removed redundant DownloadTransfer/[android] Refactor to use Transfer from common/[android] Using common TransferState directly in the UI/
263,263,16.0,0.9366999864578247,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",[android] pass magnet url if you have it to add it to torrent_params so that the download after the magnet download can start faster./[android] make sure handpicked torrent download happens on the background./
264,264,17.0,0.5091000199317932,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] better handling of action view intent (minor issue left with second open)/[android] fixes bug where torrent download dialog would keep coming back./[android] Refactor, moved adnetworks classes to offers/[android] fixed refresh of player notifier, build 263/[android] merge/[android] Fixed logic for SD permission check from MainActivity/[android] Only check SD lost permissions if in SAF/[android] Added dialog in case storage path is inaccessible/"
265,265,17.0,0.6456000208854675,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] fixed refresh of player notifier, build 263/"
266,266,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] WIP on a NumberPickerPreference/
267,267,13.0,0.8256999850273132,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Moving permanent notification updating to EngineService/[android] Restored ugly hack of killProcess due to some remaining HTTP connections/[android] Enable/disable receiver. Removed ugly hack of killProcess/[android] Improved okhttp shutdown process/
268,268,10.0,0.8416000008583069,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",[android] Updated android billing helper framework/
269,269,15.0,0.43720000982284546,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[all] restore parse of magnet peers parameter/
270,270,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","[all] after so much time, it does not make sense to have a dedicated package for the single class Logger/"
271,271,14.0,0.9136000275611877,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] cleanup/[desktop] support for build based updates. cleanup./[desktop] accommodated InstallerUpdater to new jlibtorrent/
272,272,14.0,0.8944000005722046,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] preparing update message reader to parse build number/
273,273,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[all] refactor to easy the path for jlibtorrent 1.2/
274,274,16.0,0.8944000005722046,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",[desktop] GUIMediator cleanup/[desktop] fixes bug where transfers wouldnt be shown upon a download starting./
275,275,13.0,0.970300018787384,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[common] added PEER_LOG logging in BTEngine/[all] restore parse of magnet peers parameter/[common] using new jlibtorrent API for load settings (fix log setting)/update to jlibtorrent 1.2.0.0-beta1/[common] support jlibtorrent log alerts in BTEngine/[all] update to jlibtorrent 1.1.1.39 with local jlibtorrent 1.2 API layer/[all] final refactor to new BTEngine/[all] more on BTEngine simplification/[common] BTEngine cleanup/
276,276,12.0,0.944100022315979,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",[common] improved getProgress calculation/[common] avoid possible NPE/[all] using new TrasferState from jlibtorrent2 proxy layer/[common] more changes related to jlibtorren2/
277,277,14.0,0.6833000183105469,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[Common] Fixed Monova Search Results/
278,278,17.0,0.8944000005722046,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] More edge cases and better shutdown. MusicUtils.mService renamed to MusicUtils.musicPlaybackServices Fixed log entry on Offers.dismissAndOrShutdownIfNecessary Cleanup/
279,279,9.0,0.9603999853134155,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","[android] new opt-out haptic feedback support on some UI actions/[android] More edge cases and better shutdown. MusicUtils.mService renamed to MusicUtils.musicPlaybackServices Fixed log entry on Offers.dismissAndOrShutdownIfNecessary Cleanup/[android] refactor to delete old swipe detector, no more hacks for proper gesture detection/"
280,280,17.0,0.8944000005722046,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] More edge cases and better shutdown. MusicUtils.mService renamed to MusicUtils.musicPlaybackServices Fixed log entry on Offers.dismissAndOrShutdownIfNecessary Cleanup/
281,281,17.0,0.8944000005722046,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] More edge cases and better shutdown. MusicUtils.mService renamed to MusicUtils.musicPlaybackServices Fixed log entry on Offers.dismissAndOrShutdownIfNecessary Cleanup/
282,282,17.0,0.6833000183105469,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] cleanup/
283,283,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] NPE fix on DragSortListView.measureItemAndGetHeights()/
284,284,17.0,0.9868000149726868,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] fix crash on MusicPlaybackService.getQueue() when the playlist is null, import cleanup/[android] dont crash if you cant register the apollo remote control client/[android] fixes premature music service stop during music playback, refactor, much better shutdown./[android] More edge cases and better shutdown. MusicUtils.mService renamed to MusicUtils.musicPlaybackServices Fixed log entry on Offers.dismissAndOrShutdownIfNecessary Cleanup/[android] Fixes auto-restarting on exit issue. the issue was MusicPlaybackServices shutdown logic. The way this service shuts itself down is by receiving a delayed PendingIntent (this is a way to start a service at a later time even if the app isnt running) via Androids AlarmManager. This shutdown was being scheduled in 60 seconds. If the service is started normally with Context.startService() its onStartCommand() method returns START_STICKY, but if its started with the special SHUTDOWN command, it checks if theres anything playing, and if its not playing, it releases everything and returns START_NOT_STICKY./[android] more cleanup and formatting/[android] rminor cleanup and formatting/"
285,285,15.0,0.9049999713897705,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] music playback isnt interrupted when video ad is displayed/[android] TOS dialog replaced for checkbox on wizard. Other refactors and cleanup./
286,286,8.0,0.9682999849319458,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[all] refactor to easy the path for jlibtorrent 1.2/[android] MobFoxAdnetwork tells its listener when not to act on interstitial closed/[android] make sure InMobi initialization happens on the UI thread (when invoked from the software udpater thread)/[android] refactored AdNetworks. Encapsulated logic to enable/disable networks. Created Offers.AdNetworkHelper class to reduce code repetition among networks by using composition./
287,287,15.0,0.9405999779701233,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] refactored AdNetworks. Encapsulated logic to enable/disable networks. Created Offers.AdNetworkHelper class to reduce code repetition among networks by using composition./
288,288,15.0,0.9405999779701233,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] refactored AdNetworks. Encapsulated logic to enable/disable networks. Created Offers.AdNetworkHelper class to reduce code repetition among networks by using composition./
289,289,9.0,0.864300012588501,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",[android] new opt-out haptic feedback support on some UI actions/
290,290,0.0,0.8944000005722046,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","[all] after so much time, it does not make sense to have a dedicated package for the single class Logger/"
291,291,0.0,0.7577000260353088,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","[android] lint cleanup/[all] after so much time, it does not make sense to have a dedicated package for the single class Logger/"
292,292,17.0,0.6833000183105469,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] code cleanup/
293,293,9.0,0.8416000008583069,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",[android] more haptic feedback on transfers. Build 307/[all] removed MagnetUriBuilder/
294,294,1.0,0.4652000069618225,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",[all] restore parse of magnet peers parameter/
295,295,8.0,0.9768000245094299,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] avoid double finish/[android] MobFoxAdnetwork tells its listener when not to act on interstitial closed/[android] finish or shutdown on dismiss DRY refactor All ad networks reuse logic to handle dismiss or shutdown, fixing possible case where app wouldnt be shutdown on one of the networks. Future proofing improvement on Offers.getActiveNetworks() for new network codes or configuration typos./[android] TOS dialog replaced for checkbox on wizard. Other refactors and cleanup./"
296,296,17.0,0.7092999815940857,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] build 301. waitWhileServicesAreRunning now includes a timeout parameter and a variadic interface/[android] More edge cases and better shutdown. MusicUtils.mService renamed to MusicUtils.musicPlaybackServices Fixed log entry on Offers.dismissAndOrShutdownIfNecessary Cleanup/[android] Fixes auto-restarting on exit issue. the issue was MusicPlaybackServices shutdown logic. The way this service shuts itself down is by receiving a delayed PendingIntent (this is a way to start a service at a later time even if the app isnt running) via Androids AlarmManager. This shutdown was being scheduled in 60 seconds. If the service is started normally with Context.startService() its onStartCommand() method returns START_STICKY, but if its started with the special SHUTDOWN command, it checks if theres anything playing, and if its not playing, it releases everything and returns START_NOT_STICKY./[android] MainActivity::finish() override refactor/[android] NPE opening drawer too quickly java.lang.IllegalArgumentException: No drawer view found with gravity LEFT at android.support.v4.widget.DrawerLayout.openDrawer(DrawerLayout.java:1618) at android.support.v4.app.ActionBarDrawerToggle.onOptionsItemSelected(ActionBarDrawerToggle.java:409) at com.frostwire.android.gui.activities.MainActivity.onOptionsItemSelected(MainActivity.java:804)/[android] ad_menuItem showing when menu_player visibility is set to gone/[all] after so much time, it does not make sense to have a dedicated package for the single class Logger/"
297,297,9.0,0.864300012588501,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",[android] new opt-out haptic feedback support on some UI actions/
298,298,17.0,0.9817000031471252,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] More edge cases and better shutdown. MusicUtils.mService renamed to MusicUtils.musicPlaybackServices Fixed log entry on Offers.dismissAndOrShutdownIfNecessary Cleanup/[android] Fixes auto-restarting on exit issue. the issue was MusicPlaybackServices shutdown logic. The way this service shuts itself down is by receiving a delayed PendingIntent (this is a way to start a service at a later time even if the app isnt running) via Androids AlarmManager. This shutdown was being scheduled in 60 seconds. If the service is started normally with Context.startService() its onStartCommand() method returns START_STICKY, but if its started with the special SHUTDOWN command, it checks if theres anything playing, and if its not playing, it releases everything and returns START_NOT_STICKY./"
299,299,17.0,0.5249999761581421,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[desktop] azureus folder code cleanup/
300,300,17.0,0.762499988079071,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[desktop] DRY abstraction refactor of all Http BTDownloads, cleanup./"
301,301,16.0,0.8944000005722046,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","[desktop] library DnD 2016fication tyding up as I study and remember how this all worked, might refactor into less transfer handlers in the future/"
302,302,12.0,0.5846999883651733,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","[desktop] Playlist explorer contextual popup menu refactor No need to keep pre-built pop up menues in memory, just keep the actions to build menus New methods build up menues and receive the underlying playlist to reduce the number of available actions to a minimum Create New Playlist item has right click menu refactored code to 2016 style cleanup and code compacting/"
303,303,1.0,0.8098999857902527,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",[desktop] Make MPlayerInstance independent from azureus code/[desktop] MPlayerInstance cleanup/
304,304,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[desktop] more azureus cleanup/
305,305,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[common] fixed YT regex/
306,306,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[common] fixed limetorrents details regex/
307,307,15.0,0.7623000144958496,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[common] fixed SC search/
308,308,12.0,0.9320999979972839,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",[common] fixed monova engine using the http torrent url/[common] added unit test for monova and partial fix to search (still not working)/
309,309,13.0,0.6388000249862671,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[android] MoPub progress Added close ad button and handling logic. Does not load the ad if user disabled ads. NPE onDestroy fixed. Refactor on AudioPlayerActivity::PlaybackStatus Avoid temporary crashes while the landscape mode isnt finished./[android] make sure unit is destroyed on onDestroy()/[android] unit loads. onclick doesnt do much. layout and closing button needed/[android] hides removeAds after a purchase/[android/WIP] getting things ready on Music Player activity/
310,310,13.0,0.9848999977111816,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","[android] asynchronous notification update (#262)/[android] moving updating of RemoteControlClient to asynchronous task due to bitmap operations (#260)/[android] Allow simple playback of ringtones without adversly affecting the users current playback and incidental stop/play status fix for file lists (affects both ringtone and music playback): something is playing and a ringtone is to be played the current played music will be paused a ringtone is playing and music playback is resumed/started then the ringtone will be stopped one ringtone can be played at the same time. Starting new playback will stop the old one. a ringtone that is being played will stop the playback out of the fragment or changing the tab carusel will stop playback of a ringtone. (User can no longer see the stop button, so its stopped for him)/"
311,311,14.0,0.8944000005722046,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] asynchronous blurring (#263) * [android] asynchronous blurring and improved ImageLoader API/
312,312,17.0,0.7623999714851379,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android/WIP] more on AdNetworks abstraction/[android] WIP more on AbstractAdNetwork refactor/
313,313,13.0,0.5026999711990356,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","[android/WIP] more on AdNetworks abstraction/[android] More AppLovinAdNetwork cleanup/[android] WIP, AdNetworks abstraction refactor, starting, stopping and all common logic moved up to new AbstractAdNetwork/"
314,314,17.0,0.5249999761581421,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] WIP more on AbstractAdNetwork refactor/
315,315,17.0,0.9897000193595886,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] InMobi/Offers maintenance after 0 impressions from InMobi Moved InMobi constant to InMobiAdNetwork UIUtils.inUIThread() new convenience method InMobi checks/flags to avoid double interstitial loading Offer checks to avoid too rapid network re-initialization Raised InMobi reload period from 20secs to 60secs InMobiInterstitialListener now implements InMobiInterstitial.InterstitialAdListener2, as InMobiInterstitial.InterstitialAdListener is now deprecated and will be phased out soon InMobi InterstitialReloader avoids double interstitial loading It seems InMobi changed account parameters, everything should be working but error says to contact partner. I believe there was a conflict between MoPub and InMobi and thats why we were getting the double loading of InMobi interstitials, most likely will remove InMobi manual integration next (while leaving the libraries for MoPub to use)/[android] refactor, cleanup/[android] no more broadcast receiver leaks, improved abstractions, brainfart refactors/[android] MoPub interstitial integration, ads not displaying yet./[android] boolean master :)/[android/WIP] more on AdNetworks abstraction/[android] WIP more on AbstractAdNetwork refactor/[android] WIP, AdNetworks abstraction refactor, starting, stopping and all common logic moved up to new AbstractAdNetwork/[android] added placement parameter that individual networks can map out of to obtain the right ad ids if placements are supported/[android/WIP] getting things ready on Music Player activity/"
316,316,14.0,0.8944000005722046,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] asynchronous blurring (#263) * [android] asynchronous blurring and improved ImageLoader API/
317,317,14.0,0.9735999703407288,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] better experience switching to other media types/[android] hide everything/[android] improved scroll direction detection/[android] keyword filters available manually on search/[android] new ListViewScrollDirectionDetector, installed on SearchFragments ListView/[android] no special offers in horizontal mode. fix number of slides when in landscape mode to always have an even number/"
318,318,13.0,0.9864000082015991,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","[android] Allow simple playback of ringtones without adversly affecting the users current playback and incidental stop/play status fix for file lists (affects both ringtone and music playback): something is playing and a ringtone is to be played the current played music will be paused a ringtone is playing and music playback is resumed/started then the ringtone will be stopped one ringtone can be played at the same time. Starting new playback will stop the old one. a ringtone that is being played will stop the playback out of the fragment or changing the tab carusel will stop playback of a ringtone. (User can no longer see the stop button, so its stopped for him)/[android] Fix for ringtone item selection and context menu the ability to check multiple ringtones by hiding the checkboxes there are no items for the context menu dont show it menu ignores other checked files (always can play and set as ringtone)/"
319,319,8.0,0.9524999856948853,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] feature/mobile data protection fix (#338) * [android] check if torrents should be resumed when on mobile data and mobile data saving is on * [android] seeding while on mobile data saving and no wifi/
320,320,13.0,0.6240000128746033,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[android/crash] fixes index out of bounds crash/
321,321,19.0,0.8235999941825867,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",[android] make checked listener in abstract list adapter operate on items instead of views. (#302) * [android] make checked listener in abstract list adapter operate on items instead of views. * [android] restore compundbutton info in listener callback/
322,322,5.0,0.7746000289916992,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",[android] added placement parameter that individual networks can map out of to obtain the right ad ids if placements are supported/[android] handle logic post video preview on MainActivity.onActivityResult()/
323,323,8.0,0.6833000183105469,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] PreviewPlayerActivity using AbstractActivity2/
324,324,12.0,0.9761999845504761,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",[android] make checked listener in abstract list adapter operate on items instead of views. (#302) * [android] make checked listener in abstract list adapter operate on items instead of views. * [android] restore compundbutton info in listener callback/[android] Fix for ringtone item selection and context menu the ability to check multiple ringtones by hiding the checkboxes there are no items for the context menu dont show it menu ignores other checked files (always can play and set as ringtone)/
325,325,6.0,0.5249999761581421,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[android] avoid NPE on SwipeLayout::dispatchTouchEvent. build 327/
326,326,12.0,0.9524999856948853,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",[android] Fix for ringtone item selection and context menu the ability to check multiple ringtones by hiding the checkboxes there are no items for the context menu dont show it menu ignores other checked files (always can play and set as ringtone)/
327,327,17.0,0.6833000183105469,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[desktop] lyrics support on playlists/
328,328,13.0,0.8416000008583069,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","[android] do not stop btEngine, the app never assumes btEngine is stopped./[desktop] VPN-drop guard/"
329,329,7.0,0.5626999735832214,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test",[desktop] New: Options > Advanced > Experimental/[desktop] WIP inform the user in statusbar/
330,330,19.0,0.5249999761581421,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",[desktop] WIP inform the user in statusbar/
331,331,8.0,0.8416000008583069,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] feature/delete torrent on download (#334)/
332,332,1.0,0.6830999851226807,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",[common] fixed YT player regex/[common] fixed YT/
333,333,13.0,0.5249999761581421,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[common] fix SC/
334,334,0.0,0.8100000023841858,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[common] TorLock search fixed/
335,335,5.0,0.7623999714851379,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",[common] monova filtering adjustments/
336,336,8.0,0.762499988079071,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] refactor in apollo fragments to use native components/
337,337,18.0,0.5367000102996826,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",[android] consistent layouts for recent tab (#374)/[android] refactored hierarchy design of AlbumFragment/
338,338,15.0,0.920799970626831,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] Enable icon display in toolbars overflow menu This is a refactor of a reflection hack found as a solution. See (never merged)/[android] menu refactor for compatibility with appcompat library/
339,339,14.0,0.49410000443458557,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] Enable icon display in toolbars overflow menu This is a refactor of a reflection hack found as a solution. See (never merged)/[android] removed ThemeUtils/[android] cleanup refactor in AudioPlayerActivity/[android] implementing AudioPlayerActivity with AbstractActivity/
340,340,15.0,0.9320999979972839,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] Enable icon display in toolbars overflow menu This is a refactor of a reflection hack found as a solution. See (never merged)/[android] implemented apollo BaseActivity with new AbstractActivity/
341,341,5.0,0.762499988079071,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",[android] apollo source code cleanup and formatting/
342,342,11.0,0.7623999714851379,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",[android] implemented apollo BaseActivity with new AbstractActivity/[android] implementing apollo SearchActivity with new AbstractActivity/
343,343,10.0,0.4271000027656555,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",[android] not using ThemeUtils in ImageWorker/
344,344,17.0,0.5249999761581421,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] cleanup refactor in ShuffleButton/
345,345,17.0,0.5249999761581421,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] cleanup refactor in RepeatButton/
346,346,10.0,0.45730000734329224,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",[android] dont crash if mopub couldnt load interstitial. It may happen that MoPubs network request queue cant be started because context.getCacheDir() might return null at the time of the call. See
347,347,13.0,0.9587000012397766,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","[android] post review cleanup ImageLoader revert/removal of callback passing method. AbstractListAdapter.CheckboxOnCheckedChangeListener onPostCheckedChange left over code removal. CheckableImageView does not load images in constructor, adds new loadImages() method to load images on demand by class user. CheckableImageView constructor signature refactor (Supports width, height, image uris as a array, reordering of parameters). New lines./"
348,348,14.0,0.9049999713897705,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] restored lost feature of DHT peers in transfers status bar/
349,349,1.0,0.8641999959945679,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",[android] apply distraction free setting without restarting session/
350,350,14.0,0.9848999977111816,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] GridView for My Files Images and Audio (Issue MyFiles new action toolbar on selection mode (#413) Brings out action mode on long press styled ActionMode toolbar Handles visibility of menu action depending on different factors like, number of files checked, file type, SAF Bug fix: When seeding was only enabled for WiFi, the file would still be added to transfers in Finished state. Now it doesnt add the file and it stays in the same screen as it was originally./[android] remove SupressLint warning on inflate. Also, make sure all checkboxes are unchecked when going into selection mode from long press./[android] Long clicking on Files in My Files toggles selection mode/[android] My Files > Action Bar updates Done in one day. Way to go./"
351,351,14.0,0.9546999931335449,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] UIUtils.openFile(..., boolean useFileProvider) To solve issue of FrostWire not knowing how to handle its own .apk update. Works on Android 7.0, need to test on Android 4.0, 5.0 and 6.0/[android] Old menu replaced with Material Design menu/"
352,352,9.0,0.864300012588501,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",[android] ad placement invocation adjustment to single unit/
353,353,9.0,0.9269000291824341,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",[android] feature/dont start bittorrent without vpn (#343) new UIUtils.showDismissableMessage/[android] feature/delete torrent on download (#334)/
354,354,9.0,0.864300012588501,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",[android] ad placement invocation adjustment to single unit/
355,355,9.0,0.864300012588501,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",[android] ad placement invocation adjustment to single unit/
356,356,4.0,0.6610999703407288,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","[android] ad placement invocation adjustment to single unit/[android] MainActivity/NavigationMenu refactors. NavigationMenu moved to com.frostwire.android.gui/activities.internal package. Removed unnecessary circular references to NavigationMenu on MenuDrawerToggles constructor MenuDrawerToggle no longer static so it can directly access the NavigationMenu instance that creates it More fields in NavigationMenu now final/[android] NavigationMenu extracted out of MainActivity. MainController now works with weak reference to MainActivity and enhances its functionality. Cleanup and refactoring of MainActivity, zero warnings./[android] Old menu replaced with Material Design menu/[android] better control of toolbar custom view gravity and fixed visibility issue/[android] abstracted feature of custom view in main toolbar/[android] formatting and import cleanup/"
357,357,14.0,0.967199981212616,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] InHouseAds integrated with PreviewPlayerActivity/[android] Fixes Broadcast receiver leaked error on PreviewActivity/[android] updated preview activity layouts/[android] added toggle fullscreen button, layout cleanup, lots of details/[android] more on preview activity, unregister broadcast listener on destroy, remote config value/[android] preview screen mopub integration/"
358,358,14.0,0.9269000291824341,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] GridView for My Files Images and Audio (Issue Long clicking on Files in My Files toggles selection mode/
359,359,17.0,0.9567999839782715,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] ensures creation of ConfigurationManager in onStartCommand/[android] return START_NOT_STICKY instead of 0 in onStartCommand/[android] disable presage service when shutdown (testing)/[android] method name refactor/[android] better shutdown of EngineService via intent, to allow for use of START_NOT_STICKY/[android] debug output shutting down/"
360,360,13.0,0.6830999851226807,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[desktop] avoid UI thread violation/
361,361,17.0,0.5238000154495239,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[desktop] code formatting, minor refactor, headers/"
362,362,9.0,0.864300012588501,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",[common] avoid re-download of torrent is already complete/
363,363,13.0,0.9405999779701233,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","[android] When Queue is cleared... Dont close the activity Clear the queue, refresh the adapter If a song was being played, play it after the queue has been cleared, this means the song has to be restarted (I tried using the seek method to resume playback wherever it was but this didnt work)/"
364,364,17.0,0.9907000064849854,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] avoid ANR adding to recently played/[android] avoid possible ANR by android.app.AlarmManager.cancel()/[android] NPE on MusicPlaybackService/[android] might aswell do these in the background too/[android] anti-ANR MusicPlaybackService refactors Addresses tickets and Operations like .openXXX() and .duration() may take a long time and cause ANRs if invoked in the main thread. The code has been refactored to use callbacks to the main thread and avoid the numerous ANR reports weve been getting in the google play console Also fixes a possible NPE on ensurePlayListCapacity/[android] possible NPE on a fast start and shutdown averted/[android] minor typos/[android] attempt to recover from IllegalStateException in MusicPlaybackService#getAlbumId/[android] mitigating another NPE in MusicPlaybackService#onDestroy (documented the possible reason of mShutdownIntent being null)/[android] mitigating another NPE in MusicPlaybackService#scheduleDelayedShutdown due to excessive (and bad) use of mutable state in player/[android] log StaleDataException error and return false in MusicPlaybackService#openFile/[android] log UnsupportedOperationException error and return false in MusicPlaybackService#openFile/[android] MusicPlaybackService possible Runtime Exception submitting task/[android] fixed NPE in releaseServiceUiAndStop/[android] fixed issue if visible mini player when existing the app/[android] avoid ANR, dont lock whole service object to get audioSessionId/[android] MusicPlaybackService initiation was causing ANRs. Moved initService() to a background thread The app starts way faster now/[android] possible NPE on RemoteControlClient::MetadataEditor.apply()/"
365,365,8.0,0.8481000065803528,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] one more context leak check in ImageLoader/[android] using generic ImageLoader#load method with parameters in image viewer/[android] implemented ImageLoader#load with retry using the common load/[android] using Params class in ImageLoader#load to mitigate the type-less interchangeable parameters anti-pattern with overloads/[android] ImageViewer simplifications. ImageLoader DEBUG_ERRORS flag/[android] My Files thumbnails and ImageViewer optimizations. Build 382 Build 382 File List Adapter thumbnails resolution increased from 128 pixels to 256 pixels Thumbnail loading now done using new loadBitmapAsync function (High priority, and disk cached) ImageViewer 3 image loading strategy now calls postInvalidate on Bitmap load success ImageViewers first low resolution image is now a 20th of the screens resolution ImageViewers second loaded image now uses Picassos fit() function to resize exactly to target TouchImageView ImageViewers images dont make use of the disk cache, thus not evicting thumbnails on My Files ImageViewers disk cache sizes raised from [5MB, 50MB] to closest powers of two interval [8MB, 64MB]/[android] image viewer fragment refactor, different image loading strategy test/[android] fixes bug loading high res images/[android] ImageViewer fragment (#437) * [android] ImageViewer activity, Video/Pic grid improvements * [android] Fixes selection mode issues on grids, Media Overlay View, Pinch and Zoom * [android] ads metadata to image viewer fragment * [android] set wallpaper action non-blocking * [android] ImageLoader asyncload wraps fetch() to use Picassos background threads * [android] New FileInformation action shows file information dialog * [android] add preview screen listener * [android] Cleanup and bug fixes. Fixes bug on ImageViewerFragment where after renaming an image and rotating the screen, the window title would revert to the original file name. Cleanup, license headers./"
366,366,15.0,0.9682999849319458,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","[android] ignore HTTP transfers for resume all menu/[android] fix of menu logic in TransfersFragment/[android] transfers toolbar update (#470) * [android] code reformat, missing /n * [android] moved the transfers menu dialog to toolbar menu to keep consistent throughout the app replaced the menu in transfers that opened by clicking on three dots on the toolbar that opened a dialog to an android menu. Also turned the + button on the toolbar into a menuItem. * [android] added new add_transfer string/[android] Transfers tab layout refactor. cleanup, formatting/"
367,367,14.0,0.9937000274658203,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] Refresh suggested tags when tags removed. It will no longer skip these UI updates based on rate, instead of aborting we: Make sure were not on the UI thread Wait enough to make each succesive update at least 500ms Also refactored the checkbox menu item visibility checks into a method on MyFilesFragment/[android] avoid NPE on rotations if fragments still not attached/[android] bug fix on filtered results. more on keywordtagview event handling there is still a bug when trying to change inclusion mode for a second tag, works fine with the first one only/[android] improved filtering. less aggresive filter() calling/[android] simpler logic to show filter button. SearchResultListAdapter.filter() upgrade We were getting inconsistent counts because we were only seeing the counts for the selected media type filtered search results. Now SearchResultListAdapter.filter() feeds its FilteredSearchResults with a new list that contains results matching the current keyword pipeline, regardless of their media type/[android] filter button visibility. KeywordTags still not inflating correctly/[android] keyword filters refactors/simplifications. UX upgrades, more optimizations needed/[android] recalculate suggested keywords when filter is applied/[android] KeywordDetector simplifications, cpu savings, animation stops when filter button clicked/[android] animate the filter button for a few seconds/[android] refactors to adjust to new KeywordDetector.Feature. logic improvements/[common/android] using List for histogram, better API and avoid extra memory creation/[android] Avoid any possible Context leaks when submitting results to KeywordDetector/[android] Search Filters (squashed) (#491) * [android] Search Filters (squashed) * [android] extra character on manifest * [android] KeywordDetector::HistogramUpdateRequestDispatcher cleanup/refactor/"
368,368,14.0,0.993399977684021,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] hides selection checkbox when there is no files on the list, in My Files and My Files Filter (#540) * [android] hides selection checkbox when there is no files on the list Before even when there was 0 files a user could still enter selection mode in My Files. * [android] hides selection checkbox when there is no filtered files on the list/[android] avoid illegal state exception updating header on browse peer fragment/[android] Fixes issue where My Files SearchView was being hidden right after being shown. The culprit was a runnable sent from the ToolbarActionBar which would invoke populateOptionsMenu. This runnable was sent whenever the activitys invalidateOptionsMenu() method was called. After a search, the TransfersFragment.onTime() method was found to have a periodic call to invalidateOptionsMenu() from a recent fix on the transfers menu. The fix was making sure such invalidation only happened when the TransfersFragment was visible. 14 characters. The rest of the code changes had to do with a hard coded string and tighter checks on when to perform the filtering/[android] long running Finger was causing way too many ANRs. build 395/[android] fixes last tab selected issue on rotate/[android] Tab Layout refactor (#449) * [android] added tabLayout, removed TitlePageIndicator * [android] more cleanup * [android] more cleanup and missing license headers * [android] TabLayout on search fragments SearchInputView Core functionality is there: Search counters updated OnClick listener OnSwipe listener Remembering last selected file type selected * [android] refactor RadioButtonsListener to OnTabsListener, plugs SearchFragment.onFileTypeClicked * [android] added dividers, removed unused button from tabs * [android] added new file type image for tablayout * [android] adjusted search view tablayout * [android] initial set up of browse fragment tablayout * [android] added initial set up for the transfers tab layout TODO: add the horizontal divider line below ClearableEditTextView * [android] plugged tab layout on my files. very satisfactory cleanup * [android] Simplification on swipe. More cleanup/"
369,369,8.0,0.9136000275611877,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] onResume interstitial logic/[android] onExit/onBackHome interstitial logic update, less ads/"
370,370,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",[android] ANR fix. Dont scan files on main thread/
371,371,8.0,0.864300012588501,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] refactor, moved configuration methods out from TransferManager to ConfigurationManager/"
372,372,8.0,0.5548999905586243,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] avoid context leak on DeleteFileMenuAction/[android] ImageViewer fragment (#437) * [android] ImageViewer activity, Video/Pic grid improvements * [android] Fixes selection mode issues on grids, Media Overlay View, Pinch and Zoom * [android] ads metadata to image viewer fragment * [android] set wallpaper action non-blocking * [android] ImageLoader asyncload wraps fetch() to use Picassos background threads * [android] New FileInformation action shows file information dialog * [android] add preview screen listener * [android] Cleanup and bug fixes. Fixes bug on ImageViewerFragment where after renaming an image and rotating the screen, the window title would revert to the original file name. Cleanup, license headers./"
373,373,17.0,0.6520000100135803,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] Rename refactor: BrowsePeerFragment MyFilesFragment/[android] opens single image downloads with ImageViewerActivity/[android] refactor in SendFiatTipAction for less dependencies/
374,374,8.0,0.7206000089645386,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] avoid capturing context references in startTorrentPartialDownload/[android] HandpickedTorrentDownloadDialog dialog dismiss crash fix/
375,375,8.0,0.9635000228881836,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] bug fix first display onResume interstitial/[android] onResume interstitial logic/[android] onExit/onBackHome interstitial logic update, less ads/[android] dont crash at runtime if call to commit in MainActivity#hideFragments fails with IllegalStateException/[android] properly shutdown app from DangerousPermissionsChecker/"
376,376,14.0,0.9847000241279602,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] Tab Layout refactor (#449) * [android] added tabLayout, removed TitlePageIndicator * [android] more cleanup * [android] more cleanup and missing license headers * [android] TabLayout on search fragments SearchInputView Core functionality is there: Search counters updated OnClick listener OnSwipe listener Remembering last selected file type selected * [android] refactor RadioButtonsListener to OnTabsListener, plugs SearchFragment.onFileTypeClicked * [android] added dividers, removed unused button from tabs * [android] added new file type image for tablayout * [android] adjusted search view tablayout * [android] initial set up of browse fragment tablayout * [android] added initial set up for the transfers tab layout TODO: add the horizontal divider line below ClearableEditTextView * [android] plugged tab layout on my files. very satisfactory cleanup * [android] Simplification on swipe. More cleanup/"
377,377,7.0,0.8641999959945679,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[android] pass saved instance bundle in AbstractFragment#initComponents, similar to AbstractActivity/"
378,378,8.0,0.9269000291824341,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] ignoring SecurityException while canceling all notifications in EngineService#onStartCommand/[common][all] migration to ok code refactor related to notification update/
379,379,16.0,0.8812000155448914,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",[desktop] fix issue of listening to IPv6 and random port from settings/[desktop] minor cleanup in NetworkInterfacePaneItem/
380,380,16.0,0.9524999856948853,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","[desktop] fix issue of listening to IPv6 and random port from settings/[desktop] using explicitly a random number for listening port, using zero was choosing a different random port for each pair of interface and protocol/"
381,381,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[desktop] import cleanup/
382,382,17.0,0.5371000170707703,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[all] EZTV search fixed for the longest time it wasnt adding the search keywords to the search url. test code updated/
383,383,18.0,0.9320999979972839,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","[common] give the opportunity to specify if an html page is valid to be crawled, minor refactor. For example, monova include in search results torrents that are actually removed./"
384,384,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[common] fixed SC/
385,385,0.0,0.7623999714851379,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[all] TorLock search fixed/
386,386,12.0,0.944100022315979,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","[common] using magnet url for yi-fi if torrent url fails/[common] minor code optimization/cleanup/[common] YifySearchPerformer/SearchResult using named groups, cleanup/"
387,387,14.0,0.9049999713897705,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] bug fix on song fragments clicking on a song that was already being played would not open the music player screen. other cleanups/
388,388,17.0,0.8098999857902527,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] apollos FavoriteStore and RecentStore optimizations Should avoid possible NPE in addSongId/
389,389,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] avoid crashes with NotificationManager (SecurityExceptions Issue
390,390,17.0,0.5004000067710876,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] MediaPlayer, factored mediaplayer async calls into single class/[android] issue avoid ANR updating remote control client playback state/[android] MusicPlaybackService.MultiPlayer flatened/[android] avoid context leak int MuscPlaybackService#updateNotificationAsync background task/[android] avoid context leak int MuscPlaybackService#setDataSourceImpl background task/[android] capturing only IllegalStateException and StaleDataException (partial revert of ANR/MusicPlaybackService.start fix Make sure mCurrentMediaPlayer.start() never occurs in main thread, where it can some times take too long performing isDrm() checks./[android] bug fix on song fragments clicking on a song that was already being played would not open the music player screen. other cleanups/[android] MusicPlaybackService.notifyChange() refactor/optimizations Make sure notifyChange logic always runs in background thread Avoid unnecessary database re-querying Invert string.equals() comparison to avoid possible NPEs/[android] MusicPlaybackService.changeQueueAsync refactor. isFavorite() deadlock avoidance The change on isFavorite() to not synchronize on the whole object is to help avoid the ANR reported on issue when notifyChange() is called, which is called quite a lot. A refactor on notifyChange() is coming/[android] crash fix on MusicPlaybackService (Issue"
391,391,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",[android] bugfix: resume audio playback in case interstitial takes audio focus/
392,392,15.0,0.8098999857902527,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] avoid context leak on MoPubAdNetwork.loadMoPubInterstitial/[android] ANR loading MoPub interstitial/
393,393,17.0,0.9524999856948853,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] Fix onResume interstitial display bug when app was minimized/[android] Less interstitial ads MainActivity and Search interstitials now share the same timestamp, this way we avoid all possibility of back to back interstitials/"
394,394,12.0,0.949999988079071,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","[android] removed not useful calculateDiskCacheSize The nature of this app is heavy use of disk, increasing or decreasing, the target shouldt change in each app start, also, the phone will more likely to fail before we reach the ~120MB max specified./"
395,395,15.0,0.5145000219345093,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] transfers selected tab on rotation bug fix/[android] ScrollListeners refactor./[android] Transfers > Seeding Views to encourage seeding Makes the new Seeding tab user experience a lot better when we can detect things like: is seeding disabled? or are there any transfers we should be seeding? Cleanup and fixes on TransfersFragment/[android] TransfersFragment cleanup & formatting/[android] Transfers > Seeding tab/
396,396,15.0,0.9711999893188477,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] Issue ImageViewerActivity: Swipe to the next/previous picture (#612)/[android] typo/[android] Open Menu Action to open image with external Image Viewer (#586) * [android] Open Menu Action to open image with external Image Viewer Issue * [android] removed unncessary inFullScreenMode from the ImageViewerFragment * [android] removed unnecessary characters put in by mistake/
397,397,17.0,0.9682999849319458,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] ScrollListeners refactor./[android] SearchFragment avoid context leaks/[android] using execute instead of submit to avoid unused creation of Future objects/[android] search fragment header banner (#573)/[android] bring back ratings reminder, simpler logic. cleanup./"
398,398,14.0,0.9635000228881836,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] make sure update icon is shown on first run if update available/[android] extracted SoftwareUpdaterDialog out of SoftwareUpdater/[android] consider when the last session started for the onResume Interstitial timeout duration/[android] search fragment header banner (#573)/[android] vpn offer name fix, remote vpn offer config/"
399,399,17.0,0.5249999761581421,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] fix of context leak in prepareOnMediaScannerConnectedRunnable/
400,400,17.0,0.762499988079071,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] LocalSearchEngine minor cleanup/
401,401,16.0,0.9711999893188477,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","[android] SeedAction can now be used to seed all finished transfers. Fixes bug where top menu seed all wouldnt show the dialogs pertinent to bittorrent off or seeding off. TransferFragment menu action now reuses SeedAction SeedAction cleanup, formatting and Context-leak avoidance maintenance/[android] Fixed context leak detected in SeedAction.onClick()/"
402,402,14.0,0.8675000071525574,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] issue 594, shows transfer while fetching torrent (#596) Also, when transfers are removed the transfer list is now refreshed immediatly./[android] Transfers > Seeding tab/[android] cleanup/[WIP] additional no results text on search for when connection  (#567) * [android] additional no results text on search for when connection is not available. only when no Wifi and Data are available at the moment. * [android] NetworkManager broadcast listener refactor, integration with SearchProgressView (1st leg) * [android] wording on more results * [android] get rid of extra broadcast receiver, thanks * [android] show snackbar on search fragment when connection dropped Transfers already have a few way of showing, so not adding it there yet * [android] update link to more results url * [android] flip string checks, add default switch case * [android] NetworkManager parameter refactor, url fix/"
403,403,17.0,0.4496999979019165,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] reduce on-resume interstitial delay to 2 secs 20 secs gave a real bad experience/[android] make sure update icon is shown on first run if update available/[android] move updateNavigationMenu() call out of setupDrawer() avoid possible infinite loop/[android] MainActivity update system fixes Made BroadcastReceivers static classes with weak references to activity to avoid any possible memory leaks Persist an updateAvailable boolean state to update nav menu with upgrade icon notification. updateNavigationMenu(bool) check is also done when instantiating a new nav menu cleaned constructor of unnecessary ""this."" notation rearranged member declarations, simpler (smaller sized) types to the top/[android] using LocalBroadcastManager for update notify instead of hard type dependencies and strong references between components/[android] consider when the last session started for the onResume Interstitial timeout duration/[android/bugfix] issue that showed first ad ever way too soon/[android] issue 594, shows transfer while fetching torrent (#596) Also, when transfers are removed the transfer list is now refreshed immediatly./[android] using execute instead of submit to avoid unused creation of Future objects/[android] Fix onResume interstitial display bug when app was minimized/[android] avoid back to back interstitials/[android] search fragment header banner (#573)/[android] delay onResume ad/[android] Less interstitial ads MainActivity and Search interstitials now share the same timestamp, this way we avoid all possibility of back to back interstitials/"
404,404,14.0,0.8098999857902527,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] string refactors. VPNStatusDetailActivity templeteable/
405,405,1.0,0.8812000155448914,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",[android] removed internal tracking of receivers in AbstractActivity due to lack of effectiveness/
406,406,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] avoid crashes with NotificationManager (SecurityExceptions Issue
407,407,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",[android] bugfix: resume audio playback in case interstitial takes audio focus/
408,408,15.0,0.8414999842643738,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[common] more robust handling of network errors in BaseHttpDownload/
409,409,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[common] fixed SC/
410,410,17.0,0.6833000183105469,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] code cleanup/
411,411,17.0,0.8708000183105469,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android/issue-637] Showing songs instead of albums in recent player tab (#638) * [android][WIP] issue 637 Showing song instead of albums in recent player tab * [WIP] Code compiles. Having problems with closing a cursor. * [WIP] Project compiles. Add songs to the Recent Fragment and plays as expected. Need more work to put song related information on screen * [WIP] Changes to save track duration correctly * [WIP] Fixes in RecentSongLoader * [WIP] Reusing existent interface to get song field names * [WIP] Adding licenses and cleaning up * [WIP] Removing unnecessary classes to save some symbols. RecentSongLoader removed, all the functionality was added to RecentLoader. RecentSongStore class removed, all funcionality was added to RecentStore * [WIP] Removed BaseSongFragment. RecentFragment inherits from ApolloFragment directly. Removed menu for View as in Recent tab * Removed getLastSongForArtist because isnt used * [WIP] Cleaning up * More cleaning. Updating docstring * [WIP] choosing a better name for last time played column. * Minor Fixes requested by * [WIP] Calling restartLoader using force equals in order to update the Recent list without reload the Fragment * [WIP] Almost there, fixing odd behavior in Recent List * [android] lambda refactor, NPE checks on SongAdapter * exec improvements. reloading data in a different thread./"
412,412,17.0,0.5187000036239624,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] When a song is shared attach a screenshot of the player/
413,413,17.0,0.5249999761581421,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] more apollo ImageFetcher code cleanup/
414,414,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] MyFilesFragment AsyncTask refactor/[android] apollo ImageCache AsyncTask refactor/
415,415,17.0,0.5249999761581421,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] more trivial apollo cleanup/
416,416,17.0,0.9926999807357788,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] MusicPlaybackService maintenance Avoid possible crash if Power Manager cant be obtained Avoid possible crash obtaining a lock on mFavoritesCache null, do not synchronize non-final objects/[android/issue-637] Showing songs instead of albums in recent player tab (#638) * [android][WIP] issue 637 Showing song instead of albums in recent player tab * [WIP] Code compiles. Having problems with closing a cursor. * [WIP] Project compiles. Add songs to the Recent Fragment and plays as expected. Need more work to put song related information on screen * [WIP] Changes to save track duration correctly * [WIP] Fixes in RecentSongLoader * [WIP] Reusing existent interface to get song field names * [WIP] Adding licenses and cleaning up * [WIP] Removing unnecessary classes to save some symbols. RecentSongLoader removed, all the functionality was added to RecentLoader. RecentSongStore class removed, all funcionality was added to RecentStore * [WIP] Removed BaseSongFragment. RecentFragment inherits from ApolloFragment directly. Removed menu for View as in Recent tab * Removed getLastSongForArtist because isnt used * [WIP] Cleaning up * More cleaning. Updating docstring * [WIP] choosing a better name for last time played column. * Minor Fixes requested by * [WIP] Calling restartLoader using force equals in order to update the Recent list without reload the Fragment * [WIP] Almost there, fixing odd behavior in Recent List * [android] lambda refactor, NPE checks on SongAdapter * exec improvements. reloading data in a different thread./[android] revert System.arraycopy Android Studio fixes, array out of bounds ensued left and right/[android] MusicPlaybackService cleanup/"
417,417,13.0,0.5249000191688538,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[android] more cleanup in apollo loaders code/
418,418,17.0,0.521399974822998,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] MyFilesFragment lambda cleanup, warning cleanup/"
419,419,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","[android] lambda refactor, import cleanup/"
420,420,8.0,0.9269000291824341,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] no need to pass arguments in NetworkManager#isDataWIFIUp/[android] removal of static Context reference in Librarian to avoid memory leaks/
421,421,8.0,0.920799970626831,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] syntax error. redundant logic fixes/[android] removal of static Context reference in Librarian to avoid memory leaks/
422,422,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","[android] lambda refactor, import cleanup/"
423,423,8.0,0.9524999856948853,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] refactor notification of no network notification Now the communication happens the android way, type decoupled and no inner visual component like SearchProgressView has any responsibility on it./[android] inner rename refactor in MainActivity/[android] lambda, if unwrap, explicity type cleanup/"
424,424,14.0,0.8154000043869019,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[WIP] Frostwire 2.0 (#624) * [android] new layouts for the transfer detail pages * [android] initial set up for the transfer detail activity * [android] added toolbar menu to TransferDetails * [android] Extended AbstractFragment from the remaining transfer detail screens * [android] cleanup in TransferDetailActivity * [android] added status tab to transfer details, code cleanup * [android] added status layout to torrent details, layout code cleanup and headers * [android] added resource values for transfer detail activity * [android] removed unnecessary styles * [android] adjusted style of the view_transfer_list_item * [android] replaced missing style in view_transfer_item_list_item.xml * [android] minor adjustement in dividers of view_transfer_list_item.xml * [android] Transfer detail fragments reused. cleanup on fragment classes * [android] TransferDetailActivity refactor. New AbstractTransferDetailFragment. * [android] remove setContext hack, do it the android way * [android] cleanup * [android] TransferManager::getBittorrentDownload(String infoHash) * [android] plugged BittorrentDownload object all the way to fragments Common section with title and progress plugged. Next transfer status and upload/download speed in that common section * [android] plugged transfer status string into common progress component New TransferStateStrings singleton class to map transfer states to strings TransferListAdapter refactored to use new TransferStateStrings * [android] plugged download and upload speeds on common progress panel * [android] transfer detail > status > Completed plugged renamed some of the textviews to have a _label suffix * [desktop] CommonUtils cleanup * [android] Transfer Detail > Status > Time remaining plugged * [android] Transfer Details, Status tab done * [android] abort updateDetailProgress when transfer is null * [android] transfer details layout cleanup * [android] adding a recyclerview to Details, moved mock layout for its views to fragment_transfer_detail_files_recyclerview_item.xml * [android] plugged number of files and total download size on FILES tab * [android] started pluggin ReciclerView, still not working * [android] got files showing on details > files. main transfer list broken, might transition to recycler view here too, testing with ListViewCompat (v7) * [android] cleanup * [android] cleanup * [android] TransferFragment RecyclerView refactor (broken) * [android] RecyclerView on TransfersFragment working * [android] cleanup, component lookup optimization in Transfers Fragment * [android] Progress on Transfer Detail > Details fragment * [common] BTDownload.getTorrentHandle() * [android] Details > Details > Comment section handled * [android] Transfer Detail > Details tab. Just have an issue with the Sequential Download checkbox * [android] updated torrent_details_files * [android] progress on Transfers Details > Trackers tab * [android] updated transfer detail trackers fragment layout * [android] new UIUtils.showEditTextDialog * [android] Transfer Detail > Trackers can add trackers to torrent * [android] fixed width issues with individual tracker layout * [android] minor refactor on TransferManager * [android] TransferDetailStatusFragment doesnt crash on rotation * [android] broken progress towards adding/editing trackers * [android] fix another on rotation crash * [android] uiBittorrentDownload is serialized and deserialized on screen rotation * [android] set adapter after youve set the layout manager * [android] adds and deletes trackers * [android] trackers can be removed. edit text dialog can be configured for single/multi line * [android] fixes issue where recycler views would not show items when tabs switched * [android/build] build.gradle flavorDimensions update for Android 3.0 * [android/build] build.gradle flavorDimensions update for Android 3.0 * [android] Transfer Details > Peers tab working * [andriod] fixed build.gradle to use new plugin 3.0.0 DSL * [android] Details tab cleanup. Removed Seeding On checkbox. That can be handled with action bar controls for seeding/pausing Dialog button handlers now use lambdas * [android] dont refresh tab if not visible cleanup * [android] Transfer Details > Pieces tab (not working correctly) Also lint cleanup on the activity and related fragments * [android] few minor changes to transfer detail layouts * [android] added and updated icons for the view_transfer_list_item * [android] set up for the right side transfer item actions * [android] pieces colors work, had to do r.getColor(). refactor * [android] First pass at setting the media type icon on transfer list items * [common] BittorrentDownload::getPredominantFileExtension() * [android] uses BittorrentDownloads getPredominantFileExtension() to determine transfer media icon * [android] show missing /s on transfer detail speed indicators * [android] warning cleanup on ClickAdapter<T> * [android] Details button on TransferListAdapter works. TransferListAdapter refactored so that we only have a single instance for every click listener, not individual listeners for every view created * [android] found inconsistency between download and upload rate unlimited values in libtorrent, cleanup * [android] fixed overlapping of the text on view_transfer_detail_peer_item * [android] updated TransferDetailDetails dialog title * [android] added appropriate padding to the pieces view in transfer detail * [android] fixes rotation crashes. refactors to deal with rotation if for some reason getting pieces takes a while, an indeterminate progress bar is shown in place of the pieces other rotation bugs fixed, more robust behavior * [android] files becomes the first, and default selected tab * [android] remove timer subscription responsibility from fragments Use only one TimeObserver subscriber defined in the TransferDetailActivity. Fragments still implement TimerObserver, have the single subscriber instance invoke the fragments onTime method only on the currently active fragment * [android] SectionsPagerAdapter doesnt need to keep a reference to activity Just pass a SparseArray with the string resources it actually needs Implements a new OnPageChangeListener that keeps track of what the current fragment is When current fragment changes invoke onTime() on it Implements TimerObserver on the activity which delegates an onTime() call to the current fragment * [android] isVisible() check no longer necessary also found out that in some cases, isVisible() would give a false negative * [android] more work towards avoiding issues on rotation and fast tab changes * [android] import cleanup, if unwraps * [android] stick to empty default constructors on YouTubeDownloadDialog * [android] lint warnings, import cleanups * [android] semi-aldenization of fragment code, still rotation issues * [android] move all init calls to initComponents * [android] cleanup. still issues on rotation, fragments not being added after rotation * [android] finally resolved rotation issue It seems the pager adapter and the programatic approach to adding transfers ended up in fragments not being able to be tracked correctly by their tags when the activity was reloaded. Some of the fragments we ended up with inside detailFragments[] wouldnt be added, however, we luckily have methods in the parent AbstractFragment class to keep track of fragments that have been added (since Android only provides these after API 21 or so), by looking at the added fragments Im able to find the correct instance and replace it inside our internal detailFragments array. Writing this commit message makes me want to use that list, and get rid of the internal detailFragments array, to keep track of the fragments used, they could easily be filtered if theyre instances of AbstractTransferDetailFragment * [android] edit text dialog is now an extension of AbstractDialog * [android] EditTextDialog moved to its own file, handles screen rotation * [android] removed unused vars * [android] rotation bug finally fixed * [android] inline call * [android] Transfer > Status > Time left only shown when downloading * [android] transfer detail tab layout cleanup * [android] transfer details details speed limit dialog issues When transfers are first created their upload/download rate limits come as When you want to set them to for unlimited, this does not go well. You have to use 0 for unlimited, after that, they return 0 Added a dialog closed listener that refreshes the fragment on close so new rate limit is reflected on the ui right away * [android] remove tracker dialog look is uniform. UIUtil.showYesNoDialog now uses YesNoDialog. * [android] dont hide time left textview, show 0 instead * [android] progress with transfer details action bar menu * [android] logic fix with pause/resume action bar action * [android] updated action bar icons, added new resume aciton bar icon * [android] removed double resume action That action is not on the main branch, it was added later it said resume but it was starting to seed even with seeding settings off. * [android] updated icons for resume and cancel context menu action * [android] added action bar seed icon * [android] removed repeating action menu option * [android] fixed and updated action_bar_seed icons Sorry saved them with a wrong setting, looked pixelated * [android] better logic on pause, resume, seed * [android] added clear option on details. cancel menu action uses the proper icon depending on the parameters * [android] removed Open action from details, files are already there for individual access * [android] added action_bar_stop_transfer icon * [android] seeding|pausing made left most action. delete transfer sent to the context menu and cancel transfer the 2nd action shown only if room is available * [android] use SeedAction when seedable * [android] Update Translation: Spanish * [android] Update Translation: Portuguese * [android] start of HexHiveView for piece display, bye RecyclerView in this fragment * [android] detail pieces layout cleanup * [android] updated files and trackers layouts RecyclerViewer no longer moves independently from the rest of hte scroll view better user experience on small screens and horizontal views. Fixed the files tab to open lower then top of the page. The issue was connected to recycleviewer receiving the focus. * [android] calculate height of other components to correctly measure height for hexhiveview * [android] HexHiveView sizing issues fixed No need to manually calculate bounds when the containing layout is simpler fragment_transfer_detail_pieces no longer inside ScrollView, no need to scroll as our HexHiveView will only use remaining vertical space To make this work, view_transfer_detail_progress now has a fixed height, otherwise the folder icon would go towards the center of the screen * [android] fix to the height of the detail_progress layout * [android] very wip hex drawing experiments * [android] Update Translations: French and Italian * [android] HexView advances New DrawingProperties object is used to create buffer objects and useful constants for drawing the hexagonal hive drawing functions refactored to reduce the number of parameter copies on each onDraw iteration more cleanup to be done, gotta take into consideration real pixel width of hexagon borders when moving along the hive * [android] hex fill logic implemented, wip, more fixes needed to calculate right max hex side length * [android] fixed color assigment bug, only repaint when number of pieces downloaded has changed * [android] made debug drawing an XML attribute. Better aproximation on desired hexagon length, still need to account for maximum rows. * [android] simplifications * [android] unneded file * [android] 2.0.0 (beta) build 473 * [android] stricter checks initializing UIBittorrentDownload * [android] no need to import Objects * [android] Update Translation: German * [android] NPE bullet proofing My guess is that the NPE reported on detailProgressStatusTextView.setText(transferStateStrings.get(uiBittorrentDownload.getState())); wasnt due to the view being null, but to the transferStateStrings object being null due to possible synchronization issues with TransferStateStrings.getInstance() To make sure this doesnt happen, I perform another check later on, and if weve been wrongly assigned a null value, we request the singleton again. * [android] rebasing left-overs * [android] math brainfart calculating Y position of rows fixed * [android] hex hive drawing fixes * [android] cube-piece drawing support * [android] upgrades dont animate tabs when touching tab title new CubePaint object extends Paint and calculates and keeps colors for dark left cube face and light top face out of the given fill color which is used on the right side face * [android] adds URL validation when adding or editing trackers * [android] fix bug with active time and seeding time calculation when context menu actions are invoked they invoke onTime() on the underlying activity (Context that implements TimerObserver) cleanup * [android] s/delete/remove torrent and data/ menu entry * [android] cleanup * [android] avoid Runtime error trying to open transfer with no infoHash/"
425,425,15.0,0.8416000008583069,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] attempt to fix the ANR caused by io.presage.receiver.NetworkChangeReceiver/
426,426,0.0,0.762499988079071,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[desktop] minor code cleanup/
427,427,9.0,0.949999988079071,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",[desktop] using MacOSHandler for open uri application event/[desktop] using MacOSHandler for open file application event/[desktop] more use of MacOSHandler/[desktop] added MacOSHandler utility to handle macOS application events in both java 8 and 9/
428,428,15.0,0.7924000024795532,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","[android] raise timeout to 20 seconds. 12 users reported BTEngine.ctx null at com.frostwire.android.gui.transfers.TransferManager.run (TransferManager.java:578) Galaxy J7, Xperia XA, Galaxy S8. All on Android 7.0/[android] Synchronize early access to BTEngine.instance() while .ctx is being built with CountDownLatch/"
429,429,4.0,0.5048999786376953,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",[android/desktop] changelogs. [desktop] messages.jar/
430,430,13.0,0.5249999761581421,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[common] LimeTorrentsSearchPerformer fix/
431,431,17.0,0.864300012588501,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[common] fixed yi-fi/[common] minor code refactor/[common] Zooqle search fixed/
432,432,17.0,0.762499988079071,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[common] Zooqle search fixed/
433,433,14.0,0.8641999959945679,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] ApolloFragment async refactor and cleanup/[android] tidy move inner static class to the bottom/
434,434,17.0,0.762499988079071,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] more mopub fixes/[android] AudioPlayerActivity async refactors/
435,435,8.0,0.8100000023841858,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] moving towards less use of nonsensical AsyncTask/
436,436,14.0,0.9567999839782715,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] Use NotificationCompat.Builders new constructor It receives the channel ID, so no need for fragmentation verification ifs./[android] Notifications work on Android 8.0 Issue apollo NotificationHelper fixes remove unused parameter remove unnecessary final keyword catch possible IllegalArgumentException exception on service.startForefound/"
437,437,13.0,0.5462999939918518,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","[android] capture IllegalStateException in MusicPlaybackService#openFile/[android] make sure that MusicPlaybackService#onCreate runs strict/[android] give some room to the application to recover from OOM in changeRemoteControlClientTask/[android] bug fix, notification music player goes away when music fully stopped on miniplayer thanks Android Oreo(8) notification issues fixed/[android] NPE on MusicPlaybackService.initNextMediaPlayer/[android] lambda to method reference refactor/[android] NPE on MusicPlayer::playSimple/[android] core refactor in MusicPlaybackService/[android] enable position interaction API for remote control/[android] MusicPlayback service (last) setDataSource task async refactor/[android] MusicPlaybackService MediaPlayerRunnable async refactor/[android] MusicPlaybackService changeRemoteControlClientTask async task/[android] MusicPlaybackService remoteControlClientSetPlaybackState task refactor/[android] MusicPlaybackService notifyChange task, recentsStoreAddSongId task async refactors/[android] MusicPlaybackService cancelShutDown task async refactor/[android] MusicPlaybackService updateNotificationTask async refactor/[android] array out of bounds exception getting next song in shuffle mode aparently theres a possibility for mHistory to change its size during those iterations, to avoid the issue Ive made a copy of it and iterate over the copy/"
438,438,10.0,0.9405999779701233,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",[android] refactor prebid onBanner<Load|Failed> integration to a single liner/[android] SearchHeaderBanner listener onBannerFailed prebid logic/[android] PreBid integration on Search Header Banner Also NPE protection before inflating search header banner view/
439,439,14.0,0.9269000291824341,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] 1 ConfigurationManager.instance() call per method/[android] force ad disablement right after update or on first session ever/
440,440,8.0,0.5249999761581421,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] AppLovinCustomEventBanner MoPub-2.1.3 plugin version api update/
441,441,16.0,0.6833000183105469,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",[android] refactor in ImageLoader in preparation for when Picasso release with Downloader interface removed/
442,442,17.0,0.9320999979972839,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] So long AbstractTask and ContextTask/[android] tab switching should occur on UI thread/[android] s/invokeAsync/async/[android] TransfersFragment invokeAsync refactor/[android] CalledFromWrongThreadException on TransfersFragment.updateTransferList/[android] ANR on TransfersFragment.onTime()/
443,443,8.0,0.9366999864578247,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] So long AbstractTask and ContextTask/[android] s/StartDownloadTask/AsyncStartDownload refactor Favors use of Asyncs api over buggy ContextTask/[android] remove AsyncTask in favor of our async function LoadSlideTask wasnt executed on the second run, not sure why. Maybe Android 8 restrictions/"
444,444,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] ANR on TransferDetailFilesFragment/
445,445,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] update Config threshold defaults/
446,446,17.0,0.9049999713897705,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] 1 ConfigurationManager.instance() call per method/[android] delete torrent from provider when deleting all files from torrent/
447,447,15.0,0.8098999857902527,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] s/invokeAsync/async/[android] refactor in DeleteFileMenuAction#deleteFiles to use adapter as the context/
448,448,17.0,0.920799970626831,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] better fix for YesNoDialog clicklisteners/[android] Bug fix handling YesNoDialogs without positive or negative custom listeners/[android] YesNoDialog.setOnDialogClickListeners refactor/
449,449,15.0,0.3840999901294708,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] MainActivity cleanup Dice roll refactor to use UIUtils.diceRollPassesThreshold NPE warnings fixed Removed TODO/[android] IllegalStateException on MainActivity.switchContent/[android] s/invokeAsync/async/[android] 2nd part of MainActivity::checkSDPermission async refactor. Thanks MainActivity. checkSDPermission + onDownloadCompleteNotification asyncInvoke refactors/[android] Issue Dont hog the main thread checking SD permissions on resume/
450,450,17.0,0.5299999713897705,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] s/invokeAsync/async/[android] CalledFromWrongThreadException crash on NavigationMenu.refreshMenuRemoveAdsItem/
451,451,17.0,0.9648000001907349,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] VPN suggestion updates (#681) * [android] added new strings, drawables and vpn images * [android] updated text and layout of vpn page * [android] further vpn page adjustments readded vpn_button_background_selector after previous deletion * [android] added strings, updated vpn buttons * [android] updated headers/"
452,452,8.0,0.6258999705314636,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] Use NotificationCompat.Builders new constructor It receives the channel ID, so no need for fragmentation verification ifs./[android] Notifications work on Android 8.0 Issue EngineService async refactors, no warnings. ResumeBTEngine async refactor Component enabling async refactor Notification cancellation async refactor Permanent Notification Updates start async refactor null checks syncrhonization on local variable warning fixed/[android] compilation fix/[android] One last ANR on EngineService.onStartCommand and fixes Moved the start of the NotificationUpdateDemon to a background thread, it was the only ANR coming out of here with the last build. Since shutdownSupport is already a background thread no need to spawn more subthreads for component disable and NotificationCanceller Added some try/catch protections on all those methods to avoid unexpected crashes during startup/shutdown/[android] Cancel All Notifications refactor on EngineService When starting the app the main thread could be held gettings the notification system service, which in android 7 has the possibility of a security exception (catched already), then all notifications are asked to be cancelled. This code was repeated also when shutting down, also on the main thread This has now been moved to a background runnable to make startup and shutdown faster to the user./[android] ANR on EngineService.enableComponents() Note: Maybe we should remove the code that asks for Killing Ogury on Exit, I dont think well be changing that, and that was the cause of the ANR/[android] dont stop what hasnt started Thanks Dont hold back early exit/[android] EngineServices BTEngine resume call and state change occur in background thread It was possible for this method to lock the UI thread on startup/[android] ANR on EngineService components startup Issue"
453,453,9.0,0.5407999753952026,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","[desktop] transfer tab title, filter text and buttons in the same row/[desktop] transfer splitter wont be at bottom for new transfers/[desktop] transfers splitter behavior as expected/[android] TransferDetail components foundation laid out New TransferDetailComponent is the holder component. It has a JToggleButton row to control which is the currently visible detail JPanel component. It can remember which one was the last component shown, it does so using a StringSetting (UISettings.LAST_SELECTED_TRANSFER_DETAIL_JPANEL)/"
454,454,9.0,0.944100022315979,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","[desktop] GUIUtils.toUnitBytes GUIUtils.getBytesInHuman Since we kept forgetting the old method to get bytes in human friendly units, Ive deleted the new implementation and renamed the old one/"
455,455,14.0,0.8416000008583069,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[desktop] when transfers are started they remain selected, cleanup./"
456,456,2.0,0.5971999764442444,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",[desktop] rename refactor showDetails->showSearchResultWebPage Ive renamed this method so its not confused with showing torrent transfer details components/
457,457,0.0,0.9817000031471252,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","[desktop] New: Send Feedback Dialog * [issue-632] Adding Send Feedback Dialog * [issue-632]Adding Send Feedback Dialog * [desktop] temp * [desktop][issue-632] Adding Send Feedback action * [desktop][issue-632] Updated endpoint * [desktop][issue-632] warning fixes * [desktop][issue-632] Adding Your Name field to SendFeedback Dialog * [desktop][issue-632] Adding validations to Dialog * [desktop][issue-632] improvements to Send Feedback Dialog * [desktop][issue-632] misc fixes & refactors * [desktop][issue-632] system info output in HTML or not * [desktop][issue-632] Feedback interval to 5 mins, make sure user can copy & paste long messages and layout doesnt break * [desktop] cleanup/"
458,458,16.0,0.8416000008583069,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",[common/desktop] Yify search fixes/
459,459,16.0,0.8416000008583069,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",[common/desktop] Yify search fixes/
460,460,15.0,0.8357999920845032,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] Music player saves shuffle preferences Fixes also an array out of bound when trying to play the first song of a playlist when the last shuffle mode was true. Other refactors./[android] Fix empty message for playlist detailed view (#721) * [android] Fix empty message for playlist detailed view * [android] Changing empty text for PlaylistSongFragment * [android] Cleaning up imports. Using the default constructor/
461,461,14.0,0.8812000155448914,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] Songs can be reordered in Queue Fragment (#728) [android] Songs can be reordered in Queue Fragment/
462,462,17.0,0.9824000000953674,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] AudioPlayer activity MoPubBannerView refactor/[android] Music player saves shuffle preferences Fixes also an array out of bound when trying to play the first song of a playlist when the last shuffle mode was true. Other refactors./[android] Less intrusive interstitial ads (#726) * [android] No more interstitial on resume No interstitial should be done before a transitional action, otherwise its too annoying and gets in the way of the user experience [android * [android] ok pro-guard rules * [android] interstitial logic on back pressed * [android] interstitial logic on nav menu other fixes and code added for APIs completion and consistency on MainController and MainActivity/"
463,463,17.0,0.9002000093460083,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] Music player saves shuffle preferences Fixes also an array out of bound when trying to play the first song of a playlist when the last shuffle mode was true. Other refactors./
464,464,14.0,0.861299991607666,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] NPE protections/[android] Fixes Illegal arg exception on random.nextInt()/[android] Songs can be reordered in Queue Fragment (#728) [android] Songs can be reordered in Queue Fragment/[android] Music player saves shuffle preferences Fixes also an array out of bound when trying to play the first song of a playlist when the last shuffle mode was true. Other refactors./[android ] Remember randomly played songs (#725) * [android] Cleaning up the player from the all shuffle stuff. No shuffle implementation functionality at this point"" * [android] Updated mechanism to get and set shuffle mode on and off. * [android] Introduced new logic for next and back in shuffle mode * [android] Rename method getShuffleMode to isShuffleEnabled, parameter shuffleMode to shuffleEnabled. mShuffleMode to mShuffleEnabled/[android] Album > Song (Long Press) > Add to Playlist Now shows mini-player view once the song has been added. Will not automatically start playback the first time a song has been added to the queue./"
465,465,14.0,0.9682999849319458,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android ] Remember randomly played songs (#725) * [android] Cleaning up the player from the all shuffle stuff. No shuffle implementation functionality at this point"" * [android] Updated mechanism to get and set shuffle mode on and off. * [android] Introduced new logic for next and back in shuffle mode * [android] Rename method getShuffleMode to isShuffleEnabled, parameter shuffleMode to shuffleEnabled. mShuffleMode to mShuffleEnabled/"
466,466,17.0,0.7200000286102295,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] new home screen banner unit id/[android] Less intrusive interstitial ads (#726) * [android] No more interstitial on resume No interstitial should be done before a transitional action, otherwise its too annoying and gets in the way of the user experience [android * [android] ok pro-guard rules * [android] interstitial logic on back pressed * [android] interstitial logic on nav menu other fixes and code added for APIs completion and consistency on MainController and MainActivity/[android] MoPub 5.0.0 updates/"
467,467,15.0,0.967199981212616,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","[android] using internal PrebidManager application context in calls to attachBids. It happens that although not documented, the context in attachBids is used in multiple places internally, and a context leak is possible if its not an application context./[android] improved ergonomics in PrebidManager#getInstance to ensure an application context is used/"
468,468,17.0,0.9851999878883362,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] mopub consent dialog refactor No longer called on MainActivity.onResume, only once MoPub has actually been initialized inside MoPubAdNetwork, therefore we no longer need Offers.initAdNetworks() to return boolean/[android] fixed compilation issue/[android] readyForAnotherInterstitialAsync optimization dont bother checking all the other stuff if the session didnt start long enough ago/[android] Less intrusive interstitial ads (#726) * [android] No more interstitial on resume No interstitial should be done before a transitional action, otherwise its too annoying and gets in the way of the user experience [android * [android] ok pro-guard rules * [android] interstitial logic on back pressed * [android] interstitial logic on nav menu other fixes and code added for APIs completion and consistency on MainController and MainActivity/"
469,469,11.0,0.7623000144958496,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",[all] update to jlibtorrent 1.2.0.18-RC9/
470,470,14.0,0.9320999979972839,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] new home screen banner unit id/[android] avoid crash on KitKat showing mopub banner/[android] mopub banner on promotions view/
471,471,17.0,0.5249999761581421,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] class rename refactor/
472,472,6.0,0.3652999997138977,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[android] MoPub GDPR consent dialog integration/
473,473,17.0,0.5249999761581421,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] PreviewPlayerActivity MopubBannerView refactor/
474,474,4.0,0.6832000017166138,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",[common] BTContext/BTEngine fw version for peer fingerprint support/
475,475,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] ANR fix on loadMopubInterstitial/[android] shutdown logic revision/[android] more mopub/admob integration/
476,476,10.0,0.9750000238418579,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","[android] MopubBannerView repairs Avoid ANRs, dont perform banner load on main thread Support for fallback banner loaded/dismissed listeners Dont use the banner onLoadListener after you load the fallback banner Do not destroy the banner onLoadFailed otherwise further banners cannot be loaded No need to keep reloading the fallback banner over and over onDismissBannerOnClickListener refactor and use of possibly given banner and fallback banner dismissed listeners used/"
477,477,0.0,0.8098999857902527,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] make AdMob initialize before other networks/
478,478,14.0,0.9603999853134155,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] only scan the contents folder on download finished removed hack to not scan one minute after starting/[android] Dont scan file system until one minute has passed from the last restart Otherwise it can take too long to load previous transfers/
479,479,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[desktop] CMYKJPEGImageReader cleanup/
480,480,15.0,0.5943999886512756,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","[android] SwitchPreference rendering issues fixed after sdk 28 min sdk bumped from 16 to 19 (android 4.4, app didnt really work on 4.1 and very little installs)/[android] refactor EngineService.foregroundStartForAndroidO Engine.foregroundServiceStartForAndroidO/[android] Android O services now should be started in foreground/"
481,481,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] mopub interstitial loading bugfix/
482,482,3.0,0.7623999714851379,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",[android] manually initialize vungle/
483,483,17.0,0.9405999779701233,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] save main thread cycles on ImageLoader.load/[android] NoSuchMethodError caught in ImageLoader.calculateDiskSize can occur in very old Android 4.2 now that weve upped the target/
484,484,15.0,0.864300012588501,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",[android] revert async mopubBannerView.loadMoPubBanner call when view was destroyed not being on the main thread would cause the app to crash/[android] fix ANR on PromotionsAdapter loading mopub view/
485,485,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[android] fix ANR on PromotionsAdapter loading mopub view/
486,486,8.0,0.49729999899864197,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] Engine.startService() refactor for ContextCompat.startForegroundService/[android] refactor EngineService.foregroundStartForAndroidO Engine.foregroundServiceStartForAndroidO/[android] EngineService::foregroundStartForAndroidO refactor Sorted declarations by encapsulation modifier in EngineService, prettification of the code/"
487,487,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[desktop] G+ cleanup/
488,488,12.0,0.5249999761581421,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",[android] test START_NOT_STICKY on EngineService/[android] applovin 9.4.0/
489,489,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
490,490,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
491,491,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
492,492,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
493,493,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
494,494,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
495,495,0.0,0.8100000023841858,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","[desktop] cleanup empty methods, encapsulation fixes/"
496,496,0.0,0.9366999864578247,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","[desktop] java7 explicit types fix/[desktop] declarations made final/[desktop] cleanup empty methods, encapsulation fixes/[desktop] azureus code cleanup/"
497,497,0.0,0.3774000108242035,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[desktop] remove redundant casts/
498,498,0.0,0.762499988079071,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[desktop] unused code cleanup/
499,499,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
500,500,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
501,501,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
502,502,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
503,503,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
504,504,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
505,505,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
506,506,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
507,507,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
508,508,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
509,509,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
510,510,14.0,0.5040000081062317,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/[desktop] unused media player code cleanup/
511,511,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",[desktop] code cleanup/
512,512,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
513,513,5.0,0.5249999761581421,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",[desktop] com.limegroup.gnutella.gui code formatting/
514,514,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
515,515,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
516,516,2.0,0.6830999851226807,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",[desktop] enforce immutability where possible/
517,517,13.0,0.6833000183105469,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[desktop] declaration access fixes/
518,518,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
519,519,2.0,0.6830999851226807,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",[desktop] enforce immutability where possible/
520,520,5.0,0.5249999761581421,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",[desktop] com.limegroup.gnutella.gui code formatting/
521,521,14.0,0.5249000191688538,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[desktop] code formatting/
522,522,5.0,0.5249999761581421,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",[desktop] com.limegroup.gnutella.gui code formatting/
523,523,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] play-services-ads:18.1.0, Android X migration, Pixabay cleanup/"
524,524,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] play-services-ads:18.1.0, Android X migration, Pixabay cleanup/"
525,525,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] play-services-ads:18.1.0, Android X migration, Pixabay cleanup/"
526,526,8.0,0.864300012588501,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[common/desktop] time calculations reverted to long, not double/"
527,527,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] play-services-ads:18.1.0, Android X migration, Pixabay cleanup/"
528,528,14.0,0.920799970626831,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[android] Fallback to opening frostwire.com if update dialog cant open the update .apk/
529,529,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] play-services-ads:18.1.0, Android X migration, Pixabay cleanup/"
530,530,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] play-services-ads:18.1.0, Android X migration, Pixabay cleanup/"
531,531,17.0,0.8100000023841858,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] play-services-ads:18.1.0, Android X migration, Pixabay cleanup/"
532,532,1.0,0.7623999714851379,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",[android] avoid static leak on ImageCache.flush()/
533,533,17.0,0.5249999761581421,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] Fixes issue fixes on EngineService startup/shutdown/
534,534,4.0,0.6832000017166138,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",[desktop/macos] libDispatch.dylib unsigned binary/
535,535,13.0,0.6830999851226807,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[android/WIP] Upgrading com.android.billingclient to 2.0.3/
536,536,8.0,0.6832000017166138,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] fixes on EngineService startup/shutdown/
537,537,17.0,0.7623999714851379,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[all] LimeTorrents search performer rewrite/
538,538,17.0,0.515500009059906,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[common] SC client_id update, fixes issue SC fixes avoid deserialization NPE when media null resolve url updated multitrack download fixes/[common/all] SC search updates/"
539,539,17.0,0.9750000238418579,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] dont show album art behind ad when switching songs/[android] crash only if on debug build/[android] simplifications and optimizations of mopub banner on audio player activity Thread.sleep() even done on another thread, kept slowing down the main thread for some reason, or just holding the execution on the threadpool, perhaps its a single threaded pool? hmm A deep revision on MopubBannerView, ongoing work but a lot better than yesterday/"
540,540,17.0,0.8133999705314636,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","[android] Save to playlist fixed, build 624/[android] Major MusicPlaybackService overhaul The most important playback issues are gone, playback continues through playlist, repeat modes work Theres still a glitch on the very first song played, gotta hit play again, will fix Now MusicPlaybackService is supposed to receive all of its background calls, or perhaps, all calls routed through its MusicPlayerHandler Thread/Loop. If all calls are routed through this queue, there should be no need to synchronize any resource and there should be no unexpected behaviors. No more expensive synchronized blocks, no more deadlocks. synchronized is evil. Also, no more anonymous callbacks, or nested callbacks. Since everything is done on the background and it doesnt block the UI thread, all methods that used to receive callbacks now return boolean, true for sucess, false for failure, usually on success callbacks are acted upon. Some callbacks used to get boolean results, its now very functional or a la go-lang, keep it simple stupid. anonymous callbacks, dynamic runnables and lambdas are also hard to debug/trace, its perhaps a bit more verbose but its clean and easy to read in stack traces and to step through in the debugger. Still a lot more to fix: album art not loading properly in song adapter or music activity/[android] IApolloService...??so long No need for inter process communication, stupid over complexity that leads to more uncertainty in this callback hell of a state machine. Good ridance/[android] Fixes issue where player would stop at the end of a song MusicPlaybackService is now decoupled from Asyncs.async/EngineThreadPool Do not compromise the performance of search and download when using the music player. Sending all background requests to a single HandlerThread in MusicPlaybackService makes things a lot simpler, less chances for race conditions if all requests are executed in the order theyre requested given all the synchronized blocks we inherited from Apollos codebase More fine grained intervals when notifying music player changes, depending on the change being notified There are still glitches with Repeat song mode, this code is full of overcomplexities that are hard to track down. My Music / Art Albums not loading Started debugging and doing a few fixes, album art isnt loading correctly for some reason/[android] MusicPlaybackService encapsulation fixes/[android] move image loader to its own dedicated HandlerThread This way the ImageLoader wont possibly hog the threads on EngineThreadPool and will run independently. ImageLoader.load was one of the most common Asyncs.async tasks used and would affect search and transfers performance/[common/android] Refactored (android) Async.Throttle (common) com.frostwire.util.TaskThrottle/[android] one less thread the old constraints dont apply anymore, now this service is started by the user on the first music playback, well after the engine thread pool has been created/[android] fixes after crash reports from 618 relaxed syncrhonization on entire MusicPlaybackService simplifications and cleanups multiplayer got lost (null)/[android] dont crash if you cant unbind serviceconnection listener on service shutdown/[android] cleanup/[android] cleanup/[android] bring back MusicPlaybackService initService latch/[android/apollo] Simplification of MusicPlaybackService lifecycle. Instead of constantly trying to cancel a shutdown alarm, the service is not started until its needed and stopped as soon as playback is fully stopped. Removed a bunch of unncessary stuff around that old logic/[android] bullet proof access to MusicPlaybackService::mPlayer (rare NPE crash fix attempt/[android] Fixes player notification bug where play/pause button wasnt refreshed on pause/[android] dont sweat Illegal State error onNotificationCreated()/[android] more on player notifications/[android] fix double notification issue/[android] long running music services gotta be started with startService, otherwise they will get killed/[android] use lock to access media remote control, which can trigger notifications/[android] MusicPlaybackService implements IApolloService/[android] Audio Player activity updates Improvements in thread usage and banner loading. We dont load banners unless the current song has played over 3 seconds, we do so after waiting 5 seconds/[android] MusicPlaybackService.java throttles background task submissions/[android] MusicPlaybackService updates AudioManager AudioFocusRequest based integration for Android O+ Dont call Engine.foregroundServicesStartForAndroidO(this), this seems to have been broken, it wasnt sending an icon and it included no information whatever the case we cant be using that mechanism. Initialization of repeat mode and shuffle is now done in an async task. This code would make use of the ConfigurationManager.instance() which can freeze the main thread if there are IO issues mAudioManager.abandonAudioFocusRequest(AUDIO_FOCUS_REQUEST_ for Android O+ cancelShutdown async tasks throttled to be spaced at least with 1 second in between notifyChange async task submissions are now throttled. META_CHANGE to 100ms, other changes to 200ms/"
541,541,17.0,0.762499988079071,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] PlayPauseButton async UI refresh optimizations/
542,542,6.0,0.8416000008583069,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[android] dont try reloading rewarded video manually/
543,543,14.0,0.9524999856948853,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","[android] com.unity3d.ads:unity-ads:3.4.0 new/[android] mopub-applovin adapter integration using mopub mediation dependency/[android] display test reward ad, hides ads, shows ads when time expires/"
544,544,17.0,0.949999988079071,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[android] AsyncStartDownload runs in an exclusive HandlerThread Search can occupy all of the 8 Asyncs threads and if the download is sent to that same queue it might take too long too start/[android] AsyncStartDownload: show transfers on main thread/
545,545,1.0,0.6832000017166138,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",[android] fixes timer unsubscription issues in TransfersFragment/
546,546,17.0,0.5196999907493591,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",[common/android] Refactored (android) Async.Throttle (common) com.frostwire.util.TaskThrottle/[android] SearchFragment cleanup and refactors/
547,547,8.0,0.510699987411499,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","[android] Fixes issue where an existing partial torrent transfer could not add more items from a .torrent in My files its not working 100% of the way, trying to figure out why the uiBittorrentDownload in the Transfer Detail fragment isnt updated all the times, it works most of the times. Gets wonky if the transfer is paused sometimes, but if you try again it will add all of the specified files. Gotta come back to this/"
548,548,8.0,0.6833000183105469,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[android] possible NPE on transfer removal/
549,549,13.0,0.8641999959945679,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","[android] fix buggy behavior of PromotionsAdapter, simply offer ad removal everytime/"
550,550,18.0,0.8812000155448914,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",[android] addition of reward video payment option to BuyActivity/
551,551,6.0,0.8416000008583069,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[android] dont try reloading rewarded video manually/
552,552,4.0,0.9743000268936157,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","* Added new `Pointer.limit` property, mainly useful to get the `size` of an output parameter, as returned by an adapter specified with the annotation * Renamed the `capacity` field of an adapter to `size` as it now maps to both `Pointer.limit` and `Pointer.capacity` (the latter only for new allocations)/ * Added `Pointer.put(Pointer)` method, the counterpart of `Buffer.put(Buffer)`, to call the native `memcpy()` function on two `Pointer` objects/Initial commit/"
553,553,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Initial commit/
554,554,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Initial commit/
555,555,13.0,0.569100022315979,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","* Before loading the JNI library, the `Loader` now also tries to extract and load libraries listed in the preload={...})` annotation values, and to support library names with version numbers, each value has to follow the format (or to have `Builder` use it for the compiler as well), where ""version"" is the version number found in the filename as required by the native dynamic linker, usually a short sequence of digits and dots, but it can be anything (e.g.: would map to ""libmylib.so.4.2"", ""libmylib.4.2.dylib"", and ""mylib.4.2.dll"" under Linux, Mac OS X, and Windows respectively) * Stopped using `java.net.URL` as hash key in `Loader` (very bad idea)/ * New `Loader.loadLibrary()` method similar to `System.loadLibrary()`, but before searching the library path, it tries to extract and load the librar * `Generator` now accepts on `FunctionPointer` class declarations * Added new value to cast explicitly the output of a C++ adapter object * Upgraded references of the Android NDK to version r8 * Included new command line option ""-Xcompiler"" to pass options such as ""-Wl,-static"" directly to the compiler * Made other various minor changes and enhancements/Fixed Maven build and Mac OS X `-framework` option (issue and other minor things/Initial commit/"
556,556,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Initial commit/
557,557,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Initial commit/
558,558,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Initial commit/
559,559,13.0,0.9750000238418579,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","* New `Loader.loadLibrary()` method similar to `System.loadLibrary()`, but before searching the library path, it tries to extract and load the librar * `Generator` now accepts on `FunctionPointer` class declarations * Added new value to cast explicitly the output of a C++ adapter object * Upgraded references of the Android NDK to version r8 * Included new command line option ""-Xcompiler"" to pass options such as ""-Wl,-static"" directly to the compiler * Made other various minor changes and enhancements/Initial commit/"
560,560,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Initial commit/
561,561,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Initial commit/
562,562,0.0,0.49970000982284546,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",* `Generator` would ignore `Pointer.position()` in the case of and parameters * Replaced hack to create a `Pointer` from a `Buffer` object with something more standard/ * Fixed `Pointer.equals(null)` throwing `NullPointerException` (issue * would erroneously prevent `sizeof()` operations from getting generated/
563,563,0.0,0.9320999979972839,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",* `Generator` would ignore `Pointer.position()` in the case of and parameters * Replaced hack to create a `Pointer` from a `Buffer` object with something more standard/
564,564,2.0,0.9848999977111816,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","* Exported `Loader.isLoadLibraries()`, which always returns true, except when the `Builder` loads the classes * Made it possible to specify a nested class (with a $ character in the name) on the command line * When `Pointer.limit 0`, the methods `put()`, `zero()`, and `asBuffer()` now assume a size of 1/ * Added `Pointer.withDeallocator(Pointer)` method to attach easily a custom `Deallocator` created out of a `static void deallocate(Pointer)` method in the subclass, including native ones such as static void deallocate(Pointer)`/ * Fixed memory corruption when returning by value an `std::vector<>` using an adapter * Added `Pointer.zero()` method that calls `memset(0)` on the range * For easier memory management, more than one `Pointer` now allowed to share the `deallocator` when ""casting"" them/"
565,565,17.0,0.8812000155448914,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",* Fixed callbacks not working on Android anymore (issue * Added some Javadoc to most of the code/
566,566,9.0,0.9735999703407288,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","* To help diagnose `UnsatisfiedLinkError` thrown by `Loader.load()`, they have been augmented with a potential cause originating from the ""preloading"" of libraries, whose premature deletion has also been fixed/ * Provided new annotation value to let users specify the name of the native library used by both `Builder` and `Loader`, where different classes with the same name get built together, which also works on nested classes (issue"
567,567,13.0,0.9824000000953674,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","* Upgraded references of the Android NDK to version r9 * Added new `Builder` option ""-copylibs"" that copies into the build directory any dependent shared libraries listed in the preload={...})` annotation * `Loader.getPlatformName()` can now be overridden by setting the `com.googlecode.javacpp.platform.name` system property * Refactored the loading code for into a neat `Loader.ClassProperties` class, among a few other small changes in `Loader`, `Builder`, `Generator`, and the properties/ * Included often used directories such as `/usr/local/include/` and `/usr/local/lib/` to `compiler.includepath` and `compiler.linkpath` default properties * New value lets users specify properties in common on a similarly annotated shared config class of sorts/"
568,568,14.0,0.9405999779701233,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",Released version 0.6 * Added new very preliminary `Parser` to produce Java interface files almost automatically from C/C++ header files; please refer to the new JavaCPP Presets subproject for details/
569,569,16.0,0.5677000284194946,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Released version 0.7 * Tweaked a few things to support RoboVM and target iOS, but `JNI_OnLoad()` does not appear to get called... * Upgraded references of the Android NDK to version r9c * Improved the C++ support of the `Parser` for templates and overloaded operators/ * Made `Loader.load()` work, within reason, even when all annotations and resources have been removed, for example, by ProGuard * Fixed compile error when using a `FunctionPointer` as parameter from outside its top-level enclosing class * The `Parser` now filters tokens appropriately with preprocessor directives * Improved the C++ support of the `Parser` for macros, templates, etc/"
570,570,16.0,0.9660999774932861,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","* Made `Loader.load()` work, within reason, even when all annotations and resources have been removed, for example, by ProGuard * Fixed compile error when using a `FunctionPointer` as parameter from outside its top-level enclosing class * The `Parser` now filters tokens appropriately with preprocessor directives * Improved the C++ support of the `Parser` for macros, templates, etc/"
571,571,8.0,0.9524999856948853,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Update version in the `pom.xml` file to 0.11-SNAPSHOT * Provide `UByteIndexer` and `UShortIndexer`, treating array and buffer data as unsigned integers, for ease of use * Clean up Windows `java.io.tmpdir` even when program messes with `java.class.path` (issue"
572,572,6.0,0.9692999720573425,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","* Log when `Pointer.deallocator` gets registered, garbage collected, or deallocated manually, if `Logger.isDebugEnabled()` (redirectable to SLF4J) * Make `Pointer implements AutoCloseable` to let us try-with-resources, thus bumping requirements to Java SE 7 and Android 4.0/Update version in the `pom.xml` file to 1.1-SNAPSHOT * Introduce the concept of ""owner address"" to integrate `Pointer` transparently with `std::shared_ptr`, etc (Thanks to Cyprien Noel for the idea)/ * Add new ""org.bytedeco.javacpp.nopointergc"" system property to prevent `Pointer` from registering deallocators with the garbage collector/"
573,573,5.0,0.9049999713897705,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","* Add new ""org.bytedeco.javacpp.cachedir"" system property to specify where to extract and leave native libraries to share across multiple JVM instances/"
574,574,0.0,0.9405999779701233,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","* Prevent `Generator` from initializing classes when preloading them, which can cause problems (issue bytedeco/javacpp-presets#126)/ * Add logging to `Loader.loadLibrary()` to help diagnose loading problems (issue"
575,575,19.0,0.9269000291824341,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","* Add ""org.bytedeco.javacpp.maxbytes"" system property, forcing a call to `System.gc()` when this amount of memory tracked with deallocators is reached/ * Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
576,576,18.0,0.9320999979972839,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","* Add ""org.bytedeco.javacpp.maxbytes"" system property, forcing a call to `System.gc()` when this amount of memory tracked with deallocators is reached/long pointers/squid:S00108 Nested blocks of code should not be left empty/"
577,577,17.0,0.9524999856948853,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","squid:S00108 Nested blocks of code should not be left empty/ * Fix swallowed `InterruptedException` (issue bytedeco/javacv#315)/squid:S00117 Local variable and method parameter names should comply with a naming convention squid:S1197 Array designators ""[]"" should be on the type, not the variable squid:S00115 Constant names should comply with a naming convention/"
578,578,17.0,0.9635000228881836,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","* Add ""org.bytedeco.javacpp.maxretries"" system property, the number times to call `System.gc()` before giving up (defaults to 10)/ * Deallocate native memory in a dedicated thread to reduce lock contention (issue a bit longer for `System.gc()` to reclaim memory/Add `Pointer.maxBytes()` and `totalBytes()` to monitor amount of memory tracked/"
579,579,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
580,580,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
581,581,14.0,0.9805999994277954,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","* Prevent Android system libraries from getting copied or extracted/ * Fix `IndexerTest` potentially failing with `OutOfMemoryError` (issue bytedeco/javacpp-presets#234) * Preload libraries to work around some cases when they refuse to load once renamed (issue deeplearning4j/libnd4j#235) * Fix compilation error on some `linux-ppc64le` platforms (issue deeplearning4j/libnd4j#232)/* Fix `Loader` crashing on Android (issue bytedeco/javacv#412)/ * Add the ability the specify, after a `#` character, the output filename of libraries extracted by `Loader.load()`/ * Add parameters to `Loader.load()` offering more flexibility over the platform properties and library paths/ * Fix `Loader.load()` error when called right after `Builder.build()` within the same process/"
582,582,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
583,583,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
584,584,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
585,585,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
586,586,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
587,587,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
588,588,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
589,589,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
590,590,6.0,0.762499988079071,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",* Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address 0`/
591,591,7.0,0.6906999945640564,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","* Accelerate call to `Pointer.physicalBytes()` on Linux (issue * Add ""org.bytedeco.javacpp.maxphysicalbytes"" system property to force calls to `System.gc()` based on `Pointer.physicalBytes()` * Allow strings ending with ""t"", ""g"", ""m"", etc to specify the number of bytes in system properties (issue synchronization of memory allocation to avoid `OutOfMemoryError` when low on memory/ * Synchronize memory allocation in `Pointer` when low on memory to avoid `OutOfMemoryError`/"
592,592,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
593,593,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
594,594,2.0,0.8812000155448914,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",* Make `Loader` cache libraries (in `~/.javacpp/cache/` by default) instead of using temporary files/
595,595,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
596,596,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
597,597,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
598,598,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
599,599,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
600,600,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
601,601,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
602,602,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
603,603,3.0,0.8100000023841858,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
604,604,2.0,0.954800009727478,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Update version in the `pom.xml` file to 1.3.2-SNAPSHOT * Make `Pointer.asBuffer()` thread-safe (issue version 1.3/Update version in the `pom.xml` file to 1.3-SNAPSHOT * Print memory sizes in a human-readable format with `Pointer.formatBytes()` * Map standard `malloc()`, `calloc()`, `realloc()`, and `free()` functions (issue"
605,605,16.0,0.4496999979019165,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","* Fix broken `outputDirectory` property and corresponding `-d` command line option (issue * Add `Loader.extractResources()` and `cacheResources()` methods to extract or cache all resources with given name/Update version in the `pom.xml` file to 1.3.1-SNAPSHOT * Fix potential issues with `Parser` repeating the or annotations on parameters * To support Scala singleton objects better, consider as `static` methods from objects that are not `Pointer` * Allow `Loader.extractResource()` and `cacheResource()` to extract or cache all files from a directory in a JAR file * Create version-less symbolic links to libraries in cache on those platforms where it is useful to link easily * Use `java.io.tmpdir` as fallback in `Loader.getCacheDir()`, and throw a clear exception on failure/Release version 1.2.7 * Fix `Loader` errors that could occur due to recent changes/ * Improve `Loader` handling of duplicate libraries found in different JAR files using symbolic links (useful for MKL, etc)/ * Prevent `Loader` from overwriting previously extracted and renamed libraries (issue deeplearning4j/nd4j#1460)/"
606,606,2.0,0.8416000008583069,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",* Let `Pointer` log debug messages when forced to call `System.gc()`/
607,607,1.0,0.6154000163078308,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","* Prevent `Loader` from loading system libraries, which causes problems on Android 7.x (issue bytedeco/javacv#617)/ * Avoid `Loader` issues with spaces, etc in paths to library files (issue deeplearning4j/nd4j#1564)/"
608,608,7.0,0.9320999979972839,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test",* Make public the `Pointer.formatBytes()` and `Pointer.parseBytes()` static methods/ * Fix potential formatting issues with `OutOfMemoryError` thrown from `Pointer`/
609,609,11.0,0.944100022315979,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","* Prevent race condition that could occur in `Loader.cacheResource()` (pull * Let users bundle arbitrary resources, have them extracted in cache, and used as `include` or `link` paths (pull"
610,610,18.0,0.920799970626831,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",* Try to use symbolic links in `Loader.load()` for output filenames specified with the `#` character (useful for libraries like MKL)/
611,611,16.0,0.954800009727478,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",* Create symbolic links to libraries preloaded by `Loader` as needed on Mac for renamed libraries/ * Fix potential race conditions and various issues with `Loader` that could prevent libraries like MKL from working properly/
612,612,13.0,0.8812000155448914,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",* Catch more exceptions that can occur in `Loader` when caching resources (pull
613,613,13.0,0.9049999713897705,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Add `PointerScope.getInnerScope()` that can be called from any context/ * Add `PointerScope` to manage more easily the resources of a group of `Pointer` objects/
614,614,17.0,0.9269000291824341,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","* Synchronize `Loader.loadLibrary()` to fix potential race condition (pull * Fall back on Android-friendly `System.loadLibrary()` in `Loader.load()` instead of ""java.library.path"" (issue bytedeco/javacv#970)/"
615,615,1.0,0.8098999857902527,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Add `PointerTest.testPointerPointer()` to make sure it works correctly (issue
616,616,19.0,0.9648000001907349,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","* Add `Loader.getLoadedLibraries()` method for debugging purposes and fix flaky `BuilderTest` (issue * Call `PointerScope.attach()` as part of `Pointer.deallocator()`, instead of `init()`, to support custom deallocators as well * Prevent `Parser` from appending annotations to setter methods of variables to satisfy the `Generator`/"
617,617,19.0,0.972000002861023,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","* Allow users to override platform properties via system properties starting with ""org.bytedeco.javacpp.platform.""/ * Replace calls to `Class.getResource()` with `Loader.findResource()` to work around issues with JPMS ([pull * Enhance `Loader.findResources()` with `Class.getResource()` and search among parent packages * Take shortest common package name among all user classes for the default output path of `Builder`/"
618,618,5.0,0.8944000005722046,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",* Add `Loader.loadGlobal()` to load symbols globally as often required by Python libraries (issue ContinuumIO/anaconda-issues#6401)/
619,619,14.0,0.8641999959945679,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",Clarify what values of 0 or less mean for `Pointer.maxBytes` and `maxPhysicalBytes`/
620,620,11.0,0.7623999714851379,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","* Fix `Loader.cacheResource()` with the ""jrt"" protocol as used by jlink (pull"
621,621,5.0,0.9049000144004822,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",* Consider `Pointer` values `maxBytes` or `maxPhysicalBytes` suffixed with `%` as relative to `Runtime.maxMemory()` (pull
622,622,11.0,0.975600004196167,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","* include/ffi_common.h: Delete, after moving contents to... * include/ffi_private.h: Subsume contents of ffi_common.h. * include/Makefile.am (noinst_HEADERS): Remove ffi_common.h. * include/Makefile.in: Rebuilt. * arm/ffi.c, m68k/ffi.c, mips/ffi.c, powerpc/ffi.c, s390/ffi.c, ia64/ffi.c: Include ffi_private.h, not ffi_common.h. * alpha/ffi.c, sparc/ffi.c, x86/ffi.c: Dont include ffi_common.h. * types.c, raw_api.c, java_raw_api.c, prep_cif.c: Dont include ffi_common.h. * debug.c: Include ffi_private.h instead of ffi_common.h. * mips/ffi.c (calc_n32_struct_flags): Make static. (FIX_ARGP): Remove call to debugging routine ffi_stop_here. * mips/n32.S: Include ffi_private.h. * mips/o32.S: Include ffi_private.h./Many changes. Not quite there yet./"
623,623,2.0,0.6832000017166138,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",Testsuite fixes./
624,624,12.0,0.8641999959945679,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Pulled in libffi from gcc trunk. Fixed build and install for standalone use./
625,625,2.0,0.9890999794006348,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Fix wchar_t* return when null Allow Pointer[] as function argument Fix window utils test on osx to avoid os-cast shadows git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix GetLastError bug Make ByReference derive from Memory git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Enable callback return values (e.g. sig_t signal(int sig, sig_t f)) Move native function proxy wrapping to CallbackReference from Structure git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Start on issue Main thing I wanted to do was get the second parameter to ToNativeConverter.toNative() in there so we dont have future API breakage by adding it later. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Revert r266 Integer.valueOf() and friends are 1.5+ features, so it breaks 1.4 support git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Pass the Method of the Library subclass that invoked the Function, to the FromNativeConverter. Also optimized the code in Library.Handler so only one Map lookup per function invoke is needed. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Add auto-conversion for custom types Fix X11 lib for 64-bit use git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Clean up varargs Add per-field Structure read/write Avoid automatic writes to volatile structure fields Read/wrap function pointers in Structure fields Disallow Memory/Function as declared Structure fields git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Use libffi instead of custom assembly (tested for darwin (universal), linux-i386, win32-x86 git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
626,626,16.0,0.9801999926567078,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Provide explicit dispose on NativeLibrary Cache NativeMappedConverter instances for improved performance Preliminary support for wince (improve backwards compatibility for older VMs) Provide synch after call interface for arguments that need to perform some sort of synchronization after a native call git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Allow array of struct by reference as argument Pretty-print structure info in toString git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Provide for tweaking of invocation handling between interface and native Add test for annotation preservation in proxy methods Add explicit global variable lookup Add utility method to determine Web Start native library location git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/implement struct by value add missing Pointer char function git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
627,627,18.0,0.762499988079071,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",add more return types git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
628,628,2.0,0.8944000005722046,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Pass the Method of the Library subclass that invoked the Function, to the FromNativeConverter. Also optimized the code in Library.Handler so only one Map lookup per function invoke is needed. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
629,629,16.0,0.9587000012397766,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Provide for tweaking of invocation handling between interface and native Add test for annotation preservation in proxy methods Add explicit global variable lookup Add utility method to determine Web Start native library location git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
630,630,18.0,0.5971999764442444,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","use long offsets and sizes for pointer arithmetic git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/make memory share public git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Enable VM crash protection on w32, linux Move native library init from Pointer to Native Fix Pointer.setNativeLong bug Make library initialization explicit in Pointer/NativeLibrary Javadoc cleanup git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
631,631,12.0,0.8416000008583069,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",provide memory alignment to Memory if requested git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
632,632,2.0,0.573199987411499,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Dont keep a strong reference to the library proxy Keep a mapping for the proxy on calls to loadLibrary Dont fail if no alpha on shaped window demo Clean up a few comments git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Clean up temp files on w32 Avoid empty paths in search list Add canonical out-of-date jar Clean up ant build script targets git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/extract integer type with platform-specific size remove temporary file suffix altogether git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Enable VM crash protection on w32, linux Move native library init from Pointer to Native Fix Pointer.setNativeLong bug Make library initialization explicit in Pointer/NativeLibrary Javadoc cleanup git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Clean up varargs Add per-field Structure read/write Avoid automatic writes to volatile structure fields Read/wrap function pointers in Structure fields Disallow Memory/Function as declared Structure fields git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Copy issue fix from v3 branch git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
633,633,8.0,0.49790000915527344,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Provide explicit dispose on NativeLibrary Cache NativeMappedConverter instances for improved performance Preliminary support for wince (improve backwards compatibility for older VMs) Provide synch after call interface for arguments that need to perform some sort of synchronization after a native call git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix JNLP class loader method lookup git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix bug in stdcall function mapping when using struct by value Defer size_t/off_t definition, for now git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Consolidate Structure.ByReference reads Cache native library options as well as alignments/type mappers Ensure library options are passed to callbacks Add protection around remaining Pointer read/write calls Consolidate wide character reads/writes Avoid stack overflow reading self-referential structures or loops More prettification of Structure.toString Add size_t/off_t standard types git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/reinstate w32 JAWT workaround; tests work without it, but demo code doesnt git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Provide for tweaking of invocation handling between interface and native Add test for annotation preservation in proxy methods Add explicit global variable lookup Add utility method to determine Web Start native library location git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Add a hack for OpenJDK (Soylatte) on macosx System.mapLibraryName() returns a .dylib extension now, not .jnilib. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Use simpler workaround for loading AWT/JAWT on X11-based platforms git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
634,634,11.0,0.7705000042915344,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",improve package/version information in code and manifest git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/enable ibm j9 build/run git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
635,635,18.0,0.967199981212616,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Allow arbitrary callback method names Allow specification of callback type mapper with TYPE_MAPPER Allow write with uninitialized boxed primitives in Structure Fix memory leak with callbacks called from native threads w/no java context Fix Structure derived classes to allow setting TypeMapper git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
636,636,7.0,0.9843999743461609,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","use long offsets and sizes for pointer arithmetic git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Dont try to use the library extension as the suffix. System.load() doesnt appear to care what the library you tell it to load is called. Could be a possible source of why it wont load on multi-byte charsets under windows. git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Add callback tests for smaller int types Copy generic java.nio.Buffer support from v3 branch + tests Copy missed sparc-isms from v3 branch for building sunos variants Add stubs for platform-specific jars Include libffi testsuite Remove /lib64 references as per v3 branch Auto-generate os prefix from os.name (as per v3 branch) to automatically handle new targets Update overview to include Buffer, function pointer info Include src.zip and doc.zip in dist generation git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Use libffi instead of custom assembly (tested for darwin (universal), linux-i386, win32-x86 git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
637,637,18.0,0.9814000129699707,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Consolidate structure ffi type info initialization and avoid premature GC Enable union by-value by using largest fields type info Explicitly throw IllegalArgument on bad type info Throw IllegalState on missing type info Explicitly write version/md5 info into Makefile from ant (to avoid platform-specific variances in sed) Make Structure.ByValue/ByReference public to allow client code comparisons Consolidate Structure field get/set operations Avoid extra Pointer peer lookup from native code (wmeissner) git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/implement struct by value add missing Pointer char function git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
638,638,18.0,0.6833000183105469,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",allow String[] as callback argument/return git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
639,639,8.0,0.7135000228881836,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Allow ByReference types in callbacks Clean up some Structure field error messages git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Allow Pointer[] as field in Structure Clean up error messages when Structure size calculation fails git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Clean up varargs Add per-field Structure read/write Avoid automatic writes to volatile structure fields Read/wrap function pointers in Structure fields Disallow Memory/Function as declared Structure fields git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
640,640,18.0,0.7113000154495239,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve test output git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix bug in STructure.toArray w/nested struct arrays git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Ensure initialized nested structure arrays use the right memory git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Improve error messages when illegal argument/return types are used git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Consolidate Structure.ByReference reads Cache native library options as well as alignments/type mappers Ensure library options are passed to callbacks Add protection around remaining Pointer read/write calls Consolidate wide character reads/writes Avoid stack overflow reading self-referential structures or loops More prettification of Structure.toString Add size_t/off_t standard types git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix NPE when sizing struct with a struct array field git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Allow array of struct by reference as argument Pretty-print structure info in toString git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix VM crash running test under linux-amd64 git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Properly handle default boolean mapping (int) in Structure read/write Provide Pointer.share(offset) (allow omission of size) git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Move w32 JAWT load into native code Fix Structure.ByValue for callback arg/return Perform better type checking on callback arg/return types Propagate library/symbol lookup error messages Enable loading of libraries with non-ascii names Tighten type checking in native code git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Consolidate structure ffi type info initialization and avoid premature GC Enable union by-value by using largest fields type info Explicitly throw IllegalArgument on bad type info Throw IllegalState on missing type info Explicitly write version/md5 info into Makefile from ant (to avoid platform-specific variances in sed) Make Structure.ByValue/ByReference public to allow client code comparisons Consolidate Structure field get/set operations Avoid extra Pointer peer lookup from native code (wmeissner) git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/implement struct by value add missing Pointer char function git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
641,641,8.0,0.9136000275611877,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Allow final modifier in Structure fields git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix NPE when using NativeMapped within Structure ( git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
642,642,18.0,0.8812000155448914,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",auto-write Structure.ByReference fields git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
643,643,18.0,0.762499988079071,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",struct/union improvements git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
644,644,6.0,0.7736999988555908,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","ensure JNA library can be unloaded git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix stdcall callbacks catch all exceptions when invoking callback git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Use libffi instead of custom assembly (tested for darwin (universal), linux-i386, win32-x86 git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
645,645,7.0,0.9071000218391418,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","ensure JNA library can be unloaded git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/fix indexOf return value git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/use long offsets and sizes for pointer arithmetic git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Remove unused locals git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix GetLastError bug Make ByReference derive from Memory git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Enable VM crash protection on w32, linux Move native library init from Pointer to Native Fix Pointer.setNativeLong bug Make library initialization explicit in Pointer/NativeLibrary Javadoc cleanup git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Add callback tests for smaller int types Copy generic java.nio.Buffer support from v3 branch + tests Copy missed sparc-isms from v3 branch for building sunos variants Add stubs for platform-specific jars Include libffi testsuite Remove /lib64 references as per v3 branch Auto-generate os prefix from os.name (as per v3 branch) to automatically handle new targets Update overview to include Buffer, function pointer info Include src.zip and doc.zip in dist generation git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Use libffi instead of custom assembly (tested for darwin (universal), linux-i386, win32-x86 git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
646,646,2.0,0.6377000212669373,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","allocate minimal space for invocation args, instead of MAX_NARGS git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/update project files (netbeans/eclipse) use setjmp/longjmp to recover from w32 faults instead of simply setting SP embed version resource information in w32 dll dont update last error if ffi_call faults git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Consolidate Structure.ByReference reads Cache native library options as well as alignments/type mappers Ensure library options are passed to callbacks Add protection around remaining Pointer read/write calls Consolidate wide character reads/writes Avoid stack overflow reading self-referential structures or loops More prettification of Structure.toString Add size_t/off_t standard types git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Move w32 JAWT load into native code Fix Structure.ByValue for callback arg/return Perform better type checking on callback arg/return types Propagate library/symbol lookup error messages Enable loading of libraries with non-ascii names Tighten type checking in native code git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Consolidate structure ffi type info initialization and avoid premature GC Enable union by-value by using largest fields type info Explicitly throw IllegalArgument on bad type info Throw IllegalState on missing type info Explicitly write version/md5 info into Makefile from ant (to avoid platform-specific variances in sed) Make Structure.ByValue/ByReference public to allow client code comparisons Consolidate Structure field get/set operations Avoid extra Pointer peer lookup from native code (wmeissner) git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/defer AWT toolkit initialization until actual JAWT use Make w32 dynamically load JAWT to avoid forcing toolkit init on JNA load git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Embed version and checksum into native library Print version information for java jna.jar git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Avoid error loading JAWT when running headless Use primary colors in window shape test (Dan) git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/implement struct by value add missing Pointer char function git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
647,647,7.0,0.8641999959945679,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test",Use UTF8 encoding for OSX library names git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
648,648,8.0,0.762499988079071,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",merge win64 branch git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
649,649,18.0,0.9620000123977661,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Add explicit check for correct stdcall stack pointer git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Ensure TCHAR*[] gets encoded properly on w32, wchar_t*[] on others git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Enable callback return values (e.g. sig_t signal(int sig, sig_t f)) Move native function proxy wrapping to CallbackReference from Structure git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
650,650,18.0,0.762499988079071,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",implement struct by value add missing Pointer char function git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
651,651,11.0,0.762499988079071,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Use XImage instead of XDrawRectangle point by point git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
652,652,5.0,0.8416000008583069,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Fix compilation error in file monitor example git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
653,653,18.0,0.9524999856948853,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Dont keep a strong reference to the library proxy Keep a mapping for the proxy on calls to loadLibrary Dont fail if no alpha on shaped window demo Clean up a few comments git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
654,654,18.0,0.920799970626831,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Show drop shadow on OSX BalloonManager Fix NPE on OSX BalloonManager git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix GetLastError bug Make ByReference derive from Memory git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
655,655,19.0,0.949999988079071,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",Fix OSX window transparency for 1.5+/Leopard git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Allow window masks to be set on heavyweight components (at least on w32 and x11) git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Improve transparent window drawing performance on w32 git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
656,656,19.0,0.8812000155448914,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",fix bug where struct is incorrectly passed by value instead of by reference git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
657,657,8.0,0.8944000005722046,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Fix cursor tracking on alpha-masked windows git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Work around OSX transparent window dragging bug git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
658,658,16.0,0.762499988079071,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Use XImage instead of XDrawRectangle point by point git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
659,659,16.0,0.6050000190734863,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Fix wchar_t* return when null Allow Pointer[] as function argument Fix window utils test on osx to avoid os-cast shadows git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Ensure TCHAR*[] gets encoded properly on w32, wchar_t*[] on others git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
660,660,18.0,0.8416000008583069,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Allow array of struct by reference as argument Pretty-print structure info in toString git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
661,661,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Fix NPE git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
662,662,18.0,0.970300018787384,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Allow arbitrary callback method names Allow specification of callback type mapper with TYPE_MAPPER Allow write with uninitialized boxed primitives in Structure Fix memory leak with callbacks called from native threads w/no java context Fix Structure derived classes to allow setting TypeMapper git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/merge win64 branch git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
663,663,18.0,0.762499988079071,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",implement struct by value add missing Pointer char function git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
664,664,18.0,0.5712000131607056,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Fix wchar_t* return when null Allow Pointer[] as function argument Fix window utils test on osx to avoid os-cast shadows git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Enable VM crash protection on w32, linux Move native library init from Pointer to Native Fix Pointer.setNativeLong bug Make library initialization explicit in Pointer/NativeLibrary Javadoc cleanup git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Clean up tests git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
665,665,3.0,0.5891000032424927,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",add missing import git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/make color-checking on OSX more robust git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
666,666,7.0,0.9839000105857849,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Allow lookup of OSX framework libraries by name git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix GetLastError bug Make ByReference derive from Memory git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fixup library searching for 32bit VM on 64bit OS scenarios git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Add callback tests for smaller int types Copy generic java.nio.Buffer support from v3 branch + tests Copy missed sparc-isms from v3 branch for building sunos variants Add stubs for platform-specific jars Include libffi testsuite Remove /lib64 references as per v3 branch Auto-generate os prefix from os.name (as per v3 branch) to automatically handle new targets Update overview to include Buffer, function pointer info Include src.zip and doc.zip in dist generation git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Copy fix for issue from v3 branch git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
667,667,16.0,0.724399983882904,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Add explicit test for matching linux versioned libs git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Provide explicit dispose on NativeLibrary Cache NativeMappedConverter instances for improved performance Preliminary support for wince (improve backwards compatibility for older VMs) Provide synch after call interface for arguments that need to perform some sort of synchronization after a native call git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Move w32 JAWT load into native code Fix Structure.ByValue for callback arg/return Perform better type checking on callback arg/return types Propagate library/symbol lookup error messages Enable loading of libraries with non-ascii names Tighten type checking in native code git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Provide for tweaking of invocation handling between interface and native Add test for annotation preservation in proxy methods Add explicit global variable lookup Add utility method to determine Web Start native library location git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
668,668,2.0,0.864300012588501,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Check entire linux version, not just last digit git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
669,669,18.0,0.9711999893188477,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Fix stdcall callbacks catch all exceptions when invoking callback git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Enable callback return values (e.g. sig_t signal(int sig, sig_t f)) Move native function proxy wrapping to CallbackReference from Structure git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Revert r266 Integer.valueOf() and friends are 1.5+ features, so it breaks 1.4 support git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Add auto-conversion for custom types Fix X11 lib for 64-bit use git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Use libffi instead of custom assembly (tested for darwin (universal), linux-i386, win32-x86 git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
670,670,8.0,0.9843999743461609,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Consolidate Structure.ByReference reads Cache native library options as well as alignments/type mappers Ensure library options are passed to callbacks Add protection around remaining Pointer read/write calls Consolidate wide character reads/writes Avoid stack overflow reading self-referential structures or loops More prettification of Structure.toString Add size_t/off_t standard types git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Move w32 JAWT load into native code Fix Structure.ByValue for callback arg/return Perform better type checking on callback arg/return types Propagate library/symbol lookup error messages Enable loading of libraries with non-ascii names Tighten type checking in native code git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/implement struct by value add missing Pointer char function git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
671,671,19.0,0.6833000183105469,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",allow String[] as callback argument/return git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
672,672,12.0,0.7623999714851379,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",improve w32 window masking git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
673,673,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Fix bug in stdcall function mapping when using struct by value Defer size_t/off_t definition, for now git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
674,674,18.0,0.9821000099182129,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Allow array of struct by reference as argument Pretty-print structure info in toString git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Consolidate structure ffi type info initialization and avoid premature GC Enable union by-value by using largest fields type info Explicitly throw IllegalArgument on bad type info Throw IllegalState on missing type info Explicitly write version/md5 info into Makefile from ant (to avoid platform-specific variances in sed) Make Structure.ByValue/ByReference public to allow client code comparisons Consolidate Structure field get/set operations Avoid extra Pointer peer lookup from native code (wmeissner) git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
675,675,16.0,0.9472000002861023,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Provide explicit dispose on NativeLibrary Cache NativeMappedConverter instances for improved performance Preliminary support for wince (improve backwards compatibility for older VMs) Provide synch after call interface for arguments that need to perform some sort of synchronization after a native call git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
676,676,8.0,0.8416000008583069,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Fix NPE in NativeMappedConverter git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/Fix NPE when using NativeMapped within Structure ( git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
677,677,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Fix bug in stdcall function mapping when using struct by value Defer size_t/off_t definition, for now git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/"
678,678,18.0,0.762499988079071,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",merge win64 branch git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
679,679,1.0,0.6833000183105469,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",synch libffi with (adds mingw32ce support) git-svn-id: 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80/
680,680,1.0,0.6832000017166138,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Wed Dec 10 03:10:01 UTC 2008 Steven Stallion
681,681,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",BranchChangeLogTag: Fri Jun 19 22:14:25 UTC 2009 Adam Mitz
682,682,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Mon Jul 18 13:35:36 UTC 2011 Paul Calabrese
683,683,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",ChangeLogTag: Fri Feb 10 20:49:54 UTC 2012 Adam Mitz
684,684,16.0,0.9524999856948853,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","BranchChangeLogTag: Thu May 15 20:54:58 UTC 2014 Brian Johnson Wed May 7 18:09:59 UTC 2014 Brian Johnson Transport work possible deadlocking in MulticastTransport.cpp, added logging to trace assoc failures throughout multicast, removed an unused TransportClient* arg in UdpTransport/MulticastDataLink released lock prior to calling passive_connection to alleviate deadlock with accept_datalink start_session needing to acquire lock for find_or_create_session/"
685,685,16.0,0.7799000144004822,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Multicast Transport work possible deadlocking in MulticastTransport.cpp, added logging to trace assoc failures throughout multicast, removed an unused TransportClient* arg in UdpTransport/"
686,686,16.0,0.8416000008583069,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",BranchLogTag: Wed May 7 13:45:52 UTC 2014 Peter Oschwald Tue May 6 18:00:21 UTC 2014 Peter Oschwald unused args in RtpsUdpTransport.cpp to alleviate compile warnings/
687,687,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Wed Aug 27 16:57:49 UTC 2014 Byron Harris
688,688,1.0,0.5249999761581421,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",ChangeLogTag: Wed Jan 28 18:24:36 UTC 2015 Peter Oschwald
689,689,1.0,0.8812000155448914,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",ChangeLogTag: Mon Mar 2 15:13:24 UTC 2015 Adam Mitz Fri Feb 27 22:55:08 UTC 2015 Adam Mitz Fri Feb 27 22:55:08 UTC 2015 Adam Mitz Mon Feb 16 23:19:42 UTC 2015 Adam Mitz Wed Jan 14 23:18:18 UTC 2015 Adam Mitz Wed Jan 14 21:11:32 UTC 2015 Jeff Schmitz Wed Jan 14 18:46:06 UTC 2015 Jeff Schmitz Fri Jan 9 16:16:41 UTC 2015 Jeff Schmitz Fri Jan 9 15:27:06 UTC 2015 Adam Mitz Thu Jan 8 22:37:55 UTC 2015 Jeff Schmitz Wed Jan 7 22:51:47 UTC 2015 Jeff Schmitz
690,690,5.0,0.9049999713897705,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Initial support for Wireshark 2.x, see TODO comments proto_tree_add* still needs to be handled as this has a large breaking change in wireshark/"
691,691,15.0,0.9914000034332275,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Use ACE_ERROR together with LM_ERROR, see issue * dds/DCPS/DataReaderImpl.cpp: * dds/DCPS/DataWriterImpl.cpp: * dds/DCPS/DiscoveryBase.h: * dds/DCPS/MultiTopicDataReaderBase.cpp: * dds/DCPS/RTPS/ParameterListConverter.cpp: * dds/DCPS/RTPS/Sedp.cpp: * dds/DCPS/RTPS/Spdp.cpp: * dds/DCPS/ReactorInterceptor.cpp: * dds/DCPS/RecorderImpl.cpp: * dds/DCPS/ReplayerImpl.cpp: * dds/DCPS/StaticDiscovery.cpp: * dds/DCPS/transport/framework/ReceiveListenerSet.cpp: * dds/DCPS/transport/multicast/MulticastTransport.cpp: * dds/DCPS/transport/rtps_udp/RtpsUdpDataLink.cpp: * dds/DCPS/transport/rtps_udp/RtpsUdpSendStrategy.cpp: * dds/DCPS/transport/rtps_udp/RtpsUdpTransport.cpp: * dds/DCPS/transport/shmem/ShmemTransport.cpp: * dds/DCPS/transport/tcp/TcpTransport.cpp: * dds/DCPS/transport/udp/UdpTransport.cpp: * dds/FACE/config/QosSettings.cpp: * dds/idl/ts_generator.cpp: * performance-tests/DCPS/MulticastListenerTest/Writer.cpp: * performance-tests/DCPS/SimpleE2ETest/Reader.cpp: * performance-tests/DCPS/SimpleE2ETest/Writer.cpp: * performance-tests/DCPS/TCPListenerTest/Writer.cpp: * performance-tests/DCPS/UDPListenerTest/Writer.cpp: * performance-tests/DCPS/UDPNoKeyTest/Reader.cpp: * performance-tests/DCPS/UDPNoKeyTest/Writer.cpp: * tests/DCPS/Compiler/idl_test1_main/main.cpp: * tests/DCPS/Compiler/idl_test3_main/main.cpp: * tests/DCPS/FooTest5/DataReaderListener.cpp: * tests/DCPS/ManyToMany/publisher.cpp: * tests/DCPS/ManyToMany/subscriber.cpp: * tests/DCPS/NotifyTest/subscriber.cpp: * tests/DCPS/Ownership/DataReaderListener.cpp: * tests/DCPS/Ownership/subscriber.cpp: * tests/DCPS/Presentation/main.cpp: * tests/DCPS/RtpsDiscovery/RtpsDiscoveryTest.cpp: * tests/DCPS/StaticDiscovery/DataReaderListenerImpl.cpp: * tests/FACE/Compiler/idl_test1_main/main.cpp: * tests/FACE/Compiler/idl_test3_main/main.cpp: * tests/FACE/Unit/test_check.h: * tests/transport/rtps/subscriber.cpp: * tests/transport/rtps_reliability/rtps_reliability.cpp:/"
692,692,15.0,0.9940999746322632,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Dissector: Handle invalid ITLs, mark missing ITLs Handle Segfaults in sample dissectors from invalid ITLs. This wouldnt detect if an ITL is valid, just check if a pointer is definitely invalid during the payload dissection, inform the user via the GUI that the error is most likely caused by an invalid ITL file and exit as gracefully as possible (exit()). Ideally it would just mark the packets as malformed and recover, but that code is less than ideal (using set long jump from the SIGSEGV handler). Also in this commit is using Wiresharks expert info to inform the user in the GUI that it couldnt dissect a sample payload for other reasons: No Topic to Track or No Dissector could be found for the Type./Dissector: Basic Payload Dissection in WS2 This is the basic framework for payload dissection in wireshark2. Involves adding a hf vector to the Sample_Manager Singleton, adding fields as they encountered in Sample_Dissector, and merging them with the constant DCPS fields and passing the result to wireshark when DCPS is registered. Sample Fields are left with a int that is the wireshark field id (hf_). During packet dissect, Sample_Fields are passed the wireshark field id and using that they can add data corresponding to their field. Payload type is now set on opendds.sample.payload and payload contents are added in a tree under that. Work left as of writing this is: to fix/complete registration of non string fields, involve composite types (seq, union, etc), clean up Sample_Manager usage, check/stop possible leak of dynamically generated field names and get working with tests other than Messenger./Dissector: Add Sample IDL Type to Protocol Tree Also because it uses the preset header field, DCPS packets can not be filtered by if they contain Sample Data and more specifically by IDL Type./Dissector: Fixed ""double"" DCPS packet info In the packet tree, OpenDDS packet infomation would be added twice to the packet information tree, at least in Wireshark 2.4.1 for the Messager tests. This appears to resolve that by removing a dissection call for TCP packets./"
693,693,10.0,0.9546999931335449,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Dissector: Adjustments for WS compatibility Made some changes to README Sample Dissection enabled for Wireshark 1.x. Attempted to make compatible with the next release, 2.5/2.6, but it has an assertion error on start up. 1.10 and before segfaults. Tested successfully with 1.12, 2.0, 2.2, and 2.4 on Linux./"
694,694,16.0,0.9915000200271606,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","dissector: Fixed Type Support and expert changes Added support for dissection for OpenDDS fixed-point numbers which are converted to doubles for numerical functions. If ACE_CDR::Fixed is missing, a message will be displayed that this is so. Also started marking some packets with a warning instead of marking it as malformed. For example, when an individual field fails for some reason or non essential information is unavailable, like the dissector ITL file./Dissector: Various Changes for PR Comments Removed Some Manual Memory Management: String and WString code rewritten to use TAO::String_Manager_T utf16_to_utf8 rewritten to use std::vector Various Other Small Changes/Dissector: %s %C ACE_DEBUG/Dissector: READ and error handling fix/Dissector: Fixs for PR Addressing Adams inital comments, Codacy, the Merge Conflict and a few things I found along the way./Dissector: Various Smaller Tasks Completely Removed ws_proto_tree_add_text(). Mark Packet if a determinable error occurs during sample dissection./Dissector: Serializer Refactor: Compiles, but broken/Dissector: Nexted Structs/Dissector: Namespace Resolution and Union Bug Added Sample_Base as a Parent Class to Sample_Field and Sample_Dissector. Sample_Base holds wireshark namespace and field for every context that its child class is used in. Tweaked Array and Sequence Labels and Namespace. For now using ""_e"" (for element) to access an element of the sequence or array. Removed namespace debug messages. Fixed bug in Union length calculation that caused String Key Test to hang on dissection./"
695,695,5.0,0.9635000228881836,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Dissector: Refctr SD with RCH (Compiles but broken)/Various Improvements to the Dissector Made small changes to make it more complaint with what Wireshark is expecting. Some Formatting fixes and more comments Fixed a malformed packet error where multiple samples causes a boundary error of some kind./
696,696,11.0,0.9136000275611877,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","fix security test issues (double association / volatile gap without info_dst / secure discovery), fix rtps transport test to associate correctly/"
697,697,2.0,0.982699990272522,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","There can be reasons to explicitly include directories in jars, and to extract them even if they are empty, so JarX and Build now permit that./Using the ZipFile enumeration in extract(String) was a crock. The enumeration comes back in hashed order, the resulting seek time is atrocious, and on a large enough jar, the oh-so-reliable ZipFile would eventually just barf. Now we open the file twice, the second time as a ZipInputStream. Its more reliable and faster, though it does introduce assumptions about the file being a regular file, still there when we try to open it the second time, etc. Sure would be nice if ZipFile worked better./Initial commit./"
698,698,9.0,0.5249999761581421,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/
699,699,9.0,0.762499988079071,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/Initial revision/
700,700,9.0,0.5249999761581421,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/
701,701,9.0,0.5249999761581421,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/
702,702,12.0,0.8944000005722046,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",*** empty log message ***/*** empty log message ***/Added classes to handle SPI cursors and complex set parameters./
703,703,9.0,0.5249999761581421,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/
704,704,11.0,0.8100000023841858,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Added version logic to makefile system/
705,705,9.0,0.8098999857902527,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/*** empty log message ***/*** empty log message ***/Change of license text/
706,706,17.0,0.8812000155448914,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",*** empty log message ***/Improved caching and closing of prepared statements./*** empty log message ***/*** empty log message ***/
707,707,9.0,0.5249999761581421,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/
708,708,9.0,0.5249999761581421,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/
709,709,9.0,0.5249999761581421,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/
710,710,9.0,0.5249999761581421,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/*** empty log message ***/*** empty log message ***/*** empty log message ***/*** empty log message ***/*** empty log message ***/*** empty log message ***/
711,711,1.0,0.8098999857902527,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Added SecurityManagers for trusted and untrusted versions/
712,712,15.0,0.5536999702453613,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",More forgiving getXXX methods on ResultSet with respect to types. Made use of Oid more strict. The Oid class is used instead of int. Moved TypeMap functionality into the Oid class./Added getOid method (needed for ResultSetMetaData)/
713,713,19.0,0.6833000183105469,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",Changes needed for PostgreSQL 8.1.x/
714,714,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",GCJ bug
715,715,15.0,0.9524999856948853,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Fixed bug causing primitive arrays to fail./Improved caching of class./More forgiving getXXX methods on ResultSet with respect to types. Made use of Oid more strict. The Oid class is used instead of int. Moved TypeMap functionality into the Oid class./
716,716,9.0,0.8812000155448914,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","From now getWarning throws an exception on a closed statement, according to JDBC specification/"
717,717,0.0,0.5249000191688538,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Filips DatabaseMetaData additions/
718,718,19.0,0.6833000183105469,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",Changes needed for PostgreSQL 8.1.x/
719,719,13.0,0.8944000005722046,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Backport of things missing in PostgreSQL 7.4/Backport of things missing in PostgreSQL 7.4/
720,720,16.0,0.9405999779701233,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Backport of things missing in PostgreSQL 7.4/Backport of things missing in PostgreSQL 7.4/Fixed bug# 1218/Removed some obsolete GCJ/*** empty log message ***/Added the ability to return a ResultSet Added object pooling capabilities/
721,721,1.0,0.9546999931335449,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Changes needed to accomodate OUT parameters in 8.1/Fixed bug# 1218/Runtime detection of integer-datetimes/Removed some obsolete GCJ/*** empty log message ***/Added the ability to return a ResultSet Added object pooling capabilities/Added SecurityManagers for trusted and untrusted versions/
722,722,12.0,0.6833000183105469,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Rewrite of fence mechanism/
723,723,12.0,0.8944000005722046,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Removed JavaHandle and complex MemoryContext stuff that was no longer needed./Rewrite of fence mechanism/
724,724,10.0,0.9472000002861023,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Rewrite of fence mechanism/Dropped support for versions prior to 8.0. Fixed bug causing stack check failur when backend was called from thread other than main Fixed stability issue related to GC and MemoryContexts/
725,725,12.0,0.8944000005722046,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Removed JavaHandle and complex MemoryContext stuff that was no longer needed./Rewrite of fence mechanism/
726,726,12.0,0.8944000005722046,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Removed JavaHandle and complex MemoryContext stuff that was no longer needed./Rewrite of fence mechanism/
727,727,9.0,0.9711999893188477,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Reinstated Exception traces when log level >= DEBUG1 Merged Invocation and CallContext Cleanup and refactoring/*** empty log message ***/Changes needed in order to compile with 8.0.x/Rewrite of fence mechanism/INDEX_MAX_KEYS now determined using GUC variable max_index_keys Dropped support for CYGWIN since 7.x no longer is supported/Fixed bug that caused crash when initializing using incorrect pljava.classpath/
728,728,15.0,0.4399000108242035,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Fixed bug Jar owner in the sqlj.jar_repository table changed type from oid to name./
729,729,11.0,0.46709999442100525,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Rewrite of the type mapping system/
730,730,9.0,0.5249999761581421,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",*** empty log message ***/
731,731,2.0,0.8100000023841858,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",Fixed some issues with Meta-data/
732,732,0.0,0.9320999979972839,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Batch execution of PreparedStatements failed because addBatch was only saving off the parameter values while it ignored the parameter types. Save both the types and values. Reported by Lucas Madar Bug
733,733,9.0,0.9768000245094299,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Changes are required because of move of GETSTRUCT() and timeout handling framework changes done in PG 9.3. Along with that I fixes minor issue in DDRProcessor.java that is causing ""illegal start of expression"" error. Maven did not worked for me to build it and hang endlessly while building c source code (pljava-so). As a workaround I temporarily fixed makefiles to test PG9.3 related fix that seems worked and generated pljava.jar and pljava.so files and their basic sanity seems working fine./"
734,734,6.0,0.6833000183105469,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Updated to Java7 and clang for Mac OS X Mavericks/
735,735,6.0,0.6833000183105469,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Updated to Java7 and clang for Mac OS X Mavericks/
736,736,6.0,0.6833000183105469,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Updated to Java7 and clang for Mac OS X Mavericks/
737,737,6.0,0.6833000183105469,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Updated to Java7 and clang for Mac OS X Mavericks/
738,738,2.0,0.9937999844551086,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Snippet implicitly requires its implementor-name. That way, a snippet that will test some condition and enable that implementor-name at install time can be declared to provide it, and be emitted earlier. However, not to insist on supplying a snippet that explicitly provides every implementor name ... some could just be selected by the user with SET LOCAL pljava.implementors TO ... before installing. So the implicit requirement will be counted in indegree initially, to delay such a snippet as far as practical, but only until a cycle-breaker releases it when no other progress can be made./SQL generator now reports require cycles usefully. Should have done that all along./Now Snippet can carry implementor-name. Nothing sets it yet, but it now defaults to PostgreSQL for everything, in recognition of the fact that hardly any of the PG SQL code that gets generated here will be in the exact ISO-prescribed syntax of the exact five allowed, unadorned SQL statements. Yes, that makes DDR files bigger. The zip format will compress them well./Workaround the problem reported in The annotation processor in javac runs in multiple rounds, as long as generated source files keep appearing, and one final round after they dont. This code used to save some mapping of Class objects to the javax.lang.model objects (TypeElement/TypeMirror) until the final round, which worked in Java 6, but as of 7 one gets back different model objects in different rounds, for the same types, and they dont match, so type mapping breaks. This workaround moves all those lookups to before or during round 1, when a consistent set of model objects can be looked up. So, it will work as long as all the source files that might contain pljava annotations will be found in round 1. That should always be the case unless someone is using a very fancy build with _other_ annotation processors generating new source files with pljava annotations that have to be processed in additional rounds. For now, that wont work, because their types wont seem to match what was computed in round 1. So dont do that. might refer to this problem, and promises a fix in Java 9, for what its worth./"
739,739,18.0,0.762499988079071,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Just noticed this example left the schema name off./
740,740,5.0,0.9876999855041504,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Eliminate threadlock ops in string conversion. The Java methods related to charset encoding/decoding may be called repeatedly, and they dont require the threadlock to be released and reacquired. I dont measure much difference in timing (I dont really have a good ""average text"" corpus to test on; the test case for this bug is worst case because it uses all of the biggest characters.) Even without a compelling timing difference, the Java charset encoders/ decoders arent thread safe, so I feel just that much better holding on to the lock./Only ignore the expected exception in method lookup. That way other exceptions (like a problem in a class initializer, or out of memory) will not be hidden from view. Closes to allow building pljava with Microsoft Visual C Code changes to allow compilation and linking with Microsoft Visual C. Maven build process conditionalized to to detect Visual C and adjust options appropriately. See msvc-build-notes.txt for full details. Property names updated for clarity/"
741,741,18.0,0.5432000160217285,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Conform to Point/ComplexTuple renames. Completes the work on MappedUDT, with Point and ComplexTuple now annotation examples, and the corresponding code removed from the old hand-maintained examples.ddr. In a couple last this-really-has-to-be-it review passes... some msg() calls had the Element parameter in the wrong place (and, being variadic and expecting extra parameters, didnt warn about it). whether to include length/alignment/storage in the output when like is used should explicitly depend on whether they were set explicitly (is that explicit enough?); otherwise, in some cases it wouldnt be possible to override the values copied by like. removed a goofy constant in the VarlenaUDTTest left over from very early sanity checking. Shortened unnecessary full-qualification on some enums actually in scope./Conform to BaseUDT rename, add MappedUDT. Supporting MappedUDT is much simpler. It emits a CREATE TYPE foo AS ( ... structure ... ) if a structure is provided, or not, if it isnt, followed by a SELECT sqlj.add_type_mapping( ... ), and thats it. Much javadoc also added. This commit ends with pure renames of the Point and ComplexTuple examples in preparation to make them annotation examples ... breaking compilation until they are fixed up in the next commit, but git can see where they went./Add a class annotation to make a base/scalar UDT. Add the annotation, have DDRProcessor recognize it. Check the annotated class for the required properties and members. The snippets map formerly allowed only one type of annotation on a given element. That happened to work, but not now when a class could have both a UDT and a SQLAction annotation, for example. Adapt the map to key by element and snippet class, so snippets of different classes can be hung on an element and selectively retrieved. An old comment in populateAnnotationImpl suggested it would reduce boilerplate setter code if, failing to find a setter method, the field itself could be reflectively looked up and stored. Thats done now, so setter methods are needed only when something more special has to be done. Function declarations will be synthesized automagically for the four mandatory (in, out, recv, send) methods, but to allow their properties to be individually adjusted, they can still accept Function annotations. Thats done by hanging a new subclass of FunctionImpl on those elements, that generates the right special form of declaration, and has setters that refuse changes to certain properties where that wouldnt make sense. Treat the default implementor-tag (PostgreSQL if not changed with new ddr.implementor command line property) specially. Now that everything is getting wrapped with an implementor tag by default, the implied requires=""implementor-tag"" was holding everything back until released by the cycle-breaker, sometimes in puzzling order. The implementor tag that happens to be the default one should /not/ be treated as a pending requirement. Move ComplexScalar to annotation subpackage, in preparation for revamping it as an annotation example. So git wont lose track of it, this is a breaking change, to be fixed in the next commit with edits corresponding to the move./"
742,742,2.0,0.9801999926567078,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Workaround Windows creating_extension visibility. Should now detect (in most cases?) when an extension is being created, even in versions where creating_extension isnt visible in Windows. Test depends on seeing the command in ActivePortal; I am not sure what contexts could be contrived where that wouldnt work right, but ordinary foreseeable cases seem to work. Got rid of pljavaInExtension: the idea that two cases have to be distinguished (loading PL/Java itself as an extension, or using it in the creation of some other extension) was sound, but the second case isnt something that can be checked once at load time; it needs a backend function that sqlj.install_jar can invoke whenever needed./"
743,743,13.0,0.9472000002861023,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Handle PostgreSQL 8.1+ roles correctly. At least, cover the cases that obviously mess up in execution of deployment descriptors. This represents an executive decision that PG 8.1 is no longer supported./"
744,744,13.0,0.9472000002861023,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Handle PostgreSQL 8.1+ roles correctly. At least, cover the cases that obviously mess up in execution of deployment descriptors. This represents an executive decision that PG 8.1 is no longer supported./"
745,745,15.0,0.9745000004768372,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Use PG 8.3 find_coercion_pathway API. Ok, this compiles and doesnt break any tests (and will give errors for unhandled return cases, or a warning when disregarding domain constraints). The reason it went unnoticed so long is that Type_getCoerce{In,Out} are really very seldom called. They are almost dead code--fairly contrived function declarations are needed even to force them to be called for test purposes. That seems to be mostly because Type_getCoerce{In,Out} wont be called if Type_canReplaceType returns true, which it very often does, because (a) Type_fromOid silently replaces domains with their base types, and (b) Function builds the method signature by mapping the SQL return type to a Java type and looking for a matching method, failing if none is found, rather than looking for a method by its name and parameter signature only, then validating its return type. So, essentially by construction, coercion of the return type cant ever turn out to be necessary. It could, however, if the AS string gives the Java return type explicitly, and it requires coercion to the (base type of the) SQL type. SQL/JRT has no syntax to specify the Java return type, but as an extension it seems PL/Java does: ""Function mapping"" in the wiki gives an example where it precedes class.method, separated by a space. Only the example doesnt work. Thats because the getAS code accepts the syntax only when the return type is purely alphanumeric (no dots, so non-package-qualified). Not a recent change, has been that way since 2006. An is also accepted, though, even after a package-qualified return type. The example works if retried with an SQL/JRT, it seems to me, specifies the other approach to method resolution, that is, find the method by name and parameter signature, then work out what to do with its return type, but that would be a change to current PL/Java behavior. The unimplemented warning for RELABELTYPE to a domain type essentially cant be triggered, because of the way any domain is replaced by its base type before consulting canReplaceType. Thats harmless for IN parameters. For the return type, its a type-safety hole: a PL/Java function can return a value that isnt valid for its declared result domain. Fixing that should be possible, but beyond the scope of this issue. The not-implemented errors for COERCEVIAIO and ARRAYCOERCE can be triggered, just by constructing exactly the sort of function declarations where you would expect those things to happen. They would still be arguably contrived cases, where the AS string specifies a method with different types than would naturally correspond to the SQL ones. Nevertheless, the ""Function mappping"" wiki page says ""PL/Java will use a standard PostgreSQL explicit cast when the SQL type of the parameter or return value does not correspond..."" and thats not completely true at the moment, as long as some features available in standard PostgreSQL explicit casts, like array or I/O coercion, arent yet implemented. Again, beyond the scope of issue 65. At least those cases now give clear errors, instead of crashes or Krueger numbers as I just confirmed in a build without this change, so this does in fact fix a latent bug. Note that the single-row result set writers used for functions that return composite or set-of-composite results have their own coercion logic completely unrelated to this, and implemented in SPIConnection.java. There are more moving parts here than I had hoped..../Type_getCoerceOut correct statement order. Ken Olson pointed this out as a case of dead code, but in fact it was live use of an unassigned variable just before the function call that was going to assign it. Had to be an editing mixup, has been that way a very long time, and has probably not caused more problems only because its statistically rare for a random stack location to be exactly equal to InvalidOid. Scratches the surface of A full solution for that issue will have to come later, but this much was an obvious Heisenbug with an obvious fix./"
746,746,11.0,0.9908000230789185,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Lay groundwork for typmod support. To fully support type modifiers, including the three-argument forms of input/receive functions, will take more thought than 1.5.0 can wait for. But to release 1.5.0 with the SQL generator emitting the one-argument forms would invite headaches later, because CREATE OR REPLACE FUNCTION cant change the parameter list, and DROP/reCREATE would cause cascading disruption. So, make sure the generator already emits the three-argument form, and just be sure to fail if a non-default typmod is in fact passed, with more complete support to be added later. By happy chance, it seems that PostgreSQL only very rarely passes non- default typmods to the input/receive functions--possibly only during COPY operations. (Those were the only way I was able to test this change.) In most other cases where a typmod is used, PostgreSQL relies on a typmod application cast in a separate step, not on passing the typmod to input/receive. PL/Java is already usable to implement typmodin, typmodout, and cast functions, so in limited testing it seems possible already to do types with typmods, maybe as long as COPY operations on them are not needed. Therefore, add an example that illustrates it./"
747,747,18.0,0.9620000123977661,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Add large object truncate and 64-bit offsets. PG 8.3 introduced inv_truncate, and 9.3 made offsets/lengths 64 bit. Whats nice is that the Java and JNI method signatures have always been 64-bit ready; now just stop downcasting to 32 on 9.3+ PostgreSQLs where 64 bit offsets are really accepted./"
748,748,2.0,0.9843999743461609,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Make create-extension-didnt message clearer. The only error message resulting from a CREATE EXTENSION attempt that failed (because the library had been loaded before in the session, therefore LOAD was a no-op) is one about a table that already exists. But the name of the table appears in the error message, so the table may as well be named ""see doc: do CREATE EXTENSION PLJAVA in new session"" which may serve to get the point across./Rework extension wrappers slightly. Instead of different ad-hoc ways of detecting the already-LOADed no-op issue for different CREATE EXTENSION cases, have the LOAD- invoked code always drop the loadpath table, so all extension scripts can use the same approach to force an error if it didnt happen./"
749,749,15.0,0.9269000291824341,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Rework SQLInputFromChunk using direct bytebuffers. This will allow accommodating different byte orders, using the provisions built into ByteBuffer./"
750,750,15.0,0.9269000291824341,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Rework SQLOutputToChunk using direct bytebuffers. This will allow accommodating different byte orders using the provisions built into ByteBuffer./
751,751,11.0,0.9839000105857849,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Make DEBUG1 quieter. Move a bunch of DEBUG1s to DEBUG2, leaving DEBUG1 for the initial load message that identifies PL/Java and JVM versions. (This is NOTICE if PL/Java is explicitly LOADed, so its seen by default, but in other cases you can now see it by enabling DEBUG1, and not mixed in with a lot of other stuff.)/Cluster name as property and in jps title. Make the cluster name easily available if it is set (to a nonempty string, in PostgreSQL versions where it is available), as a java system property and in the title visible to jps and jvisualvm. In passing, lose the default vmoption to lock out jvisualvm local connections. Noah Misch persuades me that PostgreSQL itself doesnt make special efforts to prevent things that already require access as the postgres user on the server host, and it would be unpostgresy to do so here. Instead, simply document how to disable attachment, if the admin so desires./"
752,752,13.0,0.967199981212616,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Handle widening of portalPos and demise of posOverflow. No need for version conditionals around isPosOverflow: it isnt in an API class, and nothing in this code base uses it, so out it goes. This eliminates the other compile-time error that was blocking compilation for PG 9.6, but its not the last thing to have had its width changed./Handle the PG 9.6 widening of SPI_processed. Includes a start on some Java 8 JDBC additions: getLargeUpdateCount() and executeLargeBatch()./"
753,753,10.0,0.8416000008583069,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Handle the PG 9.6 widening of SPI_processed. Includes a start on some Java 8 JDBC additions: getLargeUpdateCount() and executeLargeBatch()./
754,754,10.0,0.8416000008583069,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Handle the PG 9.6 widening of SPI_processed. Includes a start on some Java 8 JDBC additions: getLargeUpdateCount() and executeLargeBatch()./
755,755,3.0,0.9003000259399414,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Generate SQL for trigger transition tables. To accommodate PotgreSQL 10 transition tables, allow the Trigger annotation to specify tableOld and/or tableNew, check that they are allowed (trigger must be AFTER and include events capable of populating the table), and generate the corresponding REFERENCING OLD TABLE AS ... NEW TABLE AS ... in the trigger declaration./"
756,756,17.0,0.9136000275611877,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Keep making DEBUG1 quieter. These sites were missed in commit 1eb3bd8, trying to get the PL/Java-loaded-versions announcement to be the only thing at DEBUG1./"
757,757,16.0,0.9472000002861023,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Add example for trigger transition tables. Expand the Triggers example so the trigger actually does something, and add a new trigger to test transition table functionality in PostgreSQL 10 or later./"
758,758,14.0,0.9524999856948853,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Fulfill old todo for setting a libjvm default. By passing on the mvn command line, the downstream maintainer of a packaging system for a platform where the standard Java install location is known can build a package where the default for pljava.libjvm_location is usually right./"
759,759,3.0,0.8812000155448914,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Annotations doesnt support CREATE CONSTRAINT TRIGGER and clause FROM schema.table
760,760,19.0,0.5591999888420105,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Annotate Connection methods and add more comments. Add to the methods in SPIConnection that are specified by the Connection API (to make it easier to spot the ones that arent). For those added in JDBC 4.1, the annotation is commented out, as PL/Java 1.5 still strives to be buildable with Java 6. Once the back-compatibility horizon is Java 7 or later, those can be uncommented. It would also be fair to say this has added annotations (or commented-out annotations) through JDBC 4.2, as it didnt add any new Connection methods. The PL/Java-specific and internal methods are now easier to pick out (theyre the ones without annotations), and have some more extensive comments about what theyre doing there. Also moved one method to be nearer the stuff it pertains to. No code changes (except to add the specified generic signature on getTypeMap/setTypeMap). Indentation adjusted in a couple contiguous areas./"
761,761,7.0,0.9136000275611877,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Add a constraint-trigger example. A more interesting test would be to actually try to insert 44, and verify that the exception happens. (Oh, for the chance to use pgTAP...)./"
762,762,6.0,0.5785999894142151,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Dont repeat yourself with UDT mappings. It was never satisfying to define a UDT in Java through annotations and still have the DDR generator give ""no mapping to an SQL type"" errors for functions accepting or returning that type. It was easy enough (if tedious) to work around by adding explicit type= or annotations, but the DDR generator had all the necessary information to figure that out without such manual help. Now it does, eliminating the tedium illustrated in To accommodate mappings added from the source being compiled, the protoMappings collection is now of TypeMirror instances rather than of Class instances. Still future work: also add implied ordering dependencies for uses of the new type (such as functions that have it as parameter or return types and have to follow the type declaration, as illustrated in or that are named in it as, for example, typmod in/out functions, and have to precede it)./Types.getArrayType() does the trick./Recognize type SQLXML in the annotation processor./"
763,763,5.0,0.9801999926567078,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Use getSQLTypeName, and add a test. Provide one of the short-term solutions suggested in issue As the current PreparedStatement implementation does not get the inferred parameter types from PostgreSQL (which became possible with SPI only as recently as PostgreSQL 9.0), its setObject method must make a best effort to map in the other direction, finding the PostgreSQL type that corresponds to the Java parameter value. In one case, this is easily made much more reliable: when the Java parameter value is an SQLData instance (a UDT), and therefore has a getSQLTypeName method, unused until now. Add a test method (in the ComplexTuple UDT example) to confirm that the type is properly mapped when passed as a parameter to a PreparedStatement./"
764,764,1.0,0.7623000144958496,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Take off the training wheels./
765,765,5.0,0.9801999926567078,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Use getSQLTypeName, and add a test. Provide one of the short-term solutions suggested in issue As the current PreparedStatement implementation does not get the inferred parameter types from PostgreSQL (which became possible with SPI only as recently as PostgreSQL 9.0), its setObject method must make a best effort to map in the other direction, finding the PostgreSQL type that corresponds to the Java parameter value. In one case, this is easily made much more reliable: when the Java parameter value is an SQLData instance (a UDT), and therefore has a getSQLTypeName method, unused until now. Add a test method (in the ComplexTuple UDT example) to confirm that the type is properly mapped when passed as a parameter to a PreparedStatement./"
766,766,1.0,0.944100022315979,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Use ALLOCSET_*_SIZES macros to create contexts. These macros became available in 9.6 (ea268cd, with a ""back-patch"" appearing in 9.6.0, funnily enough). It becomes /necessary/ to use them in PG 11 (9fa6f00), at least when using AllocSetContextCreate rather than the PG11-new AllocSetContextCreateExtended. Here, just define the macros locally for PG 9.6, and use them unconditionally./"
767,767,2.0,0.8944000005722046,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",Drop GUC_LIST_QUOTE flag on pljava.implementors. It becomes disallowed for extensions to define GUC_LIST_QUOTE variables as of PG 11 (846b5a5)./
768,768,1.0,0.762499988079071,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Added TableBase::nativeToJson() currently disabled in JNI/
769,769,8.0,0.8100000023841858,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Updated tightdb version. Updated jni now warning free./
770,770,2.0,0.9524999856948853,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",Major overhaul of generic.mk/config.mk and build.sh which shall serve as a uniform front-end for building each part/Still working on script to build a complete distribution package including all language bindings/
771,771,6.0,0.9620000123977661,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",small update in error-text for Windows/Updated error-msg when we cant find the jni lib/Merge branch integration of github.com:nikuco/tightdb_java2 into integration Conflicts: src/main/java/com/tightdb/lib/TightDB.java/Better Error message when lib cant be loaded./Fixed library path configuration (issue
772,772,1.0,0.9320999979972839,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Added TableBase::distinct() Added TableBase::average() Added checks for columnType in setIndex()/Prepared and added new methods to table/view interface (issue tightdb version. Updated jni now warning free./
773,773,4.0,0.8641999959945679,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Tracking latest error handling improvements in core library/WIP on ReadTransactions/
774,774,11.0,0.8641999959945679,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Fixed create of Group(byte[]). Enabled all GroupTest tests and added new test./
775,775,19.0,0.9692999720573425,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Tracking latest error handling improvements in core library/Changes due to improved error handling in core library. Group no longer has an is_valid() method. From now on, Group instances are always valid./Fixes due to changed Group and SharedGroup constructors in core library/Fixed create of Group(byte[]). Enabled all GroupTest tests and added new test./"
776,776,10.0,0.8098999857902527,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Updated to support internal on linux/
777,777,8.0,0.7623999714851379,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Correct transcoding from UTF-16 to UTF-8/Correct transcoding from UTF-8 to UTF-16/
778,778,9.0,0.9524999856948853,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","String conversion in both directions is complete except for an important FIXME in to_jstring() in util.h/String conversion from C++ to Java done, opposite order remains/Tracking changes in core library: Renaming of column type enumeration/"
779,779,17.0,0.970300018787384,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Added experimental table method: nativeFindSortedInt (currently disabled). Add try/catch around WriteToMem() (many more needs this)./Group::BufferSpec eliminated. Using BinaryData instead./String conversion in both directions is complete except for an important FIXME in to_jstring() in util.h/String conversion from C++ to Java done, opposite order remains/Fix due to core lib change: Group::get_table_count() Group::size()/"
780,780,9.0,0.7149999737739563,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Added experimental method to Table: findSortedLong(). NOTICE it does not return if the value was actually found or not you have to check that through a get() afterwards for now./Fixes for: Merge branch master into explicit_string_size/Added experimental method: table.moveLastOver() method/Added Table::addColumn(), renameColumn(), removeColumn()/String conversion in both directions is complete except for an important FIXME in to_jstring() in util.h/String conversion from C++ to Java done, opposite order remains/Tracking changes in core library: Renaming of column type enumeration/Just another FIXME/WIP: Updated with float, double support. Still a crash./"
781,781,12.0,0.762499988079071,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",minor test added/
782,782,12.0,0.49129998683929443,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Added testcases for Mixed float and double/
783,783,4.0,0.6833000183105469,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",REnamed Group::getTableCount() to size()/
784,784,8.0,0.9524999856948853,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","refdoc: added dynamic view and query and example for Table/float: updated floats sum, average to return double instead of float./WIP: Updated with float, double support. Still a crash./"
785,785,8.0,0.9648000001907349,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Added experimental method to Table: findSortedLong(). NOTICE it does not return if the value was actually found or not you have to check that through a get() afterwards for now./Added experimental method: table.moveLastOver() method/float: updated floats sum, average to return double instead of float./WIP: Updated with float, double support. Still a crash./"
786,786,8.0,0.9711999893188477,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","refdoc: added dynamic view and query and example for Table/Added experimental method to Table: findSortedLong(). NOTICE it does not return if the value was actually found or not you have to check that through a get() afterwards for now./added a few classes in Java ref-doc/Added experimental method: table.moveLastOver() method/Added Table::addColumn(), renameColumn(), removeColumn()/float: updated floats sum, average to return double instead of float./WIP: Updated with float, double support. Still a crash./"
787,787,12.0,0.8944000005722046,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",minor test added/Added set() method to the generated Cursor classes./
788,788,7.0,0.5246999859809875,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test",Use java.io.IOException./
789,789,16.0,0.5249999761581421,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Group::mode_Normal has been renamed to Group::mode_ReadWrite in the core library/
790,790,14.0,0.5916000008583069,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Experimental Table.findSortedLong() replaced by Table.lowerBoundLong() and Table.upperBoundLong() because these provide more flexibility, and it follows similar changes in the core library/Added count*() and lookup() methods currently without tests/"
791,791,6.0,0.9136000275611877,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Now throws exception when calling a method on a closed Group() Updated tutorial and showcase a bit./Bugfix: mem-leak removed/
792,792,10.0,0.8944000005722046,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","added count(), lookup() metods. Added simple performance test/"
793,793,1.0,0.864300012588501,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Added count*() and lookup() methods currently without tests/
794,794,10.0,0.9587000012397766,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fixes for: A config step has been introduced into the build procedure./A config step has been introduced into the build procedure. The main reason is that it allows reliable uninstallation, but there are several other benefits too. Also, support for running in debug mode has been improved./"
795,795,1.0,0.7623000144958496,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Added hasChanged() to SharedGroup/
796,796,17.0,0.8416000008583069,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",Added exception handling for Group (untested)/
797,797,16.0,0.8100000023841858,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Exception handling for Table (not tested)/
798,798,13.0,0.44839999079704285,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Added equals, toString and toJson/"
799,799,16.0,0.977400004863739,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","renamed exception/add exception. Throw it when wrong type is acessed from Mixed type/Table.close() now private (added private_debug_close() instead. Added better support for detecting valid View and Query after close of table. Still not completely tested, also missing core-support./ColumnType.Long renamed to Integer/BREAKING CHANGE: Column type enum have been renamed. Is now less verbose and more java like e.g. ColumnType.ColumnTypeString is now ColumnType.STRING etc/"
800,800,2.0,0.9269000291824341,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",Fixed bug in Group.equals(). Added Table.equals and tests./Test cases added and group.equals modified/
801,801,0.0,0.5514000058174133,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Updated test of TableView and Query close() test. + other minor updates/added private_close/Renamed class util to Util./Added finalize to TableQuery/
802,802,16.0,0.9620000123977661,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Outcommented lookup in table and tableview/Query could leak./Updated test of TableView and Query close() test. + other minor updates/Renamed class util to Util./Space in public comments added + Renaming of addLong to incrementInColumn (including test cases)/
803,803,16.0,0.9829999804496765,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Lookup reenabled on typed table. Test case added, Check for null otherwise core crash/debugging datebug fixed one/merged develop into breaking/Renamed to reflect c++ renames/Outcommented lookup in table and tableview/Table.close() now private (added private_debug_close() instead. Added better support for detecting valid View and Query after close of table. Still not completely tested, also missing core-support./Renamed class util to Util./Space in public comments added + Renaming of addLong to incrementInColumn (including test cases)/"
804,804,13.0,0.9320999979972839,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Renamed class util to Util./Fixed initialisation to be threadsafe. Updated load of library file for Windows (must look into this again later created Asana task)./
805,805,1.0,0.8812000155448914,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",subtable sort test case added + try fail added more places/
806,806,13.0,0.9049999713897705,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",moved check to java and changed to illegalArgument exception/added parent object to view and query. Now sets ptr to 0 in synchronized block/
807,807,7.0,0.920799970626831,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test",close made public and added to TableOrView interface. Initial test cases added for GC case/added jni bridge to getColumnIndex/
808,808,1.0,0.9366999864578247,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","removed CloseMutex class Commented out/added jni bridge to getColumnIndex/pivot native test cases added/throw exception when setting null on string + test cases/missing "".""/"
809,809,16.0,0.8944000005722046,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",check for if group is closed when closing transactions + test case Will crash core if not detected/
810,810,1.0,0.5249999761581421,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Added maximumDate and minimumDate/
811,811,10.0,0.9768000245094299,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Refactored json methods to support standalone objects. Added stub methods + begun work on unit tests./Merge branch master into cm-primary-keys Conflicts: changelog.txt realm-annotations-processor/src/main/java/io/realm/processor/RealmProcessor.java realm-annotations-processor/src/main/java/io/realm/processor/RealmProxyClassGenerator.java realm/src/androidTest/java/io/realm/RealmAnnotationTest.java realm/src/main/java/io/realm/internal/Row.java realm/src/main/java/io/realm/internal/Table.java/Allow custom constructors but require no arg public constructor as well./Added unit tests and fixes bug found on the way/Preliminary support for primary keys in the binding + Unit tests./
812,812,5.0,0.8416000008583069,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Renamed ConcurrencyExample to ServiceExample. Added threadExample to distribution examples./
813,813,18.0,0.8416000008583069,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Renamed ConcurrencyExample to ServiceExample. Added threadExample to distribution examples./
814,814,18.0,0.8416000008583069,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Renamed ConcurrencyExample to ServiceExample. Added threadExample to distribution examples./
815,815,16.0,0.9320999979972839,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Make ViewHolder static./Fixed wrong usage of ViewHolder pattern. Fixed bad practise when inflating views./
816,816,4.0,0.9660999774932861,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Printing location of error/Move C++ Java exception mapping to a function/More debug info when converting a string from to Java fails./Revert the support for lenient UTF conversion/Fix for: Lenient UTF-8 UTF-16 transcoding (insert replacement characters)/Lenient UTF-8 UTF-16 transcoding (insert replacement characters)/Catching standard exceptions./Adding better error messages when converting to Java string/Use to_jstring() everywhere at the JNI layer./
817,817,4.0,0.9136000275611877,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Revert ""Revert ""Merge pull request from realm/kg-core-0.87.0"""" This reverts commit suggested a refactoring of the old tracing/logging system./"
818,818,3.0,0.972100019454956,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Throw NoSuchMethodError when RealmResults.indexOf() is called as the method is not implemented./Improved error message for child object sorting./Changing sort to be in-place./Added support for remove in RealmResults iterators. Additional unit tests for their usage./Proper iterators implemented for RealmResults./Root cause unit test added./Adding thread check on RealmObject. Refactoring unit tests. Updating changelog./
819,819,10.0,0.8098999857902527,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Preliminary support for primary keys in the binding + Unit tests./
820,820,4.0,0.5708000063896179,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Setting same value to a primary field no longer violates the primary key constraint./Primary keys are now indexed. Refactored Table/Row for missing cases + cleaner code. Additional unit tests./Preliminary support for primary keys in the binding + Unit tests./
821,821,10.0,0.7077000141143799,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Only check for duplicate values when switching primary key fields./Style fixes./Setting same value to a primary field no longer violates the primary key constraint./Merge branch master into cm-primary-keys Conflicts: changelog.txt realm-annotations-processor/src/main/java/io/realm/processor/RealmProcessor.java realm-annotations-processor/src/main/java/io/realm/processor/RealmProxyClassGenerator.java realm/src/androidTest/java/io/realm/RealmAnnotationTest.java realm/src/main/java/io/realm/internal/Row.java realm/src/main/java/io/realm/internal/Table.java/It is no longer possible to manually set 0 or """" in primary key fields. Refactored error checking so it is more maintainable./Primary keys are now indexed. Refactored Table/Row for missing cases + cleaner code. Additional unit tests./Preliminary support for primary keys in the binding + Unit tests./"
822,822,9.0,0.9761999845504761,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Better error reporting for error cases when creating a Realm using the File constructors./Revert ""Revert ""Merge pull request from realm/kg-core-0.87.0"""" This reverts commit 6be8edca486f53273f28553584d28bc17c5a0ddf./Mitigate the file size growing problem This change does the following: * disables the caching of Realm instances in threads without an event loop * makes the Realm class implement Closable * does reference counting for closing Realm instances * checks if the Realm instance has not been closed before any operation/"
823,823,15.0,0.8100000023841858,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Fixed issues with Null when using JSONObject./
824,824,2.0,0.8812000155448914,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",Made Thread example more resilient to monkey test events./
825,825,10.0,0.6833000183105469,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Realm Core has change namespace from tightdb to realm./Fixed unit test./
826,826,4.0,0.7623999714851379,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Queries on links can be case sensitive./
827,827,13.0,0.8812000155448914,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Fixed bug where copyToRealm() crashed when copying objects with primary key data./Moved setPrimaryKey to JNI./
828,828,9.0,0.944100022315979,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Handle null value for String in Json when updating Fix Update the objects String field to empty string when the corresponding field in Json is null./Updates due to PR feedback/Tighter check on table validation involving RealmList<> fields./
829,829,9.0,0.920799970626831,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Improve unit test for class with boolean fields in the AP/Updates due to PR feedback/Tighter check on table validation involving RealmList<> fields./
830,830,11.0,0.983299970626831,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Add analytics on annotation/Index fields with annotation And update test cases to adapt this change./Support search indexing for column int, bool, date 1. Enable the search index annotation on byte, short, int, long, boolean, and Date. 2. Enable add/remove search index in java. 3. Annotation processor test to support better detailed test cases. 4. Modify JNI test cases. 5. Update doc. 6. Add AnnotationIndexTypes to avoid polluting other test cases. This is the first PR for Implicit index to int primary keys will be handled in another PR./Simple dynamic API added./Add check on fields of type RealmList/"
831,831,1.0,0.8944000005722046,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Populate after generating the metadata/Avoid wildcard imports and execute in the background/Add analytics on annotation/
832,832,14.0,0.4562000036239624,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",Support UnreachableVersionException from Core/Realm will now throw a RealmError when Realm Core enters an unrecoverable error condition./
833,833,8.0,0.9789000153541565,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","add logic to release handover resources, update threading example with asyc transaction/using std::unique_ptr for handover + remove unnecessary begin_read in a spearate JNI call + latest fixes from fsa_handover_demo (untyped query etc.)/add support for findFirst & findAllSorted*, update UnitTests/Changing logic to handover the query from caller to background thread, handle different failure points (begin_read, import_handover), update UnitTests/Support UnreachableVersionException from Core/add retry policy + concurrency tests + perf improvement to the query/POC async query, using Core fsa_handover_demo branch/"
834,834,6.0,0.7623999714851379,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Merge remote-tracking branch origin/master into nh-async-query/
835,835,5.0,0.8812000155448914,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",add retry policy + concurrency tests + perf improvement to the query/
836,836,10.0,0.8098999857902527,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Fix a few lint warning reported by the Android Linter/
837,837,15.0,0.9366999864578247,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Add support for in-memory Realm 1. Add durability to createNativeWithImplicitTransactions. 2. Add inMemory to RealmConfiguration. 3. Support passing durability to SharedGroup constructor. 4. Add new static method getInMemoryRealm to Realm class. 5. Add test cases./
838,838,15.0,0.6521999835968018,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Suppress useless cast and raw type warnings in generated proxy classes. This warnings are reported when `-Xlint:all` is added to the compilar args. Example configuration of `build.gradle` is: ``` allprojects { gradle.projectsEvaluated { tasks.withType(JavaCompile) { options.compilerArgs options.compilerArgs } } } ```/reorder parameters of ColumnInfo class/fix for Problem: The Proxy class of each model holds column indices in static fields. These indices are good only when all Realm databases have the same column index in every column of the model class. If Realm databases have different column indices, the getter/setter of Proxy class reads/writes wrong column. Solution: This commit introduces `ColumnInfo` object that holds column indices information per Realm instance which shares the same database file./Add Nullable suport * Add boxed type support. * Add annotation. * isNull and isNotNull support all nullable types now. * equalTo query can take null as input param for nullable fields. * Add functions for nullable related migration. * JSON converter behavior changes for null value. * Update test cases./"
839,839,15.0,0.6521999835968018,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Suppress useless cast and raw type warnings in generated proxy classes. This warnings are reported when `-Xlint:all` is added to the compilar args. Example configuration of `build.gradle` is: ``` allprojects { gradle.projectsEvaluated { tasks.withType(JavaCompile) { options.compilerArgs options.compilerArgs } } } ```/reorder parameters of ColumnInfo class/fix for Problem: The Proxy class of each model holds column indices in static fields. These indices are good only when all Realm databases have the same column index in every column of the model class. If Realm databases have different column indices, the getter/setter of Proxy class reads/writes wrong column. Solution: This commit introduces `ColumnInfo` object that holds column indices information per Realm instance which shares the same database file./Add Nullable suport * Add boxed type support. * Add annotation. * isNull and isNotNull support all nullable types now. * equalTo query can take null as input param for nullable fields. * Add functions for nullable related migration. * JSON converter behavior changes for null value. * Update test cases./"
840,840,6.0,0.7623999714851379,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",fix tests/Milestone 4: before merging origin/master/
841,841,3.0,0.9855999946594238,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Change to is_not_null for links Since it is supported by core now./add distinctAsync/fix code style/Added RealmQuery.isEmpty()/optimising TableView construction/small code style fix/using TableRef/remove dynamic allocation for Handover TableView/now throwing Exception if we try to add/remove listener for unmanaged RealmObject/RealmResults/update as per round 2 feedback/Add Nullable suport * Add boxed type support. * Add annotation. * isNull and isNotNull support all nullable types now. * equalTo query can take null as input param for nullable fields. * Add functions for nullable related migration. * JSON converter behavior changes for null value. * Update test cases./update branch as per PR feedback/Milestone 4: before merging origin/master/Milestone 3: findAllSorted, findFirst & asyncTransaction working/Milestone 2: findAllAsync & findFirst working with retries sratetgies/"
842,842,19.0,0.7023000121116638,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",now throwing Exception if we try to add/remove listener for unmanaged RealmObject/RealmResults/update as per round 2 feedback/Milestone 4: before merging origin/master/Milestone 2: findAllAsync & findFirst working with retries sratetgies/
843,843,8.0,0.6833000183105469,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Update core to v0.89.8 Add RealmEncryptionNotSupportedException/
844,844,15.0,0.9692999720573425,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Add Nullable suport * Add boxed type support. * Add annotation. * isNull and isNotNull support all nullable types now. * equalTo query can take null as input param for nullable fields. * Add functions for nullable related migration. * JSON converter behavior changes for null value. * Update test cases./
845,845,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Milesonte1/
846,846,4.0,0.9635000228881836,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Adapt core changes in 0.92.0 * Remove insert_xxx methods from Table. * Table.where() changed to user a Ref instead of a raw ptr. * makeWriteLogCollector to make_client_history * advance_read/promote_to_write/rollback_and_continue_as_read need a replication ptr as param now. * Remove add_int method. * A bug fix in linkview jni. * adjust is removed from TableOrView. * Other test cases fix. ** testMoveUp & testRemove dont pass right now./
847,847,6.0,0.8812000155448914,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Merge remote-tracking branch origin/master into nh-async-query/Milestone 4: before merging origin/master/Cleanup Group.java and SharedGroup.java/
848,848,18.0,0.9472000002861023,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",More fixes for the AP/Fix the AP unit tests/Fix the unit tests of the annotations processor. Also remove one unit test which makes no sense now that users can write model classes in any way they like./update unit tests for annotation-processor/Added Realm.copyFromRealm()/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/
849,849,10.0,0.4823000133037567,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",More fixes for the AP/Fix the AP unit tests/Fix the unit tests of the annotations processor. Also remove one unit test which makes no sense now that users can write model classes in any way they like./update unit tests for annotation-processor/Added Realm.copyFromRealm()/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/
850,850,18.0,0.7476000189781189,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Remove reflection from the generated proxy code./Address the first wave of review comments/Realm.createOrUpdateObjectFromJson() now works correctly if the RealmObject class contains a primary key/
851,851,18.0,0.48159998655319214,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Remove reflection from the generated proxy code./
852,852,5.0,0.9455000162124634,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Access to RealmResults based on deleted RealmList * When the original RealmList is deleted, for most methods of RealmResults should just work without crash by just treat it like an empty RealmResults. * RealmResults.where() throws IllegalStateExecption in this case. * RealmResults.isValid() returns false in this case. This is a temp fix, check for more details. Close RealmQuery.distinct(), RealmResults.distinct()/Fix PMD error. Nested if/Applying code review recommendations and adding removed from Realm tests. Review recommendations are from and and jni layer interop code to utilize find method. Testing Adding header file Checking to ensure were on the right Realm before performing the contains check. Adding changlog Adding table view code and changing return types to jlong so we can use them in the indexOf Adding multi-realm test Removing code that should not have been in the commit Changing LinkView.cpp method to use commonly used Macro and udpated null checks. Changing LinkView.cpp method to use commonly used Macro and udpated null checks and moved contains method to override instead of overload. Using instance of instead of assignable from and using long value instead of returning a boolean. Checking for row and index validity in the cpp layer. Also removing the not found constant in order to the use the constant in TableOrView Checking for managed mode. Falling back to default impl. Updating changelog. Reverting back to to_jlong_or_not_found Fixing a few compilation issues as well as applying code review updates. Fixing managed mode bug that I accidentally introduced. Updates as per recommended, issues listed below. After changing cpp code to return tests started failing items were not found. Im on the fence if this is expected or not. Im not to familiar wit the underlying core to know if should be needed. Also added tests to check to see if a query or list contains an item that is in the same Realm, but from another query/result. This test creates a native crash. I tried to identify the root cause by pulling the tombstone and performing `ndk-stack` and `addr2line` on it but I could not properly identify the symbols and where they might be. Inspiration and HOWTO to debug NDK from here: Adding nativePtr to JNI call. Also added isLoaded() call. Not contained test failing on out of bounds exception on row indexes. Changing TableOrView#find() name to TableOrView#sourceRowIndex() as per recommendation of Adding check for parent table and re-enabled the other jni targets and fixed incorrect test. Received help with this from and RxJava support/fix 1894 changelistener not called/enhancing notifications, adding Async queries for DynamicRealmObject/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
853,853,4.0,0.9587000012397766,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Access to RealmResults based on deleted RealmList * When the original RealmList is deleted, for most methods of RealmResults should just work without crash by just treat it like an empty RealmResults. * RealmResults.where() throws IllegalStateExecption in this case. * RealmResults.isValid() returns false in this case. This is a temp fix, check for more details. Close Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
854,854,17.0,0.8944000005722046,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",Fix RealmQuery.isNotEmpty(). It is fix for Fix the native method + Renaming./Support for RealmQuery.isNotEmpty() added/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/
855,855,1.0,0.8944000005722046,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Added RealmQuery.distinct(), RealmResults.distinct()/Cleanup internal classes/"
856,856,0.0,0.9049999713897705,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Notify listeners of async RealmObject even if the result is empty/enhancing notifications, adding Async queries for DynamicRealmObject/"
857,857,10.0,0.9136000275611877,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Cleanup internal classes/enhancing notifications, adding Async queries for DynamicRealmObject/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
858,858,9.0,0.8098999857902527,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Moving actual loading to RealmCore/
859,859,4.0,0.9269000291824341,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",New Migration and Dynamic API. New Sort/Case enums. New RealmCache/Release LinkView native pointers Fix * Add abstract method to NativeObjectReference for native resource releasing. * Implement NativeObjectReference.cleanup for UncheckRow and LinkView/
860,860,4.0,0.9049999713897705,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Release LinkView native pointers Fix * Add abstract method to NativeObjectReference for native resource releasing. * Implement NativeObjectReference.cleanup for UncheckRow and LinkView/
861,861,5.0,0.9951000213623047,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Add RealmList.removeAllFromRealm and Realm.clear * Add LinkView.removeAllTargetRows. * Add Realm.clear to remove all objects from Realm. * Add RealmList.removeAllFromRealm(). * Javadoc & test case update. Close code review recommendations and adding removed from Realm tests. Review recommendations are from and and jni layer interop code to utilize find method. Testing Adding header file Checking to ensure were on the right Realm before performing the contains check. Adding changlog Adding table view code and changing return types to jlong so we can use them in the indexOf Adding multi-realm test Removing code that should not have been in the commit Changing LinkView.cpp method to use commonly used Macro and udpated null checks. Changing LinkView.cpp method to use commonly used Macro and udpated null checks and moved contains method to override instead of overload. Using instance of instead of assignable from and using long value instead of returning a boolean. Checking for row and index validity in the cpp layer. Also removing the not found constant in order to the use the constant in TableOrView Checking for managed mode. Falling back to default impl. Updating changelog. Reverting back to to_jlong_or_not_found Fixing a few compilation issues as well as applying code review updates. Fixing managed mode bug that I accidentally introduced. Updates as per recommended, issues listed below. After changing cpp code to return tests started failing items were not found. Im on the fence if this is expected or not. Im not to familiar wit the underlying core to know if should be needed. Also added tests to check to see if a query or list contains an item that is in the same Realm, but from another query/result. This test creates a native crash. I tried to identify the root cause by pulling the tombstone and performing `ndk-stack` and `addr2line` on it but I could not properly identify the symbols and where they might be. Inspiration and HOWTO to debug NDK from here: Adding nativePtr to JNI call. Also added isLoaded() call. Not contained test failing on out of bounds exception on row indexes. Changing TableOrView#find() name to TableOrView#sourceRowIndex() as per recommendation of Adding check for parent table and re-enabled the other jni targets and fixed incorrect test. Received help with this from and internal classes/Release LinkView native pointers Fix * Add abstract method to NativeObjectReference for native resource releasing. * Implement NativeObjectReference.cleanup for UncheckRow and LinkView/"
862,862,13.0,0.9284999966621399,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Using separate interfaces for async transaction also fixes RealmQuery.distinct(), RealmResults.distinct()/Fix flaky tests related with async transaction * Background realm needs to be closed before notifying other threads in async transaction. * Latch needs to be called after Realm closed./fix 1884 listener trigger/Fix crash when closing a Realm in listener Realm.copyFromRealm()/Realm.createOrUpdateObjectFromJson() now works correctly if the RealmObject class contains a primary key/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/Checking to see if a transaction is currently in progress for sync and async transaction pathways. Also adding logging. Added a test helper to assist with log assertion. Fixes Fixing typo. Removing logger after testing Removing extra line Fixing tests and adding change log message. Adding breaking change message Small cosmetic changes Adding punctuation formatting Fixing unit test./"
863,863,4.0,0.9587000012397766,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Access to RealmResults based on deleted RealmList * When the original RealmList is deleted, for most methods of RealmResults should just work without crash by just treat it like an empty RealmResults. * RealmResults.where() throws IllegalStateExecption in this case. * RealmResults.isValid() returns false in this case. This is a temp fix, check for more details. Close Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
864,864,4.0,0.9567999839782715,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Access to RealmResults based on deleted RealmList * When the original RealmList is deleted, for most methods of RealmResults should just work without crash by just treat it like an empty RealmResults. * RealmResults.where() throws IllegalStateExecption in this case. * RealmResults.isValid() returns false in this case. This is a temp fix, check for more details. Close"
865,865,8.0,0.6833000183105469,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Deprecate Realm.getInstance(Context) * Deprecate Realm.getInstance(Context). * Replease `Realm.getInstance(Context)` in examples./
866,866,12.0,0.9842000007629395,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","RealmChangeListener should provide the changes object/realm/collection as well (#2705) RealmChangeListener should provide the changes object/realm/collection as well/Interface as supplement to extending RealmObject (#2599) * Backup * Add RealmModel * Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests * fixing examples * WIP * fix Annotation processor * clean up * fix javadoc * add transformer tests & fix DynamicRealm tests * fixing tests * adding tests * adding tests * fixing tests for the realm-annotations-processor * add tests to cover more UC for RealmList * renamed POJO to RealmModel * reduce method count by using proxyState only in RealmObjectProxy * fix examples, no need to exclude RealmObject using Gson * making RealmList final, remove unused method & fixing APT test/Thread check for copyToRealm/copyToRealmOrUpdate/Fix the cast in proxy generator for CacheData Close realm and row as a field name of model class. This fixes"
867,867,12.0,0.9842000007629395,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","RealmChangeListener should provide the changes object/realm/collection as well (#2705) RealmChangeListener should provide the changes object/realm/collection as well/Interface as supplement to extending RealmObject (#2599) * Backup * Add RealmModel * Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests * fixing examples * WIP * fix Annotation processor * clean up * fix javadoc * add transformer tests & fix DynamicRealm tests * fixing tests * adding tests * adding tests * fixing tests for the realm-annotations-processor * add tests to cover more UC for RealmList * renamed POJO to RealmModel * reduce method count by using proxyState only in RealmObjectProxy * fix examples, no need to exclude RealmObject using Gson * making RealmList final, remove unused method & fixing APT test/Thread check for copyToRealm/copyToRealmOrUpdate/Fix the cast in proxy generator for CacheData Close realm and row as a field name of model class. This fixes"
868,868,12.0,0.9750000238418579,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Interface as supplement to extending RealmObject (#2599) * Backup * Add RealmModel * Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests * fixing examples * WIP * fix Annotation processor * clean up * fix javadoc * add transformer tests & fix DynamicRealm tests * fixing tests * adding tests * adding tests * fixing tests for the realm-annotations-processor * add tests to cover more UC for RealmList * renamed POJO to RealmModel * reduce method count by using proxyState only in RealmObjectProxy * fix examples, no need to exclude RealmObject using Gson * making RealmList final, remove unused method & fixing APT test/"
869,869,7.0,0.8708999752998352,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","RealmCollection iterators are now stable (#2124) This commit changes the semantics of how live RealmResults really are. Before this commit RealmResults where live _all the time_, which meant that code like the below didnt work as expected (only half the elements would be deleted): ``` for (int i 0; i results.size(); i++) { results.get(i).deleteFromRealm(); } ``` The rather unintuitive work-around was counting backwards: ``` for (int i results.size() i >=0; i--) { results.get(i).deleteFromRealm(); } ``` This commit changes the behaviour of RealmResults, so they no longer are automatically up to date. They are instead refreshed using Looper events just like changes from other threads. RealmList iterators are not live in the same way that RealmResults are but act more like standard ArrayLists, so they dont suffer from the same liveness issues. The high-level arguments for doing this change was: Pros: * All iterators now work as you would normally expect (ease of use). * The liveness of Realm objects are now all controlled through the Looper (simpler internal code / easier to document Realm behaviour). Cons: * There is a small chance that people can accidentally reference a deleted RealmObject in a RealmResult. (Can be checked through use of `RealmObject.isValid()`)/Interface as supplement to extending RealmObject (#2599) * Backup * Add RealmModel * Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests * fixing examples * WIP * fix Annotation processor * clean up * fix javadoc * add transformer tests & fix DynamicRealm tests * fixing tests * adding tests * adding tests * fixing tests for the realm-annotations-processor * add tests to cover more UC for RealmList * renamed POJO to RealmModel * reduce method count by using proxyState only in RealmObjectProxy * fix examples, no need to exclude RealmObject using Gson * making RealmList final, remove unused method & fixing APT test/Added RealmCollection APIs/Adapt core fix for the delete RealmList Update core to 0.96.1. No more exception will be thrown when access RealmResults if the base RealmList has be deleted. Instead, it will be treated as an empty RealmResults forever./"
870,870,18.0,0.9136000275611877,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Adapt core fix for the delete RealmList Update core to 0.96.1. No more exception will be thrown when access RealmResults if the base RealmList has be deleted. Instead, it will be treated as an empty RealmResults forever./"
871,871,15.0,0.9587000012397766,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Expanding time resolution into millisecs (#2679) Expanding time resolution into milliseconds but using cores new column type Timestamp. An upgrade to core version 0.100.0 is required, and some minor updates are coming from some changes in the core API./Upgrading to Realm Core 0.97.0/"
872,872,5.0,0.8098999857902527,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Multi-arguments distinct(...) for Realm, DynamicRealm, RealmQuery, and RealmResults/"
873,873,15.0,0.9901000261306763,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Expanding time resolution into millisecs (#2679) Expanding time resolution into milliseconds but using cores new column type Timestamp. An upgrade to core version 0.100.0 is required, and some minor updates are coming from some changes in the core in String, Byte, Short, Integer, and Long types can be null. (#2634) is allowed to be null for String and Boxed primitive types. Features part of this commit are 1. A annotated field in java.lang.String/Byte/Short/Integer/Long type can be null. 2. The object with nil primary key can be updated. 3. Migration checks if existing nullable type is set to be nullable in existing file. If not, throws RealmMigrationNeeded. 4. Duplicated null as PrimaryKey throws RealmPrimaryKeyConstraintException. 5. When a new empty row is added, the default value checked for duplicated is still the same empty string ("""") at Table.addEmptyRow() 6. Since there isnt equivalent term for Optional in Java 7 yet, nullable PK types are configured as following in this PR. | String PK | Number PK | Primitive PK | Table Column | Nullable | Nullable | Not Nullable |/fix the String from DynamicRealmObject.toString(). Now DynamicRealmObject.toString() shows null value as ""null"". ""class_"" prefix is removed from type name in that String. Align the format to the String from typed RealmObject./"
874,874,15.0,0.6833000183105469,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",optimize imports/
875,875,9.0,0.9472000002861023,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Added RealmCollection APIs/More checkings when modify RealmList Fix * New method LinkView.getTargetTable(). * Proper checking to add DynamicRealmObject to RealmList. * Disallow modifying with RealmObject belongs to another Realm instance./
876,876,15.0,0.6833000183105469,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Upgrading to Realm Core 0.97.0/
877,877,18.0,0.5536999702453613,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Make all methods on RealmObject and all classes in public API final (#2675) We should mark all methods that should not be overridden final. We should mark all classes that should not be inherited final./Add getPrimaryKey() method to RealmObjectSchema (#2648) Without this method it was necessary to iterator over all fields manually to find the primary key using in String, Byte, Short, Integer, and Long types can be null. (#2634) is allowed to be null for String and Boxed primitive types. Features part of this commit are 1. A annotated field in java.lang.String/Byte/Short/Integer/Long type can be null. 2. The object with nil primary key can be updated. 3. Migration checks if existing nullable type is set to be nullable in existing file. If not, throws RealmMigrationNeeded. 4. Duplicated null as PrimaryKey throws RealmPrimaryKeyConstraintException. 5. When a new empty row is added, the default value checked for duplicated is still the same empty string ("""") at Table.addEmptyRow() 6. Since there isnt equivalent term for Optional in Java 7 yet, nullable PK types are configured as following in this PR. | String PK | Number PK | Primitive PK | Table Column | Nullable | Nullable | Not Nullable |/Fixed wrong method name in exception Fixed wrong method name in exception/Added RealmCollection APIs/Add RealmObjectSchema.isPrimaryKey() * Add RealmObjectSchema.isPrimaryKey(). RealmObjectSchema.getPrimaryKey() is not added since it doesnt seem to be necessary. * isNullable & isRequired didnt throw which is wrong. * Add throws doc to hasIndex. Close"
878,878,5.0,0.7932000160217285,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Give async related vars better names asyncQueryExecutor and pendingQuery seem to be very confusing since they are used for async transaction as well./Move all query methods to RealmQuery (#2620) Currently we have the following helper query methods on Realm/DynamicRealm: allObjects() and distinct() However we have to be mindful about the number of methods in our API and from the projects we have seen so far, these methods does not seem to have been used much. Moving all query methods to RealmQuery has two advantages: 1) It helps us to reduce our overall method count and 2) It makes our API more consistent as now all RealmQuery methods are only found on RealmQuery./Deprecated 3 field sort in Realm and RealmQuery (#2619)/Make Realm.createObject(Class,PrimaryKey) public (#2622) This method is a way to solve the problem that people might have data with a primary key that is is the default value for that type, e.g a long primary key with the value 0. Once this object is saved in Realm any calls to the normal Realm.createObject() would throw an exception as it would try to assign the default value (zero) to the object before the user could set it to something else. With this method that is no longer a problem. Internally we already use this method and it is already public in the Dynamic API. The reason for not making this public earlier was due to fear of mis-use since there is no type-safe guarantee at compile time. However since it is already public in the Dynamic API and we havent seen any indication of that being misused we feel it should be safe to make this public. Also our Query API is also semi-threadsafe so there is precedence there as well./Only throw RealmException if absolutely necessary (#2618) We should avoid wrapping lower level exceptions in RealmExceptions unless there is a good reason for it. We had multiple support issues where people were confused about the RealmException and didnt understand they had to dig through the stack trace for the original exception. This change alters the behaviour of all our JSON methods so they now only convert JSONException to RealmException in order to prevent checked exceptions to reach the user. All other exceptions should be thrown directly./Interface as supplement to extending RealmObject (#2599) * Backup * Add RealmModel * Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests * fixing examples * WIP * fix Annotation processor * clean up * fix javadoc * add transformer tests & fix DynamicRealm tests * fixing tests * adding tests * adding tests * fixing tests for the realm-annotations-processor * add tests to cover more UC for RealmList * renamed POJO to RealmModel * reduce method count by using proxyState only in RealmObjectProxy * fix examples, no need to exclude RealmObject using Gson * making RealmList final, remove unused method & fixing APT test/Added RealmConfiguration.initialData() (#2602)/Remove Realm.getTable() from public API Close RealmCollection APIs/migrateRealm throws when db doesnt exsit Close ./follow lint warnings/Multi-arguments distinct(...) for Realm, DynamicRealm, RealmQuery, and RealmResults/Thread local notifactions are now triggered using the Looper instead of immediately/RealmQuery.distinctAsync(), RealmResults.distinctAsync() support./"
879,879,12.0,0.9366000294685364,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",RealmResults and RealmObjects are no longer accidentally GCed while their Observable is still alive./Realm Observables now holds a Realm instance until unsubscribed/fixed typo/
880,880,9.0,0.9660999774932861,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Merge commit ecbacf into merge-ecbacf-to-master/Use qualified name for model classes in generated code in order to avoid name conflict. (#3083) * use qualified name for model classes in generated code in order to avoid name conflict. * add model classes that test issue 3077. * update changelog for inserts (#2999) Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./Objects not in Realm are now called unmananged everywhere. (#2828)/"
881,881,9.0,0.9660999774932861,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Merge commit ecbacf into merge-ecbacf-to-master/Use qualified name for model classes in generated code in order to avoid name conflict. (#3083) * use qualified name for model classes in generated code in order to avoid name conflict. * add model classes that test issue 3077. * update changelog for inserts (#2999) Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./Objects not in Realm are now called unmananged everywhere. (#2828)/"
882,882,1.0,0.9366999864578247,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Use qualified name for model classes in generated code in order to avoid name conflict. (#3083) * use qualified name for model classes in generated code in order to avoid name conflict. * add model classes that test issue 3077. * update changelog for
883,883,9.0,0.967199981212616,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Annotation processor no longer consume annotations (#3216) This commit changes the Realm annotation processor so it no longer consumes the annotations. The only effect this has is that it is now possible to write multiple annotation processors for the same annotation. It is not possible to specify the order of annotation processors from two different libraries, so if one of the processors consumed the annotation it would be random if both ran or only one of them./"
884,884,15.0,0.5202999711036682,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Asset file as an initial dataset (#2692)/
885,885,12.0,0.6833000183105469,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Fixed unit tests./
886,886,9.0,0.9269000291824341,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Disable change listeners on IntentService threads (#3232)/add a null check to addChangeListener and removeChangeListener (#2805) * add a null check to addChangeListener and removeChangeListener in Realm and DynamicRealm (fixes * reflect review comments/
887,887,17.0,0.4875999987125397,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Avoid Table.nativeToString/nativeRowToString (#2884) Table::row_to_string Table::to_string are for debugging purpose only, it calls string::substr which might truncate an UTF-8 string at any char. Thus a crash could happend in JNI when converting to UTF-16. This will only happen when user attach a debugger and the debugger is trying to call Table.toString. Close"
888,888,0.0,0.9750000238418579,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","bulk inserts (#2999) Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./ PrimaryKey index rearrangement in migration revisited (#2920) This PR fixes two issues; 1) When a column with a smaller index than that of a primary key field gets removed, the primary key field index decrements, which causes a cached primary key index to point a wrong field. 2) When a primary key field is renamed, the primary key meta table is not updated accordingly./"
889,889,0.0,0.9049999713897705,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","bulk inserts (#2999) Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./"
890,890,0.0,0.9049999713897705,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","bulk inserts (#2999) Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./"
891,891,0.0,0.9049999713897705,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","bulk inserts (#2999) Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./"
892,892,0.0,0.9743000268936157,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",RealmObjectSchema.setClassName() transfers primary key for new class name (#3149) RealmObjectSchema.setClassName() transfers a primary key for a new class name that the renamed class can maintain the old primary key./Redundant addIndex(fieldName) removed (#2917) In RealmObjectSchema.addModifiers() there is a redundant addIndex(fieldName) called before addPrimaryKey(fieldName). This should have been removed at on RealmObjectSchema misses Index addition. (#2832) RealmObjectSchema.addPrimaryKey() is supposed to add a search index to its primary key field but it is currently missing. This is to fix the addition and removal of a search index related to primary key methods in RealmObjectSchema./
893,893,19.0,0.8734999895095825,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Disable change listeners on IntentService threads (#3232)/bulk inserts (#2999) Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./Fix async transaction problem when async query already exist. (#2780) I have rewritten how async transactions works. Before they did this: 1) commitOnBackgroundThread() post REALM_CHANGED post onSuccess runnable The problem with that approach was that the REALM_CHANGED would be swallowed if async queries existed which meant that the async transaction would call onSuccess on an old version of the Realm (making it look like it didnt work). Instead we now do this: 2) commitOnBackgroundThread post runnable that calls HandlerController.handleAsyncTransactionCompleted(runnable) The special runnable is treated as a combined REALM_CHANGED + Callback, which makes it possible for us to queue up callbacks until we are finally able to trigger all of them. Unfortunately the Handler has poor support for this so it means that we no longer can detect if such a message is in the queue. This means that it introduces a slight chance of such a event plus a real REALM_CHANGED event to be in the message queue at the same time. I considered that acceptable (since it can already happen today), and since preventing this will introduce more complexity to something that is already entirely to complex./Objects not in Realm are now called unmananged everywhere. (#2828)/"
894,894,15.0,0.5708000063896179,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Allow to specify default value of the field in models constructor (#3397) * Allow to call its accessors, and replace its field accesses with accessor calls in models constructor. fixes fixes * use field instead of checking transaction * fix a bug that acceptDefaultValue is not set correctly * reject default values when the getter of a model creates other model object * add simple test for default value * supports default value of model field * supports default value of RealmList fields * add tests for assignment in constructor and setter in constructor * update javadoc comments of createObject * always ignores the default value of primary key if the object is managed * update javadoc * add a test for default values handling in copyToRealm(). the last assertion of RealmTests.copyToRealm_defaultValuesAreIgnored() is failing now. * refactor tests * use isPrimaryKey() * fix a bug that unexpected realm object is created by default value of RealmModel/RealmList fields * remove extra ; from generated code * add more tests for default value * fix tests * fix a bug that creates unexpected objects * rename internal methods * update changelog * update CHANGELOG * review comments * update CHANGELOG * added a description of how proxy object should be created in the Javadoc comment of RealmProcessor/Supporting both additive and manual schema modes (#91) * Adding very thin wrappers for Object Stores ObjectSchema and Property. * Adding method for building object schema is proxy classes * Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode. * Disallowing destructive schema changes in additive mode./Invalidate schema cache when the schema version of Realm is changed by other process (#3409). invalidate schema cache when the schema version of Realm is changed by other process. Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./Integrate Object Store [PART2] SharedRealm (#3031) This simplified our code base a lot. Basically all APIs we need from SharedGroup/Group are wrapped in the SharedRealm. So we can just remove those classes. But we do need a few APIs from SharedGroup which is not supplied by SharedRealm because of async queries. Currently we expose those in a friend class of ShareRealm, see realm/realm-object-store#141 We are still managing Realm caches in Java although there are mechanism in OS to do the same thing. The major reason is we have method like deleteRealm needs information from cache to check if all Realm instances are closed. The ShareRealm is actually a std::shared_ptr. We hold the pointer to the std::shared_ptr in Java. Another fundamental change is that we used to have separated SharedGroups for DynamicRealm and typed Realm in the same thread. But now they are using different SharedRealm but actually the different SharedRealms are point to the same SharedGroup. And some other code cleanup./"
895,895,15.0,0.5669000148773193,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Allow to specify default value of the field in models constructor (#3397) * Allow to call its accessors, and replace its field accesses with accessor calls in models constructor. fixes fixes * use field instead of checking transaction * fix a bug that acceptDefaultValue is not set correctly * reject default values when the getter of a model creates other model object * add simple test for default value * supports default value of model field * supports default value of RealmList fields * add tests for assignment in constructor and setter in constructor * update javadoc comments of createObject * always ignores the default value of primary key if the object is managed * update javadoc * add a test for default values handling in copyToRealm(). the last assertion of RealmTests.copyToRealm_defaultValuesAreIgnored() is failing now. * refactor tests * use isPrimaryKey() * fix a bug that unexpected realm object is created by default value of RealmModel/RealmList fields * remove extra ; from generated code * add more tests for default value * fix tests * fix a bug that creates unexpected objects * rename internal methods * update changelog * update CHANGELOG * review comments * update CHANGELOG * added a description of how proxy object should be created in the Javadoc comment of RealmProcessor/Supporting both additive and manual schema modes (#91) * Adding very thin wrappers for Object Stores ObjectSchema and Property. * Adding method for building object schema is proxy classes * Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode. * Disallowing destructive schema changes in additive mode./Invalidate schema cache when the schema version of Realm is changed by other process (#3409). invalidate schema cache when the schema version of Realm is changed by other process. Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./Integrate Object Store [PART2] SharedRealm (#3031) This simplified our code base a lot. Basically all APIs we need from SharedGroup/Group are wrapped in the SharedRealm. So we can just remove those classes. But we do need a few APIs from SharedGroup which is not supplied by SharedRealm because of async queries. Currently we expose those in a friend class of ShareRealm, see realm/realm-object-store#141 We are still managing Realm caches in Java although there are mechanism in OS to do the same thing. The major reason is we have method like deleteRealm needs information from cache to check if all Realm instances are closed. The ShareRealm is actually a std::shared_ptr. We hold the pointer to the std::shared_ptr in Java. Another fundamental change is that we used to have separated SharedGroups for DynamicRealm and typed Realm in the same thread. But now they are using different SharedRealm but actually the different SharedRealms are point to the same SharedGroup. And some other code cleanup./"
896,896,2.0,0.762499988079071,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",DefaultRealmModule not created for empty Kotlin projects (#3749)/
897,897,18.0,0.9693999886512756,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Create object without setting PK is not allowed (#3379) * Create object without setting PK is not allowed * createObject must be called with PK value when creating a object with PK defined. * Creating objects from JSON must have have corresponding PK defined in the JSON object. Known issue: The default values for creating object from JSONStream will be differnt from those created by createObject. Default values from default constructor VS default values from core. This has to be addressed by
898,898,11.0,0.6521999835968018,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Add cause to RealmMigrationNeededException (#3482)/Remove deprecated constructor + add directory() (#3357) This commit simplifies the RealmConfiguration constructors and also ensures that we always have an Android context. It does so by now only having the`RealmConfiguration.Builder(context)` constructor. Custom file locations are now supported through the `directory()` builder method. This also made it possible to simply `assetFile(Context, location)` to only `assetFile(location)`. Having the Context means that we are now able to access system services and other framework classes without exposing any Android functionality in any potential interface (which will be needed to support Realm on the JVM)./Add RealmFileException to replace RealmIOException and IncompatibleLockFileException. Also it is mapped to the same name exception in ObjectStore to give user a detailed kind of file exception./"
899,899,11.0,0.8871999979019165,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Introduce global init (#3457) Realm now uses a global init function instead of Context on the RealmConfiguration.Builder/Remove deprecated constructor + add directory() (#3357) This commit simplifies the RealmConfiguration constructors and also ensures that we always have an Android context. It does so by now only having the`RealmConfiguration.Builder(context)` constructor. Custom file locations are now supported through the `directory()` builder method. This also made it possible to simply `assetFile(Context, location)` to only `assetFile(location)`. Having the Context means that we are now able to access system services and other framework classes without exposing any Android functionality in any potential interface (which will be needed to support Realm on the JVM)./Wait all async tasks done before next test (#3319) This is highly related with Below things are still guaranteed by this change after commit async transaction: * When callback function called (onSuccess/onError), the background Realm is closed. * When any change listeners called, the background Realm is closed. What is true now but it is not guaranteed in the future: * Background Realm might not be closed before REALM_CHANGED sent (not received). Due to this, to avoid the flaky tests, we have to ensure all async tasks quit peacefully before start the next test. This is implemented by wait and check in the TestRealmConfigurationFactory. NOTE: Any test, if the async tasks cannot be finished peacefully in a certain time, it has to be considered as a problem and fixed. This would be needed by OS notifications since Java wont have a precise control of sending REALM_CHANGED anymore./"
900,900,15.0,0.8812000155448914,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Introduce global init (#3457) Realm now uses a global init function instead of Context on the RealmConfiguration.Builder/
901,901,8.0,0.6833000183105469,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Add methods to RealmQuery to support byte arrays (#3285)/
902,902,16.0,0.5249000191688538,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Public Sync API (#73)/
903,903,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Fix native crash in DynamicRealmObject#setList() (#3550)/
904,904,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Fix native crash in DynamicRealmObject#setList() (#3550)/
905,905,13.0,0.6832000017166138,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Upgrade to beta-33 / 2.0.0-rc4 (#90)/
906,906,2.0,0.9916999936103821,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Use CheckedRow when creating a DynamicRealm Object. (#3551) And this removes a check of UncheckedRow in some constructors of DynamicRealmObject since CheckedRow is never passed to it./Sync facade to make spliting lib possible (#116) * Sync facade to make spliting lib possible * Add class SyncObjectServerFacade which will only exist in the sync lib. * Check if SyncObjectServerFacade exists and create an singleton instance. * Empty implementations for base ObjectServerFacade. * Add RealmConfiguration.isSyncConfiguration to make the checks faster. * Other cleanups. Close both additive and manual schema modes (#91) * Adding very thin wrappers for Object Stores ObjectSchema and Property. * Adding method for building object schema is proxy classes * Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode. * Disallowing destructive schema changes in additive mode./Integrate Object Store [PART2] SharedRealm (#3031) This simplified our code base a lot. Basically all APIs we need from SharedGroup/Group are wrapped in the SharedRealm. So we can just remove those classes. But we do need a few APIs from SharedGroup which is not supplied by SharedRealm because of async queries. Currently we expose those in a friend class of ShareRealm, see realm/realm-object-store#141 We are still managing Realm caches in Java although there are mechanism in OS to do the same thing. The major reason is we have method like deleteRealm needs information from cache to check if all Realm instances are closed. The ShareRealm is actually a std::shared_ptr. We hold the pointer to the std::shared_ptr in Java. Another fundamental change is that we used to have separated SharedGroups for DynamicRealm and typed Realm in the same thread. But now they are using different SharedRealm but actually the different SharedRealms are point to the same SharedGroup. And some other code cleanup./getFieldIndex returns null for non-existing field (#3295) Close"
907,907,11.0,0.5637000203132629,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","allow to put Realm database file on external storage. (#3591) * set the path of the directory of named pipes to fix * update CHANGELOG * add a test for issue3140 * update test * follow the chenges in object-store * skip an external storage test on the device where SELinux is not enforced * rename test * rename variable * update object-store * make SharedRealm#temporaryDirectory volatile * address findbugs error/Fixed bug in path when getting access tokens for Realms (#157)/Add cause to RealmMigrationNeededException (#3482)/Introduce global init (#3457) Realm now uses a global init function instead of Context on the RealmConfiguration.Builder/fix merge mistakes/Merge branch master into master-sync Conflicts: realm/realm-annotations-processor/src/main/java/io/realm/processor/ClassMetaData.java realm/realm-annotations-processor/src/main/java/io/realm/processor/RealmProxyClassGenerator.java realm/realm-annotations-processor/src/test/resources/io/realm/AllTypesRealmProxy.java realm/realm-annotations-processor/src/test/resources/io/realm/BooleansRealmProxy.java realm/realm-annotations-processor/src/test/resources/io/realm/NullTypesRealmProxy.java realm/realm-annotations-processor/src/test/resources/io/realm/RealmDefaultModuleMediator.java realm/realm-annotations-processor/src/test/resources/io/realm/SimpleRealmProxy.java realm/realm-library/src/main/cpp/object-store realm/realm-library/src/main/java/io/realm/Realm.java realm/realm-library/src/main/java/io/realm/RealmSchema.java realm/realm-library/src/main/java/io/realm/internal/SharedRealm.java/Supporting both additive and manual schema modes (#91) * Adding very thin wrappers for Object Stores ObjectSchema and Property. * Adding method for building object schema is proxy classes * Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode. * Disallowing destructive schema changes in additive mode./Upgrade to beta-33 / 2.0.0-rc4 (#90)/Invalidate schema cache when the schema version of Realm is changed by other process (#3409). invalidate schema cache when the schema version of Realm is changed by other process. Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./Public Sync API (#73)/Integrate Object Store [PART2] SharedRealm (#3031) This simplified our code base a lot. Basically all APIs we need from SharedGroup/Group are wrapped in the SharedRealm. So we can just remove those classes. But we do need a few APIs from SharedGroup which is not supplied by SharedRealm because of async queries. Currently we expose those in a friend class of ShareRealm, see realm/realm-object-store#141 We are still managing Realm caches in Java although there are mechanism in OS to do the same thing. The major reason is we have method like deleteRealm needs information from cache to check if all Realm instances are closed. The ShareRealm is actually a std::shared_ptr. We hold the pointer to the std::shared_ptr in Java. Another fundamental change is that we used to have separated SharedGroups for DynamicRealm and typed Realm in the same thread. But now they are using different SharedRealm but actually the different SharedRealms are point to the same SharedGroup. And some other code cleanup./Wait all async tasks done before next test (#3319) This is highly related with Below things are still guaranteed by this change after commit async transaction: * When callback function called (onSuccess/onError), the background Realm is closed. * When any change listeners called, the background Realm is closed. What is true now but it is not guaranteed in the future: * Background Realm might not be closed before REALM_CHANGED sent (not received). Due to this, to avoid the flaky tests, we have to ensure all async tasks quit peacefully before start the next test. This is implemented by wait and check in the TestRealmConfigurationFactory. NOTE: Any test, if the async tasks cannot be finished peacefully in a certain time, it has to be considered as a problem and fixed. This would be needed by OS notifications since Java wont have a precise control of sending REALM_CHANGED anymore./"
908,908,13.0,0.920799970626831,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Nh/fix 3966 (#3979) Realm migration is triggered, when the primary key definition is altered (#3966)/Fixed a bug that caused unexpected MigrationNeededException in very rare case. (#3768) In sync mode, `validateTable()` must get all fields in the tabla./"
909,909,13.0,0.920799970626831,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Nh/fix 3966 (#3979) Realm migration is triggered, when the primary key definition is altered (#3966)/Fixed a bug that caused unexpected MigrationNeededException in very rare case. (#3768) In sync mode, `validateTable()` must get all fields in the tabla./"
910,910,13.0,0.920799970626831,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Nh/fix 3966 (#3979) Realm migration is triggered, when the primary key definition is altered (#3966)/Fixed a bug that caused unexpected MigrationNeededException in very rare case. (#3768) In sync mode, `validateTable()` must get all fields in the tabla./"
911,911,1.0,0.7623999714851379,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",PermissionOffer/PermissionOfferResponse support (#4005) Added support for PermissionOffer and PermissionOfferResponse/
912,912,17.0,0.7623999714851379,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",Implement global logout (#3642)/
913,913,3.0,0.9872000217437744,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Expose schemaVersion in SyncConfig (#4058)/Disallow listeners for Results & Object on non-looper/Disallow Realm listeners on non-looper thread It is possible to have listeners for non-looper thread, and trigger them through SharedRealm.refresh(). But there are many corner cases need to be covered if we enable it. Disallow it for now to stick with current behaviour./Single daemon thread for notification (#3666) With changes in , there is only one thread will be create for listening changes for all different Realms. But an additional SharedGroup will be created in the daemon thread for determine which SharedGroup has changed since last time. That requires some changes in the java side especially the daemon thread should not be created when Realm.compactRealm called./Create capabilities for every Realm instance/Fix get DynamicRealmObject from RealmResults./Clear the memory ownership for RowNotifier/Deliver global notification through OS did_change Remove AndroidNotifier which will have a common implementation among all platforms./Back the RealmResults by collection/Experiment of Results.get()/"
914,914,5.0,0.5975000262260437,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Fix lots of minor issues/Get findAllAsync and findFirstAsync back the findAllAsync got deprecated since from OS Results point of view, the query updating will run in the background by default, then there would be no difference between findAll and findAsync. Apparently the isLoad() and load() should be deprecated as well. So like cocoa, the Query will be executed immediately if user tries to access the element in the Results. But ... That was wrong because of those use cases: 1) RxJava support, in the subscription, it needs to check isLoaded() to determine the next step. And it will be fired once after the Observable created. By always returning true from isLoaded(), that means for RxJava will always run sync query at the first time. 2) Create a RealmResults and pass it to a list adapter. Since UI will always call size() which will run the query immediately and the first time query becomes synced. So, we get all async related APIs back and keep them having the same functionality like before. The behavior wont be exact the same, but from users perspective, they are the same as before./Detect if collection itarator becomes unstable Instead of checking the version of TableView which requires exposing the TableView version from OS, we simply check if the iterator is used across detach/reattach. If reattach happens, the TableView will be synced, the iterators are not stable anymore./Disallow listeners for Results & Object on non-looper/Use ObserverPair to manage collection listeners/OS Resultss first and last/Move TableOrView.NO_MATCH to Table/Lint warnings/Distinct support/Results.contains()/Results.sort()/Wrap NotificationToken/Back the RealmResults by collection/WiP start using Object Stores Results class/"
915,915,6.0,0.864300012588501,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Modify Java docs and comments. (#4124) * Remove unnecessary an addtional space. * Modify Java docs and comments. * PR feedback. * PR feedback./Remove final modifier from all major classes. (#3911)/
916,916,10.0,0.8812000155448914,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Wire the ""like"" predicate into RealmQuery (#3992) * Wire the ""like"" predicate into RealmQuery Fixes"
917,917,16.0,0.7940000295639038,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Free the SharedRealm in phantom daemon (#4096) Treat the SharedRealm the same as other native objects. SharedRealm.close will only call Object Store Realm::close without deleting the ShareRealm pointer. Then we dont need the finalizer anymore. Fix . This is related with as well. It is possible that java close the Realm in any of the Object Stores callbacks. To avoid Object Store operating on a invalid SharedRealm pointer, binding should try to make sure after callbacks. However, it cannot be totally avoided since user could set the Realm instance to null and the instance can be GCed at any time. It is still something should be considered in the Object Store implementation./Expose schemaVersion in SyncConfig (#4058)/Init some member vars in SharedRealm constructor move initialization of RealmNotifier and Capabilities to SharedRealm constructor to make it less confusing./Use term detach/reattach for Collection snapshot Also add some comments/canceltransaction posts to reattach collections To maintain the consistency behaviour: once a collection becomes detached because of beginTransaction, it can only be reattached in the next event loop./Merge remote-tracking branch origin/master into kneth/object-store/results/Fix concurrency problem with collection list/Workaround for the Alooper and postAtFrontQueue See with snapshot to support stable iterator But something is going wrong, more like a bug in core than Object Store./Remove handler related code The event handler logic stays in OS only, see util/android/event_loop_signal.hpp All the async related logic should be platform independent in java./"
918,918,5.0,0.9821000099182129,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Merge remote-tracking branch origin/master into kneth/object-store/results/BYE BYE finalizer (#3144) NativeObject Interface * NativeObject is an interface now. The implementation should supply a function to return a native deallocator pointer as well. * NativeObjectReference doesnt necessarily to know the pointer type anymore since it can always get a pointer to the deallocator function. Use phantom reference and daemon thread to replace finalizer for destruct native objects. * The phantom reference pool is implemented as a linked list which suppose to be fast insertion/removal. * A daemon thread is created to monitor and free all phantom reference. * Delayed disposal before native object creation is not needed any more. * SharedGroup still gets freed in the caller thread with a lock on context. * Native object needs to pass a destruction function pointer in addition to the native object pointer./
919,919,5.0,0.9817000031471252,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",BYE BYE finalizer (#3144) NativeObject Interface * NativeObject is an interface now. The implementation should supply a function to return a native deallocator pointer as well. * NativeObjectReference doesnt necessarily to know the pointer type anymore since it can always get a pointer to the deallocator function. Use phantom reference and daemon thread to replace finalizer for destruct native objects. * The phantom reference pool is implemented as a linked list which suppose to be fast insertion/removal. * A daemon thread is created to monitor and free all phantom reference. * Delayed disposal before native object creation is not needed any more. * SharedGroup still gets freed in the caller thread with a lock on context. * Native object needs to pass a destruction function pointer in addition to the native object pointer./
920,920,5.0,0.9817000031471252,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",BYE BYE finalizer (#3144) NativeObject Interface * NativeObject is an interface now. The implementation should supply a function to return a native deallocator pointer as well. * NativeObjectReference doesnt necessarily to know the pointer type anymore since it can always get a pointer to the deallocator function. Use phantom reference and daemon thread to replace finalizer for destruct native objects. * The phantom reference pool is implemented as a linked list which suppose to be fast insertion/removal. * A daemon thread is created to monitor and free all phantom reference. * Delayed disposal before native object creation is not needed any more. * SharedGroup still gets freed in the caller thread with a lock on context. * Native object needs to pass a destruction function pointer in addition to the native object pointer./
921,921,5.0,0.98580002784729,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Use getTargetTable to build query on linkview otherwise RealmQueryTests.findFirst() will fail because it compares the row with the tables name first./Move TableOrView.NO_MATCH to Table/BYE BYE finalizer (#3144) NativeObject Interface * NativeObject is an interface now. The implementation should supply a function to return a native deallocator pointer as well. * NativeObjectReference doesnt necessarily to know the pointer type anymore since it can always get a pointer to the deallocator function. Use phantom reference and daemon thread to replace finalizer for destruct native objects. * The phantom reference pool is implemented as a linked list which suppose to be fast insertion/removal. * A daemon thread is created to monitor and free all phantom reference. * Delayed disposal before native object creation is not needed any more. * SharedGroup still gets freed in the caller thread with a lock on context. * Native object needs to pass a destruction function pointer in addition to the native object pointer./
922,922,6.0,0.8944000005722046,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Modify Java docs and comments. (#4124) * Remove unnecessary an addtional space. * Modify Java docs and comments. * PR feedback. * PR feedback./Remove final modifier from all major classes. (#3911)/Move TableOrView.NO_MATCH to Table/
923,923,7.0,0.944100022315979,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test",Merge remote-tracking branch origin/master into kneth/object-store/results/Expose schemaVersion in SyncConfig (#4058)/Deliver the callbacks for async transaction/Remove final modifier from all major classes. (#3911)/Remove commitTransaction(boolean)/Nh/fixes 3732 insertOrUpdate using other Realm (#3755)/
924,924,3.0,0.9524999856948853,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Add the underlying information on RealmFileException. (#3940) Add the underlying information on RealmFileException to help investigating Incompatible lock file issue./Nh/objectstore userstore (#3838) * Add a UserStore based on ObjectStore implementation/WiP start using Object Stores Results class/
925,925,15.0,0.9269000291824341,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",More docs for RealmNotifier Also added one more test case for it./Only trigger realm notifier when version changed/Clear the memory ownership for RowNotifier/
926,926,3.0,0.9049999713897705,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Nh/objectstore userstore (#3838) * Add a UserStore based on ObjectStore implementation/WiP start using Object Stores Results class/
927,927,18.0,0.9711999893188477,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Update core to 2.3.0 (#3970) Also update object-store to 99570ba6e0 . * Adapt changes from sync Client::set_error_handler is removed. Session error handler signature changed. * Session error handler called after destruction According to the doc of Session::set_error_handler, the error handler could be called after the session object is destroyed. That is a problem since the java session object can be destroyed at that time. Use a weak_ptr of JavaGlobalRef in the lambda to solve the problem. * Ignore unknown category error from sync/"
928,928,5.0,0.9829999804496765,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Feature/backlinks (#4406) * Refactor RealmObjectSchema Clean up Proxy generation Comments addressed * Fix Test, Checkstyle and Findbugs errors * Respond to comments/Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/improve performance of getters and setters in proxy classes (#4206) * improve performance of getters and setters in proxy classes This change is a part of fixes of * removed unused argment * Update CHANGELOG.md/"
929,929,5.0,0.9829999804496765,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Feature/backlinks (#4406) * Refactor RealmObjectSchema Clean up Proxy generation Comments addressed * Fix Test, Checkstyle and Findbugs errors * Respond to comments/Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/improve performance of getters and setters in proxy classes (#4206) * improve performance of getters and setters in proxy classes This change is a part of fixes of * removed unused argment * Update CHANGELOG.md/"
930,930,5.0,0.983299970626831,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Introduce DynamicRealmObject#linkingObjects(String srcClassName, String srcFieldName) (#4492)/Feature/backlinks (#4406) * Refactor RealmObjectSchema Clean up Proxy generation Comments addressed * Fix Test, Checkstyle and Findbugs errors * Respond to comments/Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/improve performance of getters and setters in proxy classes (#4206) * improve performance of getters and setters in proxy classes This change is a part of fixes of * removed unused argment * Update CHANGELOG.md/"
931,931,5.0,0.9750000238418579,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
932,932,5.0,0.9750000238418579,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
933,933,5.0,0.9750000238418579,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/"
934,934,11.0,0.8812000155448914,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Correctly report Client Reset (#4313)/Using ObjectStore SyncManager & Session (#4214) Using ObjectStores SyncManager & Session/
935,935,5.0,0.9789000153541565,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Fine grained locks for RealmCache (#4551) Separated lock for different RealmConfiguration instead of one lock on the RealmCache class. So Opening Realm instances from different configurations wont block each other. DynamicRealm which is created during opening type Realm will not be associated to any RealmCache to avoid recursive locks and multiple times initial block. (Also make the code easier.) This is for and part of implementation of ./fix API incompatibility introduced in 3.1.0 (#4455)/Refactor Schemas into separate native and realm-based implementations (#4382) * Refactor Schemas into separate native and realm-based implementations * Fix all tests * Respond to PR comments * Temporary (git add .) kludge to fix visiblity problem/
936,936,5.0,0.7171000242233276,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Introduce DynamicRealmObject#linkingObjects(String srcClassName, String srcFieldName) (#4492)/Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/Deprecate distinctXxx methods of RealmResults (#4210)/Enable listeners on RealmList (#4216) The RealmList holds a Collection which is used for listeners. Other RealmList APIs are still calling from LinkView./Add OrderedRealmCollectionSnapshot (#4172) Introduce OrderedRealmCollectionSnapshot as another type of OrderedRealmCollection. A snapshot can be created from RealmResults or RealmList. A snapshot is backed by a snapshot of OS Results. So the snapshot itself wont be updated. The size and order stay the same forever (elements inside are still live objects.). Since the RealmResults is auto-updated all the time, snapshot will be usefull when changing the results in a simple loops. This commit also moves the common code from RealmResults to OrderedRealmCollectionImpl since those can be shared with snapshot implementation. Implement"
937,937,5.0,0.9876999855041504,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Hide StandardRealmSchema class from public API. (#4444) fixes * add package private methods to RealmSchema instead of BaseRealm.getSchemaInternal()./refactor internal method name in RealmSchema (#4429)/Feature/backlinks (#4406) * Refactor RealmObjectSchema Clean up Proxy generation Comments addressed * Fix Test, Checkstyle and Findbugs errors * Respond to comments/Refactor Schemas into separate native and realm-based implementations (#4382) * Refactor Schemas into separate native and realm-based implementations * Fix all tests * Respond to PR comments * Temporary (git add .) kludge to fix visiblity problem/Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/Fix error message to show correct character limit and minor grammar fix/"
938,938,5.0,0.9049999713897705,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Feature/backlinks (#4406) * Refactor RealmObjectSchema Clean up Proxy generation Comments addressed * Fix Test, Checkstyle and Findbugs errors * Respond to comments/"
939,939,8.0,0.9218999743461609,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Clean up SharedRealm source Remove unused methods. Fix double negatives. Use auto& to avoid creating temp objects./Rename Context to NativeContext (#4597)/Dont reset SharedRealm ptr (#4478) Keep the SharedRealm ptr valid when close it. To allow the is_closed checking could throw in the Object Store and it can be converted to a friendly Java exception. Check if SharedRealm is closed when create collection iterator. Throw an ISE if it is. Fix ./Refactor Schemas into separate native and realm-based implementations (#4382) * Refactor Schemas into separate native and realm-based implementations * Fix all tests * Respond to PR comments * Temporary (git add .) kludge to fix visiblity problem/Add detailed notification for RealmObject (#4331) See Add ObjectChangeSet & RealmObjectChangeListener. Add OsObject to wrap ObjectStores Object for notifications. No more false positive notifications for RealmObject. Use ObserverPairList in ProxyState instead of normal list to solve the potential listener removal problems which is handled well by the ObserverPairList. Fix tests./Using ObjectStore SyncManager & Session (#4214) Using ObjectStores SyncManager & Session/RealmResults is always live-to-updated This is the precondition of fine grained notifications. OS will trigger the collection notification immediately when transaction begins on the local thread to compute the change set if there is any. This conflicts with Javas original RealmResults behavior the original RealmResults would only be synced in the next event loop. Also, there are some edge cases dont work well with the original RealmResults behavior, see details in So: RealmResults becomes always up-to-date again which means it will never contains a invalid row. Behavior of iteration on a RealmResults still just works, it will just iterate on snapshot of collection. This means user can still delete elements from a RealmResults inside iteration. Deletion & Modification on RealmResults in simple-for-loop wont work as expected if the changes will impact the order/elements of the results. This could be solved by the future new Collection type RealmCollectionSnapshot. Add Collection.load() and Collection.isLoaded() to support java sync queries. Test fix. wont be an issue anymore since the RealmResults is always up to date and it wont contain any invalid rows. So remove the related tests. Failure tests caused by listener being triggred with beginTransaction() Remove realmResultsListenerAddedAfterCommit. when add listener to the OS Results after commit transaction, the OS CollectionNotifier will be created at the SharedGroup version of transaction committed. So the listener wont be called anymore since the all changes already exist in current SharedGroup./Dont remove callback in AndroidRealmNotifier We still have to ensure the async transaction callbacks delivered./"
940,940,6.0,0.5246000289916992,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Rename Context to NativeContext (#4597)/
941,941,6.0,0.9049999713897705,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Rename Context to NativeContext (#4597)/Fix warnings from error prone plugin (#4339) This PR does not add the plugin, just fix the warnings. Ill add the plugin in another PR with suppressing some warnings./"
942,942,6.0,0.5246000289916992,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Rename Context to NativeContext (#4597)/
943,943,5.0,0.9136000275611877,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Rename Context to NativeContext (#4597)/Use target table to create snapshot from LinkView (#4556) Fix . Remove useless confusing `LinkView.getTable()`./
944,944,5.0,0.9865999817848206,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","fix API incompatibility introduced in 3.1.0 (#4455)/Feature/backlinks (#4406) * Refactor RealmObjectSchema Clean up Proxy generation Comments addressed * Fix Test, Checkstyle and Findbugs errors * Respond to comments/Refactor Schemas into separate native and realm-based implementations (#4382) * Refactor Schemas into separate native and realm-based implementations * Fix all tests * Respond to PR comments * Temporary (git add .) kludge to fix visiblity problem/Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/Fix error message to show correct character limit and minor grammar fix/"
945,945,12.0,0.9320999979972839,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Set log level for core logger bridge (#4389) Set the all core logger bridges log level with the global log level. Fix
946,946,5.0,0.9904999732971191,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Fine grained locks for RealmCache (#4551) Separated lock for different RealmConfiguration instead of one lock on the RealmCache class. So Opening Realm instances from different configurations wont block each other. DynamicRealm which is created during opening type Realm will not be associated to any RealmCache to avoid recursive locks and multiple times initial block. (Also make the code easier.) This is for and part of implementation of ./removed nested try-finally/address findbugs warnings/dispose OsRealmObjectSchema in OsRealmSchema.Creator/Fix OsRealmSchema leak (#4422)/refactor internal method name in RealmSchema (#4429)/Feature/backlinks (#4406) * Refactor RealmObjectSchema Clean up Proxy generation Comments addressed * Fix Test, Checkstyle and Findbugs errors * Respond to comments/Refactor Schemas into separate native and realm-based implementations (#4382) * Refactor Schemas into separate native and realm-based implementations * Fix all tests * Respond to PR comments * Temporary (git add .) kludge to fix visiblity problem/Backlinks (#4219) * Document multiple links to the same object * Fix non-object field reference bug * Load all tables before validating any * Update to gradle 3.4.1 * Fix Documentation * Fix Asynchronous UTs * Ignore, instead of throwing on, attempts to load Backlink fields w/JSON * Fix documentation and add notification unit tests * Require that backlink fields be final. * Address PR comments * Add tests for notification and distinct * Add interface methods and most of table validation * Check fields on JSON load * Fix fails in RealmTests * Respond to comments * Compile time type checking Refactor annotation handler * Improved error messages * Add Unit tests * Renamed Backlink to LinkingObjects. Added annotation processor tests. * Added annotation processor unit tests. * Add Backlink annotation/Using ObjectStore SyncManager & Session (#4214) Using ObjectStores SyncManager & Session/"
947,947,5.0,0.9320999979972839,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Use `clang-format` to format C++ code (#4307) * Formatting C++ source code using clang-format using cores rules. * How to format C++ code * Curly braces around single lines/
948,948,8.0,0.8098999857902527,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",KeepMember OsObject.notifyChangeListeners Fix Use release assertion when finding java method./Using ObjectStore SyncManager & Session (#4214) Using ObjectStores SyncManager & Session/
949,949,19.0,0.7457000017166138,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Create ProxyUtils class to simplify proxies (#4942) Move a simple task to a utility class, to reduce Proxy code size/Let Object Store handle table creations (#4674) This tries to progressively move more things about schemas to Object Store. First the concept of Schema in Object Store is not the same as what we have in Java. It is very much just a schema information holder and wont take care of the schema modifications. That says it is more like the ColumnIndices cache in the Java binding. So this commit try to: Instead of inheriting from the RealmSchema, change the OsRealmSchema/OsRealmObjectSchema to OsSchemaInfo/OsObjectSchemaInfo. They behave as a simple Java wrapper to the relevant OS objects. Add functions to Proxy classes which will create its own OsObjectSchemaInfo and those info can be used to create a OsSchemaInfo through the mediator. Call `SharedRealm.updateSchema` with the OsSchemaInfo which is got from the proxy interface to do table initialization. This will also fix a minor bug we have before, All tables are created even if the class is not in the module. Migration is still handled in the old way, and it will be solved in the future, to let Object Store handle it. ColumnIndices are still kept for now, but it should be computed from the OsSchemaInfo/OsObjectSchemaInfo in the future./Clean up the Proxies a bit, in prep for RealmInteger mods (#4770) * Clean up the Proxies a bit, in prep for RealmInteger mods/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
950,950,19.0,0.7458000183105469,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Create ProxyUtils class to simplify proxies (#4942) Move a simple task to a utility class, to reduce Proxy code size/Let Object Store handle table creations (#4674) This tries to progressively move more things about schemas to Object Store. First the concept of Schema in Object Store is not the same as what we have in Java. It is very much just a schema information holder and wont take care of the schema modifications. That says it is more like the ColumnIndices cache in the Java binding. So this commit try to: Instead of inheriting from the RealmSchema, change the OsRealmSchema/OsRealmObjectSchema to OsSchemaInfo/OsObjectSchemaInfo. They behave as a simple Java wrapper to the relevant OS objects. Add functions to Proxy classes which will create its own OsObjectSchemaInfo and those info can be used to create a OsSchemaInfo through the mediator. Call `SharedRealm.updateSchema` with the OsSchemaInfo which is got from the proxy interface to do table initialization. This will also fix a minor bug we have before, All tables are created even if the class is not in the module. Migration is still handled in the old way, and it will be solved in the future, to let Object Store handle it. ColumnIndices are still kept for now, but it should be computed from the OsSchemaInfo/OsObjectSchemaInfo in the future./Clean up the Proxies a bit, in prep for RealmInteger mods (#4770) * Clean up the Proxies a bit, in prep for RealmInteger mods/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
951,951,10.0,0.5329999923706055,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Counters: Merge MutableRealmIntegers Feature (#5017) * Counters: Partial implementations * Counters: Wired to core (#4953) * Counters: Required, Index and Documentation (#4969) * Add tests for and * Document ManagedRealmInteger * Counters: JSON for MutableRealmIntegers (#5007) * Clean up documentation/Create ProxyUtils class to simplify proxies (#4942) Move a simple task to a utility class, to reduce Proxy code size/Let Object Store handle table creations (#4674) This tries to progressively move more things about schemas to Object Store. First the concept of Schema in Object Store is not the same as what we have in Java. It is very much just a schema information holder and wont take care of the schema modifications. That says it is more like the ColumnIndices cache in the Java binding. So this commit try to: Instead of inheriting from the RealmSchema, change the OsRealmSchema/OsRealmObjectSchema to OsSchemaInfo/OsObjectSchemaInfo. They behave as a simple Java wrapper to the relevant OS objects. Add functions to Proxy classes which will create its own OsObjectSchemaInfo and those info can be used to create a OsSchemaInfo through the mediator. Call `SharedRealm.updateSchema` with the OsSchemaInfo which is got from the proxy interface to do table initialization. This will also fix a minor bug we have before, All tables are created even if the class is not in the module. Migration is still handled in the old way, and it will be solved in the future, to let Object Store handle it. ColumnIndices are still kept for now, but it should be computed from the OsSchemaInfo/OsObjectSchemaInfo in the future./Clean up the Proxies a bit, in prep for RealmInteger mods (#4770) * Clean up the Proxies a bit, in prep for RealmInteger mods/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
952,952,8.0,0.5249999761581421,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Clean up JsonHelper as prep for RealmInteger (#4795)/
953,953,0.0,0.9049999713897705,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Suppress warnings in generated code. (#4779) * Suppress warnings in generated code. * Add compile time flag controlling the SuppressWarnings annotation on generated classes/
954,954,13.0,0.9620000123977661,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Support stable IDs for sync (#4693) * Support stable IDs for sync A special column to store stable IDs are added by sync. See for details. Calling sync::create_object instead of add_empty_row to create new object. Calling sync::create_table/sync::create_table_with_primary_key instead of add_table to create a new object schema. addPrimaryKey() is not allowed for synced Realm anymore. New API RealmSchema.createWithPrimaryKeyField is added./
955,955,9.0,0.9524999856948853,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Use Object Store to handle schema migration Use Object Store to handle migration. This is for non-synced Realm. Object Store will compare the expected OsSchemaInfo with the current on on the disc. if they dont match, the migration callback supplied by java will called to do manual migration. Update Object Store to 0d0aef97b8/"
956,956,13.0,0.8416000008583069,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Re-add Sync Progress Notifications (#4415)/Fix admin users not connection correctly to ROS (#4760)/
957,957,0.0,0.6832000017166138,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Fix admin users not connection correctly to ROS (#4760)/
958,958,4.0,0.9136000275611877,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Returning sessions state (#4821) * Returning sessions state/Fix admin users not connection correctly to ROS (#4760)/
959,959,15.0,0.5537999868392944,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","dynamic realm in migration/Use Object Store to handle schema migration Use Object Store to handle migration. This is for non-synced Realm. Object Store will compare the expected OsSchemaInfo with the current on on the disc. if they dont match, the migration callback supplied by java will called to do manual migration. Update Object Store to 0d0aef97b8/Merge standard schema classes to their parents Since we changed the architecture, no need for this extra abstraction anymore./Support readOnly() on Configurations (#4575)/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
960,960,12.0,0.967199981212616,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
961,961,1.0,0.9660999774932861,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Removed unused methods from internal Table class (#4812)/Refactor object creation into OsObject (#4632) Hide the addEmptyRow from java to support future stable ID. Add bulk insertion benchmark. This wont be the final design of internal object creation API, when integration of OS object accessor, the internal API might be changed a bit since it doesnt look nice at all 5 params for the JNI call to create an object with integer primary key/"
962,962,3.0,0.9366999864578247,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Nh/android support ssl (#4591) * Expose two new SyncConfiguration options to 1: Disable TLS verification. 2: provide the trusted root CA to validate the RealmObjectServer TLS connection (since OpenSSL doesnt have access to Android keystore) fixes
963,963,12.0,0.967199981212616,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
964,964,10.0,0.3650999963283539,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add CompactOnLaunch. (#4857) * Implement CompactOnLaunch. This commit adds RealmConfiguration.compactOnLaunch and RealmConfiguration.Builder.compactOnLaunch(CompactOnLaunchCallback). It makes a Realm determines if it should be compacted. * Throw an exception if it is read-only Realm. * Add readOnly_compactOnLaunch_throws * Fix a wrong signature. * Add tests to check compactOnLaunch. * Fix tests. * Add Javadoc. * Updated CHANGELOG>md * Fix a typo * PR feedback. * PR feedback: Improve tests. * PR feedback. * Improve Javadocs sentences. * Support Proguard. * Rename more accurate. * PR feedback. * Fix Proguard. * Fix JNI code. * PR feedback: Remove 2 createConfiguration. * PR feedback * Add RealmConfiguration.Builder.compactOnLaunch(). * Fix a bug of JNI code. * Add more tests. * Improve Javadoc. * PR feedback: Add a test to check a bug. * add a test to check a bug where compactOnLaunch is called each time a Realm is opened on a new thread. * PR feedback * Imporve Javadoc. * Fix a test (Thread). * PR: fix a test./Support stable IDs for sync (#4693) * Support stable IDs for sync A special column to store stable IDs are added by sync. See for details. Calling sync::create_object instead of add_empty_row to create new object. Calling sync::create_table/sync::create_table_with_primary_key instead of add_table to create a new object schema. addPrimaryKey() is not allowed for synced Realm anymore. New API RealmSchema.createWithPrimaryKeyField is added./Use Object Store to handle schema migration Use Object Store to handle migration. This is for non-synced Realm. Object Store will compare the expected OsSchemaInfo with the current on on the disc. if they dont match, the migration callback supplied by java will called to do manual migration. Update Object Store to 0d0aef97b8/Returning sessions state (#4821) * Returning sessions state/Nh/android support ssl (#4591) * Expose two new SyncConfiguration options to 1: Disable TLS verification. 2: provide the trusted root CA to validate the RealmObjectServer TLS connection (since OpenSSL doesnt have access to Android keystore) fixes readOnly() on Configurations (#4575)/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
965,965,12.0,0.9817000031471252,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Support stable IDs for sync (#4693) * Support stable IDs for sync A special column to store stable IDs are added by sync. See for details. Calling sync::create_object instead of add_empty_row to create new object. Calling sync::create_table/sync::create_table_with_primary_key instead of add_table to create a new object schema. addPrimaryKey() is not allowed for synced Realm anymore. New API RealmSchema.createWithPrimaryKeyField is added./LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
966,966,12.0,0.6401000022888184,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",Backlink queries (#4704) * Fixing unit tests for backlink queries. * Reintroducing of a native implementation of isNotEmpty(). * Moving inverse relationships out of beta stage./LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
967,967,12.0,0.967199981212616,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
968,968,12.0,0.967199981212616,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
969,969,19.0,0.508899986743927,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Immutable RealmSchema and RealmObjectSchema (#5003) Before this change, Realm.getSchema() and DynamicRealm.getSchema() both return a mutable version RealmSchema object. That would cause issue when changing the typed Realms schema then continue to use typed inferface to access the Realm where the column indices might be changed already and had not been refreshed before the transaction commited. After this change: Realm.getSchema() returns an immutable RealmSchema object. And all RealmObjectSchema objects retrieved from that will be immutable as well./add Nullable annotation to methods that can return null (#4999) * add Nullable annotation to methods that can return null * add changelog entry for introfucinf annotation. * fix Kotlin example * fix Kotlin test/Use Object Store to handle schema migration Use Object Store to handle migration. This is for non-synced Realm. Object Store will compare the expected OsSchemaInfo with the current on on the disc. if they dont match, the migration callback supplied by java will called to do manual migration. Update Object Store to 0d0aef97b8/Let Object Store handle table creations (#4674) This tries to progressively move more things about schemas to Object Store. First the concept of Schema in Object Store is not the same as what we have in Java. It is very much just a schema information holder and wont take care of the schema modifications. That says it is more like the ColumnIndices cache in the Java binding. So this commit try to: Instead of inheriting from the RealmSchema, change the OsRealmSchema/OsRealmObjectSchema to OsSchemaInfo/OsObjectSchemaInfo. They behave as a simple Java wrapper to the relevant OS objects. Add functions to Proxy classes which will create its own OsObjectSchemaInfo and those info can be used to create a OsSchemaInfo through the mediator. Call `SharedRealm.updateSchema` with the OsSchemaInfo which is got from the proxy interface to do table initialization. This will also fix a minor bug we have before, All tables are created even if the class is not in the module. Migration is still handled in the old way, and it will be solved in the future, to let Object Store handle it. ColumnIndices are still kept for now, but it should be computed from the OsSchemaInfo/OsObjectSchemaInfo in the future./Turn off the column check for synced realms (#4706)/Fix queries not working on proguarded Realms model classes (#4690)/Support readOnly() on Configurations (#4575)/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
970,970,17.0,0.9793000221252441,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Final step for OS schema integration (#5065) Build the column indices cache from OsSchemaInfo. Not like before, the column indices are not built when the Realm instance is created. Instead, they will only be built when the relevant RealmObject needs to be accessed. Column indices cache system has been changed. Different Realm instance will not share the same cache. Instead, every Realm instance will have its own cache. So we dont rely on the schema version any more. The cache can handle the situation when the schema versions are the same but schemas are not. Refresh column indices cache when schema changes. This is also supported in multi-processes environment. Almost all of the Realm initialization work is handled by Object Store. So we have the same routine for both sync/non-sync Realm. Also the initialization routine has be simplified a lot in java side. A new class OsRealmConfig is introduced to solve the 20+ arguments need to be passed to JNI to create Realm config. The transaction for schema initialization should be cancelled if exception happens in Java migration/initialization callbacks./"
971,971,17.0,0.9793000221252441,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Final step for OS schema integration (#5065) Build the column indices cache from OsSchemaInfo. Not like before, the column indices are not built when the Realm instance is created. Instead, they will only be built when the relevant RealmObject needs to be accessed. Column indices cache system has been changed. Different Realm instance will not share the same cache. Instead, every Realm instance will have its own cache. So we dont rely on the schema version any more. The cache can handle the situation when the schema versions are the same but schemas are not. Refresh column indices cache when schema changes. This is also supported in multi-processes environment. Almost all of the Realm initialization work is handled by Object Store. So we have the same routine for both sync/non-sync Realm. Also the initialization routine has be simplified a lot in java side. A new class OsRealmConfig is introduced to solve the 20+ arguments need to be passed to JNI to create Realm config. The transaction for schema initialization should be cancelled if exception happens in Java migration/initialization callbacks./"
972,972,17.0,0.5767999887466431,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Add Supports for Primitive Lists (#5031) * Extended Annotation Processor to support it in model classes * Relaxed Generic Constraints * Support in insert/insertOrUpdate * Support in copyToRealm/copyToRealmOrUpdate * Support in copyFromRealm/Use OS List instead of cores LinkView (#5171) Replace LinkView with OS List. Code clean up. Fix a potential leak with LinkView in insertion APIs./Final step for OS schema integration (#5065) Build the column indices cache from OsSchemaInfo. Not like before, the column indices are not built when the Realm instance is created. Instead, they will only be built when the relevant RealmObject needs to be accessed. Column indices cache system has been changed. Different Realm instance will not share the same cache. Instead, every Realm instance will have its own cache. So we dont rely on the schema version any more. The cache can handle the situation when the schema versions are the same but schemas are not. Refresh column indices cache when schema changes. This is also supported in multi-processes environment. Almost all of the Realm initialization work is handled by Object Store. So we have the same routine for both sync/non-sync Realm. Also the initialization routine has be simplified a lot in java side. A new class OsRealmConfig is introduced to solve the 20+ arguments need to be passed to JNI to create Realm config. The transaction for schema initialization should be cancelled if exception happens in Java migration/initialization callbacks./"
973,973,9.0,0.7314000129699707,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Add Supports for Primitive Lists (#5031) * Extended Annotation Processor to support it in model classes * Relaxed Generic Constraints * Support in insert/insertOrUpdate * Support in copyToRealm/copyToRealmOrUpdate * Support in copyFromRealm/
974,974,15.0,0.9472000002861023,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Support Partial Sync (#5359) * Add preview support for partial sync/Remove ObjectServerUser (#5020) * remove ObjectServerUser * add multiple session tests * Add regression test for old SyncUser JSON * Using identity with authURL to identity a SyncUser/
975,975,18.0,0.9743000268936157,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Merge remote-tracking branch origin/master into mc/os-schema-migration/Add annotation to public APIs (#5044) * add annotation to parameters and suppress null related warnings * add annotation to objectserver APIs * fix compile error in example * remove annotation from equals(Object) * remove annotation from each overriding methods. Add more package-infos instead. * remove from parameters * remove from parameters * remove from parameters * remove unused import * remove unused import * add Nullability by Annotataion section to CONTRIBUTING.md * fix typo in CONTRIBUTING.md * replace assert with noinspection comment * add more annotation to address findbugs warnings/
976,976,17.0,0.9793000221252441,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Final step for OS schema integration (#5065) Build the column indices cache from OsSchemaInfo. Not like before, the column indices are not built when the Realm instance is created. Instead, they will only be built when the relevant RealmObject needs to be accessed. Column indices cache system has been changed. Different Realm instance will not share the same cache. Instead, every Realm instance will have its own cache. So we dont rely on the schema version any more. The cache can handle the situation when the schema versions are the same but schemas are not. Refresh column indices cache when schema changes. This is also supported in multi-processes environment. Almost all of the Realm initialization work is handled by Object Store. So we have the same routine for both sync/non-sync Realm. Also the initialization routine has be simplified a lot in java side. A new class OsRealmConfig is introduced to solve the 20+ arguments need to be passed to JNI to create Realm config. The transaction for schema initialization should be cancelled if exception happens in Java migration/initialization callbacks./"
977,977,18.0,0.9728999733924866,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Update realm-sync to 1.10.8 (#5129) And update object-store to 2274604de9/Add annotation to public APIs (#5044) * add annotation to parameters and suppress null related warnings * add annotation to objectserver APIs * fix compile error in example * remove annotation from equals(Object) * remove annotation from each overriding methods. Add more package-infos instead. * remove from parameters * remove from parameters * remove from parameters * remove unused import * remove unused import * add Nullability by Annotataion section to CONTRIBUTING.md * fix typo in CONTRIBUTING.md * replace assert with noinspection comment * add more annotation to address findbugs warnings/
978,978,18.0,0.9728999733924866,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Add annotation to public APIs (#5044) * add annotation to parameters and suppress null related warnings * add annotation to objectserver APIs * fix compile error in example * remove annotation from equals(Object) * remove annotation from each overriding methods. Add more package-infos instead. * remove from parameters * remove from parameters * remove from parameters * remove unused import * remove unused import * add Nullability by Annotataion section to CONTRIBUTING.md * fix typo in CONTRIBUTING.md * replace assert with noinspection comment * add more annotation to address findbugs warnings/
979,979,9.0,0.7502999901771545,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Add Supports for Primitive Lists (#5031) * Extended Annotation Processor to support it in model classes * Relaxed Generic Constraints * Support in insert/insertOrUpdate * Support in copyToRealm/copyToRealmOrUpdate * Support in copyFromRealm/
980,980,17.0,0.9793000221252441,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Final step for OS schema integration (#5065) Build the column indices cache from OsSchemaInfo. Not like before, the column indices are not built when the Realm instance is created. Instead, they will only be built when the relevant RealmObject needs to be accessed. Column indices cache system has been changed. Different Realm instance will not share the same cache. Instead, every Realm instance will have its own cache. So we dont rely on the schema version any more. The cache can handle the situation when the schema versions are the same but schemas are not. Refresh column indices cache when schema changes. This is also supported in multi-processes environment. Almost all of the Realm initialization work is handled by Object Store. So we have the same routine for both sync/non-sync Realm. Also the initialization routine has be simplified a lot in java side. A new class OsRealmConfig is introduced to solve the 20+ arguments need to be passed to JNI to create Realm config. The transaction for schema initialization should be cancelled if exception happens in Java migration/initialization callbacks./"
981,981,11.0,0.920799970626831,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Support offline client reset (#5297) * Exposing a `SyncConfiguration` that allows a user to open the backup Realm after the client reset (#4759)./
982,982,17.0,0.9793000221252441,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Final step for OS schema integration (#5065) Build the column indices cache from OsSchemaInfo. Not like before, the column indices are not built when the Realm instance is created. Instead, they will only be built when the relevant RealmObject needs to be accessed. Column indices cache system has been changed. Different Realm instance will not share the same cache. Instead, every Realm instance will have its own cache. So we dont rely on the schema version any more. The cache can handle the situation when the schema versions are the same but schemas are not. Refresh column indices cache when schema changes. This is also supported in multi-processes environment. Almost all of the Realm initialization work is handled by Object Store. So we have the same routine for both sync/non-sync Realm. Also the initialization routine has be simplified a lot in java side. A new class OsRealmConfig is introduced to solve the 20+ arguments need to be passed to JNI to create Realm config. The transaction for schema initialization should be cancelled if exception happens in Java migration/initialization callbacks./"
983,983,13.0,0.8812000155448914,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Ensure stable classes order in generated mediators (#5567) Fixes
984,984,14.0,0.6833000183105469,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",[Sync] Verify certificate using TrustManager (#5515)/
985,985,11.0,0.6830999851226807,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Align Sync APIs with Cocoa (#5835)/
986,986,9.0,0.8812000155448914,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush",Better exception message if non model class is provided as function argument/
987,987,12.0,0.6507999897003174,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Speed up Snappy uncompression, new Logger interface. Removed one copy of an uncompressed block contents changing the signature of Snappy_Uncompress() so it uncompresses into a flat array instead of a std::string. Speeds up readrandom ~10%. Instead of a combination of Env/WritableFile, we now have a Logger interface that can be easily overridden applications that want to supply their own logging. Separated out the gcc and Sun Studio parts of atomic_pointer.h so we can use asm, volatile keywords for Sun Studio. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/Small tweaks and bugfixes for Issue 18 and 19. Slight tweak to the no-overlap optimization: only push to level 2 to reduce the amount of wasted space when the same small key range is being repeatedly overwritten. Fix for Issue 18: Avoid failure on Windows by avoiding deletion of lock file until the end of DestroyDB(). Fix for Issue 19: Disregard sequence numbers when checking for overlap in sstable ranges. This fixes issue 19: when writing the same key over and over again, we would generate a sequence of sstables that were never merged together since their sequence numbers were disjoint. Dont ignore map/unmap error checks. Miscellaneous fixes for small problems Sanjay found while diagnosing issue/9 and issue/16 (corruption_testr failures). log::Reader reports the record type when it finds an unexpected type. log::Reader no longer reports an error when it encounters an expected zero record regardless of the setting of the ""checksum"" flag. Added a missing forward declaration. Documented a side-effects of larger write buffer sizes (longer recovery time). git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/"
988,988,0.0,0.9728000164031982,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Speed up Snappy uncompression, new Logger interface. Removed one copy of an uncompressed block contents changing the signature of Snappy_Uncompress() so it uncompresses into a flat array instead of a std::string. Speeds up readrandom ~10%. Instead of a combination of Env/WritableFile, we now have a Logger interface that can be easily overridden applications that want to supply their own logging. Separated out the gcc and Sun Studio parts of atomic_pointer.h so we can use asm, volatile keywords for Sun Studio. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/"
989,989,6.0,0.9585999846458435,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","A number of smaller fixes and performance improvements: Implemented Get() directly instead of building on top of a full merging iterator stack. This speeds up the ""readrandom"" benchmark by up to 15-30%. Fixed an opensource compilation problem. Added flag to control where the database is placed. Automatically compact a file when we have done enough overlapping seeks to that file. Fixed a performance bug where we would read from at least one file in a level even if none of the files overlapped the key being read. Makefile fix for Mac OSX installations that have XCode 4 without XCode 3. Unified the two occurrences of binary search in a file-list into one routine. Found and fixed a bug where we would unnecessarily search the last file when looking for a key larger than all data in the level. A fix to avoid the need for trivial move compactions and therefore gets rid of two out of five syncs in ""fillseq"". Removed the MANIFEST file write when switching to a new memtable/log-file for a 10-20% improvement on fill speed on ext4. Adding a SNAPPY setting in the Makefile for folks who have Snappy installed. Snappy compresses values and speeds up writes. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/"
990,990,12.0,0.6326000094413757,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Bugfix for issue 33; reduce lock contention in Get(), parallel benchmarks. Fix for issue 33 (non-null-terminated result from leveldb_property_value()) Support for running multiple instances of a benchmark in parallel. Reduce lock contention on Get(): (1) Do not hold the lock while searching memtables. (2) Shard block and table caches 16-ways. Benchmark for evaluating this change: $ db_bench (fillseq1 is a small hack to make sure fillseq runs once regardless of number of threads specified on the command line). git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/Speed up Snappy uncompression, new Logger interface. Removed one copy of an uncompressed block contents changing the signature of Snappy_Uncompress() so it uncompresses into a flat array instead of a std::string. Speeds up readrandom ~10%. Instead of a combination of Env/WritableFile, we now have a Logger interface that can be easily overridden applications that want to supply their own logging. Separated out the gcc and Sun Studio parts of atomic_pointer.h so we can use asm, volatile keywords for Sun Studio. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/Small tweaks and bugfixes for Issue 18 and 19. Slight tweak to the no-overlap optimization: only push to level 2 to reduce the amount of wasted space when the same small key range is being repeatedly overwritten. Fix for Issue 18: Avoid failure on Windows by avoiding deletion of lock file until the end of DestroyDB(). Fix for Issue 19: Disregard sequence numbers when checking for overlap in sstable ranges. This fixes issue 19: when writing the same key over and over again, we would generate a sequence of sstables that were never merged together since their sequence numbers were disjoint. Dont ignore map/unmap error checks. Miscellaneous fixes for small problems Sanjay found while diagnosing issue/9 and issue/16 (corruption_testr failures). log::Reader reports the record type when it finds an unexpected type. log::Reader no longer reports an error when it encounters an expected zero record regardless of the setting of the ""checksum"" flag. Added a missing forward declaration. Documented a side-effects of larger write buffer sizes (longer recovery time). git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/A number of smaller fixes and performance improvements: Implemented Get() directly instead of building on top of a full merging iterator stack. This speeds up the ""readrandom"" benchmark by up to 15-30%. Fixed an opensource compilation problem. Added flag to control where the database is placed. Automatically compact a file when we have done enough overlapping seeks to that file. Fixed a performance bug where we would read from at least one file in a level even if none of the files overlapped the key being read. Makefile fix for Mac OSX installations that have XCode 4 without XCode 3. Unified the two occurrences of binary search in a file-list into one routine. Found and fixed a bug where we would unnecessarily search the last file when looking for a key larger than all data in the level. A fix to avoid the need for trivial move compactions and therefore gets rid of two out of five syncs in ""fillseq"". Removed the MANIFEST file write when switching to a new memtable/log-file for a 10-20% improvement on fill speed on ext4. Adding a SNAPPY setting in the Makefile for folks who have Snappy installed. Snappy compresses values and speeds up writes. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/sync with upstream Fixed race condition reported by Dave Smit on the leveldb mailing list. We were not signalling waiters after a trivial move from level-0. The result was that in some cases (hard to reproduce), a write would get stuck forever waiting for the number of level-0 files to drop below its hard limit. The new code is simpler: there is just one condition variable instead of two, and the condition variable is signalled after every piece of background work finishes. Also, all compaction work (including for manual compactions) is done in the background thread, and therefore we can remove the ""compacting_"" variable. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/"
991,991,12.0,0.4334000051021576,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Bugfixes for iterator and documentation. Fix bug in Iterator::Prev where it would return the wrong key. Fixes issues 29 and 30. Added a tweak to testharness to allow running just some tests. Fixing two minor documentation errors based on issues 28 and 25. Cleanup; fix namespaces of export-to-C code. Also fix one ""const char*"" vs ""char*"" mismatch. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/Sun Studio support, and fix for test related memory fixes. LevelDB patch for Sun Studio Based on a patch submitted by Theo Schlossnagle thanks This fixes Issue 17. Fix a couple of test related memory leaks. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/Small tweaks and bugfixes for Issue 18 and 19. Slight tweak to the no-overlap optimization: only push to level 2 to reduce the amount of wasted space when the same small key range is being repeatedly overwritten. Fix for Issue 18: Avoid failure on Windows by avoiding deletion of lock file until the end of DestroyDB(). Fix for Issue 19: Disregard sequence numbers when checking for overlap in sstable ranges. This fixes issue 19: when writing the same key over and over again, we would generate a sequence of sstables that were never merged together since their sequence numbers were disjoint. Dont ignore map/unmap error checks. Miscellaneous fixes for small problems Sanjay found while diagnosing issue/9 and issue/16 (corruption_testr failures). log::Reader reports the record type when it finds an unexpected type. log::Reader no longer reports an error when it encounters an expected zero record regardless of the setting of the ""checksum"" flag. Added a missing forward declaration. Documented a side-effects of larger write buffer sizes (longer recovery time). git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/A number of smaller fixes and performance improvements: Implemented Get() directly instead of building on top of a full merging iterator stack. This speeds up the ""readrandom"" benchmark by up to 15-30%. Fixed an opensource compilation problem. Added flag to control where the database is placed. Automatically compact a file when we have done enough overlapping seeks to that file. Fixed a performance bug where we would read from at least one file in a level even if none of the files overlapped the key being read. Makefile fix for Mac OSX installations that have XCode 4 without XCode 3. Unified the two occurrences of binary search in a file-list into one routine. Found and fixed a bug where we would unnecessarily search the last file when looking for a key larger than all data in the level. A fix to avoid the need for trivial move compactions and therefore gets rid of two out of five syncs in ""fillseq"". Removed the MANIFEST file write when switching to a new memtable/log-file for a 10-20% improvement on fill speed on ext4. Adding a SNAPPY setting in the Makefile for folks who have Snappy installed. Snappy compresses values and speeds up writes. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/"
992,992,7.0,0.9405999779701233,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test",git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/Small tweaks and bugfixes for Issue 18 and 19. Slight tweak to the no-overlap optimization: only push to level 2 to reduce the amount of wasted space
993,993,19.0,0.9921000003814697,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","A number of bugfixes: Added DB::CompactRange() method. Changed manual compaction code so it breaks up compactions of big ranges into smaller compactions. Changed the code that pushes the output of memtable compactions to higher levels to obey the grandparent constraint: i.e., we must never have a single file in level L that overlaps too much data in level L+1 (to avoid very expensive L-1 compactions). Added code to pretty-print internal keys. Fixed bug where we would not detect overlap with files in level-0 because we were incorrectly using binary search on an array of files with overlapping ranges. Added ""leveldb.sstables"" property that can be used to dump all of the sstables and ranges that make up the db state. Removing post_write_snapshot support. Email to leveldb mailing list brought up no users, just confusion from one person about what it meant. Fixing static_cast char to unsigned on BIG_ENDIAN platforms. Fixes Issue 35 and Issue 36. Comment clarification to address leveldb Issue 37. Change license in posix_logger.h to match other files. A build problem where uint32 was used instead of uint32_t. Sync with upstream for Get(), dont hold mutex while writing log. Fix bug in Get: when it triggers a compaction, it could sometimes mark the compaction with the wrong level (if there was a gap in the set of levels examined for the Get). Do not hold mutex while writing to the log file or to the MANIFEST file. Added a new benchmark that runs a writer thread concurrently with reader threads. Percentiles micros/op: avg median 99 99.9 99.99 99.999 max before: 42 38 110 225 32000 42000 48000 after: 24 20 55 65 130 1100 7000 Fixed race in optimized Get. It should have been using the pinned memtables, not the current memtables. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/"
994,994,19.0,0.9890000224113464,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","A number of bugfixes: Added DB::CompactRange() method. Changed manual compaction code so it breaks up compactions of big ranges into smaller compactions. Changed the code that pushes the output of memtable compactions to higher levels to obey the grandparent constraint: i.e., we must never have a single file in level L that overlaps too much data in level L+1 (to avoid very expensive L-1 compactions). Added code to pretty-print internal keys. Fixed bug where we would not detect overlap with files in level-0 because we were incorrectly using binary search on an array of files with overlapping ranges. Added ""leveldb.sstables"" property that can be used to dump all of the sstables and ranges that make up the db state. Removing post_write_snapshot support. Email to leveldb mailing list brought up no users, just confusion from one person about what it meant. Fixing static_cast char to unsigned on BIG_ENDIAN platforms. Fixes Issue 35 and Issue 36. Comment clarification to address leveldb Issue 37. Change license in posix_logger.h to match other files. A build problem where uint32 was used instead of uint32_t. Sync with upstream"
995,995,19.0,0.9921000003814697,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","A number of bugfixes: Added DB::CompactRange() method. Changed manual compaction code so it breaks up compactions of big ranges into smaller compactions. Changed the code that pushes the output of memtable compactions to higher levels to obey the grandparent constraint: i.e., we must never have a single file in level L that overlaps too much data in level L+1 (to avoid very expensive L-1 compactions). Added code to pretty-print internal keys. Fixed bug where we would not detect overlap with files in level-0 because we were incorrectly using binary search on an array of files with overlapping ranges. Added ""leveldb.sstables"" property that can be used to dump all of the sstables and ranges that make up the db state. Removing post_write_snapshot support. Email to leveldb mailing list brought up no users, just confusion from one person about what it meant. Fixing static_cast char to unsigned on BIG_ENDIAN platforms. Fixes Issue 35 and Issue 36. Comment clarification to address leveldb Issue 37. Change license in posix_logger.h to match other files. A build problem where uint32 was used instead of uint32_t. Sync with upstream for Get(), dont hold mutex while writing log. Fix bug in Get: when it triggers a compaction, it could sometimes mark the compaction with the wrong level (if there was a gap in the set of levels examined for the Get). Do not hold mutex while writing to the log file or to the MANIFEST file. Added a new benchmark that runs a writer thread concurrently with reader threads. Percentiles micros/op: avg median 99 99.9 99.99 99.999 max before: 42 38 110 225 32000 42000 48000 after: 24 20 55 65 130 1100 7000 Fixed race in optimized Get. It should have been using the pinned memtables, not the current memtables. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/"
996,996,19.0,0.9957000017166138,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Pass systems CFLAGS, remove exit time destructor, sstable bug fix. Pass systems values of CFLAGS,LDFLAGS. Dont override OPT if its already set. Original patch by Alessio Treglia Remove 1 exit time destructor from leveldb. See Fix problem where sstable building code would pass an internal key to the user comparator. (Sync with uptream at 25436817.)/A number of fixes: Replace raw slice comparison with a call to user comparator. Added test for custom comparators. Fix end of namespace comments. Fixed bug in picking inputs for a level-0 compaction. When finding overlapping files, the covered range may expand as files are added to the input set. We now correctly expand the range when this happens instead of continuing to use the old range. For example, suppose L0 contains files with the following ranges: F1: a .. d F2: c .. g F3: f .. j and the initial compaction target is F3. We used to search for range f..j which yielded {F2,F3}. However we now expand the range as soon as another file is added. In this case, when F2 is added, we expand the range to c..j and restart the search. That picks up file F1 as well. This change fixes a bug related to deleted keys showing up incorrectly after a compaction as described in Issue 44. (Sync with upstream number of bugfixes: Added DB::CompactRange() method. Changed manual compaction code so it breaks up compactions of big ranges into smaller compactions. Changed the code that pushes the output of memtable compactions to higher levels to obey the grandparent constraint: i.e., we must never have a single file in level L that overlaps too much data in level L+1 (to avoid very expensive L-1 compactions). Added code to pretty-print internal keys. Fixed bug where we would not detect overlap with files in level-0 because we were incorrectly using binary search on an array of files with overlapping ranges. Added ""leveldb.sstables"" property that can be used to dump all of the sstables and ranges that make up the db state. Removing post_write_snapshot support. Email to leveldb mailing list brought up no users, just confusion from one person about what it meant. Fixing static_cast char to unsigned on BIG_ENDIAN platforms. Fixes Issue 35 and Issue 36. Comment clarification to address leveldb Issue 37. Change license in posix_logger.h to match other files. A build problem where uint32 was used instead of uint32_t. Sync with upstream for Get(), dont hold mutex while writing log. Fix bug in Get: when it triggers a compaction, it could sometimes mark the compaction with the wrong level (if there was a gap in the set of levels examined for the Get). Do not hold mutex while writing to the log file or to the MANIFEST file. Added a new benchmark that runs a writer thread concurrently with reader threads. Percentiles micros/op: avg median 99 99.9 99.99 99.999 max before: 42 38 110 225 32000 42000 48000 after: 24 20 55 65 130 1100 7000 Fixed race in optimized Get. It should have been using the pinned memtables, not the current memtables. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/"
997,997,19.0,0.9951000213623047,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","A number of fixes: Replace raw slice comparison with a call to user comparator. Added test for custom comparators. Fix end of namespace comments. Fixed bug in picking inputs for a level-0 compaction. When finding overlapping files, the covered range may expand as files are added to the input set. We now correctly expand the range when this happens instead of continuing to use the old range. For example, suppose L0 contains files with the following ranges: F1: a .. d F2: c .. g F3: f .. j and the initial compaction target is F3. We used to search for range f..j which yielded {F2,F3}. However we now expand the range as soon as another file is added. In this case, when F2 is added, we expand the range to c..j and restart the search. That picks up file F1 as well. This change fixes a bug related to deleted keys showing up incorrectly after a compaction as described in Issue 44. (Sync with upstream number of bugfixes: Added DB::CompactRange() method. Changed manual compaction code so it breaks up compactions of big ranges into smaller compactions. Changed the code that pushes the output of memtable compactions to higher levels to obey the grandparent constraint: i.e., we must never have a single file in level L that overlaps too much data in level L+1 (to avoid very expensive L-1 compactions). Added code to pretty-print internal keys. Fixed bug where we would not detect overlap with files in level-0 because we were incorrectly using binary search on an array of files with overlapping ranges. Added ""leveldb.sstables"" property that can be used to dump all of the sstables and ranges that make up the db state. Removing post_write_snapshot support. Email to leveldb mailing list brought up no users, just confusion from one person about what it meant. Fixing static_cast char to unsigned on BIG_ENDIAN platforms. Fixes Issue 35 and Issue 36. Comment clarification to address leveldb Issue 37. Change license in posix_logger.h to match other files. A build problem where uint32 was used instead of uint32_t. Sync with upstream for Get(), dont hold mutex while writing log. Fix bug in Get: when it triggers a compaction, it could sometimes mark the compaction with the wrong level (if there was a gap in the set of levels examined for the Get). Do not hold mutex while writing to the log file or to the MANIFEST file. Added a new benchmark that runs a writer thread concurrently with reader threads. Percentiles micros/op: avg median 99 99.9 99.99 99.999 max before: 42 38 110 225 32000 42000 48000 after: 24 20 55 65 130 1100 7000 Fixed race in optimized Get. It should have been using the pinned memtables, not the current memtables. git-svn-id: 62dab493-f737-651d-591e-8d6aee1b9529/"
998,998,3.0,0.9049999713897705,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",added group commit; drastically speeds up mult-threaded synchronous write workloads/
999,999,6.0,0.8658000230789185,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./"
1000,1000,6.0,0.8485999703407288,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Added bloom filter support. In particular, we add a new FilterPolicy class. An instance of this class can be supplied in Options when opening a database. If supplied, the instance is used to generate summaries of keys (e.g., a bloom filter) which are placed in sstables. These summaries are consulted by DB::Get() so we can avoid reading sstable blocks that are guaranteed to not contain the key we are looking for. This change provides one implementation of FilterPolicy based on bloom filters. Other changes: Updated version number to 1.4. Some build tweaks. C binding for CompactRange. A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom. Minor .gitignore update./fixed issues 66 (leaking files on disk error) and 68 (no sync of CURRENT file)/"
1001,1001,0.0,0.9320999979972839,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Support from db_bench. If bufferedio 0, then the read code path clears the OS page cache after the IO is completed. The default remains as bufferedio=1 Summary: Task ID: Blame Rev: Test Plan: Revert Plan: Differential Revision:"
1002,1002,7.0,0.9711999893188477,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","add flush interface to DB Summary: as subject. The flush will flush everything in the db. Test Plan: new test in db_test.cc Reviewers: dhruba Reviewed By: dhruba Differential Revision: disable WAL option Summary: add disable WAL option Test Plan: new testcase in db_test.cc Reviewers: dhruba Reviewed By: dhruba Differential Revision: some variables configurable for each db instance Summary: Make configurable targetFileSize, targetFileSizeMultiplier, maxBytesForLevelBase, maxBytesForLevelMultiplier, expandedCompactionFactor, maxGrandParentOverlapFactor Test Plan: N/A Reviewers: dhruba Reviewed By: dhruba Differential Revision:"
1003,1003,7.0,0.8812000155448914,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test",add zlib compression Summary: add zlib compression Test Plan: Will add more testcases Reviewers: dhruba Reviewed By: dhruba Differential Revision:
1004,1004,12.0,0.9043999910354614,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","This is the mega-patch multi-threaded compaction published in Summary: This patch allows compaction to occur in multiple background threads concurrently. If a manual compaction is issued, the system falls back to a single-compaction-thread model. This is done to ensure correctess and simplicity of code. When the manual compaction is finished, the system resumes its concurrent-compaction mode automatically. The updates to the manifest are done via group-commit approach. Test Plan: run db_bench/An configurable option to write data using write instead of mmap. Summary: We have seen that reading data via the pread call (instead of mmap) is much faster on Linux 2.6.x kernels. This patch makes an equivalent option to switch off mmaps for the write path as well. db_bench will use write() instead of mmap() to write data to a file. This change is backward compatible, the default option is to continue using mmap for writing to a file. Test Plan: ""make check all"" Differential Revision: BackupAPI should also list the length of the manifest file. Summary: The GetLiveFiles() api lists the set of sst files and the current MANIFEST file. But the database continues to append new data to the MANIFEST file even when the application is backing it up to the backup location. This means that the database-version that is stored in the MANIFEST FILE in the backup location does not correspond to the sst files returned by GetLiveFiles. This API adds a new parameter to GetLiveFiles. This new parmeter returns the current size of the MANIFEST file. Test Plan: Unit test attached. Reviewers: heyongqiang Reviewed By: heyongqiang Differential Revision: a configurable number of background threads. Summary: The background threads are necessary for compaction. For slower storage, it might be necessary to have more than one compaction thread per DB. This patch allows creating a configurable number of worker threads. The default reamins at 1 (to maintain backward compatibility). Test Plan: run all unit tests. changes to db-bench coming in a separate patch. Reviewers: heyongqiang Reviewed By: heyongqiang CC: MarkCallaghan Differential Revision: to switch off filesystem read-aheads Summary: Ability to switch off filesystem read-aheads. This change is backward-compatible: the default setting is to allow file system read-aheads. Test Plan: run benchmarks Reviewers: heyongqiang, adsharma Reviewed By: heyongqiang Differential Revision: not cache readahead-pages in the OS cache. Summary: When posix_fadvise(offset, offset) is usedm it frees up only those pages in that specified range. But the filesystem could have done some read-aheads and those get cached in the OS cache. Do not cache readahead-pages in the OS cache. Test Plan: run db_bench benchmark. Reviewers: vamsi, heyongqiang Reviewed By: heyongqiang Differential Revision: compiler warnings. Use uint64_t instead of uint. Summary: Fix compiler warnings. Use uint64_t instead of uint. Test Plan: build using Reviewers: heyongqiang Reviewed By: heyongqiang Differential Revision: a new method Env->Fsync() that issues fsync (instead of fdatasync). Summary: Introduce a new method Env->Fsync() that issues fsync (instead of fdatasync). This is needed for data durability when running on ext3 filesystems. Added options to the benchmark db_bench to generate performance numbers with either fsync or fdatasync enabled. Cleaned up Makefile to build leveldb_shell only when building the thrift leveldb server. Test Plan: build and run benchmark Reviewers: heyongqiang Reviewed By: heyongqiang Differential Revision: a scribe logger in leveldb to log leveldb deploy stats Summary: as subject. A new log is written to scribe via thrift client when a new db is opened and when there is a compaction. a new option var scribe_log_db_stats is added. Test Plan: manually checked using command ""ptail 0 leveldb_deploy_stats"" Reviewers: dhruba Differential Revision:"
1005,1005,14.0,0.944100022315979,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Fix all warnings generated by option to the compiler. Summary: The default compilation process now uses ""-Wall"" to compile. Fix all compilation error generated by gcc. Test Plan: make all check Reviewers: heyongqiang, emayanke, sheki Reviewed By: heyongqiang CC: MarkCallaghan Differential Revision:"
1006,1006,12.0,0.7330999970436096,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Flush Data at object destruction if disableWal is used. Summary: Added a conditional flush in ~DBImpl to flush. There is still a chance of writes not being persisted if there is a crash (not a clean shutdown) before the DBImpl instance is destroyed. Test Plan: modified db_test to meet the new expectations. Reviewers: dhruba, heyongqiang Differential Revision: all warnings generated by option to the compiler. Summary: The default compilation process now uses ""-Wall"" to compile. Fix all compilation error generated by gcc. Test Plan: make all check Reviewers: heyongqiang, emayanke, sheki Reviewed By: heyongqiang CC: MarkCallaghan Differential Revision: to invoke application hook for every key during compaction. Summary: There are certain use-cases where the application intends to delete older keys aftre they have expired a certian time period. One option for those applications is to periodically scan the entire database and delete appropriate keys. A better way is to allow the application to hook into the compaction process. This patch allows the application to set a method callback for every key that is being compacted. If this method returns true, then the key is not preserved in the output of the compaction. Test Plan: This is mostly to preview the proposed new public api. Since it is a public api, please do due diligence on reviewing it. I will be writing test cases for this api in mynext version of this patch. Reviewers: MarkCallaghan, heyongqiang Reviewed By: heyongqiang CC: sheki, adsharma Differential Revision: compression options configurable. These include window-bits, level and strategy for ZlibCompression Summary: Leveldb currently uses windowBits=-14 while using zlib compression.(It was earlier 15). This makes the setting configurable. Related changes here: Test Plan: make all check Reviewers: dhruba, MarkCallaghan, sheki, heyongqiang Differential Revision: test failure Summary: as subject Test Plan: db_test Reviewers: dhruba, MarkCallaghan Reviewed By: MarkCallaghan Differential Revision: a test case to make sure chaning num_levels will fail Summary: Summary: as subject Test Plan: db_test Reviewers: dhruba, MarkCallaghan Reviewed By: MarkCallaghan Differential Revision: having different compression algorithms on different levels. Summary: The leveldb API is enhanced to support different compression algorithms at different levels. This adds the option min_level_to_compress to db_bench that specifies the minimum level for which compression should be done when compression is enabled. This can be used to disable compression for levels 0 and 1 which are likely to suffer from stalls because of the CPU load for memtable flushes and (L0,L1) compaction. Level 0 is special as it gets frequent memtable flushes. Level 1 is special as it frequently gets all:all file compactions between it and level 0. But all other levels could be the same. For any level N where N > 1, the rate of sequential IO for that level should be the same. The last level is the exception because it might not be full and because files from it are not read to compact with the next larger level. The same amount of time will be spent doing compaction at any level N excluding N=0, 1 or the last level. By this standard all of those levels should use the same compression. The difference is that the loss (using more disk space) from a faster compression algorithm is less significant for N=2 than for N=3. So we might be willing to trade disk space for faster write rates with no compression for L0 and L1, snappy for L2, zlib for L3. Using a faster compression algorithm for the mid levels also allows us to reclaim some cpu without trading off much loss in disk space overhead. Also note that little is to be gained by compressing levels 0 and 1. For a 4-level tree they account for 10% of the data. For a 5-level tree they account for 1% of the data. With compression enabled: * memtable flush rate is ~18MB/second * (L0,L1) compaction rate is ~30MB/second With compression enabled but min_level_to_compress=2 * memtable flush rate is ~320MB/second * (L0,L1) compaction rate is ~560MB/second This practicaly takes the same code from but makes the leveldb api more general purpose with a few additional lines of code. Test Plan: make check Differential Revision: unit test failure caused by delaying deleting obsolete files. Summary: A previous commit 4c107587ed47af84633f8c61f65516a504d6cd98 introduced the idea that some version updates might not delete obsolete files. This means that if a unit test blindly counts the number of files in the db directory it might not represent the true state of the database. Use GetLiveFiles() insteads to count the number of live files in the database. Test Plan: make check/Trigger read compaction only if seeks to storage are incurred. Summary: In the current code, a Get() call can trigger compaction if it has to look at more than one file. This causes unnecessary compaction because looking at more than one file is a penalty only if the file is not yet in the cache. Also, th current code counts these files before the bloom filter check is applied. This patch counts a seek only if the file fails the bloom filter check and has to read in data block(s) from the storage. This patch also counts a seek if a file is not present in the file-cache, because opening a file means that its index blocks need to be read into cache. Test Plan: unit test attached. I will probably add one more unti tests. Reviewers: heyongqiang Reviewed By: heyongqiang CC: MarkCallaghan Differential Revision: BackupAPI should also list the length of the manifest file. Summary: The GetLiveFiles() api lists the set of sst files and the current MANIFEST file. But the database continues to append new data to the MANIFEST file even when the application is backing it up to the backup location. This means that the database-version that is stored in the MANIFEST FILE in the backup location does not correspond to the sst files returned by GetLiveFiles. This API adds a new parameter to GetLiveFiles. This new parmeter returns the current size of the MANIFEST file. Test Plan: Unit test attached. Reviewers: heyongqiang Reviewed By: heyongqiang Differential Revision: to take a file-lvel snapshot from leveldb. Summary: A set of apis that allows an application to backup data from the leveldb database based on a set of files. Test Plan: unint test attached. more coming soon. Reviewers: heyongqiang Reviewed By: heyongqiang Differential Revision: log in a seperate dir Summary: added a new option db_log_dir, which points the log dir. Inside that dir, in order to make log names unique, the log file name is prefixed with the leveldb data dir absolute path. Test Plan: db_test Reviewers: dhruba Reviewed By: dhruba Differential Revision: 1.5 Summary: as subject Test Plan: db_test table_test Reviewers: dhruba/Do not spin in a tight loop attempting compactions if there is a compaction error Summary: as subject. ported the change from google code leveldb 1.5 Test Plan: run db_test Reviewers: dhruba Differential Revision: concurrent multiple opens of leveldb database. Summary: The fcntl call cannot detect lock conflicts when invoked multiple times from the same thread. Use a static lockedFile Set to record the paths that are locked. A lockfile request checks to see if htis filename already exists in lockedFiles, if so, then it triggers an error. Otherwise, it inserts the filename in the lockedFiles Set. A unlock file request verifies that the filename is in the lockedFiles set and removes it from lockedFiles set. Test Plan: unit test attached Reviewers: heyongqiang Reviewed By: heyongqiang Differential Revision: ts as suffix for LOG.old files Summary: as subject and only maintain 10 log files. Test Plan: new test in db_test Reviewers: dhruba Differential Revision:"
1007,1007,3.0,0.6398000121116638,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Fixed cache key for block cache Summary: Added function to `RandomAccessFile` to generate an unique ID for that file. Currently only `PosixRandomAccessFile` has this behaviour implemented and only on Linux. Changed how key is generated in `Table::BlockReader`. Added tests to check whether the unique ID is stable, unique and not a prefix of another unique ID. Added tests to see that `Table` uses the cache more efficiently. Test Plan: make check Reviewers: chip, vamsi, dhruba Reviewed By: chip CC: leveldb Differential Revision: fallocate to prevent excessive allocation of sst files and logs Summary: On some filesystems, pre-allocation can be a considerable amount of space. xfs in our production environment pre-allocates by 1GB, for instance. By using fallocate to inform the kernel of our expected file sizes, we eliminate this wasteage (that isnt recovered until the file is closed which, in the case of LOG files, can be a considerable amount of time). Test Plan: created an xfs loopback filesystem, mounted with allocsize=4M, and ran db_stress. LOG file without this change was 4M, and with it it was 128k then grew to normal size. Reviewers: dhruba Reviewed By: dhruba CC: adsharma, leveldb Differential Revision: a number of object lifetime/ownership issues Summary: Replace manual memory management with std::unique_ptr in a number of places; not exhaustive, but this fixes a few leaks with file handles as well as clarifies semantics of the ownership of file handles with log classes. Test Plan: db_stress, make check Reviewers: dhruba Reviewed By: dhruba CC: zshao, leveldb, heyongqiang Differential Revision: WAL files to archive directory, instead of deleting. Summary: Create a directory ""archive"" in the DB directory. During DeleteObsolteFiles move the WAL files (*.log) to the Archive directory, instead of deleting. Test Plan: Created a DB using DB_Bench. Reopened it. Checked if files move. Reviewers: dhruba Reviewed By: dhruba Differential Revision:"
1008,1008,3.0,0.9682999849319458,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Fix a number of object lifetime/ownership issues Summary: Replace manual memory management with std::unique_ptr in a number of places; not exhaustive, but this fixes a few leaks with file handles as well as clarifies semantics of the ownership of file handles with log classes. Test Plan: db_stress, make check Reviewers: dhruba Reviewed By: dhruba CC: zshao, leveldb, heyongqiang Differential Revision:"
1009,1009,7.0,0.8296999931335449,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Fix poor error on num_levels mismatch and few other minor improvements Summary: Previously, if you opened a db with num_levels set lower than the database, you received the unhelpful message ""Corruption: VersionEdit: new-file entry."" Now you get a more verbose message describing the issue. Also, fix handling of compression_levels (both the run-over-the-end issue and the memory management of it). Lastly, unique_ptrify a couple of minor calls. Test Plan: make check Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: fallocate to prevent excessive allocation of sst files and logs Summary: On some filesystems, pre-allocation can be a considerable amount of space. xfs in our production environment pre-allocates by 1GB, for instance. By using fallocate to inform the kernel of our expected file sizes, we eliminate this wasteage (that isnt recovered until the file is closed which, in the case of LOG files, can be a considerable amount of time). Test Plan: created an xfs loopback filesystem, mounted with allocsize=4M, and ran db_stress. LOG file without this change was 4M, and with it it was 128k then grew to normal size. Reviewers: dhruba Reviewed By: dhruba CC: adsharma, leveldb Differential Revision: a number of object lifetime/ownership issues Summary: Replace manual memory management with std::unique_ptr in a number of places; not exhaustive, but this fixes a few leaks with file handles as well as clarifies semantics of the ownership of file handles with log classes. Test Plan: db_stress, make check Reviewers: dhruba Reviewed By: dhruba CC: zshao, leveldb, heyongqiang Differential Revision: manifest file. Summary: Check in LogAndApply if the file size is more than the limit set in Options. Things to consider : will this be expensive? Test Plan: make all check. Inputs on a new unit test? Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: fix for Leveldb manifest writing bug from Open-Source Summary: Pretty much a blind copy of the patch in open source. Hope to get this in before we make a release Test Plan: make clean check Reviewers: dhruba, heyongqiang Reviewed By: dhruba CC: leveldb Differential Revision: meta-database support. Summary: Added kMetaDatabase for meta-databases in db/filename.h along with supporting fuctions. Fixed switch in DBImpl so that it also handles kMetaDatabase. Fixed DestroyDB() that it can handle destroying meta-databases. Test Plan: make check Reviewers: sheki, emayanke, vamsi, dhruba Reviewed By: dhruba Differential Revision: API in write batch. Summary: WriteBatch is now used by the GetUpdatesSinceAPI. This API is external and will be used by the rocks server. Rocks Server and others will need to know about the Sequence Number in the WriteBatch. This public method will allow for that. Test Plan: make all check. Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: Bug in Binary Search for files containing a seq no. and delete Archived Log Files during Destroy DB. Summary: * Fixed implementation bug in Binary_Searvch introduced in * Binary search is also overflow safe. * Delete archive log files and archive dir during DestroyDB Test Plan: make check Reviewers: dhruba CC: kosievdmerwe, emayanke Differential Revision: public api to fetch the latest transaction id. Summary: Implement a interface to retrieve the most current transaction id from the database. Test Plan: Added unit test. Reviewers: sheki Reviewed By: sheki CC: leveldb Differential Revision: GetArchivalDirectoryName to filename.h Summary: filename.h has functions to do similar things. Moving code away from db_impl.cc Test Plan: make check Reviewers: dhruba Reviewed By: dhruba Differential Revision: API to enable replication. Summary: How it works: * GetUpdatesSince takes a SequenceNumber. * A LogFile with the first SequenceNumber nearest and lesser than the requested Sequence Number is found. * Seek in the logFile till the requested SeqNumber is found. * Return an iterator which contains logic to return records one by one. Test Plan: * Test case included to check the good code path. * Will update with more test-cases. * Feedback required on test-cases. Reviewers: dhruba, emayanke Reviewed By: dhruba CC: leveldb Differential Revision: WAL files to archive directory, instead of deleting. Summary: Create a directory ""archive"" in the DB directory. During DeleteObsolteFiles move the WAL files (*.log) to the Archive directory, instead of deleting. Test Plan: Created a DB using DB_Bench. Reopened it. Checked if files move. Reviewers: dhruba Reviewed By: dhruba Differential Revision: non-visible keys during a compaction even in the presense of snapshots. Summary: LevelDB should delete almost-new keys when a long-open snapshot exists. The previous behavior is to keep all versions that were created after the oldest open snapshot. This can lead to database size bloat for high-update workloads when there are long-open snapshots and long-open snapshot will be used for logical backup. By ""almost new"" I mean that the key was updated more than once after the oldest snapshot. If there were two snapshots with seq numbers s1 and s2 (s1 s2), and if we find two instances of the same key k1 that lie entirely within s1 and s2 (i.e. s1 k1 s2), then the earlier version of k1 can be safely deleted because that version is not visible in any snapshot. Test Plan: unit test attached make clean check Differential Revision: CompactionFilter api: pass in a opaque argument to CompactionFilter invocation. Summary: There are applications that operate on multiple leveldb instances. These applications will like to pass in an opaque type for each leveldb instance and this type should be passed back to the application with every invocation of the CompactionFilter api. Test Plan: Enehanced unit test for opaque parameter to CompactionFilter. Reviewers: heyongqiang Reviewed By: heyongqiang CC: MarkCallaghan, sheki, emayanke Differential Revision:"
1010,1010,12.0,0.8571000099182129,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Exit and Join the background compaction threads while running rocksdb tests Summary: The background compaction threads are never exitted and therefore caused memory-leaks while running rpcksdb tests. Have changed the PosixEnv destructor to exit and join them and changed the tests likewise The memory leaked has reduced from 320 bytes to 64 bytes in all the tests. The 64 bytes is relating to pthread_exit, but still have to figure out why. The stack-trace right now with table_test.cc 64 bytes in 1 blocks are possibly lost in loss record 4 of 5 at 0x475D8C: malloc (jemalloc.c:914) by 0x400D69E: _dl_map_object_deps (dl-deps.c:505) by 0x4013393: dl_open_worker (dl-open.c:263) by 0x400F015: _dl_catch_error (dl-error.c:178) by 0x4013B2B: _dl_open (dl-open.c:569) by 0x5D3E913: do_dlopen (dl-libc.c:86) by 0x400F015: _dl_catch_error (dl-error.c:178) by 0x5D3E9D6: __libc_dlopen_mode (dl-libc.c:47) by 0x5048BF3: pthread_cancel_init (unwind-forcedunwind.c:53) by 0x5048DC9: _Unwind_ForcedUnwind (unwind-forcedunwind.c:126) by 0x5046D9F: __pthread_unwind (unwind.c:130) by 0x50413A4: pthread_exit (pthreadP.h:289) Test Plan: make all check Reviewers: dhruba, sheki, haobo Reviewed By: dhruba CC: leveldb, chip Differential Revision: FD_CLOEXEC after each file open Summary: as subject. This is causing problem in adsconv. Ideally, this flags should be set in open. But that is only supported in Linux kernel ?2.6.23 and glibc ?2.7. Test Plan: db_test run db_test Reviewers: dhruba, MarkCallaghan, haobo Reviewed By: dhruba CC: leveldb, chip Differential Revision: env_posix cleanup Summary: 1. SetBackgroundThreads was not thread safe 2. queue_size_ does not seem necessary 3. moved condition signal after shared state change. Even though the original order is in practice ok (because the mutex is still held), it looks fishy and non-intuitive. Test Plan: make check Reviewers: dhruba Reviewed By: dhruba CC: leveldb, zshao Differential Revision: variable in constructor for PosixEnv::checkedDiskForMmap_ Summary: This caused compilation problems on some gcc platforms during the third-partyrelease Test Plan: make Reviewers: sheki Reviewed By: sheki Differential Revision: to configure bufferedio-reads, filesystem-readaheads and mmap-read-write per database. Summary: This patch allows an application to specify whether to use bufferedio, reads-via-mmaps and writes-via-mmaps per database. Earlier, there was a global static variable that was used to configure this functionality. The default setting remains the same (and is backward compatible): 1. use bufferedio 2. do not use mmaps for reads 3. use mmap for writes 4. use readaheads for reads needed for compaction I also added a parameter to db_bench to be able to explicitly specify whether to do readaheads for compactions or not. Test Plan: make check Reviewers: sheki, heyongqiang, MarkCallaghan Reviewed By: sheki CC: leveldb Differential Revision: posix_fallocate as default. Summary: Ftruncate does not throw an error on disk-full. This causes Sig-bus in the case where the database tries to issue a Put call on a full-disk. Use posix_fallocate for allocation instead of truncate. Add a check to use MMaped files only on ext4, xfs and tempfs, as posix_fallocate is very slow on ext3 and older. Test Plan: make all check Reviewers: dhruba, chip Reviewed By: dhruba CC: adsharma, leveldb Differential Revision:"
1011,1011,2.0,0.9797999858856201,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","[RocksDB] Refactor table.cc to reduce code duplication and improve readability. Summary: In table.cc, the code section that reads in BlockContent and then put it into a Block, appears at least 4 times. This is too much duplication. BlockReader is much shorter after the change and reads way better. D10077 attempted that for index block read. This is a complete cleanup. Test Plan: make check; ./db_stress Reviewers: dhruba, sheki Reviewed By: dhruba CC: leveldb Differential Revision: NULL to nullptr Summary: scripted NULL to nullptr in * include/leveldb/ * db/ * table/ * util/ Test Plan: make all check Reviewers: dhruba, emayanke Reviewed By: emayanke CC: leveldb Differential Revision:"
1012,1012,9.0,0.9430999755859375,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Fix more signed-unsigned comparisons Summary: Some comparisons left in log_test.cc and db_test.cc complained by make Test Plan: make Reviewers: dhruba, sheki Reviewed By: dhruba Differential Revision:"
1013,1013,12.0,0.5450000166893005,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","[RocksDB] Cleanup compaction filter to use a class interface, instead of function pointer and additional context pointer. Summary: This diff replaces compaction_filter_args and CompactionFilter with a single compaction_filter parameter. It gives CompactionFilter better encapsulation and a similar look to Comparator and MergeOpertor, which improves consistency of the overall interface. The change is not backward compatible. Nevertheless, the two references in fbcode are not in production yet. Test Plan: make check Reviewers: dhruba Reviewed By: dhruba CC: leveldb, zshao Differential Revision: fix compaction filter trigger condition Summary: Currently, compaction filter is run on internal key older than the oldest snapshot, which is incorrect. Compaction filter should really be run on the most recent internal key when there is no external snapshot. Test Plan: make check; db_stress Reviewers: dhruba Reviewed By: dhruba Differential Revision: Clear Archive WAL files Summary: WAL files are moved to archive directory and clear only at DB::Open. Can lead to a lot of space consumption in a Database. Added logic to periodically clear Archive Directory too. Test Plan: make all check + add unit test Reviewers: dhruba, heyongqiang Reviewed By: heyongqiang CC: leveldb Differential Revision: Support Merge operation in rocksdb Summary: This diff introduces a new Merge operation into rocksdb. The purpose of this review is mostly getting feedback from the team (everyone please) on the design. Please focus on the four files under include/leveldb/, as they spell the client visible interface change. include/leveldb/db.h include/leveldb/merge_operator.h include/leveldb/options.h include/leveldb/write_batch.h Please go over local/my_test.cc carefully, as it is a concerete use case. Please also review the impelmentation files to see if the straw man implementation makes sense. Note that, the diff does pass all make check and truly supports forward iterator over db and a version of Get thats based on iterator. Future work: Integration with compaction A raw Get implementation I am working on a wiki that explains the design and implementation choices, but coding comes just naturally and I think it might be a good idea to share the code earlier. The code is heavily commented. Test Plan: run all local tests Reviewers: dhruba, heyongqiang Reviewed By: dhruba CC: leveldb, zshao, sheki, emayanke, MarkCallaghan Differential Revision: Look at all the files, not just the first file in TransactionLogIter as BatchWrites can leave it in Limbo Summary: Transaction Log Iterator did not move to the next file in the series if there was a write batch at the end of the currentFile. The solution is if the last seq no. of the current file is RequestedSeqNo. Assume the first seqNo. of the next file has to satisfy the request. Also major refactoring around the code. Moved opening the logreader to a seperate function, got rid of goto. Test Plan: added a unit test for it. Reviewers: dhruba, heyongqiang Reviewed By: heyongqiang CC: leveldb, emayanke Differential Revision: Recover last updated sequence number from manifest also. Summary: During recovery, last_updated_manifest number was not set if there were no records in the Write-ahead log. Now check for the recovered manifest also and set last_updated_manifest file to the max value. Test Plan: unit test Reviewers: heyongqiang Reviewed By: heyongqiang CC: leveldb Differential Revision: Fix Crash on finding a db with no log files. Error out instead Summary: If the vector returned by GetUpdatesSince is empty, it is still returned to the user. This causes it throw an std::range error. The probable file list is checked and it returns an IOError status instead of OK now. Test Plan: added a unit test. Reviewers: dhruba, heyongqiang Reviewed By: heyongqiang CC: leveldb Differential Revision: non-mmapd files for Write-Ahead Files Summary: Use non mmapd files for Write-Ahead log. Earlier use of MMaped files. made the log iterator read ahead and miss records. Now the reader and writer will point to the same physical location. There is no perf regression : ./db_bench 20) with This diff : fillseq : 10.756 micros/op 185281 ops/sec; 20.5 MB/s without this dif : fillseq : 11.085 micros/op 179676 ops/sec; 19.9 MB/s Test Plan: unit test included Reviewers: dhruba, heyongqiang Reviewed By: heyongqiang CC: leveldb Differential Revision: should stall at the last record. Currently it errors out Summary: * Add a method to check if the log reader is at EOF. * If we know a record has been flushed force the log_reader to believe it is not at EOF, using a new method UnMarkEof(). This does not work with MMpaed files. Test Plan: added a unit test. Reviewers: dhruba, heyongqiang Reviewed By: heyongqiang CC: leveldb Differential Revision: to configure bufferedio-reads, filesystem-readaheads and mmap-read-write per database. Summary: This patch allows an application to specify whether to use bufferedio, reads-via-mmaps and writes-via-mmaps per database. Earlier, there was a global static variable that was used to configure this functionality. The default setting remains the same (and is backward compatible): 1. use bufferedio 2. do not use mmaps for reads 3. use mmap for writes 4. use readaheads for reads needed for compaction I also added a parameter to db_bench to be able to explicitly specify whether to do readaheads for compactions or not. Test Plan: make check Reviewers: sheki, heyongqiang, MarkCallaghan Reviewed By: sheki CC: leveldb Differential Revision: a zero-sized file while looking for a seq-no in GetUpdatesSince Summary: Rocksdb can create 0 sized log files when it is opened and closed without any operations. The GetUpdatesSince fails currently if there is a log file of size zero. This diff fixes this. If there is a log file is 0, it is removed form the probable_file_list Test Plan: unit test Reviewers: dhruba, heyongqiang Reviewed By: heyongqiang CC: leveldb Differential Revision: not allow Transaction Log Iterator to fall ahead when writer is writing the same file Summary: Store the last flushed, seq no. in db_impl. Check against it in transaction Log iterator. Do not attempt to read ahead if we do not know if the data is flushed completely. Does not work if flush is disabled. Any ideas on fixing that? * Minor change, iter->Next is called the first time automatically for * the first time. Test Plan: existing test pass. More ideas on testing this? Planning to run some stress test. Reviewers: dhruba, heyongqiang CC: leveldb Differential Revision: rate_delay_limit_milliseconds Summary: This adds the rate_delay_limit_milliseconds option to make the delay configurable in MakeRoomForWrite when the max compaction score is too high. This delay is called the Ln slowdown. This change also counts the Ln slowdown per level to make it possible to see where the stalls occur. From IO-bound performance testing, the Level N stalls occur: * with compression at the largest uncompressed level. This makes sense because compaction for compressed levels is much slower. When Lx is uncompressed and Lx+1 is compressed then files pile up at Lx because the (Lx,Lx+1)->Lx+1 compaction process is the first to be slowed by compression. * without compression at level 1 Task ID: Blame Rev: Test Plan: run with real data, added test Revert Plan: Database Impact: Memcache Impact: Other Notes: EImportant: begin *PUBLIC* platform impact section Bugzilla: end platform impact Reviewers: dhruba Reviewed By: dhruba Differential Revision: for rocksdb to compact when flushing the in-memory memtable to a file in L0. Summary: Rocks accumulates recent writes and deletes in the in-memory memtable. When the memtable is full, it writes the contents on the memtable to a file in L0. This patch removes redundant records at the time of the flush. If there are multiple versions of the same key in the memtable, then only the most recent one is dumped into the output file. The purging of redundant records occur only if the most recent snapshot is earlier than the earliest record in the memtable. Should we switch on this feature by default or should we keep this feature turned off in the default settings? Test Plan: Added test case to db_test.cc Reviewers: sheki, vamsi, emayanke, heyongqiang Reviewed By: sheki CC: leveldb Differential Revision: NULL to nullptr Summary: scripted NULL to nullptr in * include/leveldb/ * db/ * table/ * util/ Test Plan: make all check Reviewers: dhruba, emayanke Reviewed By: emayanke CC: leveldb Differential Revision: out redundant sequence numbers for kvs to increase compression efficiency Summary: The sequence numbers in each record eat up plenty of space on storage. The optimization zeroes out sequence numbers on kvs in the Lmax layer that are earlier than the earliest snapshot. Test Plan: Unit test attached. Differential Revision:"
1014,1014,7.0,0.5946000218391418,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[RocksDB] Option for incremental sync Summary: This diff added an option to control the incremenal sync frequency. db_bench has a new flag bytes_per_sync for easy tuning exercise. Test Plan: make check; db_bench Reviewers: dhruba CC: leveldb Differential Revision: Sync file to disk incrementally Summary: During compaction, we sync the output files after they are fully written out. This causes unnecessary blocking of the compaction thread and burstiness of the write traffic. This diff simply asks the OS to sync data incrementally as they are written, on the background. The hope is that, at the final sync, most of the data are already on disk and we would block less on the sync call. Thus, each compaction runs faster and we could use fewer number of compaction threads to saturate IO. In addition, the write traffic will be smoothed out, hopefully reducing the IO P99 latency too. Some quick tests show 10~20% improvement in per thread compaction throughput. Combined with posix advice on compaction read, just 5 threads are enough to almost saturate the udb flash bandwidth for 800 bytes write only benchmark. Whats more promising is that, with saturated IO, iostat shows average wait time is actually smoother and much smaller. For the write only test 800bytes test: Before the change: await occillate between 10ms and 3ms After the change: await ranges 1-3ms Will test against read-modify-write workload too, see if high read latency P99 could be resolved. Will introduce a parameter to control the sync interval in a follow up diff after cleaning up EnvOptions. Test Plan: make check; db_bench; db_stress Reviewers: dhruba CC: leveldb Differential Revision: cleanup EnvOptions Summary: This diff simplifies EnvOptions by treating it as POD, similar to Options. virtual functions are removed and member fields are accessed directly. StorageOptions is removed. Options.allow_readahead and Options.allow_readahead_compactions are deprecated. Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite Test Plan: make check; db_stress Reviewers: dhruba CC: leveldb Differential Revision: [Performance] Allow different posix advice to be applied to the same table file Summary: Current posix advice implementation ties up the access pattern hint with the creation of a file. It is not possible to apply different advice for different access (random get vs compaction read), without keeping two open files for the same table. This patch extended the RandomeAccessFile interface to accept new access hint at anytime. Particularly, we are able to set different access hint on the same table file based on when/how the file is used. Two options are added to set the access hint, after the file is first opened and after the file is being compacted. Test Plan: make check; db_stress; db_bench Reviewers: dhruba Reviewed By: dhruba CC: MarkCallaghan, leveldb Differential Revision: randomly at various points in source code for testing] Summary: This is initial version. A few ways in which this could be extended in the future are: (a) Killing from more places in source code (b) Hashing stack and using that hash in determining whether to crash. This is to avoid crashing more often at source lines that are executed more often. (c) Raising exceptions or returning errors instead of killing Test Plan: This whole thing is for testing. Here is part of output: python2.7 tools/db_crashtest2.py 600 Running db_stress db_stress retncode output LevelDB version : 1.5 Number of threads : 32 Ops per thread : 10000000 Read percentage : 50 Write-buffer-size : 4194304 Delete percentage : 30 Max key : 1000 Ratio : 320000 Num times DB reopens: 0 Batches/snapshots : 1 Purge redundant % : 50 Num keys per lock : 4 Compression : snappy No lock creation because test_batches_snapshots set 2013/04/26-17:55:17 Starting database operations Created bg thread 0x7fc1f07ff700 ... finished 60000 ops Running db_stress db_stress retncode output LevelDB version : 1.5 Number of threads : 32 Ops per thread : 10000000 Read percentage : 50 Write-buffer-size : 4194304 Delete percentage : 30 Max key : 1000 Ratio : 320000 Num times DB reopens: 0 Batches/snapshots : 1 Purge redundant % : 50 Num keys per lock : 4 Compression : snappy Created bg thread 0x7ff0137ff700 No lock creation because test_batches_snapshots set 2013/04/26-17:56:15 Starting database operations ... finished 90000 ops Revert Plan: OK Task ID: Reviewers: dhruba, emayanke Reviewed By: emayanke CC: leveldb, haobo Differential Revision:"
1015,1015,13.0,0.9944000244140625,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Expose base db object from ttl wrapper Summary: rocksdb replicaiton will need this when writing value+TS from master to slave as is Test Plan: make Reviewers: dhruba, vamsi, haobo Reviewed By: dhruba CC: leveldb Differential Revision: operator for ttl Summary: Implemented a TtlMergeOperator class which inherits from MergeOperator and is TTL aware. It strips out timestamp from existing_value and attaches timestamp to new_value, calling user-provided-Merge in between. Test Plan: make all check Reviewers: haobo, dhruba Reviewed By: haobo CC: leveldb Differential Revision: KeyMayExist to return the proper value if it can be found in memory and also check block_cache Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldnt just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test Test Plan: make all check;db_stress for 1 hour Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision: rocksdb-deletes faster using bloom filter Summary: Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving: 1. Put of delete type 2. Space in the db,and 3. Compaction time Test Plan: make all check; will run db_stress and db_bench and enhance unit-test once the basic design gets approved Reviewers: dhruba, haobo, vamsi Reviewed By: haobo CC: leveldb Differential Revision: bucketing logic in ldb-ttl Summary: [start_time, end_time) is waht Im following for the buckets and the whole time-range. Also cleaned up some code in db_ttl.* Not correcting the spacing/indenting convention for util/ldb_cmd.cc in this diff. Test Plan: python ldb_test.py, make ttl_test, Run mcrocksdb-backup tool, Run the ldb tool on 2 mcrocksdb production backups form sigmafio033.prn1 Reviewers: vamsi, haobo Reviewed By: vamsi Differential Revision: basic Multiget and simple test cases. Summary: Implemented the MultiGet operator which takes in a list of keys and returns their associated values. Currently uses std::vector as its container data structure. Otherwise, it works identically to ""Get"". Test Plan: 1. make db_test ; compile it 2. ./db_test ; test it 3. make all check ; regress / run all tests 4. make release ; (optional) compile with release settings Reviewers: haobo, MarkCallaghan, dhruba Reviewed By: dhruba CC: leveldb Differential Revision:"
1016,1016,7.0,0.9926000237464905,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[RocksDB] [MergeOperator] The new Merge Interface Uses merge sequences. Summary: Here are the major changes to the Merge Interface. It has been expanded to handle cases where the MergeOperator is not associative. It does so by stacking up merge operations while scanning through the key history (i.e.: during Get() or Compaction), until a valid Put/Delete/end-of-history is encountered; it then applies all of the merge operations in the correct sequence starting with the base/sentinel value. I have also introduced an ""AssociativeMerge"" function which allows the user to take advantage of associative merge operations (such as in the case of counters). The implementation will always attempt to merge the operations/operands themselves together when they are encountered, and will resort to the ""stacking"" method if and only if the ""associative-merge"" fails. This implementation is conjectured to allow MergeOperator to handle the general case, while still providing the user with the ability to take advantage of certain efficiencies in their own merge-operator / data-structure. NOTE: This is a preliminary diff. This must still go through a lot of review, revision, and testing. Feedback welcome Test Plan: is a preliminary diff. I have only just begun testing/debugging it. will be testing this with the existing MergeOperator use-cases and unit-tests (counters, string-append, and redis-lists) will be ""desk-checking"" and walking through the code with the help gdb. will find a way of stress-testing the new interface / implementation using db_bench, db_test, merge_test, and/or db_stress. will ensure that my tests cover all cases: Get-Memtable, Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0, Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found, end-of-history, end-of-file, etc. lot of feedback from the reviewers. Reviewers: haobo, dhruba, zshao, emayanke Reviewed By: haobo CC: leveldb Differential Revision:"
1017,1017,7.0,0.9926000237464905,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[RocksDB] [MergeOperator] The new Merge Interface Uses merge sequences. Summary: Here are the major changes to the Merge Interface. It has been expanded to handle cases where the MergeOperator is not associative. It does so by stacking up merge operations while scanning through the key history (i.e.: during Get() or Compaction), until a valid Put/Delete/end-of-history is encountered; it then applies all of the merge operations in the correct sequence starting with the base/sentinel value. I have also introduced an ""AssociativeMerge"" function which allows the user to take advantage of associative merge operations (such as in the case of counters). The implementation will always attempt to merge the operations/operands themselves together when they are encountered, and will resort to the ""stacking"" method if and only if the ""associative-merge"" fails. This implementation is conjectured to allow MergeOperator to handle the general case, while still providing the user with the ability to take advantage of certain efficiencies in their own merge-operator / data-structure. NOTE: This is a preliminary diff. This must still go through a lot of review, revision, and testing. Feedback welcome Test Plan: is a preliminary diff. I have only just begun testing/debugging it. will be testing this with the existing MergeOperator use-cases and unit-tests (counters, string-append, and redis-lists) will be ""desk-checking"" and walking through the code with the help gdb. will find a way of stress-testing the new interface / implementation using db_bench, db_test, merge_test, and/or db_stress. will ensure that my tests cover all cases: Get-Memtable, Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0, Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found, end-of-history, end-of-file, etc. lot of feedback from the reviewers. Reviewers: haobo, dhruba, zshao, emayanke Reviewed By: haobo CC: leveldb Differential Revision:"
1018,1018,13.0,0.9943000078201294,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Prefix filters for scans (v4) Summary: Similar to v2 (db and table code understands prefixes), but use ReadOptions as in v3. Also, make the CreateFilter code faster and cleaner. Test Plan: make db_test; export LEVELDB_TESTS=PrefixScan; ./db_test Reviewers: dhruba Reviewed By: dhruba CC: haobo, emayanke Differential Revision: KeyMayExist to return the proper value if it can be found in memory and also check block_cache Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldnt just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test Test Plan: make all check;db_stress for 1 hour Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision: KeyMayExist for WriteBatch-Deletes Summary: Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete. Added code to skip getting Table from disk if not already present in table_cache. Some renaming of variables. Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch. Changed KeyMayExist to not be pure virtual and provided a default implementation. Expanded unit-tests in db_test to check appropriately. Ran db_stress for 1 hour with ./db_stress Test Plan: db_stress;make check Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb, xjin Differential Revision: rocksdb-deletes faster using bloom filter Summary: Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving: 1. Put of delete type 2. Space in the db,and 3. Compaction time Test Plan: make all check; will run db_stress and db_bench and enhance unit-test once the basic design gets approved Reviewers: dhruba, haobo, vamsi Reviewed By: haobo CC: leveldb Differential Revision: measure table open io in a histogram Summary: Table is setup for compaction using Table::SetupForCompaction. So read block calls can be differentiated b/w Gets/Compaction. Use this and measure times. Test Plan: db_bench Reviewers: dhruba, haobo Reviewed By: haobo CC: leveldb, MarkCallaghan Differential Revision:"
1019,1019,13.0,0.7759000062942505,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Implement log blobs Summary: This patch adds the ability for the user to add sequences of arbitrary data (blobs) to write batches. These blobs are saved to the log along with everything else in the write batch. You can add multiple blobs per WriteBatch and the ordering of blobs, puts, merges, and deletes are preserved. Blobs are not saves to SST files. RocksDB ignores blobs in every way except for writing them to the log. Before committing this patch, I need to add some test code. But Im submitting it now so people can comment on the API. Test Plan: make check Reviewers: dhruba, haobo, vamsi Reviewed By: dhruba CC: leveldb Differential Revision: filters for scans (v4) Summary: Similar to v2 (db and table code understands prefixes), but use ReadOptions as in v3. Also, make the CreateFilter code faster and cleaner. Test Plan: make db_test; export LEVELDB_TESTS=PrefixScan; ./db_test Reviewers: dhruba Reviewed By: dhruba CC: haobo, emayanke Differential Revision: Compaction should keep DeleteMarkers unless it is the earliest file. Summary: The pre-existing code was purging a DeleteMarker if thay key did not exist in deeper levels. But in the Universal Compaction Style, all files are in Level0. For compaction runs that did not include the earliest file, we were erroneously purging the DeleteMarkers. The fix is to purge DeleteMarkers only if the compaction includes the earlist file. Test Plan: DBTest.Randomized triggers this code path. Differential Revision: unit tests for universal compaction (step 2) Summary: Continue fixing existing unit tests for universal compaction. I have tried to apply universal compaction to all unit tests those havent called ChangeOptions(). I left a few which are either apparently not applicable to universal compaction (because they check files/keys/values at level 1 or above levels), or apparently not related to compaction (e.g., open a file, open a db). I also add a new unit test for universal compaction. Good news is I didnt see any bugs during this round. Test Plan: Ran ""make all check"" yesterday. Has rebased and is rerunning Reviewers: haobo, dhruba Differential Revision: unit tests/bugs for universal compaction (first step) Summary: This is the first step to fix unit tests and bugs for universal compactiion. I added universal compaction option to ChangeOptions(), and fixed all unit tests calling ChangeOptions(). Some of these tests obviously assume more than 1 level and check file number/values in level 1 or above levels. I set kSkipUniversalCompaction for these tests. The major bug I found is manual compaction with universal compaction never stops. I have put a fix for it. I have also set universal compaction as the default compaction and found at least 20+ unit tests failing. I havent looked into the details. The next step is to check all unit tests without calling ChangeOptions(). Test Plan: make all check Reviewers: dhruba, haobo Differential Revision: KeyMayExist to return the proper value if it can be found in memory and also check block_cache Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldnt just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test Test Plan: make all check;db_stress for 1 hour Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision: KeyMayExist for WriteBatch-Deletes Summary: Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete. Added code to skip getting Table from disk if not already present in table_cache. Some renaming of variables. Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch. Changed KeyMayExist to not be pure virtual and provided a default implementation. Expanded unit-tests in db_test to check appropriately. Ran db_stress for 1 hour with ./db_stress Test Plan: db_stress;make check Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb, xjin Differential Revision: Enable manual compaction to move files back to an appropriate level. Summary: As title. This diff added an option reduce_level to CompactRange. When set to true, it will try to move the files back to the minimum level sufficient to hold the data set. Note that the default is set to true now, just to excerise it in all existing tests. Will set the default to false before check-in, for backward compatibility. Test Plan: make check; Reviewers: dhruba, emayanke CC: leveldb Differential Revision: rocksdb-deletes faster using bloom filter Summary: Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving: 1. Put of delete type 2. Space in the db,and 3. Compaction time Test Plan: make all check; will run db_stress and db_bench and enhance unit-test once the basic design gets approved Reviewers: dhruba, haobo, vamsi Reviewed By: haobo CC: leveldb Differential Revision: write amplification by merging files in L0 back into L0 Summary: There is a new option called hybrid_mode which, when switched on, causes HBase style compactions. Files from L0 are compacted back into L0. This meat of this compaction algorithm is in PickCompactionHybrid(). All files reside in L0. That means all files have overlapping keys. Each file has a time-bound, i.e. each file contains a range of keys that were inserted around the same time. The start-seqno and the end-seqno refers to the timeframe when these keys were inserted. Files that have contiguous seqno are compacted together into a larger file. All files are ordered from most recent to the oldest. The current compaction algorithm starts to look for candidate files starting from the most recent file. It continues to add more files to the same compaction run as long as the sum of the files chosen till now is smaller than the next candidate file size. This logic needs to be debated and validated. The above logic should reduce write amplification to a large extent... will publish numbers shortly. Test Plan: dbstress runs for 6 hours with no data corruption (tested so far). Differential Revision: cleanup EnvOptions Summary: This diff simplifies EnvOptions by treating it as POD, similar to Options. virtual functions are removed and member fields are accessed directly. StorageOptions is removed. Options.allow_readahead and Options.allow_readahead_compactions are deprecated. Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite Test Plan: make check; db_stress Reviewers: dhruba CC: leveldb Differential Revision: valgrind errors/Very basic Multiget and simple test cases. Summary: Implemented the MultiGet operator which takes in a list of keys and returns their associated values. Currently uses std::vector as its container data structure. Otherwise, it works identically to ""Get"". Test Plan: 1. make db_test ; compile it 2. ./db_test ; test it 3. make all check ; regress / run all tests 4. make release ; (optional) compile with release settings Reviewers: haobo, MarkCallaghan, dhruba Reviewed By: dhruba CC: leveldb Differential Revision:"
1020,1020,5.0,0.9472000002861023,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Change namespace from leveldb to rocksdb Summary: Change namespace from leveldb to rocksdb. This allows a single application to link in open-source leveldb code as well as rocksdb code into the same process. Test Plan: compile rocksdb Reviewers: emayanke Reviewed By: emayanke CC: leveldb Differential Revision:
1021,1021,7.0,0.5393999814987183,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Implement a compressed block cache. Summary: Rocksdb can now support a uncompressed block cache, or a compressed block cache or both. Lookups first look for a block in the uncompressed cache, if it is not found only then it is looked up in the compressed cache. If it is found in the compressed cache, then it is uncompressed and inserted into the uncompressed cache. It is possible that the same block resides in the compressed cache as well as the uncompressed cache at the same time. Both caches have their own individual LRU policy. Test Plan: Unit test case attached. Reviewers: kailiu, sdong, haobo, leveldb Reviewed By: haobo CC: xjin, haobo Differential Revision: feature Summary: Create a new type of file on startup if it doesnt already exist called DBID. This will store a unique number generated from boost librarys uuid header file. The use-case is to identify the case of a db losing all its data and coming back up either empty or from an image(backup/live replicas recovery) the key point to note is that DBID is not stored in a backup or db snapshot Its preferable to use Boost for uuid because: 1) A non-standard way of generating uuid is not good 2) /proc/sys/kernel/random/uuid generates a uuid but only on linux environments and the solution would not be clean 3) c++ doesnt have any direct way to get a uuid 4) Boost is a very good library that was already having linkage in rocksdb from third-party Note: I had to update the TOOLCHAIN_REV in build files to get latest verison of boost from third-party as the older version had a bug. I had to put Wno-uninitialized in Makefile because boost-1.51 has an unitialized variable and rocksdb would not comiple otherwise. Latet open-source for boost is 1.54 but is not there in third-party. I have notified the concerned people in fbcode about it. : While releasing to third-party, an additional dependency will need to be created for boost in TARGETS file. I can help identify. Test Plan: Expand db_test to test 2 cases 1) Restarting db with Id file present verify that no change to Id 2)Restarting db with Id file deleted verify that a different Id is there after reopen Also run make all check Reviewers: dhruba, haobo, kailiu, sdong Reviewed By: dhruba CC: leveldb Differential Revision: class that can randomly read and write Summary: I have implemented basic simple use case that I need for External Value Store Im working on. There is a potential for making this prettier by refactoring/combining WritableFile and RandomAccessFile, avoiding some copypasta. However, I decided to implement just the basic functionality, so I can continue working on the other diff. Test Plan: Added a unittest Reviewers: dhruba, haobo, kailiu Reviewed By: haobo CC: leveldb Differential Revision: names of properties from leveldb prefix to rocksdb prefix. Summary: Migrate names of properties from leveldb prefix to rocksdb prefix. Test Plan: make check Reviewers: emayanke, haobo Reviewed By: haobo CC: leveldb Differential Revision: apis in the Environment to clear out pages in the OS cache. Summary: Added a new api to the Environment that allows clearing out not-needed pages from the OS cache. This will be helpful when the compressed block cache replaces the OS cache. Test Plan: EnvPosixTest.InvalidateCache Reviewers: haobo Reviewed By: haobo CC: leveldb Differential Revision: Enhance Env to support two thread pools LOW and HIGH Summary: this is the ground work for separating memtable flush jobs to their own thread pool. Both SetBackgroundThreads and Schedule take a third parameter Priority to indicate which thread pool they are working on. The names LOW and HIGH are just identifiers for two different thread pools, and does not indicate real difference in priority. We can set number of threads in the pools independently. The thread pool implementation is refactored. Test Plan: make check Reviewers: dhruba, emayanke CC: leveldb Differential Revision: Added nano second stopwatch and new perf counters to track block read cost Summary: The pupose of this diff is to expose per user-call level precise timing of block read, so that we can answer questions like: a Get() costs me 100ms, is that somehow related to loading blocks from file system, or sth else? We will answer that with EXACTLY how many blocks have been read, how much time was spent on transfering the bytes from os, how much time was spent on checksum verification and how much time was spent on block decompression, just for that one Get. A nano second stopwatch was introduced to track time with higher precision. The cost/precision of the stopwatch is also measured in unit-test. On my dev box, retrieving one time instance costs about 30ns, on average. The deviation of timing results is good enough to track 100ns-1us level events. And the overhead could be safely ignored for 100us level events (10000 instances/s), for example, a viewstate thrift call. Test Plan: perf_context_test, also testing with viewstate shadow traffic. Reviewers: dhruba Reviewed By: dhruba CC: leveldb, xjin Differential Revision:"
1022,1022,5.0,0.9843999743461609,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Fixing the warning messages captured under mac os Consider using `git commit One line title && arc diff`. You will save time by running lint and unit in the background. Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os.. Test Plan: ran make in mac os Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: namespace from leveldb to rocksdb Summary: Change namespace from leveldb to rocksdb. This allows a single application to link in open-source leveldb code as well as rocksdb code into the same process. Test Plan: compile rocksdb Reviewers: emayanke Reviewed By: emayanke CC: leveldb Differential Revision: for ttl Summary: value needed to be filtered of timestamp Test Plan: ./ttl_test Reviewers: dhruba, haobo, vamsi Reviewed By: dhruba CC: leveldb Differential Revision:"
1023,1023,5.0,0.864300012588501,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Minor fixes found while trying to compile it using clang on Mac OS X/
1024,1024,14.0,0.5582000017166138,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Merge operator fixes part 1. Summary: null checks and revisions to DBIter::MergeValuesNewToOld() DBIter test to stringappend_test fix with Merge and TTL More plans for fixes later. Test Plan: clean; make stringappend_test 32; ./stringappend_test all check; Reviewers: haobo, emayanke, vamsi, dhruba Reviewed By: haobo CC: leveldb Differential Revision:"
1025,1025,10.0,0.9854000210762024,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Merge operator fixes part 1. Summary: null checks and revisions to DBIter::MergeValuesNewToOld() DBIter test to stringappend_test fix with Merge and TTL More plans for fixes later. Test Plan: clean; make stringappend_test 32; ./stringappend_test all check; Reviewers: haobo, emayanke, vamsi, dhruba Reviewed By: haobo CC: leveldb Differential Revision: for Merge Operator Summary: Updated db_bench and utilities/merge_operators.h to allow for dynamic benchmarking of merge operators in db_bench. Added a new test (--benchmarks=mergerandom), which performs a bunch of random Merge() operations over random keys. Also added a ""--merge_operator="" flag so that the tester can easily benchmark different merge operators. Currently supports the PutOperator and UInt64Add operator. Support for stringappend or list append may come later. Test Plan: 1. make db_bench 2. Test the PutOperator (simulating Put) as follows: ./db_bench 3. Test the UInt64AddOperator (simulating numeric addition) similarly: ./db_bench Reviewers: haobo, dhruba, zshao, MarkCallaghan Reviewed By: haobo CC: leveldb Differential Revision:"
1026,1026,7.0,0.9850999712944031,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Introduced a new flag non_blocking_io in ReadOptions. Summary: If ReadOptions.non_blocking_io is set to true, then KeyMayExists and Iterators will return data that is cached in RAM. If the Iterator needs to do IO from storage to serve the data, then the Iterator.status() will return Status::IsRetry(). Test Plan: Enhanced unit test DBTest.KeyMayExist to detect if there were are IOs issues from storage. Added DBTest.NonBlockingIteration to verify nonblocking Iterations. Reviewers: emayanke, haobo Reviewed By: haobo CC: leveldb Maniphest Tasks: T63 Differential Revision: ""Prefix scan: db_bench and bug fixes"" This reverts commit c2bd8f4824bda98db8699f1e08d6969cf21ef86f./Prefix scan: db_bench and bug fixes Summary: If use_prefix_filters is set and read_range>1, then the random seeks will set a the prefix filter to be the prefix of the key which was randomly selected as the target. Still need to add statistics (perhaps in a separate diff). Test Plan: ./db_bench Reviewers: dhruba Reviewed By: dhruba CC: leveldb, haobo Differential Revision:"
1027,1027,7.0,0.9244999885559082,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Fix the string format issue Summary: mac and our dev server has totally differnt definition of uint64_t, therefore fixing the warning in mac has actually made code in linux uncompileable. Test Plan: make clean && make deleting files Summary: One more fix In some cases, our filenames start with ""/"". Apparently, env_ cant handle filenames with double // Test Plan: deletefile_test does not include this line in the LOG anymore: 2013/11/12-18:11:43.150149 7fe4a6fff700 RenameFile logfile FAILED IO error: /tmp/rocksdbtest-3574/deletefile_test//000003.log: No such file or directory Reviewers: dhruba, haobo Reviewed By: haobo CC: leveldb Differential Revision: the warning messages captured under mac os Consider using `git commit One line title && arc diff`. You will save time by running lint and unit in the background. Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os.. Test Plan: ran make in mac os Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: changes in Deleting obsolete files Summary: suggestions from Renaming some variables, deprecating purge_log_after_flush, changing for loop into auto for loop. I have not implemented deleting objects outside of mutex yet because it would require a big code change we would delete object in db_impl, which currently does not know anything about object because its defined in version_edit.h (FileMetaData). We should do it at some point, though. Test Plan: Ran deletefile_test Reviewers: haobo Reviewed By: haobo CC: leveldb, haobo Differential Revision: two FindObsoleteFiles() Summary: We dont need to call FindObsoleteFiles() twice Test Plan: deletefile_test Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: up FindObsoleteFiles Summary: Heres one solution we discussed on speeding up FindObsoleteFiles. Keep a set of all files in DBImpl and update the set every time we create a file. I probably missed few other spots where we create a file. It might speed things up a bit, but makes code uglier. I dont really like it. Much better approach would be to abstract all file handling to a separate class. Think of it as layer between DBImpl and Env. Having a separate class deal with file namings and deletion would benefit both code cleanliness (especially with huge DBImpl) and speed things up. It will take a huge effort to do this, though. Lets discuss offline today. Test Plan: Ran ./db_stress, verified that files are getting deleted Reviewers: dhruba, haobo, kailiu, emayanke Reviewed By: dhruba Differential Revision: the log outside of lock Summary: Added a new call LogFlush() that flushes the log contents to the OS buffers. We never call it with lock held. We call it once for every Read/Write and often in compaction/flush process so the frequency should not be a problem. Test Plan: db_test Reviewers: dhruba, haobo, kailiu, emayanke Reviewed By: dhruba CC: leveldb Differential Revision: Generalize prefix-aware iterator to be used for more than one Seek Summary: Added a prefix_seek flag in ReadOptions to indicate that Seek is prefix aware(might not return data with different prefix), and also not bound to a specific prefix. Multiple Seeks and range scans can be invoked on the same iterator. If a specific prefix is specified, this flag will be ignored. Just a quick prototype that works for PrefixHashRep, the new lockless memtable could be easily extended with this support too. Test Plan: test it on Leaf Reviewers: dhruba, kailiu, sdong, igor Reviewed By: igor CC: leveldb Differential Revision: log retention policy based on archive size. Summary: Archive cleaning will still happen every WAL_ttl seconds but archived logs will be deleted only if archive size is greater then a WAL_size_limit value. Empty archived logs will be deleted evety WAL_ttl. Test Plan: 1. Unit tests pass. 2. Benchmark. Reviewers: emayanke, dhruba, haobo, sdong, kailiu, igor Reviewed By: emayanke CC: leveldb Differential Revision: a compressed block cache. Summary: Rocksdb can now support a uncompressed block cache, or a compressed block cache or both. Lookups first look for a block in the uncompressed cache, if it is not found only then it is looked up in the compressed cache. If it is found in the compressed cache, then it is uncompressed and inserted into the uncompressed cache. It is possible that the same block resides in the compressed cache as well as the uncompressed cache at the same time. Both caches have their own individual LRU policy. Test Plan: Unit test case attached. Reviewers: kailiu, sdong, haobo, leveldb Reviewed By: haobo CC: xjin, haobo Differential Revision: Cleaning-up After D13521 Summary: This patch is to address comments on D13521: 1. rename Table to be TableReader and make its factory function to be GetTableReader 2. move the compression type selection logic out of TableBuilder but to compaction logic 3. more accurate comments 4. Move stat name constants into BlockBasedTable implementation. 5. remove some uncleaned codes in simple_table_db_test Test Plan: pass test suites. Reviewers: haobo, dhruba, kailiu Reviewed By: haobo CC: leveldb Differential Revision: a Put fails, fail all other puts Summary: When a Put fails, it can leave database in a messy state. We dont want to pretend that everything is OK when it may not be. We fail every write following the failed one. I added checks for corruption to DBImpl::Write(). Is there anywhere else I need to add them? Test Plan: Corruption unit test. Reviewers: dhruba, haobo, kailiu Reviewed By: dhruba CC: leveldb Differential Revision: DeleteFile and DeleteWalFiles Summary: This is to simplify rocksdb public APIs and improve the code quality. Created an additional parameter to ParseFileName for log sub type and improved the code for deleting a wal file. Wrote exhaustive unit-tests in delete_file_test Unification of other redundant APIs can be taken up in a separate diff Test Plan: Expanded delete_file test Reviewers: dhruba, haobo, kailiu, sdong Reviewed By: dhruba CC: leveldb Differential Revision: the log number bug when updating MANIFEST file Summary: Crash may occur during the flushes of more than two mem tables. As the info log suggested, even when both were successfully flushed, the recovery process still pick up one of the memtables log for recovery. This diff fix the problem by setting the correct ""log number"" in MANIFEST. Test Plan: make test; deployed to leaf4 and make sure it doesnt result in crashes of this type. Reviewers: haobo, dhruba CC: leveldb Differential Revision: feature Summary: Create a new type of file on startup if it doesnt already exist called DBID. This will store a unique number generated from boost librarys uuid header file. The use-case is to identify the case of a db losing all its data and coming back up either empty or from an image(backup/live replicas recovery) the key point to note is that DBID is not stored in a backup or db snapshot Its preferable to use Boost for uuid because: 1) A non-standard way of generating uuid is not good 2) /proc/sys/kernel/random/uuid generates a uuid but only on linux environments and the solution would not be clean 3) c++ doesnt have any direct way to get a uuid 4) Boost is a very good library that was already having linkage in rocksdb from third-party Note: I had to update the TOOLCHAIN_REV in build files to get latest verison of boost from third-party as the older version had a bug. I had to put Wno-uninitialized in Makefile because boost-1.51 has an unitialized variable and rocksdb would not comiple otherwise. Latet open-source for boost is 1.54 but is not there in third-party. I have notified the concerned people in fbcode about it. : While releasing to third-party, an additional dependency will need to be created for boost in TARGETS file. I can help identify. Test Plan: Expand db_test to test 2 cases 1) Restarting db with Id file present verify that no change to Id 2)Restarting db with Id file deleted verify that a different Id is there after reopen Also run make all check Reviewers: dhruba, haobo, kailiu, sdong Reviewed By: dhruba CC: leveldb Differential Revision: Compaction to Have a Size Percentage Threshold To Decide Whether to Compress Summary: This patch adds a option for universal compaction to allow us to only compress output files if the files compacted previously did not yet reach a specified ratio, to save CPU costs in some cases. Compression is always skipped for flushing. This is because the size information is not easy to evaluate for flushing case. We can improve it later. Test Plan: add test DBTest.UniversalCompactionCompressRatio1 and DBTest.UniversalCompactionCompressRatio12 Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision: background flush thread by default and fix issues related to it Summary: Enable background flush thread in this patch and fix unit tests with: (1) After background flush, schedule a background compaction if condition satisfied; (2) Fix a bug that if universal compaction is enabled and number of levels are set to be 0, compaction will not be automatically triggered (3) Fix unit tests to wait for compaction to finish instead of flush, before checking the compaction results. Test Plan: pass all unit tests Reviewers: haobo, xjin, dhruba Reviewed By: haobo CC: leveldb Differential Revision: Function names from Compaction->Flush When they really mean Flush Summary: When I debug the unit test failures when enabling background flush thread, I feel the function names can be made clearer for people to understand. Also, if the names are fixed, in many places, some tests bugs are obvious (and some of those tests are failing). This patch is to clean it up for future maintenance. Test Plan: Run test suites. Reviewers: haobo, dhruba, xjin Reviewed By: dhruba CC: leveldb Differential Revision: option for storing transaction logs in a separate dir Summary: In some cases, you might not want to store the data log (write ahead log) files in the same dir as the sst files. An example use case is leaf, which stores sst files in tmpfs. And would like to save the log files in a separate dir (disk) to save memory. Test Plan: make all. Ran db_test test. A few test failing. P2785018. If you guys dont see an obvious problem with the code, maybe somebody from the rocksdb team could help me debug the issue here. Running this on leaf worked well. I could see logs stored on disk, and deleted appropriately after compactions. Obviously this is only one set of options. The unit tests cover different options. Seems like Im missing some edge cases. Reviewers: dhruba, haobo, leveldb CC: xinyaohu, sumeet Differential Revision: names of properties from leveldb prefix to rocksdb prefix. Summary: Migrate names of properties from leveldb prefix to rocksdb prefix. Test Plan: make check Reviewers: emayanke, haobo Reviewed By: haobo CC: leveldb Differential Revision: Still honor DisableFileDeletions when purge_log_after_memtable_flush is on Summary: as title Test Plan: make check Reviewers: emayanke Reviewed By: emayanke CC: leveldb Differential Revision: Submit mem table flush job in a different thread pool Summary: As title. This is just a quick hack and not ready for commit. fails a lot of unit test. I will test/debug it directly in ViewState shadow . Test Plan: Try it in shadow test. Reviewers: dhruba, xjin CC: leveldb Differential Revision: Move last_sequence and last_flushed_sequence_ update back into lock protected area Summary: A previous diff moved these outside of lock protected area. Moved back in now. Also moved tmp_batch_ update outside of lock protected area, as only the single write thread can access it. Test Plan: make check Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: Remove Log file immediately after memtable flush Summary: As title. The DB log file life cycle is tied up with the memtable it backs. Once the memtable is flushed to sst and committed, we should be able to delete the log file, without holding the mutex. This is part of the bigger change to avoid FindObsoleteFiles at runtime. It deals with log files. sst files will be dealt with later. Test Plan: make check; db_bench Reviewers: dhruba CC: leveldb Differential Revision: pathname relative to db dir in LogFile and cleanup AppendSortedWalsOfType Summary: So that replication can just download from wherever LogFile.Pathname is pointing them. Test Plan: make all check;./db_repl_stress Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision: ldb command to convert compaction style Summary: Add new command ""change_compaction_style"" to ldb tool. For universal->level, it shows ""nothing to do"". For level->universal, it compacts all files into a single one and moves the file to level 0. Also add check for number of files at level 1+ when opening db with universal compaction style. Test Plan: make all check. New unit test for internal convertion function. Also manully test various cmd like: ./ldb change_compaction_style Reviewers: haobo, dhruba Reviewed By: haobo CC: vamsi, emayanke Differential Revision: build caused by DeleteFile not tolerating / at the beginning Summary: db->DeleteFile calls ParseFileName to check name that was returned for sst file. Now, sst filename is returned using TableFileName which uses MakeFileName. This puts a / at the front of the name and ParseFileName doesnt like that. Changed ParseFileName to tolerate /s at the beginning. The test delet_file_test used to pass earlier because this behaviour of MakeFileName had been changed a while back to not return a / during which delete_file_test was checked in. But MakeFileName had to be reverted to add / at the front because GetLiveFiles used at many places outside rocksdb used the previous behaviour of MakeFileName. Test Plan: make;./delete_filetest;make all check Reviewers: dhruba, haobo, vamsi Reviewed By: dhruba CC: leveldb Differential Revision: DeleteFile API Summary: The DeleteFile API was removing files inside the db-lock. This is now changed to remove files outside the db-lock. The GetLiveFilesMetadata() returns the smallest and largest seqnuence number of each file as well. Test Plan: deletefile_test Reviewers: emayanke, haobo Reviewed By: haobo CC: leveldb Maniphest Tasks: T63 Differential Revision: Fix TransformRepFactory related valgrind problem Summary: Let TransformRepFactory own the passed in transform. Also make it better encapsulated. Test Plan: make valgrind_check; Reviewers: dhruba, emayanke Reviewed By: emayanke CC: leveldb Differential Revision: a new flag non_blocking_io in ReadOptions. Summary: If ReadOptions.non_blocking_io is set to true, then KeyMayExists and Iterators will return data that is cached in RAM. If the Iterator needs to do IO from storage to serve the data, then the Iterator.status() will return Status::IsRetry(). Test Plan: Enhanced unit test DBTest.KeyMayExist to detect if there were are IOs issues from storage. Added DBTest.NonBlockingIteration to verify nonblocking Iterations. Reviewers: emayanke, haobo Reviewed By: haobo CC: leveldb Maniphest Tasks: T63 Differential Revision: move stats counting outside of mutex protected region for DB::Get() Summary: As title. This is possible as tickers are atomic now. db_bench on high qps in-memory muti-thread random get workload, showed ~5% throughput improvement. Test Plan: make check; db_bench; db_stress Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: three new MemTableReps Summary: This patch adds three new MemTableReps: UnsortedRep, PrefixHashRep, and VectorRep. UnsortedRep stores keys in an std::unordered_map of std::sets. When an iterator is requested, it dumps the keys into an std::set and iterates over that. VectorRep stores keys in an std::vector. When an iterator is requested, it creates a copy of the vector and sorts it using std::sort. The iterator accesses that new vector. PrefixHashRep stores keys in an unordered_map mapping prefixes to ordered sets. I also added one API change. I added a function MemTableRep::MarkImmutable. This function is called when the rep is added to the immutable list. It doesnt do anything yet, but it seems like that could be useful. In particular, for the vectorrep, it means we could elide the extra copy and just sort in place. The only reason I havent done that yet is because the use of the ArenaAllocator complicates things (I can elaborate on this if needed). Test Plan: make check ./db_stress ./db_stress ./db_stress Reviewers: dhruba, haobo, emayanke Reviewed By: dhruba CC: leveldb Differential Revision: from Summary: Pull Marks patch and slightly revise it. I revised another place in db_impl.cc with similar new formula. Test Plan: make all check. Also run ""time ./db_bench It has run for 20+ hours and hasnt finished. Looks good so far: Installed stack trace handler for SIGILL SIGSEGV SIGBUS SIGABRT LevelDB: version 2.0 Date: Tue Aug 20 23:11:55 2013 CPU: 32 * Intel(R) Xeon(R) CPU E5-2660 0 2.20GHz CPUCache: 20480 KB Keys: 16 bytes each Values: 100 bytes each (50 bytes after compression) Entries: 2500000000 RawSize: 276565.6 MB (estimated) FileSize: 157356.3 MB (estimated) Write rate limit: 0 Compression: snappy WARNING: Assertions are enabled; benchmarks unnecessarily slow DB path: [/tmp/leveldbtest-3088/dbbench] fillseq : 7202.000 micros/op 138 ops/sec; DB path: [/tmp/leveldbtest-3088/dbbench] fillsync : 7148.000 micros/op 139 ops/sec; (2500000 ops) DB path: [/tmp/leveldbtest-3088/dbbench] fillrandom : 7105.000 micros/op 140 ops/sec; DB path: [/tmp/leveldbtest-3088/dbbench] overwrite : 6930.000 micros/op 144 ops/sec; DB path: [/tmp/leveldbtest-3088/dbbench] readrandom : 1.020 micros/op 980507 ops/sec; (0 of 2500000000 found) DB path: [/tmp/leveldbtest-3088/dbbench] readrandom : 1.021 micros/op 979620 ops/sec; (0 of 2500000000 found) DB path: [/tmp/leveldbtest-3088/dbbench] readseq : 113.000 micros/op 8849 ops/sec; DB path: [/tmp/leveldbtest-3088/dbbench] readreverse : 102.000 micros/op 9803 ops/sec; DB path: [/tmp/leveldbtest-3088/dbbench] Created bg thread 0x7f0ac17f7700 compact : 111701.000 micros/op 8 ops/sec; DB path: [/tmp/leveldbtest-3088/dbbench] readrandom : 1.020 micros/op 980376 ops/sec; (0 of 2500000000 found) DB path: [/tmp/leveldbtest-3088/dbbench] readseq : 120.000 micros/op 8333 ops/sec; DB path: [/tmp/leveldbtest-3088/dbbench] readreverse : 29.000 micros/op 34482 ops/sec; DB path: [/tmp/leveldbtest-3088/dbbench] ... finished 618100000 ops Reviewers: MarkCallaghan, haobo, dhruba, chip Reviewed By: dhruba Differential Revision: APIs to query SST file metadata and to delete specific SST files Summary: An api to query the level, key ranges, size etc for each SST file and an api to delete a specific file from the db and all associated state in the bookkeeping datastructures. Notes: Editing the manifest version does not release the obsolete files right away. However deleting the file directly will mess up the iterator. We may need a more aggressive/timely file deletion api. I have used std::unique_ptr will switch to boost:: since this is external. thoughts? Unit test is fragile right now as it expects the compaction at certain levels. Test Plan: unittest Reviewers: dhruba, vamsi, emayanke CC: zshao, leveldb, haobo Task ID: Blame Rev:/API for getting archived log files Summary: Also expanded class LogFile to have startSequene and FileSize and exposed it publicly Test Plan: make all check Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision: statistic for sequence number and implement setTickerCount Summary: statistic for sequence number is needed by wormhole. setTickerCount is demanded for this statistic. I cant simply recordTick(max_sequence) when db recovers because the statistic iobject is owned by client and may/may not be reset during reopen. Eg. statistic is reset in mcrocksdb whereas it is not in db_stress. Therefore it is best to go with setTickerCount Test Plan: ./db_stress ... and observed expected sequence number Reviewers: dhruba CC: leveldb Differential Revision:"
1028,1028,5.0,0.9472000002861023,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Change namespace from leveldb to rocksdb Summary: Change namespace from leveldb to rocksdb. This allows a single application to link in open-source leveldb code as well as rocksdb code into the same process. Test Plan: compile rocksdb Reviewers: emayanke Reviewed By: emayanke CC: leveldb Differential Revision:
1029,1029,7.0,0.9664999842643738,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Add the index/filter block cache Summary: This diff leverage the existing block cache and extend it to cache index/filter block. Test Plan: Added new tests in db_test and table_test The correctness is checked by: 1. make check 2. make valgrind_check Performance is test by: 1. 10 times of build_tools/regression_build_test.sh on two versions of rocksdb before/after the code change. Test results suggests no significant difference between them. For the two key operatons `overwrite` and `readrandom`, the average iops are both 20k and ~260k, with very small variance). 2. db_stress. Reviewers: dhruba Reviewed By: dhruba CC: leveldb, haobo, xjin Differential Revision: the warning messages captured under mac os Consider using `git commit One line title && arc diff`. You will save time by running lint and unit in the background. Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os.. Test Plan: ran make in mac os Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: failure in rocksdb unit test CompressedCache Summary: The problem was that there was only a single key-value in a block and its compressibility was less than 88%. Rocksdb refuses to compress a block unless its compresses to lesser than 88% of its original size. If a block is not compressed, it does nto get inserted into the compressed block cache. Create the test data so that multiple records fit into the same data block. This increases the compressibility of these data block. Test Plan: ./db_test Reviewers: kailiu, haobo Reviewed By: kailiu CC: leveldb Differential Revision: valgrind error in DBTest.CompressedCache Summary: Fixed valgrind error in DBTest.CompressedCache. This fixes the valgrind error (thanks to Haobo). I am still trying to reproduce the test-failure case deterministically. Test Plan: db_test Reviewers: haobo Reviewed By: haobo CC: leveldb Differential Revision: the transaction log iterator more robust Summary: strict essentially means that we MUST find the startsequence. Thus we should return if starteSequence is not found in the first file in case strict is set. This will take care of ending the iterator in case of permanent gaps due to corruptions in the log files Also created NextImpl function that will have internal variable to distinguish whether Next is being called from StartSequence or by application. Set NotFoudn::gaps status to give an indication of gaps happeneing. Polished the inline documentation at various places Test Plan: * db_repl_stress test * db_test relating to transaction log iterator * fbcode/wormhole/rocksdb/rocks_log_iterator * sigma production machine sigmafio032.prn1 Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: a compressed block cache. Summary: Rocksdb can now support a uncompressed block cache, or a compressed block cache or both. Lookups first look for a block in the uncompressed cache, if it is not found only then it is looked up in the compressed cache. If it is found in the compressed cache, then it is uncompressed and inserted into the uncompressed cache. It is possible that the same block resides in the compressed cache as well as the uncompressed cache at the same time. Both caches have their own individual LRU policy. Test Plan: Unit test case attached. Reviewers: kailiu, sdong, haobo, leveldb Reviewed By: haobo CC: xjin, haobo Differential Revision: updates for equal keys and similar sized values Summary: Currently for each put, a fresh memory is allocated, and a new entry is added to the memtable with a new sequence number irrespective of whether the key already exists in the memtable. This diff is an attempt to update the value inplace for existing keys. It currently handles a very simple case: 1. Key already exists in the current memtable. Does not inplace update values in immutable memtable or snapshot 2. Latest value type is a put ie kTypeValue 3. New value size is less than existing value, to avoid reallocating memory TODO: For a put of an existing key, deallocate memory take by values, for other value types till a kTypeValue is found, ie. remove kTypeMerge. TODO: Update the transaction log, to allow consistent reload of the memtable. Test Plan: Added a unit test verifying the inplace update. But some other unit tests broken due to invalid sequence number checks. WIll fix them next. Reviewers: xinyaohu, sumeet, haobo, dhruba CC: leveldb Differential Revision: Automatic commit by arc/If a Put fails, fail all other puts Summary: When a Put fails, it can leave database in a messy state. We dont want to pretend that everything is OK when it may not be. We fail every write following the failed one. I added checks for corruption to DBImpl::Write(). Is there anywhere else I need to add them? Test Plan: Corruption unit test. Reviewers: dhruba, haobo, kailiu Reviewed By: dhruba CC: leveldb Differential Revision: DeleteFile and DeleteWalFiles Summary: This is to simplify rocksdb public APIs and improve the code quality. Created an additional parameter to ParseFileName for log sub type and improved the code for deleting a wal file. Wrote exhaustive unit-tests in delete_file_test Unification of other redundant APIs can be taken up in a separate diff Test Plan: Expanded delete_file test Reviewers: dhruba, haobo, kailiu, sdong Reviewed By: dhruba CC: leveldb Differential Revision: feature Summary: Create a new type of file on startup if it doesnt already exist called DBID. This will store a unique number generated from boost librarys uuid header file. The use-case is to identify the case of a db losing all its data and coming back up either empty or from an image(backup/live replicas recovery) the key point to note is that DBID is not stored in a backup or db snapshot Its preferable to use Boost for uuid because: 1) A non-standard way of generating uuid is not good 2) /proc/sys/kernel/random/uuid generates a uuid but only on linux environments and the solution would not be clean 3) c++ doesnt have any direct way to get a uuid 4) Boost is a very good library that was already having linkage in rocksdb from third-party Note: I had to update the TOOLCHAIN_REV in build files to get latest verison of boost from third-party as the older version had a bug. I had to put Wno-uninitialized in Makefile because boost-1.51 has an unitialized variable and rocksdb would not comiple otherwise. Latet open-source for boost is 1.54 but is not there in third-party. I have notified the concerned people in fbcode about it. : While releasing to third-party, an additional dependency will need to be created for boost in TARGETS file. I can help identify. Test Plan: Expand db_test to test 2 cases 1) Restarting db with Id file present verify that no change to Id 2)Restarting db with Id file deleted verify that a different Id is there after reopen Also run make all check Reviewers: dhruba, haobo, kailiu, sdong Reviewed By: dhruba CC: leveldb Differential Revision: Bug: iterator.Prev() or iterator.SeekToLast() might return the first element instead of the correct one Summary: Recent patch introduced a regression bug: DBIter::FindPrevUserEntry(), which is called by DBIter::Prev() (and also implicitly if calling iterator.SeekToLast()) might do issue a seek when having skipped too many entries. If the skipped entry just before the seek() is a delete, the saved key is erased so that it seeks to the front, so Prev() would return the first element. This patch fixes the bug by not doing seek() in DBIter::FindNextUserEntry() if saved key has been erased. Test Plan: Add a test DBTest.IterPrevMaxSkip which would fail without the patch and would pass with the change. Reviewers: dhruba, xjin, haobo Reviewed By: dhruba CC: leveldb Differential Revision: Compaction to Have a Size Percentage Threshold To Decide Whether to Compress Summary: This patch adds a option for universal compaction to allow us to only compress output files if the files compacted previously did not yet reach a specified ratio, to save CPU costs in some cases. Compression is always skipped for flushing. This is because the size information is not easy to evaluate for flushing case. We can improve it later. Test Plan: add test DBTest.UniversalCompactionCompressRatio1 and DBTest.UniversalCompactionCompressRatio12 Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision: Function names from Compaction->Flush When they really mean Flush Summary: When I debug the unit test failures when enabling background flush thread, I feel the function names can be made clearer for people to understand. Also, if the names are fixed, in many places, some tests bugs are obvious (and some of those tests are failing). This patch is to clean it up for future maintenance. Test Plan: Run test suites. Reviewers: haobo, dhruba, xjin Reviewed By: dhruba CC: leveldb Differential Revision: option for storing transaction logs in a separate dir Summary: In some cases, you might not want to store the data log (write ahead log) files in the same dir as the sst files. An example use case is leaf, which stores sst files in tmpfs. And would like to save the log files in a separate dir (disk) to save memory. Test Plan: make all. Ran db_test test. A few test failing. P2785018. If you guys dont see an obvious problem with the code, maybe somebody from the rocksdb team could help me debug the issue here. Running this on leaf worked well. I could see logs stored on disk, and deleted appropriately after compactions. Obviously this is only one set of options. The unit tests cover different options. Seems like Im missing some edge cases. Reviewers: dhruba, haobo, leveldb CC: xinyaohu, sumeet Differential Revision: db_test more robust Summary: While working on D13239, I noticed that the same options are not used for opening and destroying at db. So adding that. Also added asserts for successful DestroyDB calls. Test Plan: Ran unit tests. Atleast 1 unit test is failing. They failures are a result of some past logic change. Im not really planning to fix those. But I would like to check this in. And hopefully the respective unit test owners can fix the broken tests Reviewers: leveldb, haobo CC: xinyaohu, sumeet, dhruba Differential Revision: test failure in DBTest.NumImmutableMemTable. Summary: Previous patch introduced a unit test failure in DBTest.NumImmutableMemTable because of change in property names. Test Plan: Reviewers: CC: Task ID: Blame Rev:/Migrate names of properties from leveldb prefix to rocksdb prefix. Summary: Migrate names of properties from leveldb prefix to rocksdb prefix. Test Plan: make check Reviewers: emayanke, haobo Reviewed By: haobo CC: leveldb Differential Revision: Added a property ""leveldb.num-immutable-mem-table"" so that Flush can be called without blocking, and application still has a way to check when its done also without blocking. Summary: as title Test Plan: DBTest.NumImmutableMemTable Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: namespace from leveldb to rocksdb Summary: Change namespace from leveldb to rocksdb. This allows a single application to link in open-source leveldb code as well as rocksdb code into the same process. Test Plan: compile rocksdb Reviewers: emayanke Reviewed By: emayanke CC: leveldb Differential Revision: backward compatible option in GetLiveFiles to choose whether to not Flush first Summary: As explained in comments in GetLiveFiles in db.h, this option will cause flush to be skipped in GetLiveFiles because some use-cases use GetSortedWalFiles after GetLiveFiles to generate more complete snapshots. Using GetSortedWalFiles after GetLiveFiles allows us to not Flush in GetLiveFiles first because wals have everything. Note: file deletions will be disabled before calling GLF or GSWF so live logs will not move to archive logs or get delted. Note: Manifest file is truncated to a proper value in GLF, so it will always reply from the proper wal files on a restart Test Plan: make Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision: SIGSEGV issue in universal compaction Summary: We saw SIGSEGV when set options.num_levels=1 in universal compaction style. Dug into this issue for a while, and finally found the root cause (thank Haobo for discussion). Test Plan: Add new unit test. It throws SIGSEGV without this change. Also run ""make all check"". Reviewers: haobo, dhruba CC: leveldb Differential Revision: unit test for iterator with snapshot Summary: I played with the reported bug about iterator with snapshot: I turned the original test program ( into a new unit test, but I cannot reproduce the problem. Notice lines 31-34 in above link. I have ran the new test with and without such Put() operations. Both succeed. So this diff simply adds the test, without changing any source codes. Test Plan: run new test. Reviewers: dhruba, haobo, emayanke Reviewed By: dhruba CC: leveldb Differential Revision: vector rep implementation was segfaulting because of incorrect initialization of vector. Summary: The constructor for Vector memtable has a parameter called count that specifies the capacity of the vector to be reserved at allocation time. It was incorrectly used to initialize the size of the vector. Test Plan: Enhanced db_test. Reviewers: haobo, xjin, emayanke Reviewed By: haobo CC: leveldb Differential Revision: Fix DBTest.UniversalCompactionSizeAmplification too Summary: as title Test Plan: make db_test; ./db_test Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: Fix DBTest.UniversalCompactionTrigger to reflect the correct compaction trigger condition. Summary: as title Test Plan: make db_test; ./db_test Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: a parameter to limit the maximum space amplification for universal compaction. Summary: Added a new field called max_size_amplification_ratio in the CompactionOptionsUniversal structure. This determines the maximum percentage overhead of space amplification. The size amplification is defined to be the ratio between the size of the oldest file to the sum of the sizes of all other files. If the size amplification exceeds the specified value, then min_merge_width and max_merge_width are ignored and a full compaction of all files is done. A value of 10 means that the size a database that stores 100 bytes of user data could occupy 110 bytes of physical storage. Test Plan: Unit test DBTest.UniversalCompactionSpaceAmplification added. Reviewers: haobo, emayanke, xjin Reviewed By: haobo CC: leveldb Differential Revision: was hanging because the configured options specified that more than 1 memtable need to be merged. Summary: There is an config option called Options.min_write_buffer_number_to_merge that specifies the minimum number of write buffers to merge in memory before flushing to a file in L0. But in the the case when the db is being closed, we should not be using this config, instead we should flush whatever write buffers were available at that time. Test Plan: Unit test attached. Reviewers: haobo, emayanke Reviewed By: haobo CC: leveldb Differential Revision: iterator may automatically invoke reseeks. Summary: An iterator invokes reseek if the number of sequential skips over the same userkey exceeds a configured number. This makes iter->Next() faster (bacause of fewer key compares) if a large number of adjacent internal keys in a table (sst or memtable) have the same userkey. Test Plan: Unit test DBTest.IterReseek. Reviewers: emayanke, haobo, xjin Reviewed By: xjin CC: leveldb, xjin Differential Revision: ldb command to convert compaction style Summary: Add new command ""change_compaction_style"" to ldb tool. For universal->level, it shows ""nothing to do"". For level->universal, it compacts all files into a single one and moves the file to level 0. Also add check for number of files at level 1+ when opening db with universal compaction style. Test Plan: make all check. New unit test for internal convertion function. Also manully test various cmd like: ./ldb change_compaction_style Reviewers: haobo, dhruba Reviewed By: haobo CC: vamsi, emayanke Differential Revision: a new flag non_blocking_io in ReadOptions. Summary: If ReadOptions.non_blocking_io is set to true, then KeyMayExists and Iterators will return data that is cached in RAM. If the Iterator needs to do IO from storage to serve the data, then the Iterator.status() will return Status::IsRetry(). Test Plan: Enhanced unit test DBTest.KeyMayExist to detect if there were are IOs issues from storage. Added DBTest.NonBlockingIteration to verify nonblocking Iterations. Reviewers: emayanke, haobo Reviewed By: haobo CC: leveldb Maniphest Tasks: T63 Differential Revision: three new MemTableReps Summary: This patch adds three new MemTableReps: UnsortedRep, PrefixHashRep, and VectorRep. UnsortedRep stores keys in an std::unordered_map of std::sets. When an iterator is requested, it dumps the keys into an std::set and iterates over that. VectorRep stores keys in an std::vector. When an iterator is requested, it creates a copy of the vector and sorts it using std::sort. The iterator accesses that new vector. PrefixHashRep stores keys in an unordered_map mapping prefixes to ordered sets. I also added one API change. I added a function MemTableRep::MarkImmutable. This function is called when the rep is added to the immutable list. It doesnt do anything yet, but it seems like that could be useful. In particular, for the vectorrep, it means we could elide the extra copy and just sort in place. The only reason I havent done that yet is because the use of the ArenaAllocator complicates things (I can elaborate on this if needed). Test Plan: make check ./db_stress ./db_stress ./db_stress Reviewers: dhruba, haobo, emayanke Reviewed By: dhruba CC: leveldb Differential Revision: merge_oprator a shared_ptr; and added TTL unit tests Test Plan: make all check; make release; make stringappend_test; ./stringappend_test Reviewers: haobo, emayanke Reviewed By: haobo CC: leveldb, kailiu Differential Revision: for getting archived log files Summary: Also expanded class LogFile to have startSequene and FileSize and exposed it publicly Test Plan: make all check Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision:"
1030,1030,12.0,0.5612000226974487,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Fsync directory after we create a new file Summary: Im not sure where we need to sync the directory. I implemented the function in Env() and added the dir sync just after we close the newly created file in the builder. Should I also add FsyncDir() to new files that get created by a compaction? Test Plan: Confirmed that FsyncDir is returning Status::OK() Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb, dhruba Differential Revision:"
1031,1031,7.0,0.987500011920929,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Fsync directory after we create a new file Summary: Im not sure where we need to sync the directory. I implemented the function in Env() and added the dir sync just after we close the newly created file in the builder. Should I also add FsyncDir() to new files that get created by a compaction? Test Plan: Confirmed that FsyncDir is returning Status::OK() Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb, dhruba Differential Revision: Simple Plain Table Summary: A Simple plain table format. No block structure. When creating the table reader, scanning the full table to create indexes. Test Plan:Add unit test Reviewers:haobo,dhruba,kailiu CC: Task ID: Blame Rev:/make util/env_posix.cc work under mac Summary: This diff invoves some more complicated issues in the posix environment. Test Plan: works under mac os. will need to verify dev box. Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision:"
1032,1032,5.0,0.9894000291824341,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","[RocksDB] [Column Family] Interface proposal Summary: diff is for Column Family branch> Sharing some of the work Ive done so far. This diff compiles and passes the tests. The biggest change is in options.h I broke down Options into two parts DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all. Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility. There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now I think thats what we agreed on] Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families. Please provide feedback. Test Plan: make check works, the code is backward compatible Reviewers: dhruba, haobo, sdong, kailiu, emayanke CC: leveldb Differential Revision: DBWithTTL more like StackableDB Summary: Now DBWithTTL takes DB* and can behave more like StackableDB. This saves us a lot of duplicate work by defining interfaces Test Plan: ttl_test with ASAN OK Reviewers: emayanke Reviewed By: emayanke CC: leveldb Differential Revision:"
1033,1033,5.0,0.7688999772071838,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte. Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases. Test Plan: make all check Reviewers: haobo, dhruba, kailiu Reviewed By: haobo CC: igor, leveldb Differential Revision: phase API clean up Summary: Addressed all the issues in Now most table-related modules are hidden from user land. Test Plan: make check Reviewers: sdong, haobo, dhruba CC: leveldb Differential Revision: [Performance Branch] Some Changes to PlainTable format Summary: Some changes to PlainTable format: (1) support variable key length (2) use user defined slice transformer to extract prefixes (3) Run some test cases against PlainTable in db_test and table_test Test Plan: test db_test Reviewers: haobo, kailiu CC: dhruba, igor, leveldb, nkg- Differential Revision: Simple Plain Table Summary: A Simple plain table format. No block structure. When creating the table reader, scanning the full table to create indexes. Test Plan:Add unit test Reviewers:haobo,dhruba,kailiu CC: Task ID: Blame Rev:/Add an option to table_reader_bench to access the table from DB And Iterating non-existing prefix case. Summary: This patch adds an option to table_reader_bench that queries run against DB level (which has one table). It is useful if user wants to see the extra costs DB level introduces. Test Plan: Run the benchmark with and without the new parameter Reviewers: haobo, dhruba, kailiu Reviewed By: kailiu CC: leveldb Differential Revision:"
1034,1034,13.0,0.6881999969482422,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Make table properties shareable Summary: We are going to expose properties of all tables to end users through ""some"" db interface. However, current design doesnt naturally fit for this need, which is because: 1. If a table presents in table cache, we cannot simply return the reference to its table properties, because the table may be destroy after compaction (and we dont want to hold the ref of the version). 2. Copy table properties is OK, but its slow. Thus in this diff, I change the table readers interface to return a shared pointer (for const table properties), instead a const refernce. Test Plan: `make check` passed Reviewers: haobo, sdong, dhruba Reviewed By: haobo CC: leveldb Differential Revision: Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte. Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases. Test Plan: make all check Reviewers: haobo, dhruba, kailiu Reviewed By: haobo CC: igor, leveldb Differential Revision: disable caching index/filter blocks Summary: Mixing index/filter blocks with data blocks resulted in some known issues. To make sure in next release our users wont be affected, we added a new option in BlockBasedTableFactory::TableOption to conceal this functionality for now. This patch also introduced a BlockBasedTableReader::OpenOptions, which avoids the ""infinite"" growth of parameters in BlockBasedTableReader::Open(). Test Plan: make check Reviewers: haobo, sdong, igor, dhruba Reviewed By: igor CC: leveldb, tnovak Differential Revision: the property block for the plain table Summary: This is the last diff that adds the property block to plain table. The format resembles that of the block-based table: [data block] [meta block 1: stats block] [meta block 2: future extended block] ... [meta block K: future extended block] (we may add more meta blocks in the future) [metaindex block] [index block: we only have the placeholder here, we can add persistent index block in the future] [Footer: contains magic number, handle to metaindex block and index block] Test Plan: extended existing property block test. Reviewers: haobo, sdong, dhruba CC: leveldb Differential Revision: rid of some shared_ptrs Summary: I went through all remaining shared_ptrs and removed the ones that I found not-necessary. Only GenerateCachePrefix() is called fairly often, so dont expect much perf wins. The ones that are left are accessed infrequently and I think were fine with keeping them. Test Plan: make asan_check Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision: Use raw pointer instead of shared pointer when passing Statistics object internally Summary: liveness of the statistics object is already ensured by the shared pointer in DB options. Theres no reason to pass again shared pointer among internal functions. Raw pointer is sufficient and efficient. Test Plan: make check Reviewers: dhruba, MarkCallaghan, igor Reviewed By: dhruba CC: leveldb, reconnect.grayhat Differential Revision: the ""table stats"" Summary: The primary motivation of the changes is to make it easier to figure out the inside of the tables. * rename ""table stats"" to ""table properties"" since now we have more than ""integers"" to store in the property block. * Add filter block size to the basic table properties. * Whenever a table is built, well log the table properties (the sample output is in Test Plan). * Make an api to expose deleted keys. Test Plan: Passed all existing test. and the sample output of table stats: Basic Properties data blocks: 1 entries: 1 raw key size: 9 raw average key size: 9 raw value size: 9 raw average value size: 0 data block size: 25 index block size: 27 filter block size: 18 (estimated) table size: 70 filter policy: rocksdb.BuiltinBloomFilter User collected properties: InternalKeyPropertiesCollector kDeletedKeys: 1 Reviewers: dhruba, haobo Reviewed By: dhruba CC: leveldb Differential Revision: bloom filters Summary: broke bloom filters. If filter is not in cache, we want to return true (safe thing). Am I right? Test Plan: when benchmarking I got different results when using bloom filters vs. when not using them. This fixed the issue. I will also be putting this change to the other diff, but that one will probably be in review for longer time. Reviewers: kailiu, dhruba, haobo Reviewed By: kailiu CC: leveldb Differential Revision:"
1035,1035,11.0,0.975600004196167,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Dont always compress L0 files written by memtable flush Summary: Code was always compressing L0 files written by a memtable flush when compression was enabled. Now this is done when min_level_to_compress=0 for leveled compaction and when universal_compaction_size_percent=-1 for universal compaction. Task ID: Blame Rev: Test Plan: ran db_bench with compression options Revert Plan: Database Impact: Memcache Impact: Other Notes: EImportant: begin *PUBLIC* platform impact section Bugzilla: end platform impact Reviewers: dhruba, igor, sdong Reviewed By: dhruba CC: leveldb Differential Revision:"
1036,1036,12.0,0.9908000230789185,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Fix UnmarkEOF for partial blocks Summary: Blocks in the transaction log are a fixed size, but the last block in the transaction log file is usually a partial block. When a new record is added after the reader hit the end of the file, a new physical record will be appended to the last block. ReadPhysicalRecord can only read full blocks and assumes that the file position indicator is aligned to the start of a block. If the reader is forced to read further by simply clearing the EOF flag, ReadPhysicalRecord will read a full block starting from somewhere in the middle of a real block, causing it to lose alignment and to have a partial physical record at the end of the read buffer. This will result in length mismatches and checksum failures. When the log file is tailed for replication this will cause the log iterator to become invalid, necessitating the creation of a new iterator which will have to read the log file from scratch. This diff fixes this issue by reading the remaining portion of the last block we read from. This is done when the reader is forced to read further (UnmarkEOF is called). Test Plan: Added unit tests Stress test (with replication). Check dbdir/LOG file for corruptions. Test on test tier Reviewers: emayanke, haobo, dhruba Reviewed By: haobo CC: vamsi, sheki, dhruba, kailiu, igor Differential Revision:"
1037,1037,7.0,0.4666999876499176,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[CF] Rethinking ColumnFamilyHandle and fix to dropping column families Summary: The change to the public behavior: * When opening a DB or creating new column family client gets a ColumnFamilyHandle. * As long as column family handle is alive, client can do whatever he wants with it, even drop it * Dropped column family can still be read from (using the column family handle) * Added a new call CloseColumnFamily(). Client has to close all column families that he has opened before deleting the DB * As soon as column family is closed, any calls to DB using that column family handle will fail (also any outstanding calls) Internally: * Ref-counting ColumnFamilyData * New thread-safety for ColumnFamilySet * Dropped column families are now completely dropped and their memory cleaned-up Test Plan: added some tests to column_family_test Reviewers: dhruba, haobo, kailiu, sdong CC: leveldb Differential Revision: table handle on Recover() when max_open_files Summary: This covers existing table files before DB open happens and avoids contention on table cache Test Plan: db_test Reviewers: haobo, sdong, igor, dhruba Reviewed By: haobo CC: leveldb Differential Revision: for LZ4 compression./use super_version in NewIterator() and MultiGet() function Summary: Use super_version insider NewIterator to avoid Ref() each component separately under mutex The new added bench shows NewIterator QPS increases from 515K to 719K No meaningful improvement for multiget I guess due to its relatively small cost comparing to 90 keys fetch in the test. Test Plan: unit test and db_bench Reviewers: igor, sdong Reviewed By: igor CC: leveldb, dhruba Differential Revision: cleanup Summary: Removed icmp_ from VersionSet (since its per-column-family, not per-DB-instance) Unfriended VersionSet and ColumnFamilyData (yay) Removed VersionSet::NumberLevels() Cleaned up DBImpl Test Plan: make check Reviewers: dhruba, haobo, kailiu CC: leveldb Differential Revision: to take ColumnFamilyData Summary: This removes the default implementation of LogAndApply that applied the changed to the default column family by default. It is mostly simple reformatting. Test Plan: make check Reviewers: dhruba, kailiu CC: leveldb Differential Revision: branch master into columnfamilies Conflicts: db/db_impl.cc db/db_impl.h db/db_impl_readonly.h db/db_test.cc include/rocksdb/db.h include/utilities/stackable_db.h/Tailing iterator Summary: This diff implements a special type of iterator that doesnt create a snapshot (can be used to read newly inserted data) and is optimized for doing sequential reads. TailingIterator uses current superversion number to determine whether to invalidate its internal iterators. If the version hasnt changed, it can often avoid doing expensive seeks over immutable structures (sst files and immutable memtables). Test Plan: * new unit tests * running LD with this patch Reviewers: igor, dhruba, haobo, sdong, kailiu Reviewed By: sdong CC: leveldb, lovro, march Differential Revision: Summary: I created a separate class ColumnFamilySet to keep track of column families. Before we did this in VersionSet and I believe this approach is cleaner. Let me know if you have any comments. I will commit tomorrow. Test Plan: make check Reviewers: dhruba, haobo, kailiu, sdong CC: leveldb Differential Revision: Recover() code Summary: This diff does two things: * Rethinks how we call Recover() with read_only option. Before, we call it with pointer to memtable where wed like to apply those changes to. This memtable is set in db_impl_readonly.cc and its actually DBImpl::mem_. Why dont we just apply updates to mem_ right away? It seems more intuitive. * Changes when we apply updates to manifest. Before, the process is to recover all the logs, flush it to sst files and then do one giant commit that atomically adds all recovered sst files and sets the next log number. This works good enough, but causes some small troubles for my column family approach, since I cant have one VersionEdit apply to more than single column family[1]. The change here is to commit the files recovered from logs right away. Here is the state of the world before the change: 1. Recover log 5, add new sst files to edit 2. Recover log 7, add new sst files to edit 3. Recover log 8, add new sst files to edit 4. Commit all added sst files to manifest and mark log files 5, 7 and 8 as recoverd (via SetLogNumber(9) function) After the change, well do: 1. Recover log 5, commit the new sst files and set log 5 as recovered 2. Recover log 7, commit the new sst files and set log 7 as recovered 3. Recover log 8, commit the new sst files and set log 8 as recovered The added (small) benefit is that if we fail after (2), the new recovery will only have to recover log 8. In previous case, well have to restart the recovery from the beginning. The bigger benefit will be to enable easier integration of multiple column families in Recovery code path. [1] Im happy to dicuss this decison, but I believe this is the cleanest way to go. It also makes backward compatibility much easier. We dont have a requirement of adding multiple column families atomically. Test Plan: make check Reviewers: dhruba, haobo, kailiu, sdong Reviewed By: kailiu CC: leveldb Differential Revision: initial implementation of kCompactionStopStyleSimilarSize for universal compaction/Allow callback to change size of existing value. Change return type of the callback function to an enum status to handle 3 cases. Summary: This diff fixes 2 hacks: * The callback function can modify the existing value inplace, if the merged value fits within the existing buffer size. But currently the existing buffer size is not being modified. Now the callback recieves a int* allowing the size to be modified. Since size is encoded as a varint in the internal key for memtable. It might happen that the entire value might have be copied to the new location if the new size varint is smaller than the existing size varint. * The callback function has 3 functionalities 1. Modify existing buffer inplace, and update size correspondingly. Now to indicate that, Returns 1. 2. Generate a new buffer indicating merged value. Returns 2. 3. Fails to do either of above, based on whatever application logic. Returns 0. Test Plan: Just make all for now. Im adding another unit test to test each scenario. Reviewers: dhruba, haobo Reviewed By: haobo CC: leveldb, sdong, kailiu, xinyaohu, sumeet, danguo Differential Revision: CompactRange to apply filter to every key Summary: When doing CompactRange(), we should first flush the memtable and then calculate max_level_with_files. Also, we want to compact all the levels that have files, including level `max_level_with_files`. This patch fixed the unit test. Test Plan: Added a failing unit test and a fix, so its not failing anymore. Reviewers: dhruba, haobo, sdong Reviewed By: haobo CC: leveldb, xjin Differential Revision: not to take NumLevels() Summary: I will submit a sequence of diffs that are preparing master branch for column families. There are a lot of implicit assumptions in the code that are making column family implementation hard. If I make the change only in column family branch, it will make merging back to master impossible. Most of the diffs will be simple code refactorings, so I hope we can have fast turnaround time. Feel free to grab me in person to discuss any of them. This diff removes number of level check from VersionEdit. It is used only when VersionEdit is read, not written, but has to be set when it is written. I believe it is a right thing to make VersionEdit dumb and check consistency on the caller side. This will also make it much easier to implement Column Families, since different column families can have different number of levels. Test Plan: make check Reviewers: dhruba, haobo, sdong, kailiu Reviewed By: kailiu CC: leveldb Differential Revision: memcpy outside of lock Summary: When building batch group, dont actually build a new batch since it requires heavy-weight mem copy and malloc. Only store references to the batches and build the batch group without lock held. Test Plan: `make check` I am also planning to run performance tests. The workload that will benefit from this change is readwhilewriting. I will post the results once I have them. Reviewers: dhruba, haobo, kailiu Reviewed By: haobo CC: leveldb, xjin Differential Revision: read/modify/write functionality to Put() api Summary: The application can set a callback function, which is applied on the previous value. And calculates the new value. This new value can be set, either inplace, if the previous value existed in memtable, and new value is smaller than previous value. Otherwise the new value is added normally. Test Plan: fbmake. Added unit tests. All unit tests pass. Reviewers: dhruba, haobo Reviewed By: haobo CC: sdong, kailiu, xinyaohu, sumeet, leveldb Differential Revision: Branch] If options.max_open_files set to be cache table readers in FileMetadata for Get() and NewIterator() Summary: In some use cases, table readers for all live files should always be cached. In that case, there will be an opportunity to avoid the table cache look-up while Get() and NewIterator(). We define options.max_open_files to be the mode that table readers for live files will always be kept. In that mode, table readers are cached in FileMetaData (with a reference count hold in table cache). So that when executing table_cache.Get() and table_cache.newInterator(), LRU cache checking can be by-passed, to reduce latency. Test Plan: add a test case in db_test Reviewers: haobo, kailiu Reviewed By: haobo CC: dhruba, igor, leveldb Differential Revision: Branch] A Hashed Linked List Based Mem Table Summary: Implement a mem table, in which keys are hashed based on prefixes. In each bucket, entries are organized in a sorted linked list. It has the same thread safety guarantee as skip list. The motivation is to optimize memory usage for the case that prefix hashing is primary way of seeking to the entry. Compared to hash skip list implementation, this implementation is more memory efficient, but inside each bucket, search is always linear. The target scenario is that there are only very limited number of records in each hash bucket. Test Plan: Add a test case in db_test Reviewers: haobo, kailiu, dhruba Reviewed By: haobo CC: igor, nkg-, leveldb Differential Revision: the valgrind issues/Support multi-threaded DisableFileDeletions() and EnableFileDeletions() Summary: We dont want two threads to clash if they concurrently call DisableFileDeletions() and EnableFileDeletions(). Im adding a counter that will enable file deletions only after all DisableFileDeletions() calls have been negated with EnableFileDeletions(). However, we also dont want to break the old behavior, so I added a parameter force to EnableFileDeletions(). If force is true, we will still enable file deletions after every call to EnableFileDeletions(), which is what is happening now. Test Plan: make check Reviewers: dhruba, haobo, sanketh Reviewed By: dhruba CC: leveldb Differential Revision: branch master into performance Conflicts: db/db_impl.cc db/db_test.cc db/memtable.cc db/version_set.cc include/rocksdb/statistics.h/[RocksDB] [Performance Branch] Some Changes to PlainTable format Summary: Some changes to PlainTable format: (1) support variable key length (2) use user defined slice transformer to extract prefixes (3) Run some test cases against PlainTable in db_test and table_test Test Plan: test db_test Reviewers: haobo, kailiu CC: dhruba, igor, leveldb, nkg- Differential Revision: [Column Family] Interface proposal Summary: diff is for Column Family branch> Sharing some of the work Ive done so far. This diff compiles and passes the tests. The biggest change is in options.h I broke down Options into two parts DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all. Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility. There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now I think thats what we agreed on] Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families. Please provide feedback. Test Plan: make check works, the code is backward compatible Reviewers: dhruba, haobo, sdong, kailiu, emayanke CC: leveldb Differential Revision: BackupableDB Summary: In this diff I present you BackupableDB v1. You can easily use it to backup your DB and it will do incremental snapshots for you. Lets first describe how you would use BackupableDB. Its inheriting StackableDB interface so you can easily construct it with your DB object it will add a method RollTheSnapshot() to the DB object. When you call RollTheSnapshot(), current snapshot of the DB will be stored in the backup dir. To restore, you can just call RestoreDBFromBackup() on a BackupableDB (which is a static method) and it will restore all files from the backup dir. In the next version, it will even support automatic backuping every X minutes. There are multiple things you can configure: 1. backup_env and db_env can be different, which is awesome because then you can easily backup to HDFS or wherever you feel like. 2. sync if true, it *guarantees* backup consistency on machine reboot 3. number of snapshots to keep this will keep last N snapshots around if you want, for some reason, be able to restore from an earlier snapshot. All the backuping is done in incremental fashion if we already have 00010.sst, we will not copy it again. *IMPORTANT* This is based on assumption that 00010.sst never changes two files named 00010.sst from the same DB will always be exactly the same. Is this true? I always copy manifest, current and log files. 4. You can decide if you want to flush the memtables before you backup, or youre fine with backing up the log files either way, you get a complete and consistent view of the database at a time of backup. 5. More things you can find in BackupableDBOptions Here is the directory structure I use: backup_dir/CURRENT_SNAPSHOT just 4 bytes holding the latest snapshot 0, 1, 2, ... files containing serialized version of each snapshot containing a list of files files/*.sst sst files shared between snapshots if one snapshot references 00010.sst and another one needs to backup it from the DB, it will just reference the same file files/ 0/, 1/, 2/, ... snapshot directories containing private snapshot files current, manifest and log files All the files are ref counted and deleted immediatelly when they get out of scope. Some other stuff in this diff: 1. Added GetEnv() method to the DB. Discussed with and we agreed that it seems right thing to do. 2. Fixed StackableDB interface. The way it was set up before, I was not able to implement BackupableDB. Test Plan: I have a unittest, but please dont look at this yet. I just hacked it up to help me with debugging. I will write a lot of good tests and update the diff. Also, `make asan_check` Reviewers: dhruba, haobo, emayanke Reviewed By: dhruba CC: leveldb, haobo Differential Revision: GetDbIdentity pure virtual and also implement it for StackableDB, DBWithTTL Summary: As title Test Plan: make clean and make Reviewers: igor Reviewed By: igor CC: leveldb Differential Revision: Transform Rep Summary: Lets get rid of TransformRep and its children. We have confirmed that HashSkipListRep works better with multifeed, so there is no benefit to keeping this around. This diff is mostly just deleting references to obsoleted functions. I also have a diff for fbcode that well need to push when we switch to new release. I had to expose HashSkipListRepFactory in the client header files because db_impl.cc needs access to GetTransform() function for SanitizeOptions. Test Plan: make check Reviewers: dhruba, haobo, kailiu, sdong Reviewed By: dhruba CC: leveldb Differential Revision: do compression tests if we dont have compression libs Summary: These tests fail if compression libraries are not installed. Test Plan: Manually disabled snappy, observed tests not ran. Reviewers: dhruba, kailiu Reviewed By: dhruba CC: leveldb Differential Revision: Summary: We need access to options for BackupableDB Test Plan: make check Reviewers: dhruba Reviewed By: dhruba CC: leveldb, reconnect.grayhat Differential Revision: Interface changes required for BackupableDB Summary: This is part of smaller diff that is easier to review Test Plan: make asan_check Reviewers: dhruba, haobo, emayanke Reviewed By: emayanke CC: leveldb, kailiu, reconnect.grayhat Differential Revision: users to profile a query and see bottleneck of the query Summary: Provide a framework to profile a query in detail to figure out latency bottleneck. Currently, in Get(), Put() and iterators, 2-3 simple timing is used. We can easily add more profile counters to the framework later. Test Plan: Enable this profiling in seveal existing tests. Reviewers: haobo, dhruba, kailiu, emayanke, vamsi, igor CC: leveldb Differential Revision: Conflicts: table/merger.cc/Upgrading compiler to gcc4.8.1 Summary: Finally did it the trick was in using option. This is first step to running ASAN. All of our code seems to compile just fine on 4.8.1. However, I still left fbcode.471.sh in the build_tools/ just in case. Test Plan: make clean; make Reviewers: dhruba, haobo, kailiu, emayanke, sdong Reviewed By: dhruba CC: leveldb Differential Revision:"
1038,1038,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Rename leveldb to rocksdb in C api/
1039,1039,9.0,0.9872000217437744,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Enable log info with different levels. Summary: * Now each Log related function has a variant that takes an additional argument indicating its log level, which is one of the following: DEBUG, INFO, WARN, ERROR, FATAL. * To ensure backward-compatibility, old version Log functions are kept unchanged. * Logger now has a member variable indicating its log level. Any incoming Log request which log level is lower than Loggers log level will not be output. * The output of the newer version Log will be prefixed by its log level. Test Plan: Add a LogType test in auto_roll_logger_test.cc Sample log output 2014/02/11-00:03:07.683895 7feded179840 [DEBUG] this is the message to be written to the log file 2014/02/11-00:03:07.683898 7feded179840 [INFO] this is the message to be written to the log file 2014/02/11-00:03:07.683900 7feded179840 [WARN] this is the message to be written to the log file 2014/02/11-00:03:07.683903 7feded179840 [ERROR] this is the message to be written to the log file 2014/02/11-00:03:07.683906 7feded179840 [FATAL] this is the message to be written to the log file Reviewers: dhruba, xjin, kailiu Reviewed By: kailiu CC: leveldb Differential Revision:"
1040,1040,12.0,0.4153999984264374,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Retry FS system calls on EINTR Summary: EINTR means please retry. We dont do that currenty. We should. Test Plan: make check, although it doesnt really test the new code. well just have to believe in the code Reviewers: haobo, ljin Reviewed By: haobo CC: leveldb Differential Revision: fallocation Summary: Based on my recent findings (posted in our internal group), if we use fallocate without KEEP_SIZE flag, we get superior performance of fdatasync() in append-only workloads. This diff provides an option for user to not use KEEP_SIZE flag, thus optimizing his sync performance by up to 2x-3x. At one point we also just called posix_fallocate instead of fallocate, which isnt very fast: (tl;dr it manually writes out zero bytes to allocate storage). This diff also fixes that, by first calling fallocate and then posix_fallocate if fallocate is not supported. Test Plan: make check Reviewers: dhruba, sdong, haobo, ljin Reviewed By: dhruba CC: leveldb Differential Revision: to add a function to allow users to query waiting queue length Summary: Add a function to Env so that users can query the waiting queue length of each thread pool Test Plan: add a test in env_test Reviewers: haobo Reviewed By: haobo CC: dhruba, igor, yhchiang, ljin, nkg-, leveldb Differential Revision: unused space on PosixWritableFile::Close() Summary: Blocks allocated with fallocate will take extra space on disk even if they are unused and the file is close. Now we remove the extra blocks at the end of the file by calling `ftruncate`. Test Plan: added a test to env_test Reviewers: dhruba Reviewed By: dhruba CC: leveldb Differential Revision: local pointer storage Summary: This is not a generic thread local implementation in the sense that it only takes pointer. But it does support multiple instances per thread and lets user plugin function to perform cleanup when thread exits or an instance gets destroyed. Test Plan: unit test for now Reviewers: haobo, igor, sdong, dhruba Reviewed By: igor CC: leveldb, kailiu Differential Revision:"
1041,1041,7.0,0.8176000118255615,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Improve ttl_test Summary: Our valgrind tests are failing because ttl_test is kind of flakey. This diff should fix valgrind issue and make ttl_test less flakey and much faster. Instead of relying on Env::Default() for getting current time, I expose `Env*` to all TTL functions that are interested in time. That way, I can insert a custom test Env which is then used to provide exactly the times we need. That way, we dont need to sleep anymore we control the time. Test Plan: ttl_test in normal and valgrind run Reviewers: dhruba, haobo, sdong, yhchiang Reviewed By: haobo CC: leveldb Differential Revision: for column families in TTL DB Summary: This will enable people using TTL DB to do so with multiple column families. They can also specify different TTLs for each one. TODO: Implement CreateColumnFamily() in TTL world. Test Plan: Added a very simple sanity test. Reviewers: dhruba, haobo, ljin, sdong, yhchiang Reviewed By: haobo CC: leveldb, alberts Differential Revision: Filter V1 to use old context struct to keep backward compatible Summary: The previous change D15087 changed existing compaction filter, which makes the commonly used class not backward compatible. Revert the older interface. Use a new interface for V2 instead. Test Plan: make all check Reviewers: haobo, yhchiang, igor CC: danguo, dhruba, ljin, igor, leveldb Differential Revision:"
1042,1042,2.0,0.7759000062942505,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Add share_files_with_cheksum option to BackupEngine Summary: added a new option to BackupEngine: if share_files_with_checksum is set to true, sst files are stored in shared_checksum/ and are identified by the triple (file name, checksum, file size) instead of just the file name. This option is targeted at distributed databases that want to backup their primary replica. Test Plan: unit tests and tested backup and restore on a distributed rocksdb Reviewers: igor Reviewed By: igor Differential Revision: s/us fixes/Read-only BackupEngine Summary: Read-only BackupEngine can connect to the same backup directory that is already running BackupEngine. That enables some interesting use-cases (i.e. restoring replica from primarys backup directory) Test Plan: added a unit test Reviewers: dhruba, haobo, ljin Reviewed By: ljin CC: leveldb Differential Revision: limiter for BackupableDB Summary: Might be useful if client doesnt want to effect running system during backup too much. Test Plan: added a test case Reviewers: dhruba, haobo, ljin Reviewed By: haobo CC: leveldb Differential Revision: option in BackupableDB Summary: Added an option to BackupableDB implementation that allows users to persist in-memory databases. When the restore happens with keep_log_files true, it will *) Not delete existing log files in wal_dir *) Move log files from archive directory to wal_dir, so that DB can replay them if necessary Test Plan: Added an unit test Reviewers: dhruba, ljin Reviewed By: dhruba CC: leveldb Differential Revision: in FailOverwritingBackups/More bug fixed introduced by code cleanup/CloseDB in BackupableDBTest to make valgrind happy/Some fixes to BackupableDB Summary: (1) Report corruption if backup meta file has tailing data that was not read. This should fix: (also, reported similar issue) (2) Dont use OS buffer when copying file to backup directory. We dont need the file in cache since we wont be reading it twice (3) Dont delete newer backups when somebody tries to backup the diverged DB (restore from older backup, add new data, try to backup). Rather, just fail the new backup. Test Plan: backupable_db_test Reviewers: ljin, dhruba, sdong Reviewed By: ljin CC: leveldb, sdong Differential Revision:"
1043,1043,16.0,0.9814000129699707,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Enhance partial merge to support multiple arguments Summary: * PartialMerge api now takes a list of operands instead of two operands. * Add min_pertial_merge_operands to Options, indicating the minimum number of operands to trigger partial merge. * This diff is based on Schalks previous diff (D14601), but it also includes necessary changes such as updating the pure C api for partial merge. Test Plan: * make check all * develop tests for cases where partial merge takes more than two operands. TODOs (from Schalk): * Add test with min_partial_merge_operands > 2. * Perform benchmarks to measure the performance improvements (can probably use results of task * Add description of problem to doc/index.html. * Change wiki pages to reflect the interface changes. Reviewers: haobo, igor, vamsi Reviewed By: haobo CC: leveldb, dhruba Differential Revision:"
1044,1044,16.0,0.9814000129699707,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Enhance partial merge to support multiple arguments Summary: * PartialMerge api now takes a list of operands instead of two operands. * Add min_pertial_merge_operands to Options, indicating the minimum number of operands to trigger partial merge. * This diff is based on Schalks previous diff (D14601), but it also includes necessary changes such as updating the pure C api for partial merge. Test Plan: * make check all * develop tests for cases where partial merge takes more than two operands. TODOs (from Schalk): * Add test with min_partial_merge_operands > 2. * Perform benchmarks to measure the performance improvements (can probably use results of task * Add description of problem to doc/index.html. * Change wiki pages to reflect the interface changes. Reviewers: haobo, igor, vamsi Reviewed By: haobo CC: leveldb, dhruba Differential Revision:"
1045,1045,10.0,0.9743000268936157,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fix more gflag namespace issues/Benchmark table reader wiht nanoseconds Summary: nanosecnods gave us better view of the performance, especially when some operations are fast so that micro seconds may only reveal less informative results. Test Plan: sample output: ./table_reader_bench InMemoryTableSimpleBenchmark: PlainTable num_key1: 4096 num_key2: 512 non_empty Histogram (unit: nanosecond): Count: 6291456 Average: 475.3867 StdDev: 556.05 Min: 135.0000 Median: 400.1817 Max: 33370.0000 Percentiles: P50: 400.18 P75: 530.02 P99: 887.73 P99.9: 8843.26 P99.99: 9941.21 [ 120, 140 ) 2 0.000% 0.000% [ 140, 160 ) 452 0.007% 0.007% [ 160, 180 ) 13683 0.217% 0.225% [ 180, 200 ) 54353 0.864% 1.089% [ 200, 250 ) 101004 1.605% 2.694% [ 250, 300 ) 729791 11.600% 14.294% [ 300, 350 ) 616070 9.792% 24.086% [ 350, 400 ) 1628021 25.877% 49.963% [ 400, 450 ) 647220 10.287% 60.250% [ 450, 500 ) 577206 9.174% 69.424% [ 500, 600 ) 1168585 18.574% 87.999% [ 600, 700 ) 506875 8.057% 96.055% [ 700, 800 ) 147878 2.350% 98.406% [ 800, 900 ) 42633 0.678% 99.083% [ 900, 1000 ) 16304 0.259% 99.342% [ 1000, 1200 ) 7811 0.124% 99.466% [ 1200, 1400 ) 1453 0.023% 99.490% [ 1400, 1600 ) 307 0.005% 99.494% [ 1600, 1800 ) 81 0.001% 99.496% [ 1800, 2000 ) 18 0.000% 99.496% [ 2000, 2500 ) 8 0.000% 99.496% [ 2500, 3000 ) 6 0.000% 99.496% [ 3500, 4000 ) 3 0.000% 99.496% [ 4000, 4500 ) 116 0.002% 99.498% [ 4500, 5000 ) 1144 0.018% 99.516% [ 5000, 6000 ) 1087 0.017% 99.534% [ 6000, 7000 ) 2403 0.038% 99.572% [ 7000, 8000 ) 9840 0.156% 99.728% [ 8000, 9000 ) 12820 0.204% 99.932% [ 9000, 10000 ) 3881 0.062% 99.994% [ 10000, 12000 ) 135 0.002% 99.996% [ 12000, 14000 ) 159 0.003% 99.998% [ 14000, 16000 ) 58 0.001% 99.999% [ 16000, 18000 ) 30 0.000% 100.000% [ 18000, 20000 ) 14 0.000% 100.000% [ 20000, 25000 ) 2 0.000% 100.000% [ 25000, 30000 ) 2 0.000% 100.000% [ 30000, 35000 ) 1 0.000% 100.000% Reviewers: haobo, dhruba, sdong CC: leveldb Differential Revision: table_reader_bench and add it to ""make"" Summary: Fix table_reader_bench after some interface changes. Add it to make to avoid future breaking Test Plan: make table_reader_bench and run it with different options. Reviewers: kailiu, haobo Reviewed By: haobo CC: igor, leveldb Differential Revision:"
1046,1046,5.0,0.9039000272750854,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","xxHash Summary: Originally: Im taking over to apply some finishing touches Test Plan: will add tests Reviewers: dhruba, haobo, sdong, yhchiang, ljin Reviewed By: yhchiang CC: leveldb Differential Revision: PrefixMayMatch on Seek() Summary: As a follow-up diff for add optimization to check PrefixMayMatch on Seek() Test Plan: make all check Reviewers: igor, haobo, sdong, yhchiang, dhruba Reviewed By: haobo CC: leveldb Differential Revision: SIGFAULT when running sst_dump on v2.6 db Summary: Fix the sigfault when running sst_dump on v2.6 db. Test Plan: git checkout bba6595b1f3f42cf79bb21c2d5b981ede1cc0063 make clean make db_bench ./db_bench arc patch D18039 make clean make sst_dump ./sst_dump Reviewers: igor, haobo, sdong Reviewed By: sdong CC: leveldb Differential Revision: a different approach to make sure BlockBasedTableReader can use hash index on older files Summary: A recent commit makes sure hash index can be used when reading existing files. This patch uses another way to achieve the approach: (1) Currently, always writing kBinarySearch to files, despite of BlockBasedTableOptions.IndexType setting. (2) When reading a file, read out the field, and make sure it is kBinarySearch, while always use index type by users. The reason for doing it is, to reserve kHashSearch property on disk to future. If now we write out binary index for both of kHashSearch and kBinarySearch. We have to use a new flag in the future for hash index on disk, otherwise compatibility would break. Also, we want the real index type and type shown in properties block to be consistent. Test Plan: make all check Reviewers: haobo, kailiu Reviewed By: kailiu CC: igor, ljin, yhchiang, xjin, dhruba, leveldb Differential Revision: type doesnt have to be persisted Summary: With the recent changes, there is no need to check the property block about the index block type. If user want to use it, they dont really need any disk format change; everything happens in the fly. Also another team encountered an error while reading the index type from properties. Test Plan: ran all the tests Reviewers: sdong CC: Task ID: Blame Rev:/RocksDB 2.8 to be able to read files generated by 2.6 Summary: From 2.6 to 2.7, property block name is renamed from rocksdb.stats to rocksdb.properties. Older properties were not able to be loaded. In 2.8, we seem to have added some logic that uses property block without checking null pointers, which create segment faults. In this patch, we fix it by: (1) try rocksdb.stats if rocksdb.properties is not found (2) add some null checking before consuming rep->table_properties Test Plan: make sure a file generated in 2.7 couldnt be opened now can be opened. Reviewers: haobo, igor, yhchiang Reviewed By: igor CC: ljin, xjin, dhruba, kailiu, leveldb Differential Revision: hash index for block-based table Summary: Based on previous patches, this diff eventually provides the end-to-end mechanism for users to specify the hash-index. Test Plan: Wrote several new unit tests. Reviewers: sdong, haobo, dhruba Reviewed By: sdong CC: leveldb Differential Revision: the Create() function comform the convention Summary: Moved ""Return multiple values"" a more conventional way./Fix the memory leak in table index Summary: BinarySearchIndex didnt use unique_ptr to guard the block object nor delete it in destructor, leading to valgrind failure for ""definite memory leak"". Test Plan: re-ran the failed valgrind test cases/Fix the unit test failure in devbox Summary: My last diff was developed in MacOS but in devserver environment error occurs. I dug into the problem and found the way we calcuate approximate data size is pretty out-of-date. We can use table properties to get more accurate results. Test Plan: ran ./table_test and passed Reviewers: igor, dhruba, haobo, sdong CC: leveldb Differential Revision: inconsistent code format Summary: Found some function follows camel style. When naming funciton, we have two styles: Trivially expose internal data in readonly mode: `all_lower_case()` Regular function: `CapitalizeFirstLetter()` I renames these functions. Test Plan: make Reviewers: haobo, sdong, dhruba, igor CC: leveldb Differential Revision: putting filter block to block cache Summary: This bug caused server crash issues because the filter block is too big and kept purging out of cache. Test Plan: Wrote a new unit tests to make sure it works. Reviewers: dhruba, haobo, igor, sdong Reviewed By: haobo CC: leveldb Differential Revision:"
1047,1047,2.0,0.944100022315979,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB"""" And make the default 0 for hash linked list memtable This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./"
1048,1048,7.0,0.9824000000953674,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","disable the log_number check in Recover() Summary: There is a chance that an old MANIFEST is corrupted in 2.7 but just not noticed. This check would fail them. Change it to log instead of returning a Corruption status. Test Plan: make Reviewers: haobo, igor Reviewed By: igor CC: leveldb Differential Revision: bug in VersionEdit::DebugString()/Add MaxColumnFamily to VersionEdit::DebugString()/[CF] Dont reuse dropped column family IDs Summary: Column family IDs should be unique, even if column family is dropped. To achieve this, we save max column family in manifest. Note that the diff is still not ready. Im only using differential to move the patch to my Mac machine. Test Plan: added a test to column_family_test Reviewers: dhruba, haobo CC: leveldb Differential Revision:"
1049,1049,10.0,0.6787999868392944,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Compaction with zero outputs Summary: We had a hypothesis in that empty-string internal keys might have been caused by compaction filter deleting all the entries. I added a unit test for that case. Unforutnately, everything works as expected. Test Plan: this is a test Reviewers: dhruba, haobo, sdong Reviewed By: haobo CC: leveldb Differential Revision: Summary: Originally: Im taking over to apply some finishing touches Test Plan: will add tests Reviewers: dhruba, haobo, sdong, yhchiang, ljin Reviewed By: yhchiang CC: leveldb Differential Revision: a new mem-table representation based on cuckoo hash. Summary: Major Changes * Add a new mem-table representation, HashCuckooRep, which is based cuckoo hash. Cuckoo hash uses multiple hash functions. This allows each key to have multiple possible locations in the mem-table. Put: When insert a key, it will try to find whether one of its possible locations is vacant and store the key. If none of its possible locations are available, then it will kick out a victim key and store at that location. The kicked-out victim key will then be stored at a vacant space of its possible locations or kick-out another victim. In this diff, the kick-out path (known as cuckoo-path) is found using BFS, which guarantees to be the shortest. Get: Simply tries all possible locations of a key this guarantees worst-case constant time complexity. Time complexity: O(1) for Get, and average O(1) for Put if the fullness of the mem-table is below 80%. Default using two hash functions, the number of hash functions used by the cuckoo-hash may dynamically increase if it fails to find a short-enough kick-out path. Currently, HashCuckooRep does not support iteration and snapshots, as our current main purpose of this is to optimize point access. Minor Changes * Add IsSnapshotSupported() to DB to indicate whether the current DB supports snapshots. If it returns false, then DB::GetSnapshot() will always return nullptr. Test Plan: Run existing tests. Will develop a test specifically for cuckoo hash in the next diff. Reviewers: sdong, haobo Reviewed By: sdong CC: leveldb, dhruba, igor Differential Revision: result of ReadFirstRecord() Summary: ReadFirstRecord() reads the actual log file from disk on every call. This diff introduces a cache layer on top of ReadFirstRecord(), which should significantly speed up repeated calls to GetUpdatesSince(). I also cleaned up some stuff, but the whole TransactionLogIterator could use some refactoring, especially if we see increased usage. Test Plan: make check Reviewers: haobo, sdong, dhruba Reviewed By: haobo CC: leveldb Differential Revision: calling FindFile twice in TwoLevelIterator for PlainTable Summary: this is to reclaim the regression introduced in Test Plan: make all check Reviewers: igor, haobo, sdong, dhruba, yhchiang Reviewed By: haobo CC: leveldb Differential Revision: ReadOptions.prefix and .prefix_seek Summary: also add an override option total_order_iteration if you want to use full iterator with prefix_extractor Test Plan: make all check Reviewers: igor, haobo, sdong, yhchiang Reviewed By: haobo CC: leveldb, dhruba Differential Revision: ""Better port::Mutex::AssertHeld() and AssertNotHeld()"" This reverts commit ddafceb6c2ecb83b7bdf6711ea1c30d97aeb3b8f./Temporarily disable a test case in db_test Summary: Root cause is still under investigation. Just Disable the troubling use case for now./Enable hash index for block-based table Summary: Based on previous patches, this diff eventually provides the end-to-end mechanism for users to specify the hash-index. Test Plan: Wrote several new unit tests. Reviewers: sdong, haobo, dhruba Reviewed By: sdong CC: leveldb Differential Revision: on Summary: Compiling for iOS has by default turned on which causes rocksdb to fail compiling. This diff turns on in our compile options and cleans up all functions with missing prototypes. Test Plan: compiles Reviewers: dhruba, haobo, ljin, sdong Reviewed By: ljin CC: leveldb Differential Revision: family support for DB::OpenForReadOnly() Summary: When opening DB in read-only mode, client can choose to only specify a subset of column families (""default"" column family cant be omitted, though) Test Plan: added a unit test in column_family_test Reviewers: haobo, sdong, ljin, dhruba Reviewed By: haobo CC: leveldb Differential Revision: GetProperty() test Summary: GetProperty test is flakey. Before this diff: P8635927 After: P8635945 We need to make sure the thread is done before we destruct sleeping tasks. Otherwise, bad things happen. Test Plan: See summary Reviewers: ljin, sdong, haobo, dhruba Reviewed By: ljin CC: leveldb Differential Revision: Fix a race condition in GetSortedWalFiles Summary: This patch fixed a race condition where a log file is moved to archived dir in the middle of GetSortedWalFiles. Without the fix, the log file would be missed in the result, which leads to transaction log iterator gap. A test utility SyncPoint is added to help reproducing the race condition. Test Plan: TransactionLogIteratorRace; make check Reviewers: dhruba, ljin Reviewed By: dhruba CC: leveldb Differential Revision: to use static allocated char array for saved_key_ (if it is not too long) Summary: DBIter now uses a std::string for saved_key. Based on some profiling, it could be more expensive than we though. Optimize it with the same technique as LookupKey if it is short, we copy it to a static allocated char. Otherwise, dynamically allocate memory for it. Test Plan: make all check Reviewers: haobo, ljin Reviewed By: haobo CC: dhruba, igor, yhchiang, leveldb Differential Revision: default value of some Options Summary: Since we are optimizing for server workloads, some default values are not optimized any more. We change some of those values that I feel its less prone to regression bugs. Test Plan: make all check Reviewers: dhruba, haobo, ljin, igor, yhchiang Reviewed By: igor CC: leveldb, MarkCallaghan Differential Revision: make init prefix more robust Summary: Currently if client uses kNULLString as the prefix, it will confuse compaction filter v2. This diff added a bool to indicate if the prefix has been intialized. I also added a unit test to cover this case and make sure the new code path is hit. Test Plan: db_test Reviewers: igor, haobo Reviewed By: igor CC: leveldb Differential Revision: new CompactionFilterV2 API Summary: This diff adds a new CompactionFilterV2 API that roll up the decisions of kv pairs during compactions. These kv pairs must share the same key prefix. They are buffered inside the db. typedef std::vector<Slice> SliceVector; virtual std::vector<bool> Filter(int level, const SliceVector& keys, const SliceVector& existing_values, std::vector<std::string>* new_values, std::vector<bool>* values_changed ) const 0; Application can override the Filter() function to operate on the buffered kv pairs. More details in the inline documentation. Test Plan: make check. Added unit tests to make sure Keep, Delete, Change all works. Reviewers: haobo CCs: leveldb Differential Revision: a unit test to verify compaction filter context Summary: Add unit tests to make sure CompactionFilterContext::is_manual_compaction_ and CompactionFilterContext::is_full_compaction_ are set correctly. Test Plan: run the new tests. Reviewers: haobo, igor, dhruba, yhchiang, ljin Reviewed By: haobo CC: nkg-, leveldb Differential Revision: compile issue in Mac OS Summary: Compile issues are: * Unused variable env_ * Unused fallocate_with_keep_size_ Test Plan: compiles Reviewers: dhruba, haobo, sdong Reviewed By: dhruba CC: leveldb Differential Revision: a DB property to indicate number of background errors encountered Summary: Add a property to calculate number of background errors encountered to help users build their monitoring Test Plan: Add a unit test. make all check Reviewers: haobo, igor, dhruba Reviewed By: igor CC: ljin, nkg-, yhchiang, leveldb Differential Revision: easy-to-add properties related to compaction and flushes Summary: To partly address the request raised, add three easy-to-add properties to compactions and flushes. Test Plan: run unit tests and add a new unit test to cover new properties. Reviewers: haobo, dhruba Reviewed By: dhruba CC: nkg-, leveldb Differential Revision: a bug that Prev() can hang. Summary: Prev() now can hang when there is a key with more than max_skipped number of appearance internally but all of them are newer than the sequence ID to seek. Add unit tests to confirm the bug and fix it. Test Plan: make all check Reviewers: igor, haobo Reviewed By: igor CC: ljin, yhchiang, leveldb Differential Revision: WriteBatch interface/A heuristic way to check if a memtable is full Summary: This is is based on Its not finished but I would like to give a prototype to avoid arena over-allocation while making better use of the already allocated memory blocks. Instead of check approximate memtable size, we will take a deeper look at the arena, which incorporate essential idea that suggests: flush when arena has allocated its last and the last is ""almost full"" Test Plan: N/A Reviewers: haobo, sdong Reviewed By: sdong CC: leveldb, sdong Differential Revision: fixes introduced by code cleanup/Fix bad merge of D16791 and D16767 Summary: A bad Auto-Merge caused log buffer is flushed twice. Remove the unintended one. Test Plan: Should already be tested (the code looks the same as when I ran unit tests). Reviewers: haobo, igor Reviewed By: haobo CC: ljin, yhchiang, leveldb Differential Revision: SliceTransform object ownership Summary: (1) Fix SanitizeOptions() to also check HashLinkList. The current dynamic case just happens to work because the 2 classes have the same layout. (2) Do not delete SliceTransform object in HashSkipListFactory and HashLinkListFactory destructor. Reason: SanitizeOptions() enforces prefix_extractor and SliceTransform to be the same object when Hash**Factory is used. This makes the behavior strange: when Hash**Factory is used, prefix_extractor will be released by RocksDB. If other memtable factory is used, prefix_extractor should be released by user. Test Plan: db_bench && make asan_check Reviewers: haobo, igor, sdong Reviewed By: igor CC: leveldb, dhruba Differential Revision: assertion Summary: I wrote a test that triggers assertion in MergingIterator. I have not touched that code ever, so Im looking for somebody with good understanding of the MergingIterator code to fix this. The solution is probably a one-liner. Let me know if youre willing to take a look. Test Plan: This test fails with an assertion `use_heap_ false` Reviewers: dhruba, haobo, sdong, kailiu Reviewed By: sdong CC: leveldb Differential Revision: ReadOptions to TransactionLogIterator. Summary: Add an optional input parameter ReadOptions to DB::GetUpdateSince(), which allows the verification of checksums to be disabled by setting ReadOptions::verify_checksums to false. Test Plan: Tests are done off-line and will not be included in the regular unit test. Reviewers: igor Reviewed By: igor CC: leveldb, xjin, dhruba Differential Revision: Handle failure in WriteBatch::Handler Summary: * Add ColumnFamilyHandle::GetID() function. Client needs to know column familys ID to be able to construct WriteBatch * Handle WriteBatch::Handler failure gracefully. Since WriteBatch is not a very smart function (it takes raw CF id), client can add data to WriteBatch for column family that doesnt exist. In that case, we need to gracefully return failure status from DB::Write(). To do that, I added a return Status to WriteBatch functions PutCF, DeleteCF and MergeCF. Test Plan: Added test to column_family_test Reviewers: dhruba, haobo CC: leveldb Differential Revision: improvements to CompressedCache test/Fix table properties Summary: Adapt table properties to column family world Test Plan: make check Reviewers: kailiu CC: leveldb Differential Revision: DB test to run on non-default column family Summary: This is a huge diff and it was hectic, but the idea is actually quite simple. Every operation (Put, Get, etc.) done on default column family in DBTest is now forwarded to non-default (""pikachu""). The good news is that we had zero test failures Column families look stable so far. One interesting test that I adapted for column families is MultiThreadedTest. I replaced every Put() with a WriteBatch writing to all column families concurrently. Every Put in the write batch contains unique_id. Instead of Get() I do a multiget across all column families with the same key. If atomicity holds, I expect to see the same unique_id in all column families. Test Plan: This is a test Reviewers: dhruba, haobo, kailiu, sdong CC: leveldb Differential Revision: the table properties to application Summary: Provide a public API for users to access the table properties for each SSTable. Test Plan: Added a unit tests to test the function correctness under differnet conditions. Reviewers: haobo, dhruba, sdong Reviewed By: haobo CC: leveldb Differential Revision:"
1050,1050,13.0,0.9136000275611877,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[C-API] added the possiblity to create a HashSkipList or HashLinkedList to support prefix seeks/added a test case for custom merge operator/
1051,1051,13.0,0.9793000221252441,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","In tools/db_stress.cc, set proper value in NewHashSkipListRepFactorys bucket_size Summary: Now that the arena is used to allocate space for hashskiplists bucket. The bucket size need to be set small enough to avoid ""should_flush_"" failure in memtables assertion. Test Plan: make all check Reviewers: sdong Reviewed By: sdong Subscribers: igor Differential Revision: to add an option to periodically change background thread pool size. Summary: Add an option to indicates the variation of background threads. If the flag is not 0, for every 100 milliseconds, adjust thread pool size to a value within the range. Test Plan: run db_stress Reviewers: haobo, dhruba, igor, ljin Reviewed By: ljin Subscribers: leveldb, nkg- Differential Revision:"
1052,1052,12.0,0.7433000206947327,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer",hdfs cleanup; fix to NewDirectory to comply with definition in env.h fix compile error with env_test; static casts added/hdfs cleanup and compile test against CDH 4.4./
1053,1053,14.0,0.567300021648407,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Finer report I/O stats about Flush and Compaction. Summary: This diff allows the I/O stats about Flush and Compaction to be reported in a more accurate way. Instead of measuring the size of a file, it measure I/O cost in per read / write basis. Test Plan: make all check Reviewers: sdong, igor, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: 32-bit errors Summary: Test Plan: compiles Reviewers: sdong, ljin, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision: printed when Thread terminating in the same format as posix_logger Summary: correctly fixed the issue of thread ID printing when terminating a thread. Nothing wrong with it. This diff prints the ID in the same way as in PosixLogger::logv() so that users can be more easily to correlates them. Test Plan: run env_test and make sure it prints correctly. Reviewers: igor, haobo, ljin, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: pthread_t in a more safe way/"
1054,1054,13.0,0.949999988079071,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","[Java] Correct the library loading for zlib in RocksJava. Summary: Correct the library loading for zlib in RocksJava: zlib should be loaded by loadLibrary(""z"") instead of loadLibrary(""zlib""). Test Plan: make rocksdbjava cd java make db_bench ./jdb_bench.sh Reviewers: sdong, ljin, ankgup87 Reviewed By: ankgup87 Subscribers: leveldb Differential Revision: Enable compression_ratio option in DbBenchmark.java Summary: Enable the random values in Java DB Bench to be generated based on the compression_ratio specified in the command-line arguments. Test Plan: make rocksdbjava java/jdb_bench.sh Reviewers: sdong, ankgup87, haobo Reviewed By: haobo Subscribers: leveldb Differential Revision:"
1055,1055,12.0,0.9247000217437744,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","[Java] Generalize dis-own native handle and refine dispose framework. Summary: 1. Move disOwnNativeHandle() function from RocksDB to RocksObject to allow other RocksObject to use disOwnNativeHandle() when its ownership of native handle has been transferred. 2. RocksObject now has an abstract implementation of dispose(), which does the following two things. First, it checks whether both isOwningNativeHandle() and isInitialized() return true. If so, it will call the protected abstract function dispose0(), which all the subclasses of RocksObject should implement. Second, it sets nativeHandle_ 0. This redesign ensure all subclasses of RocksObject have the same dispose behavior. 3. All subclasses of RocksObject now should implement dispose0() instead of dispose(), and dispose0() will be called only when isInitialized() returns true. Test Plan: make rocksdbjava make jtest Reviewers: dhruba, sdong, ankgup87, rsumbaly, swapnilghike, zzbennett, haobo Reviewed By: haobo Subscribers: leveldb Differential Revision:"
1056,1056,12.0,0.9222000241279602,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","[Java] Generalize dis-own native handle and refine dispose framework. Summary: 1. Move disOwnNativeHandle() function from RocksDB to RocksObject to allow other RocksObject to use disOwnNativeHandle() when its ownership of native handle has been transferred. 2. RocksObject now has an abstract implementation of dispose(), which does the following two things. First, it checks whether both isOwningNativeHandle() and isInitialized() return true. If so, it will call the protected abstract function dispose0(), which all the subclasses of RocksObject should implement. Second, it sets nativeHandle_ 0. This redesign ensure all subclasses of RocksObject have the same dispose behavior. 3. All subclasses of RocksObject now should implement dispose0() instead of dispose(), and dispose0() will be called only when isInitialized() returns true. Test Plan: make rocksdbjava make jtest Reviewers: dhruba, sdong, ankgup87, rsumbaly, swapnilghike, zzbennett, haobo Reviewed By: haobo Subscribers: leveldb Differential Revision:"
1057,1057,3.0,0.8641999959945679,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Caching methodId and fieldId is fine/Class IDs and method IDs should not be cached/
1058,1058,12.0,0.9244999885559082,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","[Java] Generalize dis-own native handle and refine dispose framework. Summary: 1. Move disOwnNativeHandle() function from RocksDB to RocksObject to allow other RocksObject to use disOwnNativeHandle() when its ownership of native handle has been transferred. 2. RocksObject now has an abstract implementation of dispose(), which does the following two things. First, it checks whether both isOwningNativeHandle() and isInitialized() return true. If so, it will call the protected abstract function dispose0(), which all the subclasses of RocksObject should implement. Second, it sets nativeHandle_ 0. This redesign ensure all subclasses of RocksObject have the same dispose behavior. 3. All subclasses of RocksObject now should implement dispose0() instead of dispose(), and dispose0() will be called only when isInitialized() returns true. Test Plan: make rocksdbjava make jtest Reviewers: dhruba, sdong, ankgup87, rsumbaly, swapnilghike, zzbennett, haobo Reviewed By: haobo Subscribers: leveldb Differential Revision:"
1059,1059,10.0,0.9959999918937683,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add DB property ""rocksdb.estimate-table-readers-mem"" Summary: Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache. Refactor the property codes to allow getting property from a version, with DB mutex not acquired. Test Plan: Add several checks of this new property in existing codes for various cases. Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: xjin, igor, leveldb Differential Revision: malloc when create data and index iterator in Get Summary: Define Block::Iter to be an independent class to be used by block_based_table_reader When creating data and index iterator, update an existing iterator rather than new one Thus malloc and free could be reduced Benchmark, Base: commit 76286ee67ef4b89579a92134b996a681c36a1331 commands: disable_auto_compactions=1 malloc: 3.30% 1.42% free: 3.59%->1.61% Test Plan: make all check run db_stress valgrind ./db_test ./table_test Reviewers: ljin, yhchiang, dhruba, igor, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: seek compaction Summary: As discussed in our internal group, we dont get much use of seek compaction at the moment, while its making code more complicated and slower in some cases. This diff removes seek compaction and (hopefully) all code that was introduced to support seek compaction. There is one test case that relied on didIO information. Ill try to find another way to implement it. Test Plan: make check Reviewers: sdong, haobo, yhchiang, ljin, dhruba Reviewed By: ljin Subscribers: leveldb Differential Revision: Make block based table hash index more adaptive Summary: Currently, RocksDB returns error if a db written with prefix hash index, is later opened without providing a prefix extractor. This is uncessarily harsh. Without a prefix extractor, we could always fallback to the normal binary index. Test Plan: unit test, also manually veried LOG that fallback did occur. Reviewers: sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: Reduce memory footprint of the blockbased table hash index. Summary: Currently, the in-memory hash index of blockbased table uses a precise hash map to track the prefix to block range mapping. In some use cases, especially when prefix itself is big, the memory overhead becomes a problem. This diff introduces a fixed hash bucket array that does not store the prefix and allows prefix collision, which is similar to the plaintable hash index, in order to reduce the memory consumption. Just a quick draft, still testing and refining. Test Plan: unit test and shadow testing Reviewers: dhruba, kailiu, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision: the hash index Summary: Materialize the hash index to avoid the soaring cpu/flash usage when initializing the database. Test Plan: existing unit tests passed Reviewers: sdong, haobo Reviewed By: sdong CC: leveldb Differential Revision:"
1060,1060,17.0,0.8812000155448914,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Materialize the hash index Summary: Materialize the hash index to avoid the soaring cpu/flash usage when initializing the database. Test Plan: existing unit tests passed Reviewers: sdong, haobo Reviewed By: sdong CC: leveldb Differential Revision:"
1061,1061,17.0,0.8812000155448914,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Materialize the hash index Summary: Materialize the hash index to avoid the soaring cpu/flash usage when initializing the database. Test Plan: existing unit tests passed Reviewers: sdong, haobo Reviewed By: sdong CC: leveldb Differential Revision:"
1062,1062,14.0,0.6930000185966492,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","use stack instead of heap memory in ReadBlockContents in some case Summary: When compression is enabled, and blocksize is not too big, use the space in stack to hold bytes read from block. Bencmark: base version: commit 8f09d53fd11a7debe1e48b73a192de3a458d37bf malloc: 1.30% 0.98% free: 1.49% 1.07% Test Plan: make all check Reviewers: ljin, yhchiang, dhruba, igor, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision:"
1063,1063,5.0,0.9524999856948853,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","PlainTable to encode to avoid to rewrite prefix when it is the same as the previous key Summary: Add a encoding feature of PlainTable to encode PlainTables keys to save some bytes for the same prefixes. The data format is documented in table/plain_table_factory.h Test Plan: Add unit test coverage in plain_table_db_test Reviewers: yhchiang, igor, dhruba, ljin, haobo Reviewed By: haobo Subscribers: nkg-, leveldb Differential Revision:"
1064,1064,14.0,0.9728999733924866,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Remove seek compaction Summary: As discussed in our internal group, we dont get much use of seek compaction at the moment, while its making code more complicated and slower in some cases. This diff removes seek compaction and (hopefully) all code that was introduced to support seek compaction. There is one test case that relied on didIO information. Ill try to find another way to implement it. Test Plan: make check Reviewers: sdong, haobo, yhchiang, ljin, dhruba Reviewed By: ljin Subscribers: leveldb Differential Revision:"
1065,1065,7.0,0.6754999756813049,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","log db path info before open Summary: 1. write db MANIFEST, CURRENT, IDENTITY, sst files, log files to log before open Test Plan: run db and check LOG file Reviewers: ljin, yhchiang, igor, dhruba, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: only one column family Summary: Currently DBImpl::Flush() triggers flushes in all column families. Instead we need to trigger just the column family specified. Test Plan: make all check Reviewers: igor, ljin, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: candidate files under options.db_log_dir in FindObsoleteFiles() Summary: In FindObsoleteFiles(), we dont scan db_log_dir. Add it. Test Plan: make all check Reviewers: ljin, igor, yhchiang Reviewed By: yhchiang Subscribers: leveldb, yhchiang Differential Revision: to schedule compactions when manual compaction finishes Summary: If there is an outstanding compaction scheduled but at the time a manual compaction is triggered, the manual compaction will preempt. In the end of the manual compaction, we should try to schedule compactions to make sure those preempted ones are not skipped. Test Plan: make all check Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: leveldb, dhruba, igor Differential Revision: SIGSEGV in travis Summary: Travis build was failing a lot. For example see This fixes it. Also, please dont put any code after SignalAll :) Test Plan: no more SIGSEGV Reviewers: yhchiang, sdong, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: valgrind failure caused by recent checked-in. Summary: Initialize un-initialized parameters Test Plan: run the failed test (c_test) Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: DB property ""rocksdb.estimate-table-readers-mem"" Summary: Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache. Refactor the property codes to allow getting property from a version, with DB mutex not acquired. Test Plan: Add several checks of this new property in existing codes for various cases. Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: xjin, igor, leveldb Differential Revision: Summary: 1. logging when create and delete manifest file 2. fix formating in table/format.cc Test Plan: make all check run db_bench, track the LOG file. Reviewers: ljin, yhchiang, igor, yufei.zhu, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: CompactRange to level 0 in level compaction Summary: I was bit by this when developing SpatialDB. In case all files are at level 0, CompactRange() will output the compacted files to level 0. This is not ideal, since read amp. is much better at level 1 and higher. Test Plan: Compacted data in SpatialDB, read manifest using ldb, verified that files are now at level 1 instead of 0. Reviewers: sdong, ljin, yhchiang, dhruba Reviewed By: dhruba Subscribers: dhruba, leveldb Differential Revision: StopWatch interface Summary: So that we can avoid calling NowSecs() in MakeRoomForWrite twice Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: statistics forward-able Summary: Make StatisticsImpl being able to forward stats to provided statistics implementation. The main purpose is to allow us to collect internal stats in the future even when user supplies custom statistics implementation. It avoids intrumenting 2 sets of stats collection code. One immediate use case is tuning advisor, which needs to collect some internal stats, users may not be interested. Test Plan: ran db_bench and see stats show up at the end of run Will run make all check since some tests rely on statistics Reviewers: yhchiang, sdong, igor Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: user to specify DB path of output file of manual compaction Summary: Add a parameter path_id to DB::CompactRange(), to indicate where the output file should be placed to. Test Plan: add a unit test Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: xjin, igor, dhruba, MarkCallaghan, leveldb Differential Revision: internal stats independent of statistics Summary: also make it aware of column family output from db_bench ``` ** Compaction Stats [default] ** Level Files Size(MB) Score Read(GB) Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) RW-Amp W-Amp Rd(MB/s) Wr(MB/s) Rn(cnt) Rnp1(cnt) Wnp1(cnt) Wnew(cnt) Comp(sec) Comp(cnt) Avg(sec) Stall(sec) Stall(cnt) Avg(ms) L0 14 956 0.9 0.0 0.0 0.0 2.7 2.7 0.0 0.0 0.0 111.6 0 0 0 0 24 40 0.612 75.20 492387 0.15 L1 21 2001 2.0 5.7 2.0 3.7 5.3 1.6 5.4 2.6 71.2 65.7 31 43 55 12 82 2 41.242 43.72 41183 1.06 L2 217 18974 1.9 16.5 2.0 14.4 15.1 0.7 15.6 7.4 70.1 64.3 17 182 185 3 241 16 15.052 0.00 0 0.00 L3 1641 188245 1.8 9.1 1.1 8.0 8.5 0.5 15.4 7.4 61.3 57.2 9 75 76 1 152 9 16.887 0.00 0 0.00 L4 4447 449025 0.4 13.4 4.8 8.6 9.1 0.5 4.7 1.9 77.8 52.7 38 79 100 21 176 38 4.639 0.00 0 0.00 Sum 6340 659201 0.0 44.7 10.0 34.7 40.6 6.0 32.0 15.2 67.7 61.6 95 379 416 37 676 105 6.439 118.91 533570 0.22 Int 0 0 0.0 1.2 0.4 0.8 1.3 0.5 5.2 2.7 59.1 65.6 3 7 9 2 20 10 2.003 0.00 0 0.00 Stalls(secs): 75.197 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 43.717 leveln_slowdown Stalls(count): 492387 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 41183 leveln_slowdown ** DB Stats ** Uptime(secs): 202.1 total, 13.5 interval Cumulative writes: 6291456 writes, 6291456 batches, 1.0 writes per batch, 4.90 ingest GB Cumulative WAL: 6291456 writes, 6291456 syncs, 1.00 writes per sync, 4.90 GB written Interval writes: 1048576 writes, 1048576 batches, 1.0 writes per batch, 836.0 ingest MB Interval WAL: 1048576 writes, 1048576 syncs, 1.00 writes per sync, 0.82 MB written Test Plan: ran it Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: leveldb Differential Revision: multiple DB directories in universal compaction style Summary: This patch adds a target size parameter in options.db_paths and universal compaction will base it to determine which DB path to place a new file. Level-style stays the same. Test Plan: Add new unit tests Reviewers: ljin, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, dhruba, igor, leveldb Differential Revision: rate limiter into rocksdb Summary: Add option and plugin rate limiter for PosixWritableFile. The rate limiter only applies to flush and compaction. WAL and MANIFEST are excluded from this enforcement. Test Plan: db_test Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: report I/O stats about Flush and Compaction. Summary: This diff allows the I/O stats about Flush and Compaction to be reported in a more accurate way. Instead of measuring the size of a file, it measure I/O cost in per read / write basis. Test Plan: make all check Reviewers: sdong, igor, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: timeout_hint_us to WriteOptions and introduce Status::TimeOut. Summary: This diff adds timeout_hint_us to WriteOptions. If its non-zero, then 1) writes associated with this options MAY be aborted when it has been waiting for longer than the specified time. If an abortion happens, associated writes will return Status::TimeOut. 2) the stall time of the associated write caused by flush or compaction will be limited by timeout_hint_us. The default value of timeout_hint_us is 0 (i.e., OFF.) The statistics of timeout writes will be recorded in WRITE_TIMEDOUT. Test Plan: export ROCKSDB_TESTS=WriteTimeoutAndDelayTest make db_test ./db_test Reviewers: igor, ljin, haobo, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Multiple DB paths (without having an interface to expose to users) Summary: In this patch, we allow RocksDB to support multiple DB paths internally. No user interface is supported yet so this patch is silent to users. Test Plan: make all check Reviewers: igor, haobo, ljin, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: compression decision to compaction picker Summary: Before this diff, were deciding enable_compression in CompactionPicker and then were deciding final compression type in DBImpl. This is kind of confusing. After the diff, the final compression type will be decided in CompactionPicker. The reason for this is that I want CompactFiles() to specify output compression type, so that people can mix and match compression styles in their compaction algorithms. This diff makes it much easier to do that. Test Plan: make check Reviewers: dhruba, haobo, sdong, yhchiang, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: logging format, use PRIu64 instead of casting Summary: Code cleaning up, since we are already using __STDC_FORMAT_MACROS in printing uint64_t, change other places. Only logging is changed. Test Plan: make all check Reviewers: ljin Reviewed By: ljin Subscribers: dhruba, yhchiang, haobo, leveldb Differential Revision: some conditions for DBImpl::MakeRoomForWrite Summary: Task 4580155. Some conditions in DBImpl::MakeRoomForWrite can be cached in ColumnFamilyData, because theirs value can be changed only during compaction, adding new memtable and/or add recalculation of compaction score. These conditions are: cfd->imm()->size() cfd->options()->max_write_buffer_number 1 cfd->current()->NumLevelFiles(0) >= cfd->options()->level0_stop_writes_trigger cfd->options()->soft_rate_limit > 0.0 && (score cfd->current()->MaxCompactionScore()) > cfd->options()->soft_rate_limit cfd->options()->hard_rate_limit > 1.0 && (score cfd->current()->MaxCompactionScore()) > cfd->options()->hard_rate_limit P.S. As its my first diff, Siying suggested to add everybody as a reviewers for this diff. Sorry, if I forgot someone or add someone by mistake. Test Plan: make all check Reviewers: haobo, xjin, dhruba, yhchiang, zagfox, ljin, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: a potential write hang Summary: Currently, when something badly happen in the DB::Write() while the write-queue contains more than one element, the current design seems to forget to clean up the queue as well as wake-up all the writers, this potentially makes rocksdb hang on writes. Test Plan: make all check Reviewers: sdong, ljin, igor, haobo Reviewed By: haobo Subscribers: leveldb Differential Revision: group metadata needed to open an SST file to a separate copyable struct Summary: We added multiple fields to FileMetaData recently and are planning to add more. This refactoring separate the minimum information for accessing the file. This object is copyable (FileMetaData is not copyable since the ref counter). I hope this refactoring can enable further improvements: (1) use it to design a more efficient data structure to speed up read queries. (2) in the future, when we add information of storage level, we can easily do the encoding, instead of enlarge this structure, which might expand memory work set for file meta data. The definition is same as current EncodedFileMetaData used in two level iterator, so now the logic in two level iterator is easier to understand. Test Plan: make all check Reviewers: haobo, igor, ljin Reviewed By: ljin Subscribers: leveldb, dhruba, yhchiang Differential Revision: Fast-path for single column family Summary: We have a perf regression of Write() even with one column family. Make fast path for single column family to avoid the perf regression. See task Test Plan: make check Reviewers: sdong, ljin Reviewed By: sdong, ljin Subscribers: leveldb Differential Revision: preallocate files in universal compaction Summary: In universal compaction, MaxFileSizeForLevel is ULLONG_MAX. Weve been preallocation files to UULONG_MAX size all these time :) Test Plan: make check Reviewers: dhruba, haobo, ljin, sdong, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision: DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision: signal cond variable if need to Summary: At the end of BackgroundCallCompaction(), we call SignalAll(), even though we dont need to. If compaction hasnt done anything and theres another compaction running, there is no need to signal on the condition variable. Doing so creates a tight feedback loop which results in log files like: wait for memtable flush compaction nothing to do wait for memtable flush compaction nothing to do This change eliminates that Test Plan: make check Also: ~ $ grep ""nothing to do"" /fast-rocksdb-tmp/rocksdb_test/column_family_test/LOG | wc 7435 ~ $ grep ""nothing to do"" /fast-rocksdb-tmp/rocksdb_test/column_family_test/LOG | wc 372 First version is before the change, second version is after the change. Reviewers: dhruba, ljin, haobo, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: stale column families less aggressively Summary: Weve seen some production issues where column family is detected as stale, although there is only one column family in the system. This is a quick fix that: 1) doesnt flush stale column families if theres only one of them 2) Use 4 as a coefficient instead of 2 for determening when a column family is stale. This will make flushing less aggressive, while still keep a nice dynamic flushing of very stale CFs. Test Plan: make check Reviewers: dhruba, haobo, ljin, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: compaction style Summary: Introducing new compaction style FIFO. FIFO compaction style has write amplification of 1 (+1 for WAL) and it deletes the oldest files when the total DB size exceeds pre-configured values. FIFO compaction style is suited for storing high-frequency event logs. Test Plan: Added a unit test Reviewers: dhruba, haobo, sdong Reviewed By: dhruba Subscribers: alberts, leveldb Differential Revision:"
1066,1066,10.0,0.977400004863739,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add support for C bindings to the compaction V2 filter mechanism. Test Plan: make c_test && ./c_test Some fixes after merge./Add PlainTableOptions Summary: Since we have a lot of options for PlainTable, add a struct PlainTableOptions to manage them Test Plan: make all check Reviewers: sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: API: update options w/ convenience funcs & fifo compaction/Fix valgrind error in c_test Summary: External contribution caused some valgrind errors: This diff fixes them Test Plan: ran valgrind Reviewers: sdong, yhchiang, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: API: Add support for compaction filter factories (v1)/C API: column family support/Support for compaction filters in the C API/"
1067,1067,4.0,0.7186999917030334,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Support Multiple DB paths (without having an interface to expose to users) Summary: In this patch, we allow RocksDB to support multiple DB paths internally. No user interface is supported yet so this patch is silent to users. Test Plan: make all check Reviewers: igor, haobo, ljin, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: group metadata needed to open an SST file to a separate copyable struct Summary: We added multiple fields to FileMetaData recently and are planning to add more. This refactoring separate the minimum information for accessing the file. This object is copyable (FileMetaData is not copyable since the ref counter). I hope this refactoring can enable further improvements: (1) use it to design a more efficient data structure to speed up read queries. (2) in the future, when we add information of storage level, we can easily do the encoding, instead of enlarge this structure, which might expand memory work set for file meta data. The definition is same as current EncodedFileMetaData used in two level iterator, so now the logic in two level iterator is easier to understand. Test Plan: make all check Reviewers: haobo, igor, ljin Reviewed By: ljin Subscribers: leveldb, dhruba, yhchiang Differential Revision:"
1068,1068,7.0,0.8166999816894531,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Fixed a signed-unsigned comparison error in db_test Summary: Fixed a signed-unsigned comparison error in db_test Test Plan: make db_test/Flush only one column family Summary: Currently DBImpl::Flush() triggers flushes in all column families. Instead we need to trigger just the column family specified. Test Plan: make all check Reviewers: igor, ljin, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: db_test and DBIter Summary: Fix old issue with DBTest.Randomized with BlockBasedTableWithWholeKeyHashIndex + added printing in DBTest.Randomized. Test Plan: make all check Reviewers: zagfox, igor, ljin, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: DB property ""rocksdb.estimate-table-readers-mem"" Summary: Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache. Refactor the property codes to allow getting property from a version, with DB mutex not acquired. Test Plan: Add several checks of this new property in existing codes for various cases. Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: xjin, igor, leveldb Differential Revision: a warning / error in signed and unsigned comparison Summary: Fixed the following compilation error detected in mac: db/db_test.cc:2524:3: note: in instantiation of function template specialization rocksdb::test::Tester::IsEq<unsigned long long, int> requested here ASSERT_EQ(int_num, 0); ^ Test Plan: make/Add DB::GetIntProperty() to return integer properties to be returned as integers Summary: We have quite some properties that are integers and we are adding more. Add a function to directly return them as an integer, instead of a string Test Plan: Add several unit test checks Reviewers: yhchiang, igor, dhruba, haobo, ljin Reviewed By: ljin Subscribers: yoshinorim, leveldb Differential Revision: DB property estimated number of keys Summary: Add a DB property of estimated number of live keys, by adding number of entries of all mem tables and all files, subtracted by all deletions in all files. Test Plan: Add the case in unit tests Reviewers: hobbymanyp, ljin Reviewed By: ljin Subscribers: MarkCallaghan, yoshinorim, leveldb, igor, dhruba Differential Revision: user to specify DB path of output file of manual compaction Summary: Add a parameter path_id to DB::CompactRange(), to indicate where the output file should be placed to. Test Plan: add a unit test Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: xjin, igor, dhruba, MarkCallaghan, leveldb Differential Revision: Prev() for merge operator Summary: Implement Prev() with merge operator for DBIterator. Request from mongoDB. Task 4673663. Test Plan: make all check Reviewers: sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: multiple DB directories in universal compaction style Summary: This patch adds a target size parameter in options.db_paths and universal compaction will base it to determine which DB path to place a new file. Level-style stays the same. Test Plan: Add new unit tests Reviewers: ljin, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, dhruba, igor, leveldb Differential Revision: seek bugfix Summary: If `NeedToSeekImmutable()` returns false, `SeekInternal()` wont reset the contents of `immutable_min_heap_`. However, since it calls `UpdateCurrent()` unconditionally, if `current_` is one of immutable iterators (previously popped from `immutable_min_heap_`), `UpdateCurrent()` will overwrite it. As a result, if old `current_` in fact pointed to the smallest entry, forward iterator will skip some records. Fix implemented in this diff pushes `current_` back to `immutable_min_heap_` before calling `UpdateCurrent()`. Test Plan: New unit test (courtesy of $ ROCKSDB_TESTS=TailingIteratorSeekToSame ./db_test Reviewers: igor, dhruba, haobo, ljin Reviewed By: ljin Subscribers: lovro, leveldb Differential Revision: checks all child iterators Summary: Forward iterator only checked `status_` and `mutable_iter_->status()`, which is not sufficient. For example, when reading exclusively from cache (kBlockCacheTier), `mutable_iter_->status()` may return kOk (e.g. theres nothing in the memtable), but one of immutable iterators could be in kIncomplete. In this case, `ForwardIterator::status()` ought to return that status instead of kOk. This diff changes `status()` to also check `imm_iters_`, `l0_iters_`, and `level_iters_`. Test Plan: ROCKSDB_TESTS=TailingIteratorIncomplete ./db_test Reviewers: ljin, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: rate limiter into rocksdb Summary: Add option and plugin rate limiter for PosixWritableFile. The rate limiter only applies to flush and compaction. WAL and MANIFEST are excluded from this enforcement. Test Plan: db_test Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: SimpleWriteTimeoutTest to avoid false alarm. Summary: SimpleWriteTimeoutTest has two parts: 1) insert two large key/values to make memtable full and expect both of them are successful; 2) insert another key / value and expect it to be timed-out. Previously we also set a timeout in the first step, but this might sometimes cause false alarm. This diff makes the first two writes run without timeout setting. Test Plan: export ROCKSDB_TESTS=Time make db_test Reviewers: sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: timeout_hint_us to WriteOptions and introduce Status::TimeOut. Summary: This diff adds timeout_hint_us to WriteOptions. If its non-zero, then 1) writes associated with this options MAY be aborted when it has been waiting for longer than the specified time. If an abortion happens, associated writes will return Status::TimeOut. 2) the stall time of the associated write caused by flush or compaction will be limited by timeout_hint_us. The default value of timeout_hint_us is 0 (i.e., OFF.) The statistics of timeout writes will be recorded in WRITE_TIMEDOUT. Test Plan: export ROCKSDB_TESTS=WriteTimeoutAndDelayTest make db_test ./db_test Reviewers: igor, ljin, haobo, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Multiple DB paths (without having an interface to expose to users) Summary: In this patch, we allow RocksDB to support multiple DB paths internally. No user interface is supported yet so this patch is silent to users. Test Plan: make all check Reviewers: igor, haobo, ljin, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: the correct part (WalDir) of the revision: Commit 6634844dba962b9a150646382f4d6531d1f2440b by sdong Two small fixes in db_test Summary: Two fixes: (1) WalDir to pick a directory under TmpDir to allow two tests running in parallel without impacting each other (2) kBlockBasedTableWithWholeKeyHashIndex is disabled by mistake (I assume). Enable it. Test Plan: ./db_test Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: nkg-, igor, dhruba, haobo, leveldb Differential Revision: ""Two small fixes in db_test"" This reverts commit 6634844dba962b9a150646382f4d6531d1f2440b./HashLinkList memtable switches a bucket to a skip list to reduce performance outliers Summary: In this patch, we enhance HashLinkList memtable to reduce performance outliers when a bucket contains too many entries. We switch to skip list for this case to enable binary search. Add threshold_use_skiplist parameter to determine when a bucket needs to switch to skip list. The new data structure is documented in comments in the codes. Test Plan: make all check set threshold_use_skiplist in several tests Reviewers: yhchiang, haobo, ljin Reviewed By: yhchiang, ljin Subscribers: nkg-, xjin, dhruba, yhchiang, leveldb Differential Revision: small fixes in db_test Summary: Two fixes: (1) WalDir to pick a directory under TmpDir to allow two tests running in parallel without impacting each other (2) kBlockBasedTableWithWholeKeyHashIndex is disabled by mistake (I assume). Enable it. Test Plan: ./db_test Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: nkg-, igor, dhruba, haobo, leveldb Differential Revision: arena to allocate memtables bloomfilter and hashskiplists buckets_ Summary: Bloomfilter and hashskiplists buckets_ allocated by memtables arena DynamicBloom: pass arena via constructor, allocate space in SetTotalBits HashSkipListRep: allocate space of buckets_ using arena. do not delete it in deconstructor because arena would take care of it. Several test files are changed. Test Plan: make all check Reviewers: ljin, haobo, yhchiang, sdong Reviewed By: sdong Subscribers: igor, dhruba Differential Revision: compaction to reclaim storage more effectively. Summary: This diff allows compaction to reclaim storage more effectively. In the current design, compactions are mainly triggered based on the file sizes. However, since deletion entries does not have value, files which have many deletion entries are less likely to be compacted. As a result, it may took a while to make deletion entries to be compacted. This diff address issue by compensating the size of deletion entries during compaction process: the size of each deletion entry in the compaction process is augmented by 2x average value size. The diff applies to both leveled and universal compacitons. Test Plan: develop CompactionDeletionTrigger make db_test ./db_test Reviewers: haobo, igor, ljin, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: Make block based table hash index more adaptive Summary: Currently, RocksDB returns error if a db written with prefix hash index, is later opened without providing a prefix extractor. This is uncessarily harsh. Without a prefix extractor, we could always fallback to the normal binary index. Test Plan: unit test, also manually veried LOG that fallback did occur. Reviewers: sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: forward iterator bug Summary: obvious Test Plan: db_test Reviewers: sdong, haobo, igor Reviewed By: igor Subscribers: leveldb Differential Revision: compaction style Summary: Introducing new compaction style FIFO. FIFO compaction style has write amplification of 1 (+1 for WAL) and it deletes the oldest files when the total DB size exceeds pre-configured values. FIFO compaction style is suited for storing high-frequency event logs. Test Plan: Added a unit test Reviewers: dhruba, haobo, sdong Reviewed By: dhruba Subscribers: alberts, leveldb Differential Revision:"
1069,1069,7.0,0.845300018787384,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Add DB property ""rocksdb.estimate-table-readers-mem"" Summary: Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache. Refactor the property codes to allow getting property from a version, with DB mutex not acquired. Test Plan: Add several checks of this new property in existing codes for various cases. Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: xjin, igor, leveldb Differential Revision: Summary: 1. logging when create and delete manifest file 2. fix formating in table/format.cc Test Plan: make all check run db_bench, track the LOG file. Reviewers: ljin, yhchiang, igor, yufei.zhu, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: compaction-related errors where number of input levels are hard-coded. Summary: Fixed compaction-related errors where number of input levels are hard-coded. Its a bug found in compaction branch. This diff will be pushed into master. Test Plan: export ROCKSDB_TESTS=Compact make db_test ./db_test also passed the tests in compaction branch Reviewers: igor, sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: Version::Get() Summary: Refactoring Version::Get() method to move file picker logic to a separate class. Test Plan: make check all Reviewers: igor, sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: multiple DB directories in universal compaction style Summary: This patch adds a target size parameter in options.db_paths and universal compaction will base it to determine which DB path to place a new file. Level-style stays the same. Test Plan: Add new unit tests Reviewers: ljin, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, dhruba, igor, leveldb Differential Revision: FileLevel in LevelFileNumIterator Summary: Use FileLevel in LevelFileNumIterator, thus use new version of findFile. Old version of findFile function is deleted. Write a function in version_set.cc to generate FileLevel from files_. Add GenerateFileLevelTest in version_set_test.cc Test Plan: make all check Reviewers: ljin, haobo, yhchiang, sdong Reviewed By: sdong Subscribers: igor, dhruba Differential Revision: compressed_levels_ in Version, allocate its space using arena. Make Version::Get, Version::FindFile faster Summary: Define CompressedFileMetaData that just contains fd, smallest_slice, largest_slice. Create compressed_levels_ in Version, the space is allocated using arena Thus increase the file meta data locality, speed up ""Get"" and ""FindFile"" benchmark with in-memory tmpfs, could have 4% improvement under ""random read"" and 2% improvement under ""read while writing"" benchmark command: ./db_bench writes_per_second=81920 Read Random: From 1.8363 ms/op, improve to 1.7587 ms/op. Read while writing: From 2.985 ms/op, improve to 2.924 ms/op. Test Plan: make all check Reviewers: ljin, haobo, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, igor Differential Revision: Multiple DB paths (without having an interface to expose to users) Summary: In this patch, we allow RocksDB to support multiple DB paths internally. No user interface is supported yet so this patch is silent to users. Test Plan: make all check Reviewers: igor, haobo, ljin, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: compaction to reclaim storage more effectively. Summary: This diff allows compaction to reclaim storage more effectively. In the current design, compactions are mainly triggered based on the file sizes. However, since deletion entries does not have value, files which have many deletion entries are less likely to be compacted. As a result, it may took a while to make deletion entries to be compacted. This diff address issue by compensating the size of deletion entries during compaction process: the size of each deletion entry in the compaction process is augmented by 2x average value size. The diff applies to both leveled and universal compacitons. Test Plan: develop CompactionDeletionTrigger make db_test ./db_test Reviewers: haobo, igor, ljin, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: group metadata needed to open an SST file to a separate copyable struct Summary: We added multiple fields to FileMetaData recently and are planning to add more. This refactoring separate the minimum information for accessing the file. This object is copyable (FileMetaData is not copyable since the ref counter). I hope this refactoring can enable further improvements: (1) use it to design a more efficient data structure to speed up read queries. (2) in the future, when we add information of storage level, we can easily do the encoding, instead of enlarge this structure, which might expand memory work set for file meta data. The definition is same as current EncodedFileMetaData used in two level iterator, so now the logic in two level iterator is easier to understand. Test Plan: make all check Reviewers: haobo, igor, ljin Reviewed By: ljin Subscribers: leveldb, dhruba, yhchiang Differential Revision: Bring back the logic of skipping key range check when there are level 0 files Summary: removed the logic of skipping file key range check when there are less than 3 level 0 files. This patch brings it back. Other than that, add another small optimization to avoid to check all the levels if most higher levels dont have any file. Test Plan: make all check Reviewers: ljin Reviewed By: ljin Subscribers: yhchiang, igor, haobo, dhruba, leveldb Differential Revision: DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision:"
1070,1070,10.0,0.9814000129699707,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Pass parsed user key to prefix extractor in V2 compaction Previously, the prefix extractor was being supplied with the RocksDB key instead of a parsed user key. This makes correct interpretation by calling application fragile or impossible./Fix leak in c_test/Add support for C bindings to the compaction V2 filter mechanism. Test Plan: make c_test && ./c_test Some fixes after merge./C API: Add test for compaction filter factories Also refactored the compaction filter tests to share some code and ensure that options were getting reset so future test results arent confused./C API: column family support/Add a test for using compaction filters via the C API/"
1071,1071,10.0,0.5863000154495239,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fix for tools Summary: Previously I made `make check` work with but there are some tools that are not compiled using `make check`. Test Plan: make all Reviewers: yhchiang, rven, ljin, sdong Reviewed By: ljin, sdong Subscribers: dhruba, leveldb Differential Revision: support for in place update for db_stress Summary: Added two flags which operate as follows: in_place_update: enable in_place_update for default column family set_in_place_one_in: toggles the value of the option inplace_update_support with a probability of 1/N Test Plan: Run db_stress with the two flags above set. Specifically tried in_place_update set to true and set_in_place_one_in set to 10,000. Reviewers: ljin, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: for dynamic options Summary: Allow SetOptions() during db_stress test Test Plan: make crash_test Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: leveldb Differential Revision: full filter for block based table. Summary: 1. Make filter_block.h a base class. Derive block_based_filter_block and full_filter_block. The previous one is the traditional filter block. The full_filter_block is newly added. It would generate a filter block that contain all the keys in SST file. 2. When querying a key, table would first check if full_filter is available. If not, it would go to the exact data block and check using block_based filter. 3. User could choose to use full_filter or tradional(block_based_filter). They would be stored in SST file with different meta index name. ""filter.filter_policy"" or ""full_filter.filter_policy"". Then, Table reader is able to know the fllter block type. 4. Some optimizations have been done for full_filter_block, thus it requires a different interface compared to the original one in filter_policy.h. 5. Actual implementation of filter bits coding/decoding is placed in util/bloom_impl.cc Benchmark: base commit 1d23b5c470844c1208301311f0889eca750431c0 Command: db_bench disable_auto_compactions=1 Read QPS increase for about 30% from 2230002 to 2991411. Test Plan: make all check valgrind db_test db_stress 0 ./auto_sanity_test.sh Reviewers: igor, yhchiang, ljin, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: assert to db Put in db_stress test Summary: 1. assert db->Put to be true in db_stress 2. begin column family with name ""1"". Test Plan: 1. ./db_stress Reviewers: ljin, yhchiang, dhruba, sdong, igor Reviewed By: sdong, igor Subscribers: leveldb Differential Revision: block based table related options BlockBasedTableOptions Summary: I will move compression related options in a separate diff since this diff is already pretty lengthy. I guess I will also need to change JNI accordingly :( Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: igor Subscribers: leveldb Differential Revision:"
1072,1072,12.0,0.9405999779701233,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Apply InfoLogLevel to the logs in util/env_hdfs.cc Summary: Apply InfoLogLevel to the logs in util/env_hdfs.cc Test Plan: make Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba Differential Revision:"
1073,1073,2.0,0.9883000254631042,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","DB::Open() to automatically increase thread pool size if it is smaller than max number of parallel compactions or flushes Summary: With the patch, thread pool size will be automatically increased if DBs options ask for more parallelism of compactions or flushes. Too many users have been confused by the API. Change it to make it harder for users to make mistakes Test Plan: Add two unit tests to cover the function. Reviewers: yhchiang, rven, igor, MarkCallaghan, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: fallocate(FALLOC_FL_PUNCH_HOLE) to release unused blocks at the end of file Summary: ftruncate does not always free preallocated unused space at the end of file. In some cases, we pin too much disk space than it should Test Plan: env_test Reviewers: sdong, rven, yhchiang, igor Reviewed By: igor Subscribers: nkg-, dhruba, leveldb Differential Revision: timing/Use chrono for timing Summary: Since we depend on C++11, we might as well use it for timing, instead of this platform-depended code. Test Plan: Ran autovector_test, which reports time and confirmed that output is similar to master Reviewers: ljin, sdong, yhchiang, rven, dhruba Reviewed By: dhruba Subscribers: dhruba, leveldb Differential Revision:"
1074,1074,7.0,0.9136000275611877,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","JNI changes corresponding to BlockBasedTableOptions migration Summary: as title Test Plan: tested on my mac make rocksdbjava make jtest Reviewers: sdong, igor, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision:"
1075,1075,4.0,0.9868000149726868,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",[RocksJava] Hardening RocksIterator RocksIterator will sometimes Sigsegv on dispose. Mainly thats related to dispose order. If the related RocksDB instance is freed beforehand RocksIterator.dispose() will fail. Within this commit there is a major change to RocksIterator. RocksIterator will hold a private reference to the RocksDB instance which created the RocksIterator. So even if RocksDB is freed in the same GC cycle the RocksIterator instances will be freed prior to related RocksDB instances. Another aspect targets the dispose logic if the RocksDB is freed previously and already gc`ed. On dispose of a RocksIterator the dispose logic will check if the RocksDB instance points to an initialized DB. If not the dispose logic will not perform any further action. The crash can be reproduced by using the related test provided within this commit. Related information: This relates to facebook rocksdb-dev group post about SigSegv on RocksIterator.dispose()./
1076,1076,10.0,0.9896000027656555,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","[RocksJava] Column family support This commit includes the support for the following functionalities: Single Get/Put operations WriteBatch operations Single iterator functionality Open database with column families Open database with column families Read/Only Create column family Drop column family Properties of column families Listing of column families Fully backwards comptabile implementation Multi Iterator support MultiGet KeyMayExist Option to create missing column families on open In addition there is are two new Tests: Test of ColumnFamily functionality Test of Read only feature to open subsets of column families Basic test to test the KeyMayExist feature What is not supported currently using RocksJava: Custom ColumnFamilyOptions The following targets work as expected: make rocksdbjava make jtest Test environment: Ubuntu 14.04(LTS, x64), Java 1.7.0_65(OpenJDK IcedTea 2.5.2), g++ 4.8.2, kernel 3.13.0-35-generix/Addressing review comments (adding a env variable to override temp directory)/RocksDB static build Make file changes to download and build the dependencies .Load the shared library when RocksDB is initialized/"
1077,1077,5.0,0.944100022315979,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","[Java] Add purgeOldBackups API Summary: 1. Check status of CreateNewBackup. If status is not OK, then throw. 2. Add purgeOldBackups API Test Plan: make test make sample Reviewers: haobo, sdong, zzbennett, swapnilghike, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision:"
1078,1078,12.0,0.642799973487854,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","[RocksJava] ColumnFamily custom Options API extension ********************* *************************** ******** ************* ******** ******** *********** ******** ******** ********* ******** ************************************* ************************************* ************************************* ****** *** *** ****** ****** *** *** *** ****** ****** *** ****** *************************** *********************/Iterator support for Write Batches/[RocksJava] BackupInfos & Restore-/BackupableDB enhancements Summary: BackupableDB deleteBackup method BackupableDB purgeOldBackups bugfix BackupInfos now available in Restorable-/BackupableDB Extended BackupableDBTest to cover more of the currently implemented functionality. Test Plan: make rocksdbjava make jtest Differential Revision: code style problems identified by lint/Add locking to comparator jni callback methods which must be thread-safe/Feature Implement Java API for Comparator and Slice. Allows use of either byte[] or DirectByteBuffer for accessing underlying data./[Java] Fixed 32-bit overflowing issue when converting jlong to size_t Summary: Fixed 32-bit overflowing issue when converting jlong to size_t by capping jlong to std::numeric_limits<size_t>::max(). Test Plan: make rocksdbjava make jtest Reviewers: ankgup87, ljin, sdong, igor Reviewed By: igor Subscribers: leveldb Differential Revision:"
1079,1079,5.0,0.944100022315979,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","[Java] Add purgeOldBackups API Summary: 1. Check status of CreateNewBackup. If status is not OK, then throw. 2. Add purgeOldBackups API Test Plan: make test make sample Reviewers: haobo, sdong, zzbennett, swapnilghike, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision:"
1080,1080,10.0,0.8974999785423279,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Turn on and fix all the errors Summary: We need to turn on for mobile. See D1671432 (internal phabricator) for details. This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables. Test Plan: compiles Reviewers: ljin, rven, yhchiang, sdong Reviewed By: yhchiang Subscribers: bobbaldwin, dhruba, leveldb Differential Revision: Flush functionality RocksJava now supports also flush functionality of RocksDB./[RocksJava] improvements Minor corrections to resolve build problems with RocksJava code./[RocksJava] KeyMayExist w/o ColumnFamilies/Merge with ColumnFamilies & Hardening CFHandle Summary: ColumnFamilyHandles face the same problem as RocksIterator previously so used methods were also applied for ColumnFamilyHandles. Another problem with CF was that Options passed to CFs were always filled with default values. To enable Merge, all parts of the database must share the same merge functionality which is not possible using default values. So from now on every CF will inherit from db options. Changes to RocksDB: merge can now take also a cfhandle Changes to MergeTest: Corrected formatting Included also GC tests Extended tests to cover CF related parts Corrected paths to cleanup properly within the test process Reduced verbosity of the test Test Plan: make rocksdbjava make jtest Subscribers: dhruba Differential Revision: Support Snapshots Summary: Snapshots integration into RocksJava. Added support for the following functionalities: getSnapshot releaseSnapshot ReadOptions support to set a Snapshot ReadOptions support to retrieve Snapshot SnapshotTest Test Plan: make rocksdbjava make jtest Differential Revision: merge functions to RocksDBJava Summary: Added support for the merge operation to RocksJava. You can specify a merge function to be used on the current database. The merge function can either be one of the functions defined in utilities/merge_operators.h, which can be specified through its corresponding name, or a user-created function that needs to be encapsulated in a JNI object in order to be used. Examples are provided for both use cases. Test Plan: There are unit test in MergeTest.java Reviewers: ankgup87 Subscribers: vladb38 Differential Revision: Column family support This commit includes the support for the following functionalities: Single Get/Put operations WriteBatch operations Single iterator functionality Open database with column families Open database with column families Read/Only Create column family Drop column family Properties of column families Listing of column families Fully backwards comptabile implementation Multi Iterator support MultiGet KeyMayExist Option to create missing column families on open In addition there is are two new Tests: Test of ColumnFamily functionality Test of Read only feature to open subsets of column families Basic test to test the KeyMayExist feature What is not supported currently using RocksJava: Custom ColumnFamilyOptions The following targets work as expected: make rocksdbjava make jtest Test environment: Ubuntu 14.04(LTS, x64), Java 1.7.0_65(OpenJDK IcedTea 2.5.2), g++ 4.8.2, kernel 3.13.0-35-generix/Add block based table config options/"
1081,1081,13.0,0.5088000297546387,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",ttl/ttl_test.cc: prefer prefix ++operator for non-primitive types Signed-off-by: Danny Al-Gaaf
1082,1082,10.0,0.949999988079071,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","move block based table related options BlockBasedTableOptions Summary: I will move compression related options in a separate diff since this diff is already pretty lengthy. I guess I will also need to change JNI accordingly :( Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: igor Subscribers: leveldb Differential Revision:"
1083,1083,5.0,0.975600004196167,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","use GetContext to replace callback function pointer Summary: Intead of passing callback function pointer and its arg on Table::Get() interface, passing GetContext. This makes the interface cleaner and possible better perf. Also adding a fast pass for SaveValue() Test Plan: make all check Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: CuckooHash table format to table_reader_bench Summary: Make table_reader_bench cover all the three table formats. Test Plan: Run it using three options Reviewers: radheshyamb, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision:"
1084,1084,14.0,0.9959999918937683,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Fix compaction bug in Cuckoo Table Builder. Use kvs_.size() instead of num_entries in FileSize() method. Summary: Fix compaction bug in Cuckoo Table Builder. Use kvs_.size() instead of num_entries in FileSize() method. Also added tests. Test Plan: make check all Also ran db_bench to generate multiple files. Reviewers: sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: Cuckoo Table Reader performance. Inlined hash function and number of buckets a power of two. Summary: Use inlined hash functions instead of function pointer. Make number of buckets a power of two and use bitwise and instead of mod. After these changes, we get almost 50% improvement in performance. Results: With 120000000 items, utilization is 89.41%, number of hash functions: 2. Time taken per op is 0.231us (4.3 Mqps) with batch size of 0 Time taken per op is 0.229us (4.4 Mqps) with batch size of 0 Time taken per op is 0.185us (5.4 Mqps) with batch size of 0 With 120000000 items, utilization is 89.41%, number of hash functions: 2. Time taken per op is 0.108us (9.3 Mqps) with batch size of 10 Time taken per op is 0.100us (10.0 Mqps) with batch size of 10 Time taken per op is 0.103us (9.7 Mqps) with batch size of 10 With 120000000 items, utilization is 89.41%, number of hash functions: 2. Time taken per op is 0.101us (9.9 Mqps) with batch size of 25 Time taken per op is 0.098us (10.2 Mqps) with batch size of 25 Time taken per op is 0.097us (10.3 Mqps) with batch size of 25 With 120000000 items, utilization is 89.41%, number of hash functions: 2. Time taken per op is 0.100us (10.0 Mqps) with batch size of 50 Time taken per op is 0.097us (10.3 Mqps) with batch size of 50 Time taken per op is 0.097us (10.3 Mqps) with batch size of 50 With 120000000 items, utilization is 89.41%, number of hash functions: 2. Time taken per op is 0.102us (9.8 Mqps) with batch size of 100 Time taken per op is 0.098us (10.2 Mqps) with batch size of 100 Time taken per op is 0.115us (8.7 Mqps) with batch size of 100 With 100000000 items, utilization is 74.51%, number of hash functions: 2. Time taken per op is 0.201us (5.0 Mqps) with batch size of 0 Time taken per op is 0.155us (6.5 Mqps) with batch size of 0 Time taken per op is 0.152us (6.6 Mqps) with batch size of 0 With 100000000 items, utilization is 74.51%, number of hash functions: 2. Time taken per op is 0.089us (11.3 Mqps) with batch size of 10 Time taken per op is 0.084us (11.9 Mqps) with batch size of 10 Time taken per op is 0.086us (11.6 Mqps) with batch size of 10 With 100000000 items, utilization is 74.51%, number of hash functions: 2. Time taken per op is 0.087us (11.5 Mqps) with batch size of 25 Time taken per op is 0.085us (11.7 Mqps) with batch size of 25 Time taken per op is 0.093us (10.8 Mqps) with batch size of 25 With 100000000 items, utilization is 74.51%, number of hash functions: 2. Time taken per op is 0.094us (10.6 Mqps) with batch size of 50 Time taken per op is 0.094us (10.7 Mqps) with batch size of 50 Time taken per op is 0.093us (10.8 Mqps) with batch size of 50 With 100000000 items, utilization is 74.51%, number of hash functions: 2. Time taken per op is 0.092us (10.9 Mqps) with batch size of 100 Time taken per op is 0.089us (11.2 Mqps) with batch size of 100 Time taken per op is 0.088us (11.3 Mqps) with batch size of 100 With 80000000 items, utilization is 59.60%, number of hash functions: 2. Time taken per op is 0.154us (6.5 Mqps) with batch size of 0 Time taken per op is 0.168us (6.0 Mqps) with batch size of 0 Time taken per op is 0.190us (5.3 Mqps) with batch size of 0 With 80000000 items, utilization is 59.60%, number of hash functions: 2. Time taken per op is 0.081us (12.4 Mqps) with batch size of 10 Time taken per op is 0.077us (13.0 Mqps) with batch size of 10 Time taken per op is 0.083us (12.1 Mqps) with batch size of 10 With 80000000 items, utilization is 59.60%, number of hash functions: 2. Time taken per op is 0.077us (13.0 Mqps) with batch size of 25 Time taken per op is 0.073us (13.7 Mqps) with batch size of 25 Time taken per op is 0.073us (13.7 Mqps) with batch size of 25 With 80000000 items, utilization is 59.60%, number of hash functions: 2. Time taken per op is 0.076us (13.1 Mqps) with batch size of 50 Time taken per op is 0.072us (13.8 Mqps) with batch size of 50 Time taken per op is 0.072us (13.8 Mqps) with batch size of 50 With 80000000 items, utilization is 59.60%, number of hash functions: 2. Time taken per op is 0.077us (13.0 Mqps) with batch size of 100 Time taken per op is 0.074us (13.6 Mqps) with batch size of 100 Time taken per op is 0.073us (13.6 Mqps) with batch size of 100 With 70000000 items, utilization is 52.15%, number of hash functions: 2. Time taken per op is 0.190us (5.3 Mqps) with batch size of 0 Time taken per op is 0.186us (5.4 Mqps) with batch size of 0 Time taken per op is 0.184us (5.4 Mqps) with batch size of 0 With 70000000 items, utilization is 52.15%, number of hash functions: 2. Time taken per op is 0.079us (12.7 Mqps) with batch size of 10 Time taken per op is 0.070us (14.2 Mqps) with batch size of 10 Time taken per op is 0.072us (14.0 Mqps) with batch size of 10 With 70000000 items, utilization is 52.15%, number of hash functions: 2. Time taken per op is 0.080us (12.5 Mqps) with batch size of 25 Time taken per op is 0.072us (14.0 Mqps) with batch size of 25 Time taken per op is 0.071us (14.1 Mqps) with batch size of 25 With 70000000 items, utilization is 52.15%, number of hash functions: 2. Time taken per op is 0.082us (12.1 Mqps) with batch size of 50 Time taken per op is 0.071us (14.1 Mqps) with batch size of 50 Time taken per op is 0.073us (13.6 Mqps) with batch size of 50 With 70000000 items, utilization is 52.15%, number of hash functions: 2. Time taken per op is 0.080us (12.5 Mqps) with batch size of 100 Time taken per op is 0.077us (13.0 Mqps) with batch size of 100 Time taken per op is 0.078us (12.8 Mqps) with batch size of 100 Test Plan: make check all make valgrind_check make asan_check Reviewers: sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: a cache friendly version of Cuckoo Hash Summary: This implements a cache friendly version of Cuckoo Hash in which, in case of collission, we try to insert in next few locations. The size of the neighborhood to check is taken as an input parameter in builder and stored in the table. Test Plan: make check all cuckoo_table_{db,reader,builder}_test Reviewers: sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision:"
1085,1085,10.0,0.5530999898910522,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Turn on Summary: ...and fix all the errors :) Jim suggested turning on because it helped him fix number of critical bugs in fbcode. I think its a good idea to be clean. Test Plan: compiles Reviewers: yhchiang, rven, sdong, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: InfoLogLevel to the logs in table/block_based_table_reader.cc Summary: Apply InfoLogLevel to the logs in table/block_based_table_reader.cc Also, add missing checks for the returned status in BlockBasedTable::Open Test Plan: make Reviewers: sdong, ljin, igor Reviewed By: igor Subscribers: dhruba Differential Revision: reloading filter on Get() if cache_index_and_filter_blocks false Summary: This fixes the case that filter policy is missing in SST file, but we open the table with filter policy on and cache_index_and_filter_blocks false. The current behavior is that we will try to load it every time on Get() but fail. Test Plan: unit test Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: GetContext to replace callback function pointer Summary: Intead of passing callback function pointer and its arg on Table::Get() interface, passing GetContext. This makes the interface cleaner and possible better perf. Also adding a fast pass for SaveValue() Test Plan: make all check Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: make format on PR full filter for block based table. Summary: 1. Make filter_block.h a base class. Derive block_based_filter_block and full_filter_block. The previous one is the traditional filter block. The full_filter_block is newly added. It would generate a filter block that contain all the keys in SST file. 2. When querying a key, table would first check if full_filter is available. If not, it would go to the exact data block and check using block_based filter. 3. User could choose to use full_filter or tradional(block_based_filter). They would be stored in SST file with different meta index name. ""filter.filter_policy"" or ""full_filter.filter_policy"". Then, Table reader is able to know the fllter block type. 4. Some optimizations have been done for full_filter_block, thus it requires a different interface compared to the original one in filter_policy.h. 5. Actual implementation of filter bits coding/decoding is placed in util/bloom_impl.cc Benchmark: base commit 1d23b5c470844c1208301311f0889eca750431c0 Command: db_bench disable_auto_compactions=1 Read QPS increase for about 30% from 2230002 to 2991411. Test Plan: make all check valgrind db_test db_stress 0 ./auto_sanity_test.sh Reviewers: igor, yhchiang, ljin, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: ImmutableOptions Summary: As a preparation to support updating some options dynamically, Id like to first introduce ImmutableOptions, which is a subset of Options that cannot be changed during the course of a DB lifetime without restart. ColumnFamily will keep both Options and ImmutableOptions. Any component below ColumnFamily should only take ImmutableOptions in their constructor. Other options should be taken from APIs, which will be allowed to adjust dynamically. I am yet to make changes to memtable and other related classes to take ImmutableOptions in their ctor. That can be done in a seprate diff as this one is already pretty big. Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: to allow total order seek for block-based table when hash index is enabled Summary: as title Test Plan: table_test Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: block based table related options BlockBasedTableOptions Summary: I will move compression related options in a separate diff since this diff is already pretty lengthy. I guess I will also need to change JNI accordingly :( Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: igor Subscribers: leveldb Differential Revision:"
1086,1086,5.0,0.9635000228881836,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Turn on and fix all the errors Summary: We need to turn on for mobile. See D1671432 (internal phabricator) for details. This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables. Test Plan: compiles Reviewers: ljin, rven, yhchiang, sdong Reviewed By: yhchiang Subscribers: bobbaldwin, dhruba, leveldb Differential Revision: improvement/"
1087,1087,10.0,0.949999988079071,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","move block based table related options BlockBasedTableOptions Summary: I will move compression related options in a separate diff since this diff is already pretty lengthy. I guess I will also need to change JNI accordingly :( Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: igor Subscribers: leveldb Differential Revision:"
1088,1088,5.0,0.8245999813079834,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Turn on and fix all the errors Summary: We need to turn on for mobile. See D1671432 (internal phabricator) for details. This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables. Test Plan: compiles Reviewers: ljin, rven, yhchiang, sdong Reviewed By: yhchiang Subscribers: bobbaldwin, dhruba, leveldb Differential Revision: make format on PR naked calls to operator new and delete (Fixes This replaces a mishmash of pointers in the Block and BlockContents classes with std::unique_ptr. It also changes the semantics of BlockContents to be limited to use as a constructor parameter for Block objects, as it owns any block buffers handed to it./[unit test] CompactRange should fail if we dont have space Summary: See t5106397. Also, few more changes: 1. in unit tests, the assumption is that writes will be dropped when there is no space left on device. I changed the wording around it. 2. InvalidArgument() errors are only when user-provided arguments are invalid. When the file is corrupted, we need to return Status::Corruption Test Plan: make check Reviewers: sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: PerfStepTimer to stop on destruct This eliminates the need to remember to call PERF_TIMER_STOP when a section has been timed. This allows more useful design with the perf timers and enables possible return value optimizations. Simplistic example: class Foo { public: Foo(int v) : m_v(v); private: int m_v; } Foo makeFrobbedFoo(int *errno) { *errno 0; return Foo(); } Foo bar(int *errno) { PERF_TIMER_GUARD(some_timer); return makeFrobbedFoo(errno); } int main(int argc, char[] argv) { Foo f; int errno; f bar(&errno); if (errno) return return 0; } After bar() is called, perf_context.some_timer would be incremented as if Stop(&perf_context.some_timer) was called at the end, and the compiler is still able to produce optimizations on the return value from makeFrobbedFoo() through to main()./"
1089,1089,16.0,0.7853999733924866,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","in_table_factory.cc: use correct format specifier Use %zu instead of %zd since size_t and uint32_t are unsigned. Fix for: [table/plain_table_factory.cc:55]: (warning) %zd in format string (no. 1) requires ssize_t but the argument type is size_t {aka unsigned long}. [table/plain_table_factory.cc:58]: (warning) %zd in format string (no. 1) requires ssize_t but the argument type is size_t {aka unsigned long}. Signed-off-by: Danny Al-Gaaf ImmutableOptions Summary: As a preparation to support updating some options dynamically, Id like to first introduce ImmutableOptions, which is a subset of Options that cannot be changed during the course of a DB lifetime without restart. ColumnFamily will keep both Options and ImmutableOptions. Any component below ColumnFamily should only take ImmutableOptions in their constructor. Other options should be taken from APIs, which will be allowed to adjust dynamically. I am yet to make changes to memtable and other related classes to take ImmutableOptions in their ctor. That can be done in a seprate diff as this one is already pretty big. Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: table options Summary: Add a virtual function in table factory that will print table options Test Plan: make release Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision:"
1090,1090,10.0,0.9919000267982483,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Turn back on Summary: It turns out that has different rules for gcc than clang. Previous commit fixed clang. This commits fixes the rest of the warnings for gcc. Test Plan: compiles Reviewers: ljin, yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: reloading filter on Get() if cache_index_and_filter_blocks false Summary: This fixes the case that filter policy is missing in SST file, but we open the table with filter policy on and cache_index_and_filter_blocks false. The current behavior is that we will try to load it every time on Get() but fail. Test Plan: unit test Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: Summary: removed reference to options in WriteBatch and DBImpl::Get() Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: ImmutableOptions Summary: As a preparation to support updating some options dynamically, Id like to first introduce ImmutableOptions, which is a subset of Options that cannot be changed during the course of a DB lifetime without restart. ColumnFamily will keep both Options and ImmutableOptions. Any component below ColumnFamily should only take ImmutableOptions in their constructor. Other options should be taken from APIs, which will be allowed to adjust dynamically. I am yet to make changes to memtable and other related classes to take ImmutableOptions in their ctor. That can be done in a seprate diff as this one is already pretty big. Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: to allow total order seek for block-based table when hash index is enabled Summary: as title Test Plan: table_test Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: block based table related options BlockBasedTableOptions Summary: I will move compression related options in a separate diff since this diff is already pretty lengthy. I guess I will also need to change JNI accordingly :( Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: igor Subscribers: leveldb Differential Revision:"
1091,1091,14.0,0.9983999729156494,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","use GetContext to replace callback function pointer Summary: Intead of passing callback function pointer and its arg on Table::Get() interface, passing GetContext. This makes the interface cleaner and possible better perf. Also adding a fast pass for SaveValue() Test Plan: make all check Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: add one option to allow identity function for the first hash function Summary: MurmurHash becomes expensive when we do millions Get() a second in one thread. Add this option to allow the first hash function to use identity function as hash function. It results in QPS increase from 3.7M/s to ~4.3M/s. I did not observe improvement for end to end RocksDB performance. This may be caused by other bottlenecks that I will address in a separate diff. Test Plan: ``` rocksdb] ./cuckoo_table_reader_test Test CuckooReaderTest.WhenKeyExists Test CuckooReaderTest.WhenKeyExistsWithUint64Comparator Test CuckooReaderTest.CheckIterator Test CuckooReaderTest.CheckIteratorUint64 Test CuckooReaderTest.WhenKeyNotFound Test CuckooReaderTest.TestReadPerformance With 125829120 items, utilization is 93.75%, number of hash functions: 2. Time taken per op is 0.272us (3.7 Mqps) with batch size of 0, of found keys 125829120 With 125829120 items, utilization is 93.75%, number of hash functions: 2. Time taken per op is 0.138us (7.2 Mqps) with batch size of 10, of found keys 125829120 With 125829120 items, utilization is 93.75%, number of hash functions: 2. Time taken per op is 0.142us (7.1 Mqps) with batch size of 25, of found keys 125829120 With 125829120 items, utilization is 93.75%, number of hash functions: 2. Time taken per op is 0.142us (7.0 Mqps) with batch size of 50, of found keys 125829120 With 125829120 items, utilization is 93.75%, number of hash functions: 2. Time taken per op is 0.144us (6.9 Mqps) with batch size of 100, of found keys 125829120 With 104857600 items, utilization is 78.12%, number of hash functions: 2. Time taken per op is 0.201us (5.0 Mqps) with batch size of 0, of found keys 104857600 With 104857600 items, utilization is 78.12%, number of hash functions: 2. Time taken per op is 0.121us (8.3 Mqps) with batch size of 10, of found keys 104857600 With 104857600 items, utilization is 78.12%, number of hash functions: 2. Time taken per op is 0.123us (8.1 Mqps) with batch size of 25, of found keys 104857600 With 104857600 items, utilization is 78.12%, number of hash functions: 2. Time taken per op is 0.121us (8.3 Mqps) with batch size of 50, of found keys 104857600 With 104857600 items, utilization is 78.12%, number of hash functions: 2. Time taken per op is 0.112us (8.9 Mqps) with batch size of 100, of found keys 104857600 With 83886080 items, utilization is 62.50%, number of hash functions: 2. Time taken per op is 0.251us (4.0 Mqps) with batch size of 0, of found keys 83886080 With 83886080 items, utilization is 62.50%, number of hash functions: 2. Time taken per op is 0.107us (9.4 Mqps) with batch size of 10, of found keys 83886080 With 83886080 items, utilization is 62.50%, number of hash functions: 2. Time taken per op is 0.099us (10.1 Mqps) with batch size of 25, of found keys 83886080 With 83886080 items, utilization is 62.50%, number of hash functions: 2. Time taken per op is 0.100us (10.0 Mqps) with batch size of 50, of found keys 83886080 With 83886080 items, utilization is 62.50%, number of hash functions: 2. Time taken per op is 0.116us (8.6 Mqps) with batch size of 100, of found keys 83886080 With 73400320 items, utilization is 54.69%, number of hash functions: 2. Time taken per op is 0.189us (5.3 Mqps) with batch size of 0, of found keys 73400320 With 73400320 items, utilization is 54.69%, number of hash functions: 2. Time taken per op is 0.095us (10.5 Mqps) with batch size of 10, of found keys 73400320 With 73400320 items, utilization is 54.69%, number of hash functions: 2. Time taken per op is 0.096us (10.4 Mqps) with batch size of 25, of found keys 73400320 With 73400320 items, utilization is 54.69%, number of hash functions: 2. Time taken per op is 0.098us (10.2 Mqps) with batch size of 50, of found keys 73400320 With 73400320 items, utilization is 54.69%, number of hash functions: 2. Time taken per op is 0.105us (9.5 Mqps) with batch size of 100, of found keys 73400320 rocksdb] ./cuckoo_table_reader_test Test CuckooReaderTest.WhenKeyExists Test CuckooReaderTest.WhenKeyExistsWithUint64Comparator Test CuckooReaderTest.CheckIterator Test CuckooReaderTest.CheckIteratorUint64 Test CuckooReaderTest.WhenKeyNotFound Test CuckooReaderTest.TestReadPerformance With 125829120 items, utilization is 93.75%, number of hash functions: 2. Time taken per op is 0.230us (4.3 Mqps) with batch size of 0, of found keys 125829120 With 125829120 items, utilization is 93.75%, number of hash functions: 2. Time taken per op is 0.086us (11.7 Mqps) with batch size of 10, of found keys 125829120 With 125829120 items, utilization is 93.75%, number of hash functions: 2. Time taken per op is 0.088us (11.3 Mqps) with batch size of 25, of found keys 125829120 With 125829120 items, utilization is 93.75%, number of hash functions: 2. Time taken per op is 0.083us (12.1 Mqps) with batch size of 50, of found keys 125829120 With 125829120 items, utilization is 93.75%, number of hash functions: 2. Time taken per op is 0.083us (12.1 Mqps) with batch size of 100, of found keys 125829120 With 104857600 items, utilization is 78.12%, number of hash functions: 2. Time taken per op is 0.159us (6.3 Mqps) with batch size of 0, of found keys 104857600 With 104857600 items, utilization is 78.12%, number of hash functions: 2. Time taken per op is 0.078us (12.8 Mqps) with batch size of 10, of found keys 104857600 With 104857600 items, utilization is 78.12%, number of hash functions: 2. Time taken per op is 0.080us (12.6 Mqps) with batch size of 25, of found keys 104857600 With 104857600 items, utilization is 78.12%, number of hash functions: 2. Time taken per op is 0.080us (12.5 Mqps) with batch size of 50, of found keys 104857600 With 104857600 items, utilization is 78.12%, number of hash functions: 2. Time taken per op is 0.082us (12.2 Mqps) with batch size of 100, of found keys 104857600 With 83886080 items, utilization is 62.50%, number of hash functions: 2. Time taken per op is 0.154us (6.5 Mqps) with batch size of 0, of found keys 83886080 With 83886080 items, utilization is 62.50%, number of hash functions: 2. Time taken per op is 0.077us (13.0 Mqps) with batch size of 10, of found keys 83886080 With 83886080 items, utilization is 62.50%, number of hash functions: 2. Time taken per op is 0.077us (12.9 Mqps) with batch size of 25, of found keys 83886080 With 83886080 items, utilization is 62.50%, number of hash functions: 2. Time taken per op is 0.078us (12.8 Mqps) with batch size of 50, of found keys 83886080 With 83886080 items, utilization is 62.50%, number of hash functions: 2. Time taken per op is 0.079us (12.6 Mqps) with batch size of 100, of found keys 83886080 With 73400320 items, utilization is 54.69%, number of hash functions: 2. Time taken per op is 0.218us (4.6 Mqps) with batch size of 0, of found keys 73400320 With 73400320 items, utilization is 54.69%, number of hash functions: 2. Time taken per op is 0.083us (12.0 Mqps) with batch size of 10, of found keys 73400320 With 73400320 items, utilization is 54.69%, number of hash functions: 2. Time taken per op is 0.085us (11.7 Mqps) with batch size of 25, of found keys 73400320 With 73400320 items, utilization is 54.69%, number of hash functions: 2. Time taken per op is 0.086us (11.6 Mqps) with batch size of 50, of found keys 73400320 With 73400320 items, utilization is 54.69%, number of hash functions: 2. Time taken per op is 0.078us (12.8 Mqps) with batch size of 100, of found keys 73400320 ``` Reviewers: sdong, igor, yhchiang Reviewed By: igor Subscribers: leveldb Differential Revision: Cuckoo Table Reader performance. Inlined hash function and number of buckets a power of two. Summary: Use inlined hash functions instead of function pointer. Make number of buckets a power of two and use bitwise and instead of mod. After these changes, we get almost 50% improvement in performance. Results: With 120000000 items, utilization is 89.41%, number of hash functions: 2. Time taken per op is 0.231us (4.3 Mqps) with batch size of 0 Time taken per op is 0.229us (4.4 Mqps) with batch size of 0 Time taken per op is 0.185us (5.4 Mqps) with batch size of 0 With 120000000 items, utilization is 89.41%, number of hash functions: 2. Time taken per op is 0.108us (9.3 Mqps) with batch size of 10 Time taken per op is 0.100us (10.0 Mqps) with batch size of 10 Time taken per op is 0.103us (9.7 Mqps) with batch size of 10 With 120000000 items, utilization is 89.41%, number of hash functions: 2. Time taken per op is 0.101us (9.9 Mqps) with batch size of 25 Time taken per op is 0.098us (10.2 Mqps) with batch size of 25 Time taken per op is 0.097us (10.3 Mqps) with batch size of 25 With 120000000 items, utilization is 89.41%, number of hash functions: 2. Time taken per op is 0.100us (10.0 Mqps) with batch size of 50 Time taken per op is 0.097us (10.3 Mqps) with batch size of 50 Time taken per op is 0.097us (10.3 Mqps) with batch size of 50 With 120000000 items, utilization is 89.41%, number of hash functions: 2. Time taken per op is 0.102us (9.8 Mqps) with batch size of 100 Time taken per op is 0.098us (10.2 Mqps) with batch size of 100 Time taken per op is 0.115us (8.7 Mqps) with batch size of 100 With 100000000 items, utilization is 74.51%, number of hash functions: 2. Time taken per op is 0.201us (5.0 Mqps) with batch size of 0 Time taken per op is 0.155us (6.5 Mqps) with batch size of 0 Time taken per op is 0.152us (6.6 Mqps) with batch size of 0 With 100000000 items, utilization is 74.51%, number of hash functions: 2. Time taken per op is 0.089us (11.3 Mqps) with batch size of 10 Time taken per op is 0.084us (11.9 Mqps) with batch size of 10 Time taken per op is 0.086us (11.6 Mqps) with batch size of 10 With 100000000 items, utilization is 74.51%, number of hash functions: 2. Time taken per op is 0.087us (11.5 Mqps) with batch size of 25 Time taken per op is 0.085us (11.7 Mqps) with batch size of 25 Time taken per op is 0.093us (10.8 Mqps) with batch size of 25 With 100000000 items, utilization is 74.51%, number of hash functions: 2. Time taken per op is 0.094us (10.6 Mqps) with batch size of 50 Time taken per op is 0.094us (10.7 Mqps) with batch size of 50 Time taken per op is 0.093us (10.8 Mqps) with batch size of 50 With 100000000 items, utilization is 74.51%, number of hash functions: 2. Time taken per op is 0.092us (10.9 Mqps) with batch size of 100 Time taken per op is 0.089us (11.2 Mqps) with batch size of 100 Time taken per op is 0.088us (11.3 Mqps) with batch size of 100 With 80000000 items, utilization is 59.60%, number of hash functions: 2. Time taken per op is 0.154us (6.5 Mqps) with batch size of 0 Time taken per op is 0.168us (6.0 Mqps) with batch size of 0 Time taken per op is 0.190us (5.3 Mqps) with batch size of 0 With 80000000 items, utilization is 59.60%, number of hash functions: 2. Time taken per op is 0.081us (12.4 Mqps) with batch size of 10 Time taken per op is 0.077us (13.0 Mqps) with batch size of 10 Time taken per op is 0.083us (12.1 Mqps) with batch size of 10 With 80000000 items, utilization is 59.60%, number of hash functions: 2. Time taken per op is 0.077us (13.0 Mqps) with batch size of 25 Time taken per op is 0.073us (13.7 Mqps) with batch size of 25 Time taken per op is 0.073us (13.7 Mqps) with batch size of 25 With 80000000 items, utilization is 59.60%, number of hash functions: 2. Time taken per op is 0.076us (13.1 Mqps) with batch size of 50 Time taken per op is 0.072us (13.8 Mqps) with batch size of 50 Time taken per op is 0.072us (13.8 Mqps) with batch size of 50 With 80000000 items, utilization is 59.60%, number of hash functions: 2. Time taken per op is 0.077us (13.0 Mqps) with batch size of 100 Time taken per op is 0.074us (13.6 Mqps) with batch size of 100 Time taken per op is 0.073us (13.6 Mqps) with batch size of 100 With 70000000 items, utilization is 52.15%, number of hash functions: 2. Time taken per op is 0.190us (5.3 Mqps) with batch size of 0 Time taken per op is 0.186us (5.4 Mqps) with batch size of 0 Time taken per op is 0.184us (5.4 Mqps) with batch size of 0 With 70000000 items, utilization is 52.15%, number of hash functions: 2. Time taken per op is 0.079us (12.7 Mqps) with batch size of 10 Time taken per op is 0.070us (14.2 Mqps) with batch size of 10 Time taken per op is 0.072us (14.0 Mqps) with batch size of 10 With 70000000 items, utilization is 52.15%, number of hash functions: 2. Time taken per op is 0.080us (12.5 Mqps) with batch size of 25 Time taken per op is 0.072us (14.0 Mqps) with batch size of 25 Time taken per op is 0.071us (14.1 Mqps) with batch size of 25 With 70000000 items, utilization is 52.15%, number of hash functions: 2. Time taken per op is 0.082us (12.1 Mqps) with batch size of 50 Time taken per op is 0.071us (14.1 Mqps) with batch size of 50 Time taken per op is 0.073us (13.6 Mqps) with batch size of 50 With 70000000 items, utilization is 52.15%, number of hash functions: 2. Time taken per op is 0.080us (12.5 Mqps) with batch size of 100 Time taken per op is 0.077us (13.0 Mqps) with batch size of 100 Time taken per op is 0.078us (12.8 Mqps) with batch size of 100 Test Plan: make check all make valgrind_check make asan_check Reviewers: sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: a user comparator for comparing Uint64 slices. Summary: New Uint64 comparator Modify Reader and Builder to take custom user comparators instead of bytewise comparator Modify logic for choosing unused user key in builder Modify iterator logic in reader test changes Test Plan: cuckoo_table_{builder,reader,db}_test make check all Reviewers: ljin, sdong Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: Prepare method in CuckooTableReader Summary: Implement Prepare method Rewrite performance tests in cuckoo_table_reader_test to write new file only if one doesnt already exist. Add performance tests for batch lookup along with prefetching. Test Plan: ./cuckoo_table_reader_test Results (We get better results if we used int64 comparator instead of string comparator (TBD in future diffs)): With 100000000 items and hash table ratio 0.500000, number of hash functions used: 2. Time taken per op is 0.208us (4.8 Mqps) with batch size of 0 With 100000000 items and hash table ratio 0.500000, number of hash functions used: 2. Time taken per op is 0.182us (5.5 Mqps) with batch size of 10 With 100000000 items and hash table ratio 0.500000, number of hash functions used: 2. Time taken per op is 0.161us (6.2 Mqps) with batch size of 25 With 100000000 items and hash table ratio 0.500000, number of hash functions used: 2. Time taken per op is 0.161us (6.2 Mqps) with batch size of 50 With 100000000 items and hash table ratio 0.500000, number of hash functions used: 2. Time taken per op is 0.163us (6.1 Mqps) with batch size of 100 With 100000000 items and hash table ratio 0.600000, number of hash functions used: 3. Time taken per op is 0.252us (4.0 Mqps) with batch size of 0 With 100000000 items and hash table ratio 0.600000, number of hash functions used: 3. Time taken per op is 0.192us (5.2 Mqps) with batch size of 10 With 100000000 items and hash table ratio 0.600000, number of hash functions used: 3. Time taken per op is 0.195us (5.1 Mqps) with batch size of 25 With 100000000 items and hash table ratio 0.600000, number of hash functions used: 3. Time taken per op is 0.191us (5.2 Mqps) with batch size of 50 With 100000000 items and hash table ratio 0.600000, number of hash functions used: 3. Time taken per op is 0.194us (5.1 Mqps) with batch size of 100 With 100000000 items and hash table ratio 0.750000, number of hash functions used: 3. Time taken per op is 0.228us (4.4 Mqps) with batch size of 0 With 100000000 items and hash table ratio 0.750000, number of hash functions used: 3. Time taken per op is 0.185us (5.4 Mqps) with batch size of 10 With 100000000 items and hash table ratio 0.750000, number of hash functions used: 3. Time taken per op is 0.186us (5.4 Mqps) with batch size of 25 With 100000000 items and hash table ratio 0.750000, number of hash functions used: 3. Time taken per op is 0.189us (5.3 Mqps) with batch size of 50 With 100000000 items and hash table ratio 0.750000, number of hash functions used: 3. Time taken per op is 0.188us (5.3 Mqps) with batch size of 100 With 100000000 items and hash table ratio 0.900000, number of hash functions used: 3. Time taken per op is 0.325us (3.1 Mqps) with batch size of 0 With 100000000 items and hash table ratio 0.900000, number of hash functions used: 3. Time taken per op is 0.196us (5.1 Mqps) with batch size of 10 With 100000000 items and hash table ratio 0.900000, number of hash functions used: 3. Time taken per op is 0.199us (5.0 Mqps) with batch size of 25 With 100000000 items and hash table ratio 0.900000, number of hash functions used: 3. Time taken per op is 0.196us (5.1 Mqps) with batch size of 50 With 100000000 items and hash table ratio 0.900000, number of hash functions used: 3. Time taken per op is 0.209us (4.8 Mqps) with batch size of 100 Reviewers: sdong, yhchiang, igor, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision:"
1092,1092,10.0,0.32589998841285706,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Turn on and fix all the errors Summary: We need to turn on for mobile. See D1671432 (internal phabricator) for details. This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables. Test Plan: compiles Reviewers: ljin, rven, yhchiang, sdong Reviewed By: yhchiang Subscribers: bobbaldwin, dhruba, leveldb Differential Revision: EventListener and GetDatabaseMetaData Summary: This diff adds three sets of APIs to RocksDB. GetColumnFamilyMetaData * This APIs allow users to obtain the current state of a RocksDB instance on one column family. * See GetColumnFamilyMetaData in include/rocksdb/db.h EventListener * A virtual class that allows users to implement a set of call-back functions which will be called when specific events of a RocksDB instance happens. * To register EventListener, simply insert an EventListener to ColumnFamilyOptions::listeners CompactFiles * CompactFiles API inputs a set of file numbers and an output level, and RocksDB will try to compact those files into the specified level. Example * Example code can be found in example/compact_files_example.cc, which implements a simple external compactor using EventListener, GetColumnFamilyMetaData, and CompactFiles API. Test Plan: listener_test compactor_test example/compact_files_example export ROCKSDB_TESTS=CompactFiles db_test export ROCKSDB_TESTS=MetaData db_test Reviewers: ljin, igor, rven, sdong Reviewed By: sdong Subscribers: MarkCallaghan, dhruba, leveldb Differential Revision: pending_outputs_ Summary: Heres a prototype of redesigning pending_outputs_. This way, we dont have to expose pending_outputs_ to other classes (CompactionJob, FlushJob, MemtableList). DBImpl takes care of it. Still have to write some comments, but should be good enough to start the discussion. Test Plan: make check, will also run stress test Reviewers: ljin, sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: InfoLogLevel to the logs in db/db_impl.cc Summary: Apply InfoLogLevel to the logs in db/db_impl.cc Test Plan: db_test db_bench Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: leveldb, MarkCallaghan, dhruba Differential Revision: naming convention of getters in version_set.h Summary: Enforce the accessier naming convention in functions in version_set.h Test Plan: make all check Reviewers: ljin, yhchiang, rven, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: on Summary: ...and fix all the errors :) Jim suggested turning on because it helped him fix number of critical bugs in fbcode. I think its a good idea to be clean. Test Plan: compiles Reviewers: yhchiang, rven, sdong, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: VersionBuilder unit testable Summary: Rename Version::Builder to VersionBuilder and expose its definition to a header. Make VerisonBuilder not reference Version or ColumnFamilyData, only working with VersionStorageInfo. Add version_builder_test which has a simple test. Test Plan: make all check Reviewers: rven, yhchiang, igor, ljin Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: Summary: Decoupling code that deals with archived log files outside of DBImpl. That will make this code easier to reason about and test. It will also make the code easier to improve, because an improver doesnt have to understand DBImpl code in entirety. Test Plan: added test Reviewers: ljin, yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: CompactionPicker more easily tested Summary: Make compaction picker easier to test. The basic idea is to separate a minimum subcomponent of Version to VersionStorageInfo, which just responsible to LSM tree. A stub VersionStorageInfo can then be easily created and passed into compaction picker so that we can check the outputs. It now passes most tests. Still two things need to be done: (1) deal with the FIFO compactions file size. (2) write an example test to make sure the interface can do the job. Add a compaction_picker_test to make sure compaction picker codes can be easily unit tested. Test Plan: Pass all unit tests and compaction_picker_test Reviewers: yhchiang, rven, igor, ljin Reviewed By: ljin Subscribers: leveldb, dhruba Differential Revision: Summary: Abstract out FlushProcess and take it out of DBImpl. This also includes taking DeletionState outside of DBImpl. Currently this diff is only doing the refactoring. Future work includes: 1. Decoupling flush_process.cc, make it depend on less state 2. Write flush_process_test, which will mock out everything that FlushProcess depends on and test it in isolation Test Plan: make check Reviewers: rven, yhchiang, sdong, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: inplace_update options Summary: Make inplace_update_support and inplace_update_num_locks dynamic. inplace_callback becomes immutable We are almost free of references to cfd->options() in db_impl Test Plan: unit test Reviewers: igor, yhchiang, rven, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: max_sequential_skip_in_iterations Summary: This is not a critical options. Making it dynamic so that we can remove more reference to cfd->options() Test Plan: unit test Reviewers: yhchiang, sdong, igor Reviewed By: igor Subscribers: leveldb Differential Revision: number of keys in DB Stats Summary: It is useful to print out number of keys in DB Stats Test Plan: ./db_bench fillrandom 1000000 16 and watch the outputs in LOG files Reviewers: MarkCallaghan, ljin, yhchiang, igor Reviewed By: igor Subscribers: leveldb Differential Revision: MutableCFOptions in SetOptions Summary: as title Test Plan: make release Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: leveldb Differential Revision: disable_auto_compactions Summary: Add more tests as well Test Plan: unit test Reviewers: igor, sdong, yhchiang Reviewed By: sdong Subscribers: leveldb Differential Revision: max_write_buffer_number dynamic Summary: as title Test Plan: unit test Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: stopping writes on bg_error_ Summary: This might have caused If were stopping writes and bg_error comes along, we will never unblock the write. Test Plan: compiles Reviewers: ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: RocksDB version Summary: This will be much easier than reviewing git shas we currently have in our LOGs Test Plan: none Reviewers: sdong, yhchiang, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: compaction related options changeable Summary: make compaction related options changeable. Most of changes are tedious, following the same convention: grabs MutableCFOptions at the beginning of compaction under mutex, then pass it throughout the job and register it in SuperVersion at the end. Test Plan: make all check Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: fix object handling, remove double lines Fix for: [db/db_impl.cc:4039]: (error) Instance of StopWatch object is destroyed immediately. [db/db_impl.cc:4042]: (error) Instance of StopWatch object is destroyed immediately. Signed-off-by: Danny Al-Gaaf allow_thread_local Summary: See Test Plan: compiles Reviewers: sdong, yhchiang, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: for memtable related options Summary: as title Test Plan: make all check I will think a way to set up stress test for this Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: leveldb Differential Revision: Summary: This diff just moves the write thread control out of the DBImpl. I will need this as I will control column family data concurrency by only accessing some data in the write thread. That way, we wont have to lock our accesses to column family hash table (mappings from IDs to CFDs). Test Plan: make check Reviewers: sdong, yhchiang, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: WAL synced Summary: Uhm... Test Plan: nope Reviewers: sdong, yhchiang, tnovak, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: run background jobs (flush, compactions) when bg_error_ is set Summary: If bg_error_ is set, that means that we mark DB read only. However, current behavior still continues the flushes and compactions, even though bg_error_ is set. On the other hand, if bg_error_ is set, we will return Status::OK() from CompactRange(), although the compaction didnt actually succeed. This is clearly not desired behavior. I found this when I was debugging t5132159, although Im pretty sure these arent related. Also, when were shutting down, its dangerous to exit RunManualCompaction(), since that will destruct ManualCompaction object. Background compaction job might still hold a reference to manual_compaction_ and this will lead to undefined behavior. I changed the behavior so that we only exit RunManualCompaction when manual compaction job is marked done. Test Plan: make check Reviewers: sdong, ljin, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision: model for flushing memtables Summary: When memtable is full it calls the registered callback. That callback then registers column family as needing the flush. Every write checks if there are some column families that need to be flushed. This completely eliminates the need for MakeRoomForWrite() function and simplifies our Write code-path. There is some complexity with the concurrency when the column family is dropped. I made it a bit less complex by dropping the column family from the write thread in Let me know if you want to discuss this. Test Plan: make check works. Ill also run db_stress with creating and dropping column families for a while. Reviewers: yhchiang, sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: db recovery Summary: Avoid creating unnecessary sst files while db opening Test Plan: make all check Reviewers: sdong, igor Reviewed By: igor Subscribers: zagfox, yhchiang, ljin, leveldb Differential Revision: references to cfd->options() in DBImpl Summary: I found it is almost impossible to get rid of this function in a single batch. I will take a step by step approach Test Plan: make release Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: leveldb Differential Revision: Do not wait for background threads when there is nothing in mem table Summary: When we have multiple column families, users can issue Flush() on every column families to make sure everything is flushes, even if some of them might be empty. By skipping the waiting for empty cases, it can be greatly speed up. Still wait for peoples comments before writing unit tests for it. Test Plan: Will write a unit test to make sure it is correct. Reviewers: ljin, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: instead of pull-model for managing Write stalls Summary: Introducing WriteController, which is a source of truth about per-DB write delays. Lets define an DB epoch as a period where there are no flushes and compactions (i.e. new epoch is started when flush or compaction finishes). Each epoch can either: * proceed with all writes without delay * delay all writes by fixed time * stop all writes The three modes are recomputed at each epoch change (flush, compaction), rather than on every write (which is currently the case). When we have a lot of column families, our current pull behavior adds a big overhead, since we need to loop over every column family for every write. With new push model, overhead on Write code-path is minimal. This is just the start. Next step is to also take care of stalls introduced by slow memtable flushes. The final goal is to eliminate function MakeRoomForWrite(), which currently needs to be called for every column family by every write. Test Plan: make check for now. Ill add some unit tests later. Also, perf test. Reviewers: dhruba, yhchiang, MarkCallaghan, sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: column family from write thread Summary: If we drop column family only from (single) write thread, we can be sure that nobody will drop the column family while were writing (and our mutex is released). This greatly simplifies my patch thats getting rid of MakeRoomForWrite(). Test Plan: make check, but also running stress test Reviewers: ljin, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: options_ to db_options_ in DBImpl to avoid confusion Summary: as title Test Plan: make release Reviewers: sdong, igor Reviewed By: igor Subscribers: leveldb Differential Revision: path with arena==nullptr from NewInternalIterator Summary: Simply code by removing code path which does not use Arena from NewInternalIterator Test Plan: make all check make valgrind_check Reviewers: sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: ImmutableOptions Summary: As a preparation to support updating some options dynamically, Id like to first introduce ImmutableOptions, which is a subset of Options that cannot be changed during the course of a DB lifetime without restart. ColumnFamily will keep both Options and ImmutableOptions. Any component below ColumnFamily should only take ImmutableOptions in their constructor. Other options should be taken from APIs, which will be allowed to adjust dynamically. I am yet to make changes to memtable and other related classes to take ImmutableOptions in their ctor. That can be done in a seprate diff as this one is already pretty big. Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: SanitizeDBOptionsByCFOptions() in the right place Summary: It only covers Open() with default column family right now Test Plan: make release Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: missing column families Summary: Before this diff, whenever we Write to non-existing column family, Write() would fail. This diff adds an option to not fail a Write() when WriteBatch points to non-existing column family. MongoDB said this would be useful for them, since they might have a transaction updating an index that was dropped by another thread. This way, they dont have to worry about checking if all indexes are alive on every write. They dont care if they lose writes to dropped index. Test Plan: added a small unit test Reviewers: sdong, yhchiang, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: PerfStepTimer to stop on destruct This eliminates the need to remember to call PERF_TIMER_STOP when a section has been timed. This allows more useful design with the perf timers and enables possible return value optimizations. Simplistic example: class Foo { public: Foo(int v) : m_v(v); private: int m_v; } Foo makeFrobbedFoo(int *errno) { *errno 0; return Foo(); } Foo bar(int *errno) { PERF_TIMER_GUARD(some_timer); return makeFrobbedFoo(errno); } int main(int argc, char[] argv) { Foo f; int errno; f bar(&errno); if (errno) return return 0; } After bar() is called, perf_context.some_timer would be incremented as if Stop(&perf_context.some_timer) was called at the end, and the compiler is still able to produce optimizations on the return value from makeFrobbedFoo() through to main()./Dont let flush preempt compaction in certain cases Summary: I have an application configured with 16 background threads. Write rates are high. L0->L1 compactions is very slow and it limits the concurrency of the system. While its happening, other 15 threads are idle. However, when there is a need of a flush, that one thread busy with L0->L1 is doing flush, instead of any other 15 threads that are just sitting there. This diff prevents that. If there are threads that are idle, we dont let flush preempt compaction. Test Plan: Will run stress test Reviewers: ljin, sdong, yhchiang Reviewed By: sdong, yhchiang Subscribers: dhruba, leveldb Differential Revision: let other compactions run when manual compaction runs Summary: Based on discussions from t4982833. This is just a short-term fix, I plan to revamp manual compaction process as part of t4982812. Also, I think we should schedule automatic compactions at the very end of manual compactions, not when were done with one level. I made that change as part of this diff. Let me know if you disagree. Test Plan: make check for now Reviewers: sdong, tnovak, yhchiang, ljin Reviewed By: yhchiang Subscribers: leveldb Differential Revision: ios compile Summary: No __thread for ios. Test Plan: compile works for ios now Reviewers: ljin, dhruba Reviewed By: dhruba Subscribers: leveldb Differential Revision: is-file-deletions-enabled property Summary: Add property rocksdb.is-file-deletions-enable which equals disable_delete_obsole_file_ Test Plan: make all check Reviewers: sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: block based table related options BlockBasedTableOptions Summary: I will move compression related options in a separate diff since this diff is already pretty lengthy. I guess I will also need to change JNI accordingly :( Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: igor Subscribers: leveldb Differential Revision: Options sanitization and add MmapReadRequired() to TableFactory Summary: Currently, PlainTable must use mmap_reads. When PlainTable is used but allow_mmap_reads is not set, rocksdb will fail in flush. This diff improve Options sanitization and add MmapReadRequired() to TableFactory. Test Plan: export ROCKSDB_TESTS=PlainTableOptionsSanitizeTest make db_test ./db_test Reviewers: sdong, ljin Reviewed By: ljin Subscribers: you, leveldb Differential Revision: purging logs from separate log directory Summary: 1. Support purging info logs from a separate paths from DB path. Refactor the codes of generating info log prefixes so that it can be called when generating new files and scanning log directory. 2. Fix the bug of not scanning multiple DB paths (should only impact multiple DB paths) Test Plan: Add unit test for generating and parsing info log files Add end-to-end test in db_test Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: leveldb, igor, dhruba Differential Revision:"
1093,1093,5.0,0.9789000153541565,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Turn on and fix all the errors Summary: We need to turn on for mobile. See D1671432 (internal phabricator) for details. This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables. Test Plan: compiles Reviewers: ljin, rven, yhchiang, sdong Reviewed By: yhchiang Subscribers: bobbaldwin, dhruba, leveldb Differential Revision: allow_thread_local Summary: See Test Plan: compiles Reviewers: sdong, yhchiang, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: block based table related options BlockBasedTableOptions Summary: I will move compression related options in a separate diff since this diff is already pretty lengthy. I guess I will also need to change JNI accordingly :( Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: igor Subscribers: leveldb Differential Revision:"
1094,1094,12.0,0.6614999771118164,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Turn on and fix all the errors Summary: We need to turn on for mobile. See D1671432 (internal phabricator) for details. This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables. Test Plan: compiles Reviewers: ljin, rven, yhchiang, sdong Reviewed By: yhchiang Subscribers: bobbaldwin, dhruba, leveldb Differential Revision: compile error in db/compaction.cc and db/compaction_picker.cc Summary: Fixed compile error in db/compaction.cc and db/compaction_picker.cc Test Plan: make/CompactFiles, EventListener and GetDatabaseMetaData Summary: This diff adds three sets of APIs to RocksDB. GetColumnFamilyMetaData * This APIs allow users to obtain the current state of a RocksDB instance on one column family. * See GetColumnFamilyMetaData in include/rocksdb/db.h EventListener * A virtual class that allows users to implement a set of call-back functions which will be called when specific events of a RocksDB instance happens. * To register EventListener, simply insert an EventListener to ColumnFamilyOptions::listeners CompactFiles * CompactFiles API inputs a set of file numbers and an output level, and RocksDB will try to compact those files into the specified level. Example * Example code can be found in example/compact_files_example.cc, which implements a simple external compactor using EventListener, GetColumnFamilyMetaData, and CompactFiles API. Test Plan: listener_test compactor_test example/compact_files_example export ROCKSDB_TESTS=CompactFiles db_test export ROCKSDB_TESTS=MetaData db_test Reviewers: ljin, igor, rven, sdong Reviewed By: sdong Subscribers: MarkCallaghan, dhruba, leveldb Differential Revision: naming convention of getters in version_set.h Summary: Enforce the accessier naming convention in functions in version_set.h Test Plan: make all check Reviewers: ljin, yhchiang, rven, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: on Summary: ...and fix all the errors :) Jim suggested turning on because it helped him fix number of critical bugs in fbcode. I think its a good idea to be clean. Test Plan: compiles Reviewers: yhchiang, rven, sdong, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: CompactionPicker more easily tested Summary: Make compaction picker easier to test. The basic idea is to separate a minimum subcomponent of Version to VersionStorageInfo, which just responsible to LSM tree. A stub VersionStorageInfo can then be easily created and passed into compaction picker so that we can check the outputs. It now passes most tests. Still two things need to be done: (1) deal with the FIFO compactions file size. (2) write an example test to make sure the interface can do the job. Add a compaction_picker_test to make sure compaction picker codes can be easily unit tested. Test Plan: Pass all unit tests and compaction_picker_test Reviewers: yhchiang, rven, igor, ljin Reviewed By: ljin Subscribers: leveldb, dhruba Differential Revision: InfoLogLevel to the logs in db/compaction_picker.cc Summary: Apply InfoLogLevel to the logs in db/compaction_picker.cc Test Plan: make Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: UniversalCompactionPicker,LevelCompactionPicker and FIFOCompactionPicker from VersionSet Summary: as title Test Plan: make release I will run make all check for all stacked diffs before commit Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: compaction related options changeable Summary: make compaction related options changeable. Most of changes are tedious, following the same convention: grabs MutableCFOptions at the beginning of compaction under mutex, then pass it throughout the job and register it in SuperVersion at the end. Test Plan: make all check Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: compaction picker: use double for potential overflow Summary: There is a possible overflow case in universal compaction picker. Use double to make the logic straight-forward Test Plan: make all check Reviewers: yhchiang, igor, MarkCallaghan, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: concurrency issue in CompactionPicker Summary: I am currently working on a project that uses RocksDB. While debugging some perf issues, I came up across interesting compaction concurrency issue. Namely, I had 15 idle threads and a good comapction to do, but CompactionPicker returned ""Compaction nothing to do"". Heres how Internal stats looked: 2014/08/22-08:08:04.551982 7fc7fc3f5700 DUMPING STATS 2014/08/22-08:08:04.552000 7fc7fc3f5700 ** Compaction Stats [default] ** Level Files Size(MB) Score Read(GB) Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) RW-Amp W-Amp Rd(MB/s) Wr(MB/s) Rn(cnt) Rnp1(cnt) Wnp1(cnt) Wnew(cnt) Comp(sec) Comp(cnt) Avg(sec) Stall(sec) Stall(cnt) Avg(ms) L0 7/5 353 1.0 0.0 0.0 0.0 2.3 2.3 0.0 0.0 0.0 9.4 0 0 0 0 247 46 5.359 8.53 1 8526.25 L1 2/2 86 1.3 2.6 1.9 0.7 2.6 1.9 2.7 1.3 24.3 24.0 39 19 71 52 109 11 9.938 0.00 0 0.00 L2 26/0 833 1.3 5.7 1.7 4.0 5.2 1.2 6.3 3.0 15.6 14.2 47 112 147 35 373 44 8.468 0.00 0 0.00 L3 12/0 505 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0.000 0.00 0 0.00 Sum 47/7 1778 0.0 8.3 3.6 4.6 10.0 5.4 8.1 4.4 11.6 14.1 86 131 218 87 728 101 7.212 8.53 1 8526.25 Int 0/0 0 0.0 2.4 0.8 1.6 2.7 1.2 11.5 6.1 12.0 13.6 20 43 63 20 203 23 8.845 0.00 0 0.00 Flush(GB): accumulative 2.266, interval 0.444 Stalls(secs): 0.000 level0_slowdown, 0.000 level0_numfiles, 8.526 memtable_compaction, 0.000 leveln_slowdown_soft, 0.000 leveln_slowdown_hard Stalls(count): 0 level0_slowdown, 0 level0_numfiles, 1 memtable_compaction, 0 leveln_slowdown_soft, 0 leveln_slowdown_hard ** DB Stats ** Uptime(secs): 336.8 total, 60.4 interval Cumulative writes: 61584000 writes, 6480589 batches, 9.5 writes per batch, 1.39 GB user ingest Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, 0.00 GB written Interval writes: 11235257 writes, 1175050 batches, 9.6 writes per batch, 259.9 MB user ingest Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, 0.00 MB written To see what happened, go here: * The for loop started with level 1, because it has the worst score. * PickCompactionBySize on L429 returned nullptr because all files were being compacted * ExpandWhileOverlapping(c) returned true (because thats what it does when it gets nullptr?) * for loop break-ed, never trying compactions for level 2 :( :( This bug was present at least since January. I have no idea how we didnt find this sooner. Test Plan: Unit testing compaction picker is hard. I tested this by running my service and observing L0->L1 and L2->L3 compactions in parallel. However, for long-term, I opened the task is currently refactoring CompactionPicker, hopefully the new version will be unit-testable ;) Heres how my compactions look like after the patch: 2014/08/22-08:50:02.166699 7f3400ffb700 DUMPING STATS 2014/08/22-08:50:02.166722 7f3400ffb700 ** Compaction Stats [default] ** Level Files Size(MB) Score Read(GB) Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) RW-Amp W-Amp Rd(MB/s) Wr(MB/s) Rn(cnt) Rnp1(cnt) Wnp1(cnt) Wnew(cnt) Comp(sec) Comp(cnt) Avg(sec) Stall(sec) Stall(cnt) Avg(ms) L0 8/5 404 1.5 0.0 0.0 0.0 4.3 4.3 0.0 0.0 0.0 9.6 0 0 0 0 463 88 5.260 0.00 0 0.00 L1 2/2 60 0.9 4.8 3.9 0.8 4.7 3.9 2.4 1.2 23.9 23.6 80 23 131 108 204 19 10.747 0.00 0 0.00 L2 23/3 697 1.0 11.6 3.5 8.1 10.9 2.8 6.4 3.1 17.7 16.6 95 242 317 75 669 92 7.268 0.00 0 0.00 L3 58/14 2207 0.3 6.2 1.6 4.6 5.9 1.3 7.4 3.6 14.6 13.9 43 121 159 38 436 36 12.106 0.00 0 0.00 Sum 91/24 3368 0.0 22.5 9.1 13.5 25.8 12.4 11.2 6.0 13.0 14.9 218 386 607 221 1772 235 7.538 0.00 0 0.00 Int 0/0 0 0.0 3.2 0.9 2.3 3.6 1.3 15.3 8.0 12.4 13.7 24 66 89 23 266 27 9.838 0.00 0 0.00 Flush(GB): accumulative 4.336, interval 0.444 Stalls(secs): 0.000 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 0.000 leveln_slowdown_soft, 0.000 leveln_slowdown_hard Stalls(count): 0 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 0 leveln_slowdown_soft, 0 leveln_slowdown_hard ** DB Stats ** Uptime(secs): 577.7 total, 60.1 interval Cumulative writes: 116960736 writes, 11966220 batches, 9.8 writes per batch, 2.64 GB user ingest Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, 0.00 GB written Interval writes: 11643735 writes, 1206136 batches, 9.7 writes per batch, 269.2 MB user ingest Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, 0.00 MB written Yay for concurrent L0->L1 and L2->L3 compactions Reviewers: sdong, yhchiang, ljin Reviewed By: yhchiang Subscribers: yhchiang, leveldb Differential Revision:"
1095,1095,0.0,0.949999988079071,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Turn on Summary: ...and fix all the errors :) Jim suggested turning on because it helped him fix number of critical bugs in fbcode. I think its a good idea to be clean. Test Plan: compiles Reviewers: yhchiang, rven, sdong, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision:"
1096,1096,10.0,0.5544000267982483,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Turn on and fix all the errors Summary: We need to turn on for mobile. See D1671432 (internal phabricator) for details. This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables. Test Plan: compiles Reviewers: ljin, rven, yhchiang, sdong Reviewed By: yhchiang Subscribers: bobbaldwin, dhruba, leveldb Differential Revision: EventListener and GetDatabaseMetaData Summary: This diff adds three sets of APIs to RocksDB. GetColumnFamilyMetaData * This APIs allow users to obtain the current state of a RocksDB instance on one column family. * See GetColumnFamilyMetaData in include/rocksdb/db.h EventListener * A virtual class that allows users to implement a set of call-back functions which will be called when specific events of a RocksDB instance happens. * To register EventListener, simply insert an EventListener to ColumnFamilyOptions::listeners CompactFiles * CompactFiles API inputs a set of file numbers and an output level, and RocksDB will try to compact those files into the specified level. Example * Example code can be found in example/compact_files_example.cc, which implements a simple external compactor using EventListener, GetColumnFamilyMetaData, and CompactFiles API. Test Plan: listener_test compactor_test example/compact_files_example export ROCKSDB_TESTS=CompactFiles db_test export ROCKSDB_TESTS=MetaData db_test Reviewers: ljin, igor, rven, sdong Reviewed By: sdong Subscribers: MarkCallaghan, dhruba, leveldb Differential Revision: pending_outputs_ Summary: Heres a prototype of redesigning pending_outputs_. This way, we dont have to expose pending_outputs_ to other classes (CompactionJob, FlushJob, MemtableList). DBImpl takes care of it. Still have to write some comments, but should be good enough to start the discussion. Test Plan: make check, will also run stress test Reviewers: ljin, sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: PartialCompactionFailure Test more robust again. Summary: Make PartialCompactionFailure Test more robust again by blocking background compaction until we simulate the file creation error. Test Plan: export ROCKSDB_TESTS=PartialCompactionFailure ./db_test Reviewers: sdong, igor, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: PartialCompactionFailure Test more robust. Summary: Make PartialCompactionFailure Test more robust. Test Plan: export ROCKSDB_TESTS=PartialCompactionFailure ./db_test Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: back on Summary: It turns out that has different rules for gcc than clang. Previous commit fixed clang. This commits fixes the rest of the warnings for gcc. Test Plan: compiles Reviewers: ljin, yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: DBTest.GroupCommitTest: artificially slowdown log writing to trigger group commit Summary: In order to avoid random failure of DBTest.GroupCommitTest, artificially sleep 100 microseconds in each log writing. Test Plan: Run the test in a machine where valgrind version of the test always fails multiple times and see it always succeed. Reviewers: igor, yhchiang, rven, ljin Reviewed By: ljin Subscribers: leveldb, dhruba Differential Revision: sure WAL is synced for DB::Write() if write batch is empty Summary: This patch makes it a contract that if an empty write batch is passed to DB::Write() and WriteOptions.sync true, fsync is called to WAL. Test Plan: A new unit test Reviewers: ljin, rven, yhchiang, igor Reviewed By: igor Subscribers: dhruba, MarkCallaghan, leveldb Differential Revision: to use single background compaction Summary: Now DBTest.DynamicMemtableOptions sets background compaction to be 4, without actually increasing thread pool size (even before the feature of automatic increasing it). To make sure the behavior stays the same after the automatic thread pool increasing, set it back to 1. Hopefully it can fix the occasional failure of the test. Test Plan: Run the test Reviewers: igor, ljin Reviewed By: ljin Subscribers: leveldb, dhruba Differential Revision: to automatically increase thread pool size if it is smaller than max number of parallel compactions or flushes Summary: With the patch, thread pool size will be automatically increased if DBs options ask for more parallelism of compactions or flushes. Too many users have been confused by the API. Change it to make it harder for users to make mistakes Test Plan: Add two unit tests to cover the function. Reviewers: yhchiang, rven, igor, MarkCallaghan, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: TestMemEnv and use it in db_test Summary: TestMemEnv simulates all Env APIs using in-memory data structures. We can use it to speed up db_test run, which is now reduced ~7mins when it is enabled. We can also add features to simulate power/disk failures in the next step TestMemEnv is derived from helper/mem_env mem_env can not be used for rocksdb since some of its APIs do not give the same results as env_posix. And its file read/write is not thread safe Test Plan: make all ./db_test ./env_mem_test Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: remove default value for ChangeFilterOptions() and ChangeCompactionOptions() Summary: So now all open() in db_test should get options from callsite. And destroy() always uses the last used options saved on open() I will start to integrate env_mem in the next diff Test Plan: make all check Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: options clean up part 4 Summary: as title Test Plan: as part 1 Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: options clean up part 3 Summary: as title Test Plan: same as part 1 Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: options clean up part 2 Summary: as title Test Plan: same as part 1 Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: options clean up part 1 Summary: DBTest has several functions (Reopen(), TryReopen(), ChangeOptins(), etc that takes a pointer to options), depending on if it is nullptr, it uses different options underneath. This makes it really hard to track what options is used in different test case. We should just kill the default value and make it being passed into explicitly. It is going to be very hairy. I will start with simple ones. Test Plan: make db_test stacked diffs, will run test with full stack Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: the robustness of PartialCompactionFailure test again. Summary: Improve the robustness of PartialCompactionFailure test again. Test Plan: ./db_test/Improve the robustnesss of PartialCompactionFailure test. Summary: Improve the robustness of PartialCompactionFailure test. Test Plan: ./db_test/Fix the bug where compaction does not fail when RocksDB cant create a new file. Summary: This diff has two fixes. 1. Fix the bug where compaction does not fail when RocksDB cant create a new file. 2. When NewWritableFiles() fails in OpenCompactionOutputFiles(), previously such fail-to-created file will be still be included as a compaction output. This patch also fixes this bug. 3. Allow VersionEdit::EncodeTo() to return Status and add basic check. Test Plan: ./version_edit_test export ROCKSDB_TESTS=FileCreationRandomFailure ./db_test Reviewers: ljin, sdong, nkg-, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: AtomicPointer Summary: RocksDB already depends on C++11, so we might as well all the goodness that C++11 provides. This means that we dont need AtomicPointer anymore. The less things in port/, the easier it will be to port to other platforms. Test Plan: make check + careful visual review verifying that NoBarried got memory_order_relaxed, while Acquire/Release methods got memory_order_acquire and memory_order_release Reviewers: rven, yhchiang, ljin, sdong Reviewed By: ljin Subscribers: leveldb Differential Revision: max_sequential_skip_in_iterations Summary: This is not a critical options. Making it dynamic so that we can remove more reference to cfd->options() Test Plan: unit test Reviewers: yhchiang, sdong, igor Reviewed By: igor Subscribers: leveldb Differential Revision: SIGSEGV when declaring Arena after ScopedArenaIterator/Sanitize block-based table index type and check prefix_extractor Summary: Respond to issue reported Change the Sanitize signature to take both DBOptions and CFOptions Test Plan: unit test Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: leveldb Differential Revision: DynamicMemtableOptions test Summary: as title Test Plan: make release Reviewers: igor Reviewed By: igor Subscribers: leveldb Differential Revision: build failure Summary: missed default value during merge Test Plan: ./db_test Reviewers: igor, sdong, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision: soft_rate_limit and hard_rate_limit Summary: as title Test Plan: unit test I am only able to build the test case for hard_rate_limit. soft_rate_limit is essentially the same thing as hard_rate_limit Reviewers: igor, sdong, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision: disable_auto_compactions Summary: Add more tests as well Test Plan: unit test Reviewers: igor, sdong, yhchiang Reviewed By: sdong Subscribers: leveldb Differential Revision: max_write_buffer_number dynamic Summary: as title Test Plan: unit test Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: compile error on Mac: default arguments for lambda expressions Summary: Fixed the following compile error on Mac. db/db_test.cc:8618:52: error: C++11 forbids default arguments for lambda expressions [-Werror,-Wlambda-extensions] auto gen_l0_kb [this](int start, int size, int stride 1) { ^ ~ 1 error generated. Test Plan: db_test/add db_test for changing memtable size Summary: The test only covers changing write_buffer_size. Other changable parameters such bloom bits/probes are not obvious how to test. Suggestions are welcome Test Plan: db_test Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: leveldb Differential Revision: compaction related options changeable Summary: make compaction related options changeable. Most of changes are tedious, following the same convention: grabs MutableCFOptions at the beginning of compaction under mutex, then pass it throughout the job and register it in SuperVersion at the end. Test Plan: make all check Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: for better CuckooTable performance Summary: Add the MultiGet API to allow prefetching. With file size of 1.5G, I configured it to have 0.9 hash ratio that can fill With 115M keys and result in 2 hash functions, the lookup QPS is ~4.9M/s vs. 3M/s for Get(). It is tricky to set the parameters right. Since files size is determined by power-of-two factor, that means of keys is fixed in each file. With big file size (thus smaller of files), we will have more chance to waste lot of space in the last file lower space utilization as a result. Using smaller file size can improve the situation, but that harms lookup speed. Test Plan: db_bench Reviewers: yhchiang, sdong, igor Reviewed By: sdong Subscribers: leveldb Differential Revision: Summary: Add a CompactedDBImpl that will enabled when calling OpenForReadOnly() and the DB only has one level (>0) of files. As a performan comparison, CuckooTable performs 2.1M/s with CompactedDBImpl vs. 1.78M/s with ReadOnlyDBImpl. Test Plan: db_bench Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: unit tests errors Summary: Those were introduced with because the flushing behavior changed when max_background_flushes is > 0. Test Plan: make check Reviewers: ljin, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: use_mmap_reads Summary: We currently dont test mmap reads as part of db_test. Piggyback it on kWalDir test config. Test Plan: make check Reviewers: ljin, sdong, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision: sync manifest when disableDataSync true Summary: As we discussed offline Test Plan: compiles Reviewers: yhchiang, sdong, ljin, dhruba Reviewed By: sdong Subscribers: leveldb Differential Revision: Summary: 1. wrap a filter policy like what fbcode/multifeed/rocksdb/MultifeedRocksDbKey.h to ensure that rocksdb works fine after filterpolicy interface change Test Plan: 1. valgrind ./bloom_test Reviewers: ljin, igor, yhchiang, dhruba, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: valgrind test Summary: Get valgrind to stop complaining about uninitialized value Test Plan: valgrind not complaining anymore Reviewers: sdong, yhchiang, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: model for flushing memtables Summary: When memtable is full it calls the registered callback. That callback then registers column family as needing the flush. Every write checks if there are some column families that need to be flushed. This completely eliminates the need for MakeRoomForWrite() function and simplifies our Write code-path. There is some complexity with the concurrency when the column family is dropped. I made it a bit less complex by dropping the column family from the write thread in Let me know if you want to discuss this. Test Plan: make check works. Ill also run db_stress with creating and dropping column families for a while. Reviewers: yhchiang, sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: test] CompactRange should fail if we dont have space Summary: See t5106397. Also, few more changes: 1. in unit tests, the assumption is that writes will be dropped when there is no space left on device. I changed the wording around it. 2. InvalidArgument() errors are only when user-provided arguments are invalid. When the file is corrupted, we need to return Status::Corruption Test Plan: make check Reviewers: sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: SimpleWriteTimeoutTest Summary: In column familys SanitizeOptions() [1], we make sure that min_write_buffer_number_to_merge is normal value. However, this test depended on the fact that setting min_write_buffer_number_to_merge to be bigger than max_write_buffer_number will cause a deadlock. Im not sure how it worked before. This diff fixes it by scheduling sleeping background task, which will actually block any attempts of flushing. [1] Test Plan: the test works now Reviewers: yhchiang, sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: db recovery Summary: Avoid creating unnecessary sst files while db opening Test Plan: make all check Reviewers: sdong, igor Reviewed By: igor Subscribers: zagfox, yhchiang, ljin, leveldb Differential Revision: stop level trigger-0 before slowdown level-0 trigger Summary: ... Test Plan: Cant repro the test failure, but lets see what jenkins says Reviewers: zagfox, sdong, ljin Reviewed By: sdong, ljin Subscribers: leveldb Differential Revision: Do not wait for background threads when there is nothing in mem table Summary: When we have multiple column families, users can issue Flush() on every column families to make sure everything is flushes, even if some of them might be empty. By skipping the waiting for empty cases, it can be greatly speed up. Still wait for peoples comments before writing unit tests for it. Test Plan: Will write a unit test to make sure it is correct. Reviewers: ljin, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: full filter for block based table. Summary: 1. Make filter_block.h a base class. Derive block_based_filter_block and full_filter_block. The previous one is the traditional filter block. The full_filter_block is newly added. It would generate a filter block that contain all the keys in SST file. 2. When querying a key, table would first check if full_filter is available. If not, it would go to the exact data block and check using block_based filter. 3. User could choose to use full_filter or tradional(block_based_filter). They would be stored in SST file with different meta index name. ""filter.filter_policy"" or ""full_filter.filter_policy"". Then, Table reader is able to know the fllter block type. 4. Some optimizations have been done for full_filter_block, thus it requires a different interface compared to the original one in filter_policy.h. 5. Actual implementation of filter bits coding/decoding is placed in util/bloom_impl.cc Benchmark: base commit 1d23b5c470844c1208301311f0889eca750431c0 Command: db_bench disable_auto_compactions=1 Read QPS increase for about 30% from 2230002 to 2991411. Test Plan: make all check valgrind db_test db_stress 0 ./auto_sanity_test.sh Reviewers: igor, yhchiang, ljin, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: column family from write thread Summary: If we drop column family only from (single) write thread, we can be sure that nobody will drop the column family while were writing (and our mutex is released). This greatly simplifies my patch thats getting rid of MakeRoomForWrite(). Test Plan: make check, but also running stress test Reviewers: ljin, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: memory leak in unit test DBIteratorBoundTest Summary: fixed memory leak in unit test DBIteratorBoundTest Test Plan: ran valgrind test on my unit test Reviewers: sdong Differential Revision: path with arena==nullptr from NewInternalIterator Summary: Simply code by removing code path which does not use Arena from NewInternalIterator Test Plan: make all check make valgrind_check Reviewers: sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: a new ReadOptions parameter iterate_upper_bound/Dont do memtable lookup in db_impl_readonly if memtables are empty while opening db. Summary: In DBImpl::Recover method, while loading memtables, also check if memtables are empty. Use this in DBImplReadonly to determine whether to lookup memtable or not. Test Plan: db_test make check all Reviewers: sdong, yhchiang, ljin, igor Reviewed By: ljin Subscribers: leveldb Differential Revision: timeout to 50ms instead of 3./move block based table related options BlockBasedTableOptions Summary: I will move compression related options in a separate diff since this diff is already pretty lengthy. I guess I will also need to change JNI accordingly :( Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: igor Subscribers: leveldb Differential Revision: Options sanitization and add MmapReadRequired() to TableFactory Summary: Currently, PlainTable must use mmap_reads. When PlainTable is used but allow_mmap_reads is not set, rocksdb will fail in flush. This diff improve Options sanitization and add MmapReadRequired() to TableFactory. Test Plan: export ROCKSDB_TESTS=PlainTableOptionsSanitizeTest make db_test ./db_test Reviewers: sdong, ljin Reviewed By: ljin Subscribers: you, leveldb Differential Revision: purging logs from separate log directory Summary: 1. Support purging info logs from a separate paths from DB path. Refactor the codes of generating info log prefixes so that it can be called when generating new files and scanning log directory. 2. Fix the bug of not scanning multiple DB paths (should only impact multiple DB paths) Test Plan: Add unit test for generating and parsing info log files Add end-to-end test in db_test Reviewers: yhchiang, ljin Reviewed By: ljin Subscribers: leveldb, igor, dhruba Differential Revision:"
1097,1097,10.0,0.4302000105381012,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fixed GetEstimatedActiveKeys Summary: Fixed a bug in GetEstimatedActiveKeys which does not normalized the sampled information correctly. Add a test in version_builder_test. Test Plan: version_builder_test Reviewers: ljin, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: on and fix all the errors Summary: We need to turn on for mobile. See D1671432 (internal phabricator) for details. This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables. Test Plan: compiles Reviewers: ljin, rven, yhchiang, sdong Reviewed By: yhchiang Subscribers: bobbaldwin, dhruba, leveldb Differential Revision: rid of mutex in CompactionJobs state Summary: Based on feedback in the diff, we shouldnt keep db_mutex in CompactionJobs state. This diff removes db_mutex from CompactionJob state, by making next_file_number_ atomic. That way we only need to pass the lock to InstallCompactionResults() because of LogAndApply() Test Plan: make check Reviewers: ljin, yhchiang, rven, sdong Reviewed By: sdong Subscribers: sdong, dhruba, leveldb Differential Revision: EventListener and GetDatabaseMetaData Summary: This diff adds three sets of APIs to RocksDB. GetColumnFamilyMetaData * This APIs allow users to obtain the current state of a RocksDB instance on one column family. * See GetColumnFamilyMetaData in include/rocksdb/db.h EventListener * A virtual class that allows users to implement a set of call-back functions which will be called when specific events of a RocksDB instance happens. * To register EventListener, simply insert an EventListener to ColumnFamilyOptions::listeners CompactFiles * CompactFiles API inputs a set of file numbers and an output level, and RocksDB will try to compact those files into the specified level. Example * Example code can be found in example/compact_files_example.cc, which implements a simple external compactor using EventListener, GetColumnFamilyMetaData, and CompactFiles API. Test Plan: listener_test compactor_test example/compact_files_example export ROCKSDB_TESTS=CompactFiles db_test export ROCKSDB_TESTS=MetaData db_test Reviewers: ljin, igor, rven, sdong Reviewed By: sdong Subscribers: MarkCallaghan, dhruba, leveldb Differential Revision: back on Summary: It turns out that has different rules for gcc than clang. Previous commit fixed clang. This commits fixes the rest of the warnings for gcc. Test Plan: compiles Reviewers: ljin, yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: InfoLogLevel to the logs in db/version_set.cc Summary: Apply InfoLogLevel to the logs in db/version_set.cc Test Plan: make Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: naming convention of getters in version_set.h Summary: Enforce the accessier naming convention in functions in version_set.h Test Plan: make all check Reviewers: ljin, yhchiang, rven, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: BaseReferencedVersionBuilders destructor order Summary: BaseReferencedVersionBuilder now unreference version before destructing VersionBuilder, which is wrong. Fix it. Test Plan: make all check valgrind test to tests that used to fail Reviewers: igor, yhchiang, rven, ljin Reviewed By: ljin Subscribers: leveldb, dhruba Differential Revision: on Summary: ...and fix all the errors :) Jim suggested turning on because it helped him fix number of critical bugs in fbcode. I think its a good idea to be clean. Test Plan: compiles Reviewers: yhchiang, rven, sdong, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: VersionBuilder unit testable Summary: Rename Version::Builder to VersionBuilder and expose its definition to a header. Make VerisonBuilder not reference Version or ColumnFamilyData, only working with VersionStorageInfo. Add version_builder_test which has a simple test. Test Plan: make all check Reviewers: rven, yhchiang, igor, ljin Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: CompactionPicker more easily tested Summary: Make compaction picker easier to test. The basic idea is to separate a minimum subcomponent of Version to VersionStorageInfo, which just responsible to LSM tree. A stub VersionStorageInfo can then be easily created and passed into compaction picker so that we can check the outputs. It now passes most tests. Still two things need to be done: (1) deal with the FIFO compactions file size. (2) write an example test to make sure the interface can do the job. Add a compaction_picker_test to make sure compaction picker codes can be easily unit tested. Test Plan: Pass all unit tests and compaction_picker_test Reviewers: yhchiang, rven, igor, ljin Reviewed By: ljin Subscribers: leveldb, dhruba Differential Revision: the bug where compaction does not fail when RocksDB cant create a new file. Summary: This diff has two fixes. 1. Fix the bug where compaction does not fail when RocksDB cant create a new file. 2. When NewWritableFiles() fails in OpenCompactionOutputFiles(), previously such fail-to-created file will be still be included as a compaction output. This patch also fixes this bug. 3. Allow VersionEdit::EncodeTo() to return Status and add basic check. Test Plan: ./version_edit_test export ROCKSDB_TESTS=FileCreationRandomFailure ./db_test Reviewers: ljin, sdong, nkg-, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: LevelFileNumIterator and LevelFileIteratorState anonymous Summary: No need to expose them in .h Test Plan: make release Reviewers: igor, yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: FileLevel to LevelFilesBrief / unfriend CompactedDBImpl Summary: We have several different types of data structures for file information. FileLevel is kinda of confusing since it only contains file range and fd. Rename it to LevelFilesBrief to make it clear. Unfriend CompactedDBImpl as a by product Test Plan: make release / make all will run full test with all stacked diffs Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: Compaction and CompactionPicker from VersionSet Summary: as title Test Plan: running make all check Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: inplace_update options Summary: Make inplace_update_support and inplace_update_num_locks dynamic. inplace_callback becomes immutable We are almost free of references to cfd->options() in db_impl Test Plan: unit test Reviewers: igor, yhchiang, rven, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: up DB::Open() and Version creation by limiting the number of FileMetaData initialization. Summary: This diff speeds up DB::Open() and Version creation by limiting the number of FileMetaData initialization. The behavior of Version::UpdateAccumulatedStats() is changed as follows: * It only initializes the first 20 uninitialized FileMetaData from file. This guarantees the size of the latest 20 files will always be compensated when they have any deletion entries. Previously it may initialize all FileMetaData by loading all files at DB::Open(). * In case none the first 20 files has any data entry, UpdateAccumulatedStats() will initialize the FileMetaData of the oldest file. Test Plan: db_test Reviewers: igor, sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: compaction related options changeable Summary: make compaction related options changeable. Most of changes are tedious, following the same convention: grabs MutableCFOptions at the beginning of compaction under mutex, then pass it throughout the job and register it in SuperVersion at the end. Test Plan: make all check Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: GetContext to replace callback function pointer Summary: Intead of passing callback function pointer and its arg on Table::Get() interface, passing GetContext. This makes the interface cleaner and possible better perf. Also adding a fast pass for SaveValue() Test Plan: make all check Reviewers: igor, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: sync manifest when disableDataSync true Summary: As we discussed offline Test Plan: compiles Reviewers: yhchiang, sdong, ljin, dhruba Reviewed By: sdong Subscribers: leveldb Differential Revision: version_set options_ to db_options_ to avoid confusion Summary: as title Test Plan: make release Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: leveldb Differential Revision: references to cfd->options() in DBImpl Summary: I found it is almost impossible to get rid of this function in a single batch. I will take a step by step approach Test Plan: make release Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: leveldb Differential Revision: instead of pull-model for managing Write stalls Summary: Introducing WriteController, which is a source of truth about per-DB write delays. Lets define an DB epoch as a period where there are no flushes and compactions (i.e. new epoch is started when flush or compaction finishes). Each epoch can either: * proceed with all writes without delay * delay all writes by fixed time * stop all writes The three modes are recomputed at each epoch change (flush, compaction), rather than on every write (which is currently the case). When we have a lot of column families, our current pull behavior adds a big overhead, since we need to loop over every column family for every write. With new push model, overhead on Write code-path is minimal. This is just the start. Next step is to also take care of stalls introduced by slow memtable flushes. The final goal is to eliminate function MakeRoomForWrite(), which currently needs to be called for every column family by every write. Test Plan: make check for now. Ill add some unit tests later. Also, perf test. Reviewers: dhruba, yhchiang, MarkCallaghan, sdong, ljin Reviewed By: ljin Subscribers: leveldb Differential Revision: retrying to read property block from a table when it does not exist. Summary: Avoid retrying to read property block from a table when it does not exist in updating stats for compensating deletion entries. In addition, ReadTableProperties() now returns Status::NotFound instead of Status::Corruption when table properties does not exist in the file. Test Plan: make db_test export ROCKSDB_TESTS=CompactionDeleteionTrigger ./db_test Reviewers: ljin, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision:"
1098,1098,10.0,0.949999988079071,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","move block based table related options BlockBasedTableOptions Summary: I will move compression related options in a separate diff since this diff is already pretty lengthy. I guess I will also need to change JNI accordingly :( Test Plan: make all check Reviewers: yhchiang, igor, sdong Reviewed By: igor Subscribers: leveldb Differential Revision:"
1099,1099,5.0,0.9269000291824341,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Turn back on Summary: It turns out that has different rules for gcc than clang. Previous commit fixed clang. This commits fixes the rest of the warnings for gcc. Test Plan: compiles Reviewers: ljin, yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1100,1100,1.0,0.987500011920929,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Change db_stress to work with format_version 2/Fix compile warning in db_stress Summary: Fix compile warning in db_stress Test Plan: make db_stress/Fix compile warning in db_stress.cc on Mac Summary: Fix the following compile warning in db_stress.cc on Mac tools/db_stress.cc:1688:52: error: format specifies type unsigned long but the argument has type ::google::uint64 (aka unsigned long long) [-Werror,-Wformat] fprintf(stdout, ""DB-write-buffer-size: %lu\n"", FLAGS_db_write_buffer_size); ~~~ ^~~~~~~~~~~~~~~~~~~~~~~~~~ %llu Test Plan: make/Enforce write buffer memory limit across column families Summary: Introduces a new class for managing write buffer memory across column families. We supplement ColumnFamilyOptions::write_buffer_size with ColumnFamilyOptions::write_buffer, a shared pointer to a WriteBuffer instance that enforces memory limits before flushing out to disk. Test Plan: Added SharedWriteBuffer unit test to db_test.cc Reviewers: sdong, rven, ljin, igor Reviewed By: igor Subscribers: tnovak, yhchiang, dhruba, xjin, MarkCallaghan, yoshinorim Differential Revision: rocksdb::ToString() to address cases where std::to_string is not available. Summary: In some environment such as android, the c++ library does not have std::to_string. This path adds rocksdb::ToString(), which wraps std::to_string when std::to_string is not available, and implements std::to_string in the other case. Test Plan: make dbg ./db_test make clean make dbg OPT=-DOS_ANDROID ./db_test Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1101,1101,1.0,0.9587000012397766,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Add rocksdb::ToString() to address cases where std::to_string is not available. Summary: In some environment such as android, the c++ library does not have std::to_string. This path adds rocksdb::ToString(), which wraps std::to_string when std::to_string is not available, and implements std::to_string in the other case. Test Plan: make dbg ./db_test make clean make dbg OPT=-DOS_ANDROID ./db_test Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1102,1102,1.0,0.9829999804496765,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Fix fault_injestion_test Summary: A bug in MockEnv causes fault_injestion_test to fail. I dont know why it doesnt fail every time but it doesnt seem to be right. Test Plan: Run fault_injestion_test Also run db_test with MEM_ENV=1 until the first failure. Reviewers: yhchiang, rven, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: add a unit test to allow parallel compactions and multiple levels Summary: Add a new test case in fault_injection_test, which covers parallel compactions and multiple levels. Use MockEnv to run the new test case to speed it up. Improve MockEnv to avoid DestoryDB(), previously failed when deleting lock files. Test Plan: Run ./fault_injection_test, including valgrind Reviewers: rven, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1103,1103,6.0,0.9829999804496765,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Move GetThreadList() feature under Env. Summary: GetThreadList() feature depends on the thread creation and destruction, which is currently handled under Env. This patch moves GetThreadList() feature under Env to better manage the dependency of GetThreadList() feature on thread creation and destruction. Renamed ThreadStatusImpl to ThreadStatusUpdater. Add ThreadStatusUtil, which is a static class contains utility functions for ThreadStatusUpdater. Test Plan: run db_test, thread_list_test and db_bench and verify the life cycle of Env and ThreadStatusUpdater is properly managed. Reviewers: igor, sdong Reviewed By: sdong Subscribers: ljin, dhruba, leveldb Differential Revision: openable snapshots Summary: Store links to live files in directory on same disk Test Plan: Take snapshot and open it. Added a test GetSnapshotLink in db_test. Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1104,1104,6.0,0.8641999959945679,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",[RocksJava] Rebased + integrated CF tests/[RocksJava] Merged & rebased to HEAD/
1105,1105,10.0,0.9567999839782715,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Adjustment to NativeLibraryLoader to allow native library to be loaded from either java.library.path or from extracting from the Jar. Means that the test in the build do not need to rely on the Jar, useful when creating similar builds (and executing tests) from Maven/Make sure to use the correct Java classloader for loading the RocksDB Native Library/Minor tidyup and use Java 7 for file copying/"
1106,1106,3.0,0.6272000074386597,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","[RocksJava] Cleanup portal.h Simple Java Native Objects usually are represented using the same functionality but within different classes. With this commit a template class was introduced to remove the redundant impelementation to a certain extent./Abstract duplicate code on key and value slice objects into generic methods/Implement WBWIRocksIterator for WriteBatchWithIndex in the Java API/[RocksJava] ColumnFamilyDescriptor alignment with listColumnFamilies Summary: Previous to this commit ColumnFamilyDescriptor took a String as name for the ColumnFamily name. String is however encoding dependent which is bad because listColumnFamilies returns byte arrays without any encoding information. All public API call were deprecated and flagged to be removed in 3.10.0 Test Plan: make rocksdbjava make test mvn rocksjni.pom package Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision:"
1107,1107,8.0,0.949999988079071,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",[RocksJava] Integrated changes from D29019./[RocksJava] Backupable/Restorable DB update 3.8.0 GarbageCollectMethod() available. GetCorruptedBackups() available./[RocksJava] BackupableDBOptions alginment + 3.8 Updated the BackupableDBOptions functionality to 3.8.0. Aligned Options implementation with remaining source code. Invented test-case./
1108,1108,18.0,0.9886999726295471,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","[RocksJava] CF Name shall handle bytes correctly Summary: Bytes are currently misinterpreted by the Java if the byte array contains zero bytes within its content. For Strings thats usually not useful. As the Java API allows every kind of byte array values it might be the case that zero padding might happen. Test Plan: make rocksdbjava make jtest Reviewers: adamretter, yhchiang, ankgup87 Subscribers: dhruba Differential Revision: ReadOptions support in Iterators The methods: newIterator iterators support now also ReadOptions. That allows a user of the Java API to retrieve RocksIterator instances on a snapshot./[RocksJava] ColumnFamily name JNI correction Previous to this commit there was a problem with unterminated String usage as jByteArrays are not zero terminated./[RocksJava] ColumnFamilyDescriptor alignment with listColumnFamilies Summary: Previous to this commit ColumnFamilyDescriptor took a String as name for the ColumnFamily name. String is however encoding dependent which is bad because listColumnFamilies returns byte arrays without any encoding information. All public API call were deprecated and flagged to be removed in 3.10.0 Test Plan: make rocksdbjava make test mvn rocksjni.pom package Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision: CompactRange support manual range compaction support in RocksJava/[RocksJava] GetIntProperty in RocksDB Expose GetIntProperty methods to RocksJava. As the integer(64-Bit) value is no integer in Java the method is aligned with the return type which is long./"
1109,1109,13.0,0.9603999853134155,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",[RocksJava] Comparator tests for CF Added AbstractComparatorTest. Fixed a bug in the JNI Part about Java comparators/[RocksJava] Sigsegv fix for MergerOperatorByName/[RocksJava] Convenience methods for Options RocksDB introduced in 3.7.0 convenience methods for getting ColumnFamilyOptions and DBOptions instances from predefined configuration structures. There is now also a method in RocksJava to load DBOptions as well as ColumnFamilyOptions from a predefined Properties based configuration./
1110,1110,1.0,0.920799970626831,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","MultiGet for DBWithTTL Summary: This is a feature request from rocksdbs user. I didnt even realize we dont support multigets on TTL DB :) Test Plan: added a unit test Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1111,1111,13.0,0.640999972820282,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Fix broken test in 31b02d. Summary: CorruptionTest for backupable_db_test did not call GarbageCollect() after deleting a corrupt backup, which sometimes lead to test failures as the newly created backup would reuse the same backup ID and files and fail the consistency check. Moved around some of the test logic to ensure that GarbageCollect() is called at the right time. Test Plan: Run backupable_db_test eight times and make sure it passes repeatedly. Also run make check to make sure other tests dont fail. Reviewers: igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: Backup Engine. Summary: Improve the backup engine by not deleting the corrupted backup when it is detected; instead leaving it to the client to delete the corrupted backup. Also add a BackupEngine::Open() call. Test Plan: Add check to CorruptionTest inside backupable_db_test to check that the corrupt backups are not deleted. The previous version of the code failed this test as backups were deleted, but after the changes in this commit, this test passes. Run make check to ensure that no other tests fail. Reviewers: sdong, benj, sanketh, sumeet, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1112,1112,10.0,0.9728999733924866,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","CompactionJobTest Summary: This is just a simple test that passes two files though a compaction. It shows the framework so that people can continue building new compaction *unit* tests. In the future we might want to move some Compaction* tests from DBTest here. For example, CompactBetweenSnapshot seems a good candidate. Hopefully this test can be simpler when we mock out VersionSet. Test Plan: this is a test Reviewers: ljin, rven, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1113,1113,7.0,0.9939000010490417,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Remember whole key/prefix filtering on/off in SST file Summary: Remember whole key or prefix filtering on/off in SST files. If user opens the DB with a different setting that cannot be satisfied while reading the SST file, ignore the bloom filter. Test Plan: Add a unit test for it Reviewers: yhchiang, igor, rven Reviewed By: rven Subscribers: leveldb, dhruba Differential Revision: to use prefix bloom filter when filter is not block based Summary: Get() now doesnt make use of bloom filter if it is prefix based. Add the check. Didnt touch block based bloom filter. I cant fully reason whether it is correct to do that. But its straight-forward to for full bloom filter. Test Plan: make all check Add a test case in DBTest Reviewers: rven, yhchiang, igor Reviewed By: igor Subscribers: MarkCallaghan, leveldb, dhruba, yoshinorim Differential Revision: BlockBasedTable version better compressed block format Summary: This diff adds BlockBasedTable format_version 2. New format version brings better compressed block format for these compressions: 1) Zlib encode decompressed size in compressed block header 2) BZip2 encode decompressed size in compressed block header 3) LZ4 and LZ4HC instead of doing memcpy of size_t encode size as varint32. memcpy is very bad because the DB is not portable accross big/little endian machines or even platforms where size_t might be 8 or 4 bytes. It does not affect format for snappy. If you write a new database with format_version 2, it will not be readable by RocksDB versions before 3.10. DB::Open() will return corruption in that case. Test Plan: Added a new test in db_test. I will also run db_bench and verify VSIZE when block_cache 1GB Reviewers: yhchiang, rven, MarkCallaghan, dhruba, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: footer versions bigger than 1 Summary: In this diff I add another parameter to BlockBasedTableOptions that will let users specify block based tables format. This will greatly simplify block based tables format changes in the future. First format change that this will support is encoding decompressed size in Zlib and BZip2 blocks. This diff is blocking Test Plan: Added a unit tests. More tests to come as part of Reviewers: dhruba, MarkCallaghan, yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: routine to BlockBasedTableReader Summary: Added necessary routines for dumping block based SST with block filter Test Plan: Added ""raw"" mode to utility sst_dump Reviewers: sdong, rven Reviewed By: rven Subscribers: dhruba Differential Revision:"
1114,1114,7.0,0.9918000102043152,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","New BlockBasedTable version better compressed block format Summary: This diff adds BlockBasedTable format_version 2. New format version brings better compressed block format for these compressions: 1) Zlib encode decompressed size in compressed block header 2) BZip2 encode decompressed size in compressed block header 3) LZ4 and LZ4HC instead of doing memcpy of size_t encode size as varint32. memcpy is very bad because the DB is not portable accross big/little endian machines or even platforms where size_t might be 8 or 4 bytes. It does not affect format for snappy. If you write a new database with format_version 2, it will not be readable by RocksDB versions before 3.10. DB::Open() will return corruption in that case. Test Plan: Added a new test in db_test. I will also run db_bench and verify VSIZE when block_cache 1GB Reviewers: yhchiang, rven, MarkCallaghan, dhruba, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: footer versions bigger than 1 Summary: In this diff I add another parameter to BlockBasedTableOptions that will let users specify block based tables format. This will greatly simplify block based tables format changes in the future. First format change that this will support is encoding decompressed size in Zlib and BZip2 blocks. This diff is blocking Test Plan: Added a unit tests. More tests to come as part of Reviewers: dhruba, MarkCallaghan, yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: routine to BlockBasedTableReader Summary: Added necessary routines for dumping block based SST with block filter Test Plan: Added ""raw"" mode to utility sst_dump Reviewers: sdong, rven Reviewed By: rven Subscribers: dhruba Differential Revision:"
1115,1115,1.0,0.954800009727478,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Enforce write buffer memory limit across column families Summary: Introduces a new class for managing write buffer memory across column families. We supplement ColumnFamilyOptions::write_buffer_size with ColumnFamilyOptions::write_buffer, a shared pointer to a WriteBuffer instance that enforces memory limits before flushing out to disk. Test Plan: Added SharedWriteBuffer unit test to db_test.cc Reviewers: sdong, rven, ljin, igor Reviewed By: igor Subscribers: tnovak, yhchiang, dhruba, xjin, MarkCallaghan, yoshinorim Differential Revision:"
1116,1116,1.0,0.9587000012397766,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Add rocksdb::ToString() to address cases where std::to_string is not available. Summary: In some environment such as android, the c++ library does not have std::to_string. This path adds rocksdb::ToString(), which wraps std::to_string when std::to_string is not available, and implements std::to_string in the other case. Test Plan: make dbg ./db_test make clean make dbg OPT=-DOS_ANDROID ./db_test Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1117,1117,10.0,0.8093000054359436,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fix DestroyDB Summary: When DestroyDB() finds a wal file in the DB directory, it assumes it is actually in WAL directory. This can lead to confusion, since it reports IO error when it tries to delete wal file from DB directory. For example: This change will fix our unit tests. Test Plan: unit tests work Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: a counter for collecting the wait time on db mutex. Summary: Add a counter for collecting the wait time on db mutex. Also add MutexWrapper and CondVarWrapper for measuring wait time. Test Plan: ./db_test export ROCKSDB_TESTS=MutexWaitStats ./db_test verify stats output using db_bench make clean make release ./db_bench Sample output: rocksdb.db.mutex.wait.micros COUNT : 7546866 Reviewers: MarkCallaghan, rven, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: compaction listener. Summary: This adds a listener for compactions, and gives some useful statistics on each compaction pass. Test Plan: Unit tests. Reviewers: sdong, igor, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: DBImpl::log_dir_unsynced_ to log_dir_synced_ Summary: log_dir_unsynced_ is a confusing name. Rename it to log_dir_synced_ and flip the value. Test Plan: Run ./fault_injection_test Reviewers: rven, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: WAL Directory and DB Path if different from DB directory Summary: 1. If WAL directory is different from db directory. Sync the directory after creating a log file under it. 2. After creating an SST file, sync its parent directory instead of DB directory. 3. change the check of kResetDeleteUnsyncedFiles in fault_injection_test. Since we changed the behavior to sync log files parent directory after first WAL sync, instead of creating, kResetDeleteUnsyncedFiles will not guarantee to show post sync updates. Test Plan: make all check Reviewers: yhchiang, rven, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: data race Summary: We should not be calling InternalStats methods outside of the mutex. Test Plan: COMPILE_WITH_TSAN=1 m db_test && ROCKSDB_TESTS=CompactionTrigger ./db_test failing before the diff, works now Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: GetThreadList() to indicate a thread is doing Compaction. Summary: Allow GetThreadList() to indicate a thread is doing Compaction. Test Plan: export ROCKSDB_TESTS=ThreadStatus ./db_test Reviewers: ljin, igor, sdong Reviewed By: sdong Subscribers: leveldb, dhruba, jonahcohen, rven Differential Revision: Stats Dump to print total stall time Summary: Add printing of stall time in DB Stats: Sample outputs: ** DB Stats ** Uptime(secs): 53.2 total, 1.7 interval Cumulative writes: 625940 writes, 625939 keys, 625940 batches, 1.0 writes per batch, 0.49 GB user ingest, stall micros: 50691070 Cumulative WAL: 625940 writes, 625939 syncs, 1.00 writes per sync, 0.49 GB written Interval writes: 10859 writes, 10859 keys, 10859 batches, 1.0 writes per batch, 8.7 MB user ingest, stall micros: 1692319 Interval WAL: 10859 writes, 10859 syncs, 1.00 writes per sync, 0.01 MB written Test Plan: make all check verify printing using db_bench Reviewers: igor, yhchiang, rven, MarkCallaghan Reviewed By: MarkCallaghan Subscribers: leveldb, dhruba Differential Revision: column family concurrency Summary: This patch changes concurrency guarantees around ColumnFamilySet::column_families_ and ColumnFamilySet::column_families_data_. Before: * When mutating: lock DB mutex and spin lock * When reading: lock DB mutex OR spin lock After: * When mutating: lock DB mutex and be in write thread * When reading: lock DB mutex or be in write thread That way, we eliminate the spin lock that protects these hash maps and simplify concurrency. That means we dont need to lock the spin lock during writing, since writing is mutually exclusive with column family create/drop (the only operations that mutate those hash maps). With these new restrictions, I also needed to move column family create to the write thread (column family drop was already in the write thread). Even though we dont need to lock the spin lock during write, impact on performance should be minimal the spin lock is almost never busy, so locking it is almost free. This addresses task t5116919. Test Plan: make check Stress test with lots and lots of column family drop and create: time ./db_stress Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: compaction summary log for trivial move Summary: When trivial move commit is done, we log the summary of the input version instead of current. This is inconsistent with other log messages and confusing. Test Plan: compiles Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: GetThreadList() feature under Env. Summary: GetThreadList() feature depends on the thread creation and destruction, which is currently handled under Env. This patch moves GetThreadList() feature under Env to better manage the dependency of GetThreadList() feature on thread creation and destruction. Renamed ThreadStatusImpl to ThreadStatusUpdater. Add ThreadStatusUtil, which is a static class contains utility functions for ThreadStatusUpdater. Test Plan: run db_test, thread_list_test and db_bench and verify the life cycle of Env and ThreadStatusUpdater is properly managed. Reviewers: igor, sdong Reviewed By: sdong Subscribers: ljin, dhruba, leveldb Differential Revision: up FindObsoleteFiles() Summary: There are two versions of FindObsoleteFiles(): * full scan, which is executed every 6 hours (and its terribly slow) * no full scan, which is executed every time a background process finishes and iterator is deleted This diff is optimizing the second case (no full scan). Heres what we do before the diff: * Get the list of obsolete files (files with ref==0). Some files in obsolete_files set might actually be live. * Get the list of live files to avoid deleting files that are live. * Delete files that are in obsolete_files and not in live_files. After this diff: * The only files with ref==0 that are still live are files that have been part of move compaction. Dont include moved files in obsolete_files. * Get the list of obsolete files (which exclude moved files). * No need to get the list of live files, since all files in obsolete_files need to be deleted. Ill post the benchmark results, but you can get the feel of it here: This depends on D30123. P.S. We should do full scan only in failure scenarios, not every 6 hours. Ill do this in a follow-up diff. Test Plan: One new unit test. Made sure that unit test fails if we dont have a `if (f->moved)` safeguard in ~Version. make check Big number of compactions and flushes: ./db_stress Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: system for scheduling background work Summary: When scaling to higher number of column families, the worst bottleneck was MaybeScheduleFlushOrCompaction(), which did a for loop over all column families while holding a mutex. This patch addresses the issue. The approach is similar to our earlier efforts: instead of a pull-model, where we do something for every column family, we can do a push-based model when we detect that column family is ready to be flushed/compacted, we add it to the flush_queue_/compaction_queue_. That way we dont need to loop over every column family in MaybeScheduleFlushOrCompaction. Here are the performance results: Command: ./db_bench Before the patch: fillrandom : 26.950 micros/op 37105 ops/sec; 4.1 MB/s After the patch: fillrandom : 17.404 micros/op 57456 ops/sec; 6.4 MB/s Next bottleneck is VersionSet::AddLiveFiles, which is painfully slow when we have a lot of files. This is coming in the next patch, but when I removed that code, heres what I got: fillrandom : 7.590 micros/op 131758 ops/sec; 14.6 MB/s Test Plan: make check two stress tests: Big number of compactions and flushes: ./db_stress max_background_flushes=0, to verify that this case also works correctly ./db_stress Reviewers: ljin, rven, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: the file copy out of the mutex. Summary: We now release the mutex before copying the files in the case of the trivial move. This path does not use the compaction job. Test Plan: DBTest.LevelCompactionThirdPath Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: scalability of DB::GetSnapshot() Summary: Now DB::GetSnapshot() doesnt scale to more column families, as it needs to go through all the column families to find whether snapshot is supported. This patch optimizes it. Test Plan: Add unit tests to cover negative cases. make all check Reviewers: yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: DBProperty to return number of snapshots and time for oldest snapshot Summary: Add a counter in SnapshotList to show number of snapshots. Also a unix timestamp in every snapshot. Add two DB Properties to return number of snapshots and timestamp of the oldest one. Test Plan: Add unit test checking Reviewers: yhchiang, rven, igor Reviewed By: igor Subscribers: leveldb, dhruba, MarkCallaghan Differential Revision: Moved(GB) to Compaction IO stats Summary: Adds counter for bytes moved (files pushed down a level rather than compacted) to compaction IO stats as Moved(GB). From the output removed these infrequently used columns: RW-Amp, Rn(cnt), Rnp1(cnt), Wnp1(cnt), Wnew(cnt). Example old output: Level Files Size(MB) Score Read(GB) Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) RW-Amp W-Amp Rd(MB/s) Wr(MB/s) Rn(cnt) Rnp1(cnt) Wnp1(cnt) Wnew(cnt) Comp(sec) Comp(cnt) Avg(sec) Stall(sec) Stall(cnt) Avg(ms) RecordIn RecordDrop L0 0/0 0 0.0 0.0 0.0 0.0 2130.8 2130.8 0.0 0.0 0.0 109.1 0 0 0 0 20002 25068 0.798 28.75 182059 0.16 0 0 L1 142/0 509 1.0 4618.5 2036.5 2582.0 4602.1 2020.2 4.5 2.3 88.5 88.1 24220 701246 1215528 514282 53466 4229 12.643 0.00 0 0.002032745988 300688729 Example new output: Level Files Size(MB) Score Read(GB) Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) Stall(sec) Stall(cnt) Avg(ms) RecordIn RecordDrop L0 7/0 13 1.8 0.0 0.0 0.0 0.6 0.6 0.0 0.0 0.0 14.7 44 353 0.124 0.03 626 0.05 0 0 L1 9/0 16 1.6 0.0 0.0 0.0 0.0 0.0 0.6 0.0 0.0 0.0 0 0 0.000 0.00 0 0.00 0 0 Task ID: Blame Rev: Test Plan: make check, run db_bench and look at output Revert Plan: Database Impact: Memcache Impact: Other Notes: EImportant: begin *PUBLIC* platform impact section Bugzilla: end platform impact Reviewers: igor Reviewed By: igor Subscribers: dhruba Differential Revision: write buffer memory limit across column families Summary: Introduces a new class for managing write buffer memory across column families. We supplement ColumnFamilyOptions::write_buffer_size with ColumnFamilyOptions::write_buffer, a shared pointer to a WriteBuffer instance that enforces memory limits before flushing out to disk. Test Plan: Added SharedWriteBuffer unit test to db_test.cc Reviewers: sdong, rven, ljin, igor Reviewed By: igor Subscribers: tnovak, yhchiang, dhruba, xjin, MarkCallaghan, yoshinorim Differential Revision: GetThreadList API Summary: Add GetThreadList API, which allows developer to track the status of each process. Currently, calling GetThreadList will only get the list of background threads in RocksDB with their thread-id and thread-type (priority) set. Will add more support on this in the later diffs. ThreadStatus currently has the following properties: // An unique ID for the thread. const uint64_t thread_id; // The type of the thread, it could be ROCKSDB_HIGH_PRIORITY, // ROCKSDB_LOW_PRIORITY, and USER_THREAD const ThreadType thread_type; // The name of the DB instance where the thread is currently // involved with. It would be set to empty string if the thread // does not involve in any DB operation. const std::string db_name; // The name of the column family where the thread is currently // It would be set to empty string if the thread does not involve // in any column family. const std::string cf_name; // The event that the current thread is involved. // It would be set to empty string if the information about event // is not currently available. Test Plan: ./thread_list_test export ROCKSDB_TESTS=GetThreadList ./db_test Reviewers: rven, igor, sdong, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: clean JobContext Summary: This way we can gurantee that old MemTables get destructed before DBImpl gets destructed, which might be useful if we want to make them depend on state from DBImpl. Test Plan: make check with asserts in JobContexts destructor Reviewers: ljin, sdong, yhchiang, rven, jonahcohen Reviewed By: jonahcohen Subscribers: dhruba, leveldb Differential Revision: SIGSEGV Summary: As a short-term fix, lets go back to previous way of calculating NeedsCompaction(). SIGSEGV happens because NeedsCompaction() can happen before super_version (and thus MutableCFOptions) is initialized. Test Plan: make check Reviewers: ljin, sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: NeedsCompaction() from VersionStorageInfo to CompactionPicker Summary: Move NeedsCompaction() from VersionStorageInfo to CompactionPicker to allow different compaction strategy to have their own way to determine whether doing compaction is necessary. When compaction style is set to kCompactionStyleNone, then NeedsCompaction() will always return false. Test Plan: export ROCKSDB_TESTS=Compact ./db_test Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1118,1118,10.0,0.9136000275611877,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Added some more wrappers and wrote a test for backup in C/Allow creating backups from C/Add cuckoo table options to the C interface/
1119,1119,7.0,0.994700014591217,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Rewritten system for scheduling background work Summary: When scaling to higher number of column families, the worst bottleneck was MaybeScheduleFlushOrCompaction(), which did a for loop over all column families while holding a mutex. This patch addresses the issue. The approach is similar to our earlier efforts: instead of a pull-model, where we do something for every column family, we can do a push-based model when we detect that column family is ready to be flushed/compacted, we add it to the flush_queue_/compaction_queue_. That way we dont need to loop over every column family in MaybeScheduleFlushOrCompaction. Here are the performance results: Command: ./db_bench Before the patch: fillrandom : 26.950 micros/op 37105 ops/sec; 4.1 MB/s After the patch: fillrandom : 17.404 micros/op 57456 ops/sec; 6.4 MB/s Next bottleneck is VersionSet::AddLiveFiles, which is painfully slow when we have a lot of files. This is coming in the next patch, but when I removed that code, heres what I got: fillrandom : 7.590 micros/op 131758 ops/sec; 14.6 MB/s Test Plan: make check two stress tests: Big number of compactions and flushes: ./db_stress max_background_flushes=0, to verify that this case also works correctly ./db_stress Reviewers: ljin, rven, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Allow Level-Style Compaction to Place Files in Different Paths Summary: Allow Level-style compaction to place files in different paths This diff provides the code for task 4854591. We now support level-compaction to place files in different paths by specifying them in db_paths along with the minimum level for files to store in that path. Test Plan: ManualLevelCompactionOutputPathId in db_test.cc Reviewers: yhchiang, MarkCallaghan, dhruba, yoshinorim, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: rocksdb::ToString() to address cases where std::to_string is not available. Summary: In some environment such as android, the c++ library does not have std::to_string. This path adds rocksdb::ToString(), which wraps std::to_string when std::to_string is not available, and implements std::to_string in the other case. Test Plan: make dbg ./db_test make clean make dbg OPT=-DOS_ANDROID ./db_test Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: SIGSEGV Summary: As a short-term fix, lets go back to previous way of calculating NeedsCompaction(). SIGSEGV happens because NeedsCompaction() can happen before super_version (and thus MutableCFOptions) is initialized. Test Plan: make check Reviewers: ljin, sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: NeedsCompaction() from VersionStorageInfo to CompactionPicker Summary: Move NeedsCompaction() from VersionStorageInfo to CompactionPicker to allow different compaction strategy to have their own way to determine whether doing compaction is necessary. When compaction style is set to kCompactionStyleNone, then NeedsCompaction() will always return false. Test Plan: export ROCKSDB_TESTS=Compact ./db_test Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1120,1120,10.0,0.8319000005722046,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Remember whole key/prefix filtering on/off in SST file Summary: Remember whole key or prefix filtering on/off in SST files. If user opens the DB with a different setting that cannot be satisfied while reading the SST file, ignore the bloom filter. Test Plan: Add a unit test for it Reviewers: yhchiang, igor, rven Reviewed By: rven Subscribers: leveldb, dhruba Differential Revision: deleting obsolete files Summary: For description of the bug, see comment in db_test. The fix is pretty straight forward. Test Plan: added unit test. eventually we need better testing of FOF/POF process. Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: RocksDB stats GET_HIT_L0 and GET_HIT_L1 Summary: In statistics.h , added tickers. In version_set.cc, Added a getter method for hit_file_level_ in the class FilePicker Added a line in the Get() method in case of a found, increment the corresponding counters based on the level of the file respectively. Corresponding task: Personal fork: Test Plan: In terminal, ``` make db_test ROCKSDB_TESTS=L0L1L2AndUpHitCounter ./db_test ``` Or to use debugger, ``` make db_test export ROCKSDB_TESTS=L0L1L2AndUpHitCounter gdb db_test ``` Reviewers: rven, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: deleting obsolete files Summary: This diff basically reverts D30249 and also adds a unit test that was failing before this patch. I have no idea how I didnt catch this terrible bug when writing a diff, sorry about that :( I think we should redesign our system of keeping track of and deleting files. This is already a second bug in this critical piece of code. Ill think of few ideas. BTW this diff is also a regression when running lots of column families. I plan to revisit this separately. Test Plan: added a unit test Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: DestroyDB Summary: When DestroyDB() finds a wal file in the DB directory, it assumes it is actually in WAL directory. This can lead to confusion, since it reports IO error when it tries to delete wal file from DB directory. For example: This change will fix our unit tests. Test Plan: unit tests work Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: ""Fix wal_dir not getting cleaned"" This reverts commit f36d394aeddf420661e54a1a0a54fcc790c9cffb./Add a counter for collecting the wait time on db mutex. Summary: Add a counter for collecting the wait time on db mutex. Also add MutexWrapper and CondVarWrapper for measuring wait time. Test Plan: ./db_test export ROCKSDB_TESTS=MutexWaitStats ./db_test verify stats output using db_bench make clean make release ./db_bench Sample output: rocksdb.db.mutex.wait.micros COUNT : 7546866 Reviewers: MarkCallaghan, rven, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: wal_dir not getting cleaned/db_test: fix a data race in SpecialEnv Summary: db_tests test class SpecialEnv has a thread unsafe variable rnd_ but it can be accessed by multiple threads. It is complained by TSAN. Protect it by a mutex. Test Plan: Run the test Reviewers: yhchiang, rven, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: to use prefix bloom filter when filter is not block based Summary: Get() now doesnt make use of bloom filter if it is prefix based. Add the check. Didnt touch block based bloom filter. I cant fully reason whether it is correct to do that. But its straight-forward to for full bloom filter. Test Plan: make all check Add a test case in DBTest Reviewers: rven, yhchiang, igor Reviewed By: igor Subscribers: MarkCallaghan, leveldb, dhruba, yoshinorim Differential Revision: for supporting cross functional tests for inplace_update Summary: This diff containes the changes to the code and db_test for supporting cross functional tests for inplace_update Test Plan: Run XF with inplace_test and also without Reviewers: igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: functional test infrastructure for RocksDB. Summary: This Diff provides the implementation of the cross functional test infrastructure. This provides the ability to test a single feature with every existing regression test in order to identify issues with interoperability between features. Test Plan: Reference implementation of inplace update support cross functional test. Able to find interoperability issues with inplace support and ran all of db_test. Will add separate diff for those changes. Reviewers: igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: return fixed length prefix, or full key if key is shorter than the fixed length Summary: Add CappedFixTransform, which is the same as fixed length prefix extractor, except that when slice is shorter than the fixed length, it will use the full key. Test Plan: Add a test case for db_test options_test and a new test Reviewers: yhchiang, rven, igor Reviewed By: igor Subscribers: MarkCallaghan, leveldb, dhruba, yoshinorim Differential Revision: Snapshots SequenceNumber Summary: Requested here: It might also help with mongo. I dont see a reason why we shouldnt expose this info. Test Plan: make check Reviewers: sdong, yhchiang, rven Reviewed By: rven Subscribers: dhruba, leveldb Differential Revision: fprintf to stderr instead of stdout in test/New BlockBasedTable version better compressed block format Summary: This diff adds BlockBasedTable format_version 2. New format version brings better compressed block format for these compressions: 1) Zlib encode decompressed size in compressed block header 2) BZip2 encode decompressed size in compressed block header 3) LZ4 and LZ4HC instead of doing memcpy of size_t encode size as varint32. memcpy is very bad because the DB is not portable accross big/little endian machines or even platforms where size_t might be 8 or 4 bytes. It does not affect format for snappy. If you write a new database with format_version 2, it will not be readable by RocksDB versions before 3.10. DB::Open() will return corruption in that case. Test Plan: Added a new test in db_test. I will also run db_bench and verify VSIZE when block_cache 1GB Reviewers: yhchiang, rven, MarkCallaghan, dhruba, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: GetThreadList() to indicate a thread is doing Compaction. Summary: Allow GetThreadList() to indicate a thread is doing Compaction. Test Plan: export ROCKSDB_TESTS=ThreadStatus ./db_test Reviewers: ljin, igor, sdong Reviewed By: sdong Subscribers: leveldb, dhruba, jonahcohen, rven Differential Revision: up FindObsoleteFiles() Summary: There are two versions of FindObsoleteFiles(): * full scan, which is executed every 6 hours (and its terribly slow) * no full scan, which is executed every time a background process finishes and iterator is deleted This diff is optimizing the second case (no full scan). Heres what we do before the diff: * Get the list of obsolete files (files with ref==0). Some files in obsolete_files set might actually be live. * Get the list of live files to avoid deleting files that are live. * Delete files that are in obsolete_files and not in live_files. After this diff: * The only files with ref==0 that are still live are files that have been part of move compaction. Dont include moved files in obsolete_files. * Get the list of obsolete files (which exclude moved files). * No need to get the list of live files, since all files in obsolete_files need to be deleted. Ill post the benchmark results, but you can get the feel of it here: This depends on D30123. P.S. We should do full scan only in failure scenarios, not every 6 hours. Ill do this in a follow-up diff. Test Plan: One new unit test. Made sure that unit test fails if we dont have a `if (f->moved)` safeguard in ~Version. make check Big number of compactions and flushes: ./db_stress Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Allow Level-Style Compaction to Place Files in Different Paths Summary: Allow Level-style compaction to place files in different paths This diff provides the code for task 4854591. We now support level-compaction to place files in different paths by specifying them in db_paths along with the minimum level for files to store in that path. Test Plan: ManualLevelCompactionOutputPathId in db_test.cc Reviewers: yhchiang, MarkCallaghan, dhruba, yoshinorim, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: DBProperty to return number of snapshots and time for oldest snapshot Summary: Add a counter in SnapshotList to show number of snapshots. Also a unix timestamp in every snapshot. Add two DB Properties to return number of snapshots and timestamp of the oldest one. Test Plan: Add unit test checking Reviewers: yhchiang, rven, igor Reviewed By: igor Subscribers: leveldb, dhruba, MarkCallaghan Differential Revision: write buffer memory limit across column families Summary: Introduces a new class for managing write buffer memory across column families. We supplement ColumnFamilyOptions::write_buffer_size with ColumnFamilyOptions::write_buffer, a shared pointer to a WriteBuffer instance that enforces memory limits before flushing out to disk. Test Plan: Added SharedWriteBuffer unit test to db_test.cc Reviewers: sdong, rven, ljin, igor Reviewed By: igor Subscribers: tnovak, yhchiang, dhruba, xjin, MarkCallaghan, yoshinorim Differential Revision: GetThreadList API Summary: Add GetThreadList API, which allows developer to track the status of each process. Currently, calling GetThreadList will only get the list of background threads in RocksDB with their thread-id and thread-type (priority) set. Will add more support on this in the later diffs. ThreadStatus currently has the following properties: // An unique ID for the thread. const uint64_t thread_id; // The type of the thread, it could be ROCKSDB_HIGH_PRIORITY, // ROCKSDB_LOW_PRIORITY, and USER_THREAD const ThreadType thread_type; // The name of the DB instance where the thread is currently // involved with. It would be set to empty string if the thread // does not involve in any DB operation. const std::string db_name; // The name of the column family where the thread is currently // It would be set to empty string if the thread does not involve // in any column family. const std::string cf_name; // The event that the current thread is involved. // It would be set to empty string if the information about event // is not currently available. Test Plan: ./thread_list_test export ROCKSDB_TESTS=GetThreadList ./db_test Reviewers: rven, igor, sdong, ljin Reviewed By: ljin Subscribers: dhruba, leveldb Differential Revision: openable snapshots Summary: Store links to live files in directory on same disk Test Plan: Take snapshot and open it. Added a test GetSnapshotLink in db_test. Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: a unit test for behavior when merge operator and compaction filter co-exist. Summary: Add a unit test in db_test to verify the behavior when both of merge operator and compaction filter apply to a key when merging. Test Plan: Run the new test Reviewers: ljin, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1121,1121,7.0,0.6427000164985657,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Add compaction listener. Summary: This adds a listener for compactions, and gives some useful statistics on each compaction pass. Test Plan: Unit tests. Reviewers: sdong, igor, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: rocksdb::ToString() to address cases where std::to_string is not available. Summary: In some environment such as android, the c++ library does not have std::to_string. This path adds rocksdb::ToString(), which wraps std::to_string when std::to_string is not available, and implements std::to_string in the other case. Test Plan: make dbg ./db_test make clean make dbg OPT=-DOS_ANDROID ./db_test Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: listener_test to avoid possible false alarm Summary: Improve listener_test to avoid possible false alarm Test Plan: ./listener_test/"
1122,1122,10.0,0.9957000017166138,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Added RocksDB stats GET_HIT_L0 and GET_HIT_L1 Summary: In statistics.h , added tickers. In version_set.cc, Added a getter method for hit_file_level_ in the class FilePicker Added a line in the Get() method in case of a found, increment the corresponding counters based on the level of the file respectively. Corresponding task: Personal fork: Test Plan: In terminal, ``` make db_test ROCKSDB_TESTS=L0L1L2AndUpHitCounter ./db_test ``` Or to use debugger, ``` make db_test export ROCKSDB_TESTS=L0L1L2AndUpHitCounter gdb db_test ``` Reviewers: rven, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: a counter for collecting the wait time on db mutex. Summary: Add a counter for collecting the wait time on db mutex. Also add MutexWrapper and CondVarWrapper for measuring wait time. Test Plan: ./db_test export ROCKSDB_TESTS=MutexWaitStats ./db_test verify stats output using db_bench make clean make release ./db_bench Sample output: rocksdb.db.mutex.wait.micros COUNT : 7546866 Reviewers: MarkCallaghan, rven, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: data race Summary: Added requirement that ComputeCompactionScore() be executed in mutex, since its accessing being_compacted bool, which can be mutated by other threads. Also added more comments about thread safety of FileMetaData, since it was a bit confusing. However, it seems that FileMetaData doesnt have data races (except being_compacted) Test Plan: Ran 100 ConvertCompactionStyle tests with thread sanitizer. On master some failures. With this patch none. Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: accurate message for compaction applied to a different version Test Plan: Compile. Run it. Reviewers: yhchiang, dhruba, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: up FindObsoleteFiles() Summary: There are two versions of FindObsoleteFiles(): * full scan, which is executed every 6 hours (and its terribly slow) * no full scan, which is executed every time a background process finishes and iterator is deleted This diff is optimizing the second case (no full scan). Heres what we do before the diff: * Get the list of obsolete files (files with ref==0). Some files in obsolete_files set might actually be live. * Get the list of live files to avoid deleting files that are live. * Delete files that are in obsolete_files and not in live_files. After this diff: * The only files with ref==0 that are still live are files that have been part of move compaction. Dont include moved files in obsolete_files. * Get the list of obsolete files (which exclude moved files). * No need to get the list of live files, since all files in obsolete_files need to be deleted. Ill post the benchmark results, but you can get the feel of it here: This depends on D30123. P.S. We should do full scan only in failure scenarios, not every 6 hours. Ill do this in a follow-up diff. Test Plan: One new unit test. Made sure that unit test fails if we dont have a `if (f->moved)` safeguard in ~Version. make check Big number of compactions and flushes: ./db_stress Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: builders in VersionSet::DumpManifest Summary: Reported by bootcamper This causes ldb tool to fail the assertion in ~ColumnFamilyData() Test Plan: ./ldb put a1 b1 ./ldb manifest_dump Reviewers: sdong, yhchiang, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: NeedsCompaction() from VersionStorageInfo to CompactionPicker Summary: Move NeedsCompaction() from VersionStorageInfo to CompactionPicker to allow different compaction strategy to have their own way to determine whether doing compaction is necessary. When compaction style is set to kCompactionStyleNone, then NeedsCompaction() will always return false. Test Plan: export ROCKSDB_TESTS=Compact ./db_test Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1123,1123,6.0,0.9916999936103821,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","rocksdb: Fixed Dead assignment and Dead initialization scan-build warnings Summary: This diff contains trivial fixes for 6 scan-build warnings: **db/c_test.c** `db` variable is never read. Removed assignment. scan-build report: **db/db_iter.cc** `skipping` local variable is assigned to false. Then in the next switch block the only ""non return"" case assign `skipping` to true, the rest cases dont use it and all do return. scan-build report: **db/log_reader.cc** In `bool Reader::SkipToInitialBlock()` `offset_in_block` local variable is assigned to 0 `if (offset_in_block > kBlockSize 6)` and then never used. Removed the assignment and renamed it to `initial_offset_in_block` to avoid confusion. scan-build report: In `bool Reader::ReadRecord(Slice* record, std::string* scratch)` local variable `in_fragmented_record` in switch case `kFullType` block is assigned to false and then does `return` without use. In the other switch case `kFirstType` block the same `in_fragmented_record` is assigned to false, but later assigned to true without prior use. Removed assignment for both cases. scan-build reprots: **table/plain_table_key_coding.cc** Local variable `user_key_size` is assigned when declared. But then in both places where it is used assigned to `static_cast<uint32_t>(key.size() 8)`. Changed to initialize the variable to the proper value in declaration. scan-build report: **tools/db_stress.cc** Missing `break` in switch case block. This seems to be a bug. Added missing `break`. Test Plan: Make sure all tests are passing and scan-build does not report Dead assignment and Dead initialization bugs. ```lang=bash % make check % make analyze ``` Reviewers: meyering, igor, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1124,1124,2.0,0.9711999893188477,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","rocksdb: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1125,1125,9.0,0.9783999919891357,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Allow GetThreadList() to report operation stage. Summary: Allow GetThreadList() to report operation stage. Test Plan: ./thread_list_test ./db_bench \ \ \ export ROCKSDB_TESTS=ThreadStatus ./db_test Sample output ThreadID ThreadType cfName Operation OP_StartTime ElapsedTime Stage State 140116265861184 Low Pri 140116270055488 Low Pri 140116274249792 High Pri column_family_name_000005 Flush 2015/03/10-14:58:11 0 us FlushJob::WriteLevel0Table 140116400078912 Low Pri column_family_name_000004 Compaction 2015/03/10-14:58:11 0 us CompactionJob::FinishCompactionOutputFile 140116358135872 Low Pri column_family_name_000006 Compaction 2015/03/10-14:58:10 1 us CompactionJob::FinishCompactionOutputFile 140116341358656 Low Pri 140116295221312 High Pri default Flush 2015/03/10-14:58:11 0 us FlushJob::WriteLevel0Table 140116324581440 Low Pri column_family_name_000009 Compaction 2015/03/10-14:58:11 0 us CompactionJob::ProcessKeyValueCompaction 140116278444096 Low Pri 140116299415616 Low Pri column_family_name_000008 Compaction 2015/03/10-14:58:11 0 us CompactionJob::FinishCompactionOutputFile 140116291027008 High Pri column_family_name_000001 Flush 2015/03/10-14:58:11 0 us FlushJob::WriteLevel0Table 140116286832704 Low Pri column_family_name_000002 Compaction 2015/03/10-14:58:11 0 us CompactionJob::FinishCompactionOutputFile 140116282638400 Low Pri Reviewers: rven, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1126,1126,10.0,0.9916999936103821,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Build for CYGWIN Summary: Make it build for CYGWIN. Need to define ""-std=gnu++11"" instead of ""-std=c++11"" and use some replacement functions. Test Plan: Build it and run some unit tests in CYGWIN Reviewers: yhchiang, rven, anthony, kradhakrishnan, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: up rocksDB close call. Summary: On RocksDB, when there are multiple instances doing flushes/compactions in the background, the close call takes a long time because the flushes/compactions need to complete before the database can shut down. If another instance is using the background threads and the compaction for this instance is in the queue since it has been scheduled, we still cannot shutdown. We now remove the scheduled background tasks which have not yet started running, so that shutdown is speeded up. Test Plan: DB Test added. Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Fix Division by zero scan-build warning Summary: scan-build complains with division by zero warning in a test. Added an assertion to prevent this. scan-build report: Test Plan: Make sure scan-build does not report Division by zero and all tests are passing. ```lang=bash % make analyze % make check ``` Reviewers: igor, meyering Reviewed By: meyering Subscribers: sdong, dhruba, leveldb Differential Revision:"
1127,1127,7.0,0.7027999758720398,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[RocksJava] Add compression per level to API Summary: RocksDB offers the possibility to set different compression types on a per level basis. This shall be also available using RocksJava. Test Plan: make rocksdbjava make jtest Reviewers: adamretter, yhchiang, ankgup87 Subscribers: dhruba Differential Revision: 32-Bit adjustments Summary: Before this change overflowing size_t values led to a checked Exception. After that change: size_t overflows on 32-Bit architecture throw now an IllegalArgumentException, which removes the necessity for a developer to catch these Exceptions explicitly. This is especially an advantage for developers targeting 64-Bit systems because it is not necessary anymore to catch exceptions which are never thrown on a 64-Bit system. Test Plan: make clean jclean rocksdbjava jtest mvn rocksjni.pom package Reviewers: adamretter, yhchiang, ankgup87 Subscribers: dhruba Differential Revision: JNI Logger callback Summary: Within this commit a new AbstractLogger was introduced which allows to handle log messages at an application level. Log messages are passed up to Java using a JNI callback. This allows a Java-Developer to use common Java APIs for log messages e.g. SLF4J, LOG4J, etc. Within this commit no new dependencies were introduced, which keeps the RocksDB API clean and doesn`t force a developer to use a predefined high-level Java API. Another feature is to dynamically set a custom loggers verbosity at runtime using its public method `setInfoLogLevel` and to retrieve the currently active level using the `infoLogLevel` method. Test Plan: make clean jclean rocksdbjava jtest mvn rocksjni.pom package Reviewers: adamretter, ankgup87, yhchiang Subscribers: dhruba Differential Revision: Logging JNI callback Within this commit a new AbstractLogger was introduced which pushes info log messages all the way up to Java./"
1128,1128,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","[RocksJava] Final usage correction Summary: Introduced final keyword to parameters with immutable values and classes which should not be derived. Test Plan: make rocksdbjava make jtest Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision:"
1129,1129,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","[RocksJava] Final usage correction Summary: Introduced final keyword to parameters with immutable values and classes which should not be derived. Test Plan: make rocksdbjava make jtest Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision:"
1130,1130,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","[RocksJava] Final usage correction Summary: Introduced final keyword to parameters with immutable values and classes which should not be derived. Test Plan: make rocksdbjava make jtest Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision:"
1131,1131,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","[RocksJava] Final usage correction Summary: Introduced final keyword to parameters with immutable values and classes which should not be derived. Test Plan: make rocksdbjava make jtest Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision:"
1132,1132,7.0,0.8352000117301941,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[RocksJava] Add compression per level to API Summary: RocksDB offers the possibility to set different compression types on a per level basis. This shall be also available using RocksJava. Test Plan: make rocksdbjava make jtest Reviewers: adamretter, yhchiang, ankgup87 Subscribers: dhruba Differential Revision: Final usage correction Summary: Introduced final keyword to parameters with immutable values and classes which should not be derived. Test Plan: make rocksdbjava make jtest Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision:"
1133,1133,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","[RocksJava] Final usage correction Summary: Introduced final keyword to parameters with immutable values and classes which should not be derived. Test Plan: make rocksdbjava make jtest Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision:"
1134,1134,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","[RocksJava] Final usage correction Summary: Introduced final keyword to parameters with immutable values and classes which should not be derived. Test Plan: make rocksdbjava make jtest Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision:"
1135,1135,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","[RocksJava] Final usage correction Summary: Introduced final keyword to parameters with immutable values and classes which should not be derived. Test Plan: make rocksdbjava make jtest Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision:"
1136,1136,11.0,0.8944000005722046,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","[RocksJava] Final usage correction Summary: Introduced final keyword to parameters with immutable values and classes which should not be derived. Test Plan: make rocksdbjava make jtest Reviewers: yhchiang, adamretter, ankgup87 Subscribers: dhruba Differential Revision:"
1137,1137,12.0,0.6122000217437744,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Fix BackupEngine Summary: In D28521 we removed GarbageCollect() from BackupEngines constructor. The reason was that opening BackupEngine on HDFS was very slow and in most cases we didnt have any garbage. We allowed the user to call GarbageCollect() when it detects some garbage files in his backup directory. Unfortunately, this left us vulnerable to an interesting issue. Lets say we started a backup and copied files {1, 3} but the backup failed. On another host, we restore DB from backup and generate {1, 3, 5}. Since {1, 3} is already there, we will not overwrite. However, these files might be from a different database so their contents might be different. See internal task t6781803 for more info. Now, when were copying files and we discover a file already there, we check: 1. if the file is not referenced from any backups, we overwrite the file. 2. if the file is referenced from other backups AND the checksums dont match, we fail the backup. This will only happen if user is using a single backup directory for backing up two different databases. 3. if the file is referenced from other backups AND the checksums match, its all good. We skip the copy and go copy the next file. Test Plan: Added new test to backupable_db_test. The test fails before this patch. Reviewers: sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: use ASSERT_TRUE, not ASSERT_EQ(true; same for false Summary: The usage Im fixing here caused trouble on Fedora 21 when compiling with the current gcc version 4.9.2 20150212 (Red Hat 4.9.2-6) (GCC): db/write_controller_test.cc: In member function virtual void rocksdb::WriteControllerTest_SanityTest_Test::TestBody(): db/write_controller_test.cc:23:165: error: converting false to pointer type for argument 1 of char testing::internal::IsNullLiteralHelper(testing::internal::Secret*) [-Werror=conversion-null] ASSERT_EQ(false, controller.IsStopped()); ^ This change was induced mechanically via: git grep ASSERT_EQ\(false|xargs perl s/ASSERT_EQ\(false, /ASSERT_FALSE(/ git grep ASSERT_EQ\(true|xargs perl s/ASSERT_EQ\(true, /ASSERT_TRUE(/ Except for the three in utilities/backupable/backupable_db_test.cc for which I ended up reformatting (joining lines) in the result. As for why this problem is exhibited with that version of gcc, and none of the others Ive used (from 4.8.1 through gcc-5.0.0 and newer), I suspect its a bug in F21s gcc that has been fixed in gcc-5.0.0. Test Plan: ""make"" now succeed on Fedora 21 Reviewers: ljin, rven, igor.sugak, yhchiang, sdong, igor Reviewed By: igor Subscribers: dhruba Differential Revision: switch to gtest Summary: Our existing test notation is very similar to what is used in gtest. It makes it easy to adopt what is different. In this diff I modify existing [[ | test fixture ]] classes to inherit from `testing::Test`. Also for unit tests that use fixture class, `TEST` is replaced with `TEST_F` as required in gtest. There are several custom `main` functions in our existing tests. To make this transition easier, I modify all `main` functions to fallow gtest notation. But eventually we can remove them and use implementation of `main` that gtest provides. ```lang=bash % cat ~/transform files=$(git ls-files *test\.cc) for file in $files do if grep ""rocksdb::test::RunAllTests()"" $file then if grep ^class \w+Test { $file then perl s/^(class \w+Test) {/${1}: public testing::Test {/g $file perl s/^(TEST)/${1}_F/g $file fi perl s/(int main.*\{)/${1}::testing::InitGoogleTest(&argc, argv);/g $file perl s/rocksdb::test::RunAllTests/RUN_ALL_TESTS/g $file fi done % sh ~/transform % make format ``` Second iteration of this diff contains only scripted changes. Third iteration contains manual changes to fix last errors and make it compilable. Test Plan: Build and notice no errors. ```lang=bash % USE_CLANG=1 make check ``` Tests are still testing. Reviewers: meyering, sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: Replace ASSERT* with EXPECT* in functions that does not return void value Summary: gtest does not use exceptions to fail a unit test by design, and `ASSERT*`s are implemented using `return`. As a consequence we cannot use `ASSERT*` in a function that does not return `void` value ([[ | 1]]), and have to fix our existing code. This diff does this in a generic way, with no manual changes. In order to detect all existing `ASSERT*` that are used in functions that doesnt return void value, I change the code to generate compile errors for such cases. In `util/testharness.h` I defined `EXPECT*` assertions, the same way as `ASSERT*`, and redefined `ASSERT*` to return `void`. Then executed: ```lang=bash % USE_CLANG=1 make all 2> build.log % perl print ""-- "".$F[0].""\n"" if /: error:/ \ build.log | xargs 1 perl s/ASSERT/EXPECT/g if $. $number % make format ``` After that I reverted back change to `ASSERT*` in `util/testharness.h`. But preserved introduced `EXPECT*`, which is the same as `ASSERT*`. This will be deleted once switched to gtest. This diff is independent and contains manual changes only in `util/testharness.h`. Test Plan: Make sure all tests are passing. ```lang=bash % USE_CLANG=1 make check ``` Reviewers: igor, lgalanis, sdong, yufei.zhu, rven, meyering Reviewed By: meyering Subscribers: dhruba, leveldb Differential Revision: a bug in ReadOnlyBackupEngine Summary: This diff fixes a bug introduced by D28521. Read-only backup engine can delete a backup that is later than the latest we never check the condition. I also added a bunch of logging that will help with debugging cases like this in the future. See more discussion at t6218248. Test Plan: Added a unit test that was failing before the change. Also, see new LOG file contents: Reviewers: benj, sanketh, sumeet, yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1138,1138,14.0,0.8812000155448914,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Return fbson Summary: mac compile is fixed in fbson, so it can be returned back from 7ce1b2c Test Plan: make all check make valgrind_check Reviewers: golovachalexander, igor Reviewed By: igor Subscribers: dhruba Differential Revision: ""Fbson to Json"" This reverts commit 7ce1b2c19c00f303214305ac4cc9a67296ede84a./"
1139,1139,10.0,0.9049999713897705,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fix build Test Plan: Running make all Reviewers: sdong Reviewed By: sdong Subscribers: rven, yhchiang, igor, meyering, dhruba Differential Revision:"
1140,1140,7.0,0.9869999885559082,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","A new call back to TablePropertiesCollector to allow users know the entry is add, delete or merge Summary: Currently users have no idea a key is add, delete or merge from TablePropertiesCollector call back. Add a new function to add it. Also refactor the codes so that (1) make table property collector and internal table property collector two separate data structures with the later one now exposed (2) table builders only receive internal table properties Test Plan: Add cases in table_properties_collector_test to cover both of old and new ways of using TablePropertiesCollector. Reviewers: yhchiang, igor.sugak, rven, igor Reviewed By: rven, igor Subscribers: meyering, yoshinorim, maykov, leveldb, dhruba Differential Revision: the way options.compression_per_level is used when options.level_compaction_dynamic_level_bytes=true Summary: Change the way options.compression_per_level is used when options.level_compaction_dynamic_level_bytes=true so that options.compression_per_level[1] determines compression for the level L0 is merged to, options.compression_per_level[2] to the level after that, etc. Test Plan: run all tests Reviewers: rven, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: yoshinorim, leveldb, dhruba Differential Revision:"
1141,1141,10.0,0.9817000031471252,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","fix typos/rocksdb: Fix scan-build memory warning in table/block_based_table_reader.cc Summary: scan-build is reporting two memory leak bugs in `table/block_based_table_reader.cc`. They are both false positives. In both cases we allocate memory in `ReadBlockFromFile` if `s.ok()`. Then after the function `ReadBlockFromFile` returns we check for the same variable if `s.ok()` and then use the memory that was allocated. The bugs reported by scan-build is if `ReadBlockFromFile` allocates memory and returns, but for some reason status `s` is not the same and `s.ok() true`. In this case scan-build is concerned that memory owner transfer is not explicit. I modified `ReadBlockFromFile` to accept `std::unique_ptr<Block>*` as a parameter, instead of raw pointer. scan-build reports: Test Plan: Make sure scan-build does not report these bugs and all tests are passing. ```lang=bash % make check % make analyze ``` Reviewers: sdong, lgalanis, meyering, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1142,1142,12.0,0.5993000268936157,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Adding stats for the merge and filter operation Summary: We have addded new stats and perf_context for measuring the merge and filter operation time consumption. We have bounded all the merge operations within the GUARD statment and collected the total time for these operations in the DB. Test Plan: WIP Reviewers: rven, yhchiang, kradhakrishnan, igor, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: switch to gtest Summary: Our existing test notation is very similar to what is used in gtest. It makes it easy to adopt what is different. In this diff I modify existing [[ | test fixture ]] classes to inherit from `testing::Test`. Also for unit tests that use fixture class, `TEST` is replaced with `TEST_F` as required in gtest. There are several custom `main` functions in our existing tests. To make this transition easier, I modify all `main` functions to fallow gtest notation. But eventually we can remove them and use implementation of `main` that gtest provides. ```lang=bash % cat ~/transform files=$(git ls-files *test\.cc) for file in $files do if grep ""rocksdb::test::RunAllTests()"" $file then if grep ^class \w+Test { $file then perl s/^(class \w+Test) {/${1}: public testing::Test {/g $file perl s/^(TEST)/${1}_F/g $file fi perl s/(int main.*\{)/${1}::testing::InitGoogleTest(&argc, argv);/g $file perl s/rocksdb::test::RunAllTests/RUN_ALL_TESTS/g $file fi done % sh ~/transform % make format ``` Second iteration of this diff contains only scripted changes. Third iteration contains manual changes to fix last errors and make it compilable. Test Plan: Build and notice no errors. ```lang=bash % USE_CLANG=1 make check ``` Tests are still testing. Reviewers: meyering, sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: Small refactoring before migrating to gtest Summary: These changes are necessary to make tests look more generic, and avoid feature conflicts with gtest. Test Plan: Make sure no build errors, and all test are passing. ``` % make check ``` Reviewers: igor, meyering Reviewed By: meyering Subscribers: dhruba, leveldb Differential Revision: functionality to pre-fetch blocks specified by a key range to BlockBasedTable implementation. Summary: Pre-fetching is a common operation performed by data stores for disk/flash based systems as part of database startup. This is part of task 5197184. Test Plan: Run the newly added unit test Reviewers: rven, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: add missing 5th arg in TestArgs initializer Summary: Adding and to CXXFLAGS provoked this failure: table/table_test.cc:1854:56: error: missing initializer for member rocksdb::TestArgs::format_version [-Werror=missing-field-initializers] TestArgs args { DB_TEST, false, 16, kNoCompression }; ^ Add the missing, 5th value (format_version). Test Plan: Run ""make EXTRA_CXXFLAGS=-W and see fewer errors. Reviewers: ljin, sdong, igor.sugak, igor Reviewed By: igor Subscribers: dhruba Differential Revision:"
1143,1143,7.0,0.6478999853134155,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Add more table properties to EventLogger Summary: Example output: {""time_micros"": 1431463794310521, ""job"": 353, ""event"": ""table_file_creation"", ""file_number"": 387, ""file_size"": 86937, ""table_info"": {""data_size"": ""81801"", ""index_size"": ""9751"", ""filter_size"": ""0"", ""raw_key_size"": ""23448"", ""raw_average_key_size"": ""24.000000"", ""raw_value_size"": ""990571"", ""raw_average_value_size"": ""1013.890481"", ""num_data_blocks"": ""245"", ""num_entries"": ""977"", ""filter_policy_name"": """", ""kDeletedKeys"": ""0""}} Also fixed a bug where BuildTable() in recovery was passing Env::IOHigh argument into paranoid_checks_file parameter. Test Plan: make check + check out the output in the log Reviewers: sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: CompactionJob Summary: Couple changes: 1. instead of SnapshotList, just take a vector of snapshots 2. dont take a separate parameter is_snapshots_supported. If there are snapshots in the list, that means they are supported. I actually think we should get rid of this notion of snapshots not being supported. 3. dont pass in mutable_cf_options as a parameter. Lifetime of mutable_cf_options is a bit tricky to maintain, so its better to not pass it in for the whole compaction job. We only really need it when we install the compaction results. Test Plan: make check Reviewers: sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: crashes in stats and compaction filter for db_ttl_impl Summary: fix crashes in stats and compaction filter for db_ttl_impl Test Plan: Ran build with lots of debugging Reviewers: yhchiang, igor, rven Reviewed By: igor Subscribers: rven, dhruba Differential Revision: GetApproximateSizes() to use lesser CPU cycles. Summary: CPU profiling reveals GetApproximateSizes as a bottleneck for performance. The current implementation is sub-optimal, it scans every file in every level to compute the result. We can take advantage of the fact that all levels above 0 are sorted in the increasing order of key ranges and use binary search to locate the starting index. This can reduce the number of comparisons required to compute the result. Test Plan: We have good test coverage. Run the tests. Reviewers: sdong, igor, rven, dynamike Subscribers: dynamike, maykov, dhruba, leveldb Differential Revision: bunch of more events into EventLogger Summary: Added these events: * Recovery start, finish and also when recovery creates a file * Trivial move * Compaction start, finish and when compaction creates a file * Flush start, finish Also includes small fix to EventLogger Also added option ROCKSDB_PRINT_EVENTS_TO_STDOUT which is useful when we debug things. Ive spent far too much time chasing LOG files. Still didnt get sst table properties in JSON. They are written very deeply into the stack. Ill address in separate diff. TODO: * Write specification. Lets first use this for a while and figure out whats good data to put here, too. After that well write spec * Write tools that parse and analyze LOGs. This can be in python or go. Good intern task. Test Plan: Ran db_bench with ROCKSDB_PRINT_EVENTS_TO_STDOUT. Heres the output: Reviewers: sdong, yhchiang, rven, MarkCallaghan, kradhakrishnan, anthony Reviewed By: anthony Subscribers: dhruba, leveldb Differential Revision: of trivial move of dynamic level Summary: D36669 introduces a bug that trivial moved data is not going to specific level but the next level, which will incorrectly be level 1 for level 0 compaciton if base level is not level 1. Fixing it by appreciating the output level Test Plan: Run all tests Reviewers: MarkCallaghan, rven, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: up compression logging Summary: Now we add warnings when user configures compression and the compression is not supported. Test Plan: Configured compression to non-supported values. Observed messages in my log: 2015/03/26-12:17:57.586341 7ffb8a496840 [WARN] Compression type chosen for level 2 is not supported: LZ4. RocksDB will not compress data on level 2. 2015/03/26-12:19:10.768045 7f36f15c5840 [WARN] Compression type chosen is not supported: LZ4. RocksDB will not compress data. Reviewers: rven, sdong, yhchiang Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: new call back to TablePropertiesCollector to allow users know the entry is add, delete or merge Summary: Currently users have no idea a key is add, delete or merge from TablePropertiesCollector call back. Add a new function to add it. Also refactor the codes so that (1) make table property collector and internal table property collector two separate data structures with the later one now exposed (2) table builders only receive internal table properties Test Plan: Add cases in table_properties_collector_test to cover both of old and new ways of using TablePropertiesCollector. Reviewers: yhchiang, igor.sugak, rven, igor Reviewed By: rven, igor Subscribers: meyering, yoshinorim, maykov, leveldb, dhruba Differential Revision: trivial move if compression level is different Summary: Check compression level of start_level with output_compression before allowing trivial move Test Plan: New DBTest CompressLevelCompactionThirdPath added Reviewers: igor, yhchiang, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Compactions with Small Files Summary: With this change, we use L1 and up to store compaction outputs in universal compaction. The compaction pick logic stays the same. Outputs are stored in the largest ""level"" as possible. If options.num_levels=1, it behaves all the same as now. Test Plan: 1) convert most of existing unit tests for universal comapaction to include the option of one level and multiple levels. 2) add a unit test to cover parallel compaction in universal compaction and run it in one level and multiple levels 3) add unit test to migrate from multiple level setting back to one level setting 4) add a unit test to insert keys to trigger multiple rounds of compactions and verify results. Reviewers: rven, kradhakrishnan, yhchiang, igor Reviewed By: igor Subscribers: meyering, leveldb, MarkCallaghan, dhruba Differential Revision: up old log files in background threads Summary: Cleaning up log files can do heavy IO, since we call ftruncate() in the destructor. We dont want to call ftruncate() in user threads. This diff moves cleaning to background threads (flush and compaction) Test Plan: make check, will also run valgrind Reviewers: yhchiang, rven, MarkCallaghan, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: the DB properties string definitions. Summary: Assign the string properties to const string variables under the DB::Properties namespace. This helps catch typos during compilation and also consolidates the property definition in one place. Test Plan: Run rocksdb unit tests Reviewers: sdong, yoshinorim, igor Subscribers: dhruba Differential Revision: delete files when column family is dropped Summary: To understand the bug read t5943287 and check out the new test in column_family_test (ReadDroppedColumnFamily), iter 0. RocksDB contract allowes you to read a drop column family as long as there is a live reference. However, since our iteration ignores dropped column families, AddLiveFiles() didnt mark files of a dropped column families as live. So we deleted them. In this patch I no longer ignore dropped column families in the iteration. I think this behavior was confusing and it also led to this bug. Now if an iterator client wants to ignore dropped column families, he needs to do it explicitly. Test Plan: Added a new unit test that is failing on master. Unit test succeeds now. Reviewers: sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: unused parameter in CancelAllBackgroundWork Summary: Some suggestions for cleanup from Igor. Test Plan: Regression tests. Reviewers: igor Reviewed By: igor Subscribers: dhruba Differential Revision: up rocksDB close call. Summary: On RocksDB, when there are multiple instances doing flushes/compactions in the background, the close call takes a long time because the flushes/compactions need to complete before the database can shut down. If another instance is using the background threads and the compaction for this instance is in the queue since it has been scheduled, we still cannot shutdown. We now remove the scheduled background tasks which have not yet started running, so that shutdown is speeded up. Test Plan: DB Test added. Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Summary: Heres my proposal for making our LOGs easier to read by machines. The idea is to dump all events as JSON objects. JSON is easy to read by humans, but more importantly, its easy to read by machines. That way, we can parse this, load into SQLite/mongo and then query or visualize. I started with table_create and table_delete events, but if everybody agrees, Ill continue by adding more events (flush/compaction/etc etc) Test Plan: Ran db_bench. Observed: 2015/01/15-14:13:25.788019 1105ef000 EVENT_LOG_v1 {""time_micros"": 1421360005788015, ""event"": ""table_file_creation"", ""file_number"": 12, ""file_size"": 1909699} 2015/01/15-14:13:25.956500 110740000 EVENT_LOG_v1 {""time_micros"": 1421360005956498, ""event"": ""table_file_deletion"", ""file_number"": 12} Reviewers: yhchiang, rven, dhruba, MarkCallaghan, lgalanis, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: a bug where CompactFiles wont delete obsolete files until flush. Summary: Fixed a bug where CompactFiles wont delete obsolete files until flush. Test Plan: ./compact_files_test export ROCKSDB_TESTS=CompactFiles ./db_test Reviewers: rven, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: a mechanism to inform Rocksdb that it is shutting down Summary: Provide an API which enables users to infor Rocksdb that it is shutting down. Test Plan: db_test Reviewers: sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: a bug in stall time counter. Improve its output format. Summary: Fix a bug in stall time counter. Improve its output format. Test Plan: export ROCKSDB_TESTS=Timeout ./db_test ./db_bench sample output: Uptime(secs): 35.8 total, 0.0 interval Cumulative writes: 359590 writes, 359589 keys, 183047 batches, 2.0 writes per batch, 0.04 GB user ingest, stall seconds: 1786.008 ms Cumulative WAL: 359591 writes, 183046 syncs, 1.96 writes per sync, 0.04 GB written Interval writes: 253 writes, 253 keys, 128 batches, 2.0 writes per batch, 0.0 MB user ingest, stall time: 0 us Interval WAL: 253 writes, 128 syncs, 1.96 writes per sync, 0.00 MB written Reviewers: MarkCallaghan, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: config errors with L0 file count triggers Test Plan: Run ""make clean && make all check"" Reviewers: rven, igor, yhchiang, kradhakrishnan, MarkCallaghan, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: do not relink every single binary just for a timestamp Summary: Prior to this change, ""make check"" would always waste a lot of time relinking 60+ binaries. With this change, it does that only when the generated file, util/build_version.cc, changes, and that happens only when the date changes or when the current git SHA changes. This change makes some other improvements: before, there was no rule to build a deleted util/build_version.cc. If it was somehow removed, any attempt to link a program would fail. There is no longer any need for the separate file, build_tools/build_detect_version. Its functionality is now in the Makefile. * Makefile (DEPFILES): Dont filter-out util/build_version.cc. No need, and besides, removing that dependency was wrong. (date, git_sha, gen_build_version): New helper variables. (util/build_version.cc): New rule, to create this file and update it only if it would contain new information. * build_tools/build_detect_platform: Remove file. * db/db_impl.cc: Now, print only date (not the time). * util/build_version.h (rocksdb_build_compile_time): Remove declaration. No longer used. Test Plan: Run ""make check"" twice, and note that the second time no linking is performed. Remove util/build_version.cc and ensure that any ""make"" command regenerates it before doing anything else. Run this: strings librocksdb.a|grep _build_. That prints output including the following: rocksdb_build_git_date:2015-02-19 rocksdb_build_git_sha:2.8.fb-1792-g3cb6cc0 Reviewers: ljin, sdong, igor Reviewed By: igor Subscribers: dhruba Differential Revision: job_id for flush and compaction Summary: It would be good to assing background job their IDs. Two benefits: 1) makes LOGs more readable 2) I might use it in my EventLogger, which will try to make our LOG easier to read/query/visualize Test Plan: ran rocksdb, read the LOG Reviewers: sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
1144,1144,2.0,0.9711999893188477,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","rocksdb: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1145,1145,7.0,0.8295999765396118,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Reset parent_index and base_index when picking files marked for compaction Summary: This caused a crash of our MongoDB + RocksDB instance. PickCompactionBySize() sets its own parent_index. We never reset this parent_index when picking PickFilesMarkedForCompactionExperimental(). So we might end up doing SetupOtherInputs() with parent_index that was set by PickCompactionBySize, although were using compaction calculated using PickFilesMarkedForCompactionExperimental. Test Plan: Added a unit test that fails with assertion on master. Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: is manual compaction Summary: When reporting compaction that was started because of SuggestCompactRange() we should treat it as manual compaction. Test Plan: none Reviewers: yhchiang, rven Reviewed By: rven Subscribers: dhruba, leveldb Differential Revision: GetRange Function Summary: Optimize GetRange Function by checking the level of the files Test Plan: pass make all check Reviewers: rven, yhchiang, igor Reviewed By: igor Subscribers: dhruba Differential Revision: typos/Fix CompactRange for universal compaction with num_levels > 1 Summary: CompactRange for universal compaction with num_levels > 1 seems to have a bug. The unit test also has a bug so it doesnt capture the problem. Fix it. Revert the compact range to the logic equivalent to num_levels=1. Always compact all files together. It should also fix DBTest.IncreaseUniversalCompactionNumLevels. The issue was that options.write_buffer_size 100 10 and options.write_buffer_size 100 10 are not used in later test scenarios. So write_buffer_size of 4MB was used. The compaction trigger condition is not anymore obvious as expected. Test Plan: Run the new test and all test suites Reviewers: yhchiang, rven, kradhakrishnan, anthony, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: an assertion in CompactionPicker Summary: Reading CompactionPicker I noticed this dangerous substraction of two unsigned integers. We should assert to mark this as safe. Test Plan: make check Reviewers: anthony, rven, yhchiang, sdong Reviewed By: sdong Subscribers: kradhakrishnan, dhruba, leveldb Differential Revision: experimental API MarkForCompaction() Summary: Some Mongo+Rocks datasets in Parses environment are not doing compactions very frequently. During the quiet period (with no IO), wed like to schedule compactions so that our reads become faster. Also, aggressively compacting during quiet periods helps when write bursts happen. In addition, we also want to compact files that are containing deleted key ranges (like old oplog keys). All of this is currently not possible with CompactRange() because its single-threaded and blocks all other compactions from happening. Running CompactRange() risks an issue of blocking writes because we generate too much Level 0 files before the compaction is over. Stopping writes is very dangerous because they hold transaction locks. We tried running manual compaction once on Mongo+Rocks and everything fell apart. MarkForCompaction() solves all of those problems. This is very light-weight manual compaction. It is lower priority than automatic compactions, which means it shouldnt interfere with background process keeping the LSM tree clean. However, if no automatic compactions need to be run (or we have extra background threads available), we will start compacting files that are marked for compaction. Test Plan: added a new unit test Reviewers: yhchiang, rven, MarkCallaghan, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: bug in ExpandWhileOverlapping() Summary: If ExpandWhileOverlapping() we dont clear inputs. Thats a bug introduced by my recent patch However, we have no tests covering ExpandWhileOverlapping(). I created a task t6771252 to add ExpandWhileOverlapping() tests. Test Plan: make check Reviewers: sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: to allow a callback with an argument and use it to get DBTest.DynamicLevelCompressionPerLevel2 more straight-forward Summary: Allow users to give a callback function with parameter using sync point, so more complicated verification can be done in tests. Use it in DBTest.DynamicLevelCompressionPerLevel2 so that failures will be more easy to debug. Test Plan: Run all tests. Run DBTest.DynamicLevelCompressionPerLevel2 with valgrind check. Reviewers: rven, yhchiang, anthony, kradhakrishnan, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: compile warning on CLANG Summary: oops Test Plan: compiles now Reviewers: sdong, yhchiang Subscribers: dhruba, leveldb Differential Revision: Compaction class easier to use Summary: The goal of this diff is to make Compaction class easier to use. This should also make new compaction algorithms easier to write (like CompactFiles from and dynamic leveled and multi-leveled universal from Here are couple of things demonstrating that Compaction class is hard to use: 1. we have two constructors of Compaction class 2. theres this thing called grandparents_, but it appears to only be setup for leveled compaction and not compactfiles 3. its easy to introduce a subtle and dangerous bug like this: D36225 4. SetupBottomMostLevel() is hard to understand and it shouldnt be. See this comment: It also made it harder for to write CompactFiles, as evidenced by this: The problem is that we create Compaction object, which holds a lot of state, and then pass it around to some functions. After those functions are done mutating, then we call couple of functions on Compaction object, like SetupBottommostLevel() and MarkFilesBeingCompacted(). It is very hard to see whats happening with all that Compactions state while its travelling across different functions. If youre writing a new PickCompaction() function you need to try really hard to understand what are all the functions you need to run on Compaction object and what state you need to setup. My proposed solution is to make important parts of Compaction immutable after construction. PickCompaction() should calculate compaction inputs and then pass them onto Compaction object once they are finalized. That makes it easy to create a new compaction just provide all the parameters to the constructor and youre done. No need to call confusing functions after you created your object. This diff doesnt fully achieve that goal, but it comes pretty close. Here are some of the changes: * have one Compaction constructor instead of two. * inputs_ is constant after construction * MarkFilesBeingCompacted() is now private to Compaction class and automatically called on construction/destruction. * SetupBottommostLevel() is gone. Compaction figures it out on its own based on the input. * CompactionPickers functions are not passing around Compaction object anymore. They are only passing around the state that they need. Test Plan: make check make asan_check make valgrind_check Reviewers: rven, anthony, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, yhchiang, dhruba, leveldb Differential Revision: Compactions with Small Files Summary: With this change, we use L1 and up to store compaction outputs in universal compaction. The compaction pick logic stays the same. Outputs are stored in the largest ""level"" as possible. If options.num_levels=1, it behaves all the same as now. Test Plan: 1) convert most of existing unit tests for universal comapaction to include the option of one level and multiple levels. 2) add a unit test to cover parallel compaction in universal compaction and run it in one level and multiple levels 3) add unit test to migrate from multiple level setting back to one level setting 4) add a unit test to insert keys to trigger multiple rounds of compactions and verify results. Reviewers: rven, kradhakrishnan, yhchiang, igor Reviewed By: igor Subscribers: meyering, leveldb, MarkCallaghan, dhruba Differential Revision: up compactions_in_progress_ Summary: Suprisingly, the only way we use this vector is to keep track of level0 compactions. Thus, I simplified it. Test Plan: make check Reviewers: rven, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: the way options.compression_per_level is used when options.level_compaction_dynamic_level_bytes=true Summary: Change the way options.compression_per_level is used when options.level_compaction_dynamic_level_bytes=true so that options.compression_per_level[1] determines compression for the level L0 is merged to, options.compression_per_level[2] to the level after that, etc. Test Plan: run all tests Reviewers: rven, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: yoshinorim, leveldb, dhruba Differential Revision: to allow RocksDB to pick size bases of levels dynamically. Summary: When having fixed max_bytes_for_level_base, the ratio of size of largest level and the second one can range from 0 to the multiplier. This makes LSM tree frequently irregular and unpredictable. It can also cause poor space amplification in some cases. In this improvement (proposed by Igor Kabiljo), we introduce a parameter option.level_compaction_use_dynamic_max_bytes. When turning it on, RocksDB is free to pick a level base in the range of (options.max_bytes_for_level_base/options.max_bytes_for_level_multiplier, options.max_bytes_for_level_base] so that real level ratios are close to options.max_bytes_for_level_multiplier. Test Plan: New unit tests and pass tests suites including valgrind. Reviewers: MarkCallaghan, rven, yhchiang, igor, ikabiljo Reviewed By: ikabiljo Subscribers: yoshinorim, ikabiljo, dhruba, leveldb Differential Revision:"
1146,1146,2.0,0.5730999708175659,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Fix make unity build compiler warning about ""stats"" shadowing global variable Summary: Fix the make unity build. The local stats variable name was shadowing a global stats variable. Test Plan: Run the build OPT=-DTRAVIS V=1 make unity Reviewers: sdong, igor Reviewed By: igor Subscribers: dhruba Differential Revision: the DB properties string definitions. Summary: Assign the string properties to const string variables under the DB::Properties namespace. This helps catch typos during compilation and also consolidates the property definition in one place. Test Plan: Run rocksdb unit tests Reviewers: sdong, yoshinorim, igor Subscribers: dhruba Differential Revision: a DB Property For Number of Deletions in Memtables Summary: Add a DB property for number of deletions in memtables. It can sometimes help people debug slowness because of too many deletes. Test Plan: Add test cases. Reviewers: rven, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: leveldb, dhruba, yoshinorim Differential Revision: compaction IO stats to handle large file counts Summary: The output did not have space for 6-digit file counts or for 3-digit counts of files being compacted. This adds space for that while preserving existing alignment. See Task ID: Blame Rev: Test Plan: run db_bench, look at output Revert Plan: Database Impact: Memcache Impact: Other Notes: EImportant: begin *PUBLIC* platform impact section Bugzilla: end platform impact Reviewers: igor Reviewed By: igor Subscribers: dhruba Differential Revision: RecordIn/RecordOut human readable Summary: I had hard time understanding these big numbers. Heres how the output looks like now: Test Plan: db_bench Reviewers: sdong, MarkCallaghan Reviewed By: MarkCallaghan Subscribers: dhruba, leveldb Differential Revision: negative Wnew Summary: we are using uint64_t for Wnew this is not correct since this value can be negative Test Plan: run db_bench and check what happens when Wnew is Reviewers: sdong, igor Reviewed By: igor Subscribers: dhruba Differential Revision: a bug in stall time counter. Improve its output format. Summary: Fix a bug in stall time counter. Improve its output format. Test Plan: export ROCKSDB_TESTS=Timeout ./db_test ./db_bench sample output: Uptime(secs): 35.8 total, 0.0 interval Cumulative writes: 359590 writes, 359589 keys, 183047 batches, 2.0 writes per batch, 0.04 GB user ingest, stall seconds: 1786.008 ms Cumulative WAL: 359591 writes, 183046 syncs, 1.96 writes per sync, 0.04 GB written Interval writes: 253 writes, 253 keys, 128 batches, 2.0 writes per batch, 0.0 MB user ingest, stall time: 0 us Interval WAL: 253 writes, 128 syncs, 1.96 writes per sync, 0.00 MB written Reviewers: MarkCallaghan, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: to allow RocksDB to pick size bases of levels dynamically. Summary: When having fixed max_bytes_for_level_base, the ratio of size of largest level and the second one can range from 0 to the multiplier. This makes LSM tree frequently irregular and unpredictable. It can also cause poor space amplification in some cases. In this improvement (proposed by Igor Kabiljo), we introduce a parameter option.level_compaction_use_dynamic_max_bytes. When turning it on, RocksDB is free to pick a level base in the range of (options.max_bytes_for_level_base/options.max_bytes_for_level_multiplier, options.max_bytes_for_level_base] so that real level ratios are close to options.max_bytes_for_level_multiplier. Test Plan: New unit tests and pass tests suites including valgrind. Reviewers: MarkCallaghan, rven, yhchiang, igor, ikabiljo Reviewed By: ikabiljo Subscribers: yoshinorim, ikabiljo, dhruba, leveldb Differential Revision: rocksdb.num-live-versions: number of live versions Summary: Add a DB property about live versions. It can be helpful to figure out whether there are files not live but not yet deleted, in some use cases. Test Plan: make all check Reviewers: rven, yhchiang, igor Reviewed By: igor Subscribers: yoshinorim, dhruba, leveldb Differential Revision:"
1147,1147,7.0,0.9898999929428101,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Add a DB Property For Number of Deletions in Memtables Summary: Add a DB property for number of deletions in memtables. It can sometimes help people debug slowness because of too many deletes. Test Plan: Add test cases. Reviewers: rven, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: leveldb, dhruba, yoshinorim Differential Revision: to allow RocksDB to pick size bases of levels dynamically. Summary: When having fixed max_bytes_for_level_base, the ratio of size of largest level and the second one can range from 0 to the multiplier. This makes LSM tree frequently irregular and unpredictable. It can also cause poor space amplification in some cases. In this improvement (proposed by Igor Kabiljo), we introduce a parameter option.level_compaction_use_dynamic_max_bytes. When turning it on, RocksDB is free to pick a level base in the range of (options.max_bytes_for_level_base/options.max_bytes_for_level_multiplier, options.max_bytes_for_level_base] so that real level ratios are close to options.max_bytes_for_level_multiplier. Test Plan: New unit tests and pass tests suites including valgrind. Reviewers: MarkCallaghan, rven, yhchiang, igor, ikabiljo Reviewed By: ikabiljo Subscribers: yoshinorim, ikabiljo, dhruba, leveldb Differential Revision: rocksdb.num-live-versions: number of live versions Summary: Add a DB property about live versions. It can be helpful to figure out whether there are files not live but not yet deleted, in some use cases. Test Plan: make all check Reviewers: rven, yhchiang, igor Reviewed By: igor Subscribers: yoshinorim, dhruba, leveldb Differential Revision:"
1148,1148,7.0,0.5742999911308289,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Universal Compaction with multiple levels wont allocate up to output size Summary: Universal compactions with multiple levels should use file preallocation size based on file size if output level is not level 0 Test Plan: Run all tests. Reviewers: igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: is manual compaction Summary: When reporting compaction that was started because of SuggestCompactRange() we should treat it as manual compaction. Test Plan: none Reviewers: yhchiang, rven Reviewed By: rven Subscribers: dhruba, leveldb Differential Revision: hang with large write batches and column families. Summary: This diff fixes a hang reported by a Github user. Multiple large write batches with column families cause a hang. The issue was caused by not doing flushes/compaction when the write controller was stopped. Test Plan: Create a DBTest from the users test case Reviewers: igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: CompactRange for universal compaction with num_levels > 1 Summary: CompactRange for universal compaction with num_levels > 1 seems to have a bug. The unit test also has a bug so it doesnt capture the problem. Fix it. Revert the compact range to the logic equivalent to num_levels=1. Always compact all files together. It should also fix DBTest.IncreaseUniversalCompactionNumLevels. The issue was that options.write_buffer_size 100 10 and options.write_buffer_size 100 10 are not used in later test scenarios. So write_buffer_size of 4MB was used. The compaction trigger condition is not anymore obvious as expected. Test Plan: Run the new test and all test suites Reviewers: yhchiang, rven, kradhakrishnan, anthony, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: experimental API MarkForCompaction() Summary: Some Mongo+Rocks datasets in Parses environment are not doing compactions very frequently. During the quiet period (with no IO), wed like to schedule compactions so that our reads become faster. Also, aggressively compacting during quiet periods helps when write bursts happen. In addition, we also want to compact files that are containing deleted key ranges (like old oplog keys). All of this is currently not possible with CompactRange() because its single-threaded and blocks all other compactions from happening. Running CompactRange() risks an issue of blocking writes because we generate too much Level 0 files before the compaction is over. Stopping writes is very dangerous because they hold transaction locks. We tried running manual compaction once on Mongo+Rocks and everything fell apart. MarkForCompaction() solves all of those problems. This is very light-weight manual compaction. It is lower priority than automatic compactions, which means it shouldnt interfere with background process keeping the LSM tree clean. However, if no automatic compactions need to be run (or we have extra background threads available), we will start compacting files that are marked for compaction. Test Plan: added a new unit test Reviewers: yhchiang, rven, MarkCallaghan, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: to allow a callback with an argument and use it to get DBTest.DynamicLevelCompressionPerLevel2 more straight-forward Summary: Allow users to give a callback function with parameter using sync point, so more complicated verification can be done in tests. Use it in DBTest.DynamicLevelCompressionPerLevel2 so that failures will be more easy to debug. Test Plan: Run all tests. Run DBTest.DynamicLevelCompressionPerLevel2 with valgrind check. Reviewers: rven, yhchiang, anthony, kradhakrishnan, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: Compaction class easier to use Summary: The goal of this diff is to make Compaction class easier to use. This should also make new compaction algorithms easier to write (like CompactFiles from and dynamic leveled and multi-leveled universal from Here are couple of things demonstrating that Compaction class is hard to use: 1. we have two constructors of Compaction class 2. theres this thing called grandparents_, but it appears to only be setup for leveled compaction and not compactfiles 3. its easy to introduce a subtle and dangerous bug like this: D36225 4. SetupBottomMostLevel() is hard to understand and it shouldnt be. See this comment: It also made it harder for to write CompactFiles, as evidenced by this: The problem is that we create Compaction object, which holds a lot of state, and then pass it around to some functions. After those functions are done mutating, then we call couple of functions on Compaction object, like SetupBottommostLevel() and MarkFilesBeingCompacted(). It is very hard to see whats happening with all that Compactions state while its travelling across different functions. If youre writing a new PickCompaction() function you need to try really hard to understand what are all the functions you need to run on Compaction object and what state you need to setup. My proposed solution is to make important parts of Compaction immutable after construction. PickCompaction() should calculate compaction inputs and then pass them onto Compaction object once they are finalized. That makes it easy to create a new compaction just provide all the parameters to the constructor and youre done. No need to call confusing functions after you created your object. This diff doesnt fully achieve that goal, but it comes pretty close. Here are some of the changes: * have one Compaction constructor instead of two. * inputs_ is constant after construction * MarkFilesBeingCompacted() is now private to Compaction class and automatically called on construction/destruction. * SetupBottommostLevel() is gone. Compaction figures it out on its own based on the input. * CompactionPickers functions are not passing around Compaction object anymore. They are only passing around the state that they need. Test Plan: make check make asan_check make valgrind_check Reviewers: rven, anthony, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, yhchiang, dhruba, leveldb Differential Revision: move to cover multiple input levels Summary: Now trivial move is only triggered when moving from level n to n+1. With dynamic level base, it is possible that file is moved from level 0 to level n, while levels from 1 to n-1 are empty. Extend trivial move to this case. Test Plan: Add a more unit test of sequential loading. Non-trivial compaction happened without the patch and now doesnt happen. Reviewers: rven, yhchiang, MarkCallaghan, igor Reviewed By: igor Subscribers: leveldb, dhruba, IslamAbdelRahman Differential Revision: clean up sync points in test cleaning up Summary: In some db_test tests sync points are not cleared which will cause unexpected results in the next tests. Clean them up in test cleaning up. Test Plan: Run the same tests that used to fail: build using USE_CLANG=1 and run ./db_test Reviewers: rven, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: trivial move if compression level is different Summary: Check compression level of start_level with output_compression before allowing trivial move Test Plan: New DBTest CompressLevelCompactionThirdPath added Reviewers: igor, yhchiang, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Compactions with Small Files Summary: With this change, we use L1 and up to store compaction outputs in universal compaction. The compaction pick logic stays the same. Outputs are stored in the largest ""level"" as possible. If options.num_levels=1, it behaves all the same as now. Test Plan: 1) convert most of existing unit tests for universal comapaction to include the option of one level and multiple levels. 2) add a unit test to cover parallel compaction in universal compaction and run it in one level and multiple levels 3) add unit test to migrate from multiple level setting back to one level setting 4) add a unit test to insert keys to trigger multiple rounds of compactions and verify results. Reviewers: rven, kradhakrishnan, yhchiang, igor Reviewed By: igor Subscribers: meyering, leveldb, MarkCallaghan, dhruba Differential Revision: stats for the merge and filter operation Summary: We have addded new stats and perf_context for measuring the merge and filter operation time consumption. We have bounded all the merge operations within the GUARD statment and collected the total time for these operations in the DB. Test Plan: WIP Reviewers: rven, yhchiang, kradhakrishnan, igor, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: WAL directory before running db_test Summary: DBTest doesnt clean up wal directory. It might cause failure after a failure test run. Fix it. Test Plan: Run unit tests Try open DB with non-empty db_path/wal. Reviewers: rven, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: switch to gtest Summary: Our existing test notation is very similar to what is used in gtest. It makes it easy to adopt what is different. In this diff I modify existing [[ | test fixture ]] classes to inherit from `testing::Test`. Also for unit tests that use fixture class, `TEST` is replaced with `TEST_F` as required in gtest. There are several custom `main` functions in our existing tests. To make this transition easier, I modify all `main` functions to fallow gtest notation. But eventually we can remove them and use implementation of `main` that gtest provides. ```lang=bash % cat ~/transform files=$(git ls-files *test\.cc) for file in $files do if grep ""rocksdb::test::RunAllTests()"" $file then if grep ^class \w+Test { $file then perl s/^(class \w+Test) {/${1}: public testing::Test {/g $file perl s/^(TEST)/${1}_F/g $file fi perl s/(int main.*\{)/${1}::testing::InitGoogleTest(&argc, argv);/g $file perl s/rocksdb::test::RunAllTests/RUN_ALL_TESTS/g $file fi done % sh ~/transform % make format ``` Second iteration of this diff contains only scripted changes. Third iteration contains manual changes to fix last errors and make it compilable. Test Plan: Build and notice no errors. ```lang=bash % USE_CLANG=1 make check ``` Tests are still testing. Reviewers: meyering, sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: Replace ASSERT* with EXPECT* in functions that does not return void value Summary: gtest does not use exceptions to fail a unit test by design, and `ASSERT*`s are implemented using `return`. As a consequence we cannot use `ASSERT*` in a function that does not return `void` value ([[ | 1]]), and have to fix our existing code. This diff does this in a generic way, with no manual changes. In order to detect all existing `ASSERT*` that are used in functions that doesnt return void value, I change the code to generate compile errors for such cases. In `util/testharness.h` I defined `EXPECT*` assertions, the same way as `ASSERT*`, and redefined `ASSERT*` to return `void`. Then executed: ```lang=bash % USE_CLANG=1 make all 2> build.log % perl print ""-- "".$F[0].""\n"" if /: error:/ \ build.log | xargs 1 perl s/ASSERT/EXPECT/g if $. $number % make format ``` After that I reverted back change to `ASSERT*` in `util/testharness.h`. But preserved introduced `EXPECT*`, which is the same as `ASSERT*`. This will be deleted once switched to gtest. This diff is independent and contains manual changes only in `util/testharness.h`. Test Plan: Make sure all tests are passing. ```lang=bash % USE_CLANG=1 make check ``` Reviewers: igor, lgalanis, sdong, yufei.zhu, rven, meyering Reviewed By: meyering Subscribers: dhruba, leveldb Differential Revision: up rocksDB close call. Summary: On RocksDB, when there are multiple instances doing flushes/compactions in the background, the close call takes a long time because the flushes/compactions need to complete before the database can shut down. If another instance is using the background threads and the compaction for this instance is in the queue since it has been scheduled, we still cannot shutdown. We now remove the scheduled background tasks which have not yet started running, so that shutdown is speeded up. Test Plan: DB Test added. Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: a typo / test failure in ThreadStatusSingleCompaction Summary: Fix a typo / test failure in ThreadStatusSingleCompaction Test Plan: export ROCKSDB_TESTS=ThreadStatus ./db_test/Allow GetThreadList() to report operation stage. Summary: Allow GetThreadList() to report operation stage. Test Plan: ./thread_list_test ./db_bench \ \ \ export ROCKSDB_TESTS=ThreadStatus ./db_test Sample output ThreadID ThreadType cfName Operation OP_StartTime ElapsedTime Stage State 140116265861184 Low Pri 140116270055488 Low Pri 140116274249792 High Pri column_family_name_000005 Flush 2015/03/10-14:58:11 0 us FlushJob::WriteLevel0Table 140116400078912 Low Pri column_family_name_000004 Compaction 2015/03/10-14:58:11 0 us CompactionJob::FinishCompactionOutputFile 140116358135872 Low Pri column_family_name_000006 Compaction 2015/03/10-14:58:10 1 us CompactionJob::FinishCompactionOutputFile 140116341358656 Low Pri 140116295221312 High Pri default Flush 2015/03/10-14:58:11 0 us FlushJob::WriteLevel0Table 140116324581440 Low Pri column_family_name_000009 Compaction 2015/03/10-14:58:11 0 us CompactionJob::ProcessKeyValueCompaction 140116278444096 Low Pri 140116299415616 Low Pri column_family_name_000008 Compaction 2015/03/10-14:58:11 0 us CompactionJob::FinishCompactionOutputFile 140116291027008 High Pri column_family_name_000001 Flush 2015/03/10-14:58:11 0 us FlushJob::WriteLevel0Table 140116286832704 Low Pri column_family_name_000002 Compaction 2015/03/10-14:58:11 0 us CompactionJob::FinishCompactionOutputFile 140116282638400 Low Pri Reviewers: rven, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: stalls in preshutdown tests Summary: The tests using sync_point for intent to shutdown stop compaction and this results in stalls if too many rows are written. We now limit the number of rows written to prevent stalls, since the focus of the test is to cancel background work, which is being correctly tested. This fixes a Jenkins issue. Test Plan: DBTest.PreShutdown* Reviewers: sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: the way options.compression_per_level is used when options.level_compaction_dynamic_level_bytes=true Summary: Change the way options.compression_per_level is used when options.level_compaction_dynamic_level_bytes=true so that options.compression_per_level[1] determines compression for the level L0 is merged to, options.compression_per_level[2] to the level after that, etc. Test Plan: run all tests Reviewers: rven, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: yoshinorim, leveldb, dhruba Differential Revision: a mechanism to inform Rocksdb that it is shutting down Summary: Provide an API which enables users to infor Rocksdb that it is shutting down. Test Plan: db_test Reviewers: sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: to allow RocksDB to pick size bases of levels dynamically. Summary: When having fixed max_bytes_for_level_base, the ratio of size of largest level and the second one can range from 0 to the multiplier. This makes LSM tree frequently irregular and unpredictable. It can also cause poor space amplification in some cases. In this improvement (proposed by Igor Kabiljo), we introduce a parameter option.level_compaction_use_dynamic_max_bytes. When turning it on, RocksDB is free to pick a level base in the range of (options.max_bytes_for_level_base/options.max_bytes_for_level_multiplier, options.max_bytes_for_level_base] so that real level ratios are close to options.max_bytes_for_level_multiplier. Test Plan: New unit tests and pass tests suites including valgrind. Reviewers: MarkCallaghan, rven, yhchiang, igor, ikabiljo Reviewed By: ikabiljo Subscribers: yoshinorim, ikabiljo, dhruba, leveldb Differential Revision: columnfamily option optimize_filters_for_hits to optimize for key hits only Summary: Summary: Added a new option to ColumnFamllyOptions optimize_filters_for_hits. This option can be used in the case where most accesses to the store are key hits and we dont need to optimize performance for key misses. This is useful when you have a very large database and most of your lookups succeed. The option allows the store to not store and use filters in the last level (the largest level which contains data). These filters can take a large amount of space for large databases (in memory and on-disk). For the last level, these filters are only useful for key misses and not for key hits. If we are not optimizing for key misses, we can choose to not store these filters for that level. This option is only provided for BlockBasedTable. We skip the filters when we are compacting Test Plan: 1. Modified db_test toalso run tests with an additonal option (skip_filters_on_last_level) 2. Added another unit test to db_test which specifically tests that filters are being skipped Reviewers: rven, igor, sdong Reviewed By: sdong Subscribers: lgalanis, yoshinorim, MarkCallaghan, rven, dhruba, leveldb Differential Revision: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: config errors with L0 file count triggers Test Plan: Run ""make clean && make all check"" Reviewers: rven, igor, yhchiang, kradhakrishnan, MarkCallaghan, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: iterator Summary: This is a diff for managed iterator. A managed iterator is a wrapper around an iterator which saves the options for that iterator as well as the current key/value so that the underlying iterator and its associated memory can be released when it is aged out automatically or on the request of the user. Will provide the automatic release as a follow-up diff. Test Plan: Managed* tests in db_test and XF tests for managed iterator Reviewers: igor, yhchiang, anthony, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: GetThreadList to reflect flush activity. Summary: Allow GetThreadList to reflect flush activity. Test Plan: Developed ThreadStatusFlush test and updated ThreadStatusMultiCompaction test. ./db_test ./thread_list_test Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba Differential Revision:"
1149,1149,7.0,0.8707000017166138,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Fix clang build Summary: fix build Test Plan: works Reviewers: kradhakrishnan Subscribers: dhruba, leveldb Differential Revision: GetApproximateSizes() to use lesser CPU cycles. Summary: CPU profiling reveals GetApproximateSizes as a bottleneck for performance. The current implementation is sub-optimal, it scans every file in every level to compute the result. We can take advantage of the fact that all levels above 0 are sorted in the increasing order of key ranges and use binary search to locate the starting index. This can reduce the number of comparisons required to compute the result. Test Plan: We have good test coverage. Run the tests. Reviewers: sdong, igor, rven, dynamike Subscribers: dynamike, maykov, dhruba, leveldb Differential Revision: out SetMaxPossibleForUserKey() and SetMinPossibleForUserKey Summary: Based on feedback from D37083. Are all of these correct? In some spaces it seems like were doing SetMaxPossibleForUserKey() although we want the smallest possible internal key for user key. Test Plan: make check Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: max score in level summary Summary: Add more logging to help debugging issues. Test Plan: Run test suites Reviewers: yhchiang, rven, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: Compactions with Small Files Summary: With this change, we use L1 and up to store compaction outputs in universal compaction. The compaction pick logic stays the same. Outputs are stored in the largest ""level"" as possible. If options.num_levels=1, it behaves all the same as now. Test Plan: 1) convert most of existing unit tests for universal comapaction to include the option of one level and multiple levels. 2) add a unit test to cover parallel compaction in universal compaction and run it in one level and multiple levels 3) add unit test to migrate from multiple level setting back to one level setting 4) add a unit test to insert keys to trigger multiple rounds of compactions and verify results. Reviewers: rven, kradhakrishnan, yhchiang, igor Reviewed By: igor Subscribers: meyering, leveldb, MarkCallaghan, dhruba Differential Revision: stats for the merge and filter operation Summary: We have addded new stats and perf_context for measuring the merge and filter operation time consumption. We have bounded all the merge operations within the GUARD statment and collected the total time for these operations in the DB. Test Plan: WIP Reviewers: rven, yhchiang, kradhakrishnan, igor, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: delete files when column family is dropped Summary: To understand the bug read t5943287 and check out the new test in column_family_test (ReadDroppedColumnFamily), iter 0. RocksDB contract allowes you to read a drop column family as long as there is a live reference. However, since our iteration ignores dropped column families, AddLiveFiles() didnt mark files of a dropped column families as live. So we deleted them. In this patch I no longer ignore dropped column families in the iteration. I think this behavior was confusing and it also led to this bug. Now if an iterator client wants to ignore dropped column families, he needs to do it explicitly. Test Plan: Added a new unit test that is failing on master. Unit test succeeds now. Reviewers: sdong, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: to allow RocksDB to pick size bases of levels dynamically. Summary: When having fixed max_bytes_for_level_base, the ratio of size of largest level and the second one can range from 0 to the multiplier. This makes LSM tree frequently irregular and unpredictable. It can also cause poor space amplification in some cases. In this improvement (proposed by Igor Kabiljo), we introduce a parameter option.level_compaction_use_dynamic_max_bytes. When turning it on, RocksDB is free to pick a level base in the range of (options.max_bytes_for_level_base/options.max_bytes_for_level_multiplier, options.max_bytes_for_level_base] so that real level ratios are close to options.max_bytes_for_level_multiplier. Test Plan: New unit tests and pass tests suites including valgrind. Reviewers: MarkCallaghan, rven, yhchiang, igor, ikabiljo Reviewed By: ikabiljo Subscribers: yoshinorim, ikabiljo, dhruba, leveldb Differential Revision: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1150,1150,13.0,0.8810999989509583,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Removing duplicate code in db_bench/db_stress, fixing typos Summary: While working on single delete support for db_bench, I realized that db_bench/db_stress contain a bunch of duplicate code related to copmression and found some typos. This patch removes duplicate code, typos and a redundant in internal_stats.cc. Test Plan: make db_stress && make db_bench && ./db_bench Reviewers: yhchiang, sdong, rven, anthony, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: format"" against last 10 commits Summary: This helps Windows port to format their changes, as discussed. Might have formatted some other codes too becasue last 10 commits include more. Test Plan: Build it. Reviewers: anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: Change] Improve EventListener::OnFlushCompleted interface Summary: EventListener::OnFlushCompleted() now passes a structure instead of a list of parameters. This minimizes the API change in the future. Test Plan: listener_test compact_files_test example/compact_files_example Reviewers: kradhakrishnan, sdong, IslamAbdelRahman, rven, igor Reviewed By: rven, igor Subscribers: IslamAbdelRahman, rven, dhruba, leveldb Differential Revision: EventListener::OnTableFileCreated() Summary: Add EventListener::OnTableFileCreated(), which will be called when a table file is created. This patch is part of the EventLogger and EventListener integration. Test Plan: Augment existing test in db/listener_test.cc Reviewers: anthony, kradhakrishnan, rven, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: db_stress Summary: Fixed db_stress by correcting the verification of column family names in the Listener of db_stress Test Plan: db_stress Reviewers: igor, sdong Subscribers: dhruba, leveldb Differential Revision: a compile warning in db_stress Summary: Fixed the following compile warning in db_stress: error: OnCompactionCompleted overrides a member function but is not marked override [-Werror,-Winconsistent-missing-override] Test Plan: make db_stress Reviewers: sdong, igor, anthony Subscribers: dhruba, leveldb Differential Revision: EventListener in stress test. Summary: Include EventListener in stress test. Test Plan: make blackbox_crash_test whitebox_crash_test Reviewers: anthony, igor, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1151,1151,2.0,0.5117999911308289,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Compression sizes option for sst_dump_tool Summary: Added a new feature to sst_dump_tool.cc to allow a user to see the sizes of the different compression algorithms on an .sst file. Usage: ./sst_dump ./sst_dump Note: If you do not set a block size, it will default to 16kb Test Plan: manual test and the write a unit test Reviewers: IslamAbdelRahman, anthony, yhchiang, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision:"
1152,1152,11.0,0.9921000003814697,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","cleaned up PosixMmapFile a little Summary: has left PosixMmapFile in some weird state. This diff removes pending_sync_ that was now unused, fixes indentation and prevents Fsync() from calling both fsync() and fdatasync(). Test Plan: `make check` Reviewers: sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: rate_limiter, write buffering, most perf context instrumentation and most random kill out of Env Summary: We want to keep Env a think layer for better portability. Less platform dependent codes should be moved out of Env. In this patch, I create a wrapper of file readers and writers, and put rate limiting, write buffering, as well as most perf context instrumentation and random kill out of Env. It will make it easier to maintain multiple Env in the future. Test Plan: Run all existing unit tests. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: the latest changes from github/master/Add Env::GetThreadID(), which returns the ID of the current thread. Summary: Add Env::GetThreadID(), which returns the ID of the current thread. In addition, make GetThreadList() and InfoLog use same unique ID for the same thread. Test Plan: db_test listener_test Reviewers: igor, rven, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: times in perf_context and iostats_context Summary: We occasionally get write stalls (>1s Write() calls) on HDD under read load. The following timers explain almost all of the stalls: perf_context.db_mutex_lock_nanos perf_context.db_condition_wait_nanos iostats_context.open_time iostats_context.allocate_time iostats_context.write_time iostats_context.range_sync_time iostats_context.logger_time In my experiments each of these occasionally takes >1s on write path under some workload. There are rare cases when Write() takes long but none of these takes long. Test Plan: Added code to our application to write the listed timings to log for slow writes. They usually add up to almost exactly the time Write() call took. Reviewers: rven, yhchiang, sdong Reviewed By: sdong Subscribers: march, dhruba, tnovak Differential Revision:"
1153,1153,2.0,0.9472000002861023,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Add EventListener::OnTableFileCreated() Summary: Add EventListener::OnTableFileCreated(), which will be called when a table file is created. This patch is part of the EventLogger and EventListener integration. Test Plan: Augment existing test in db/listener_test.cc Reviewers: anthony, kradhakrishnan, rven, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1154,1154,10.0,0.9136000275611877,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Expose the BackupEngine from the Java API Summary: Merge pull request by adamretter Exposes BackupEngine from C++ to the Java API. Previously only BackupableDB was available Test Plan: BackupEngineTest.java Reviewers: fyrz, igor, ankgup87, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision:"
1155,1155,5.0,0.977400004863739,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","[RocksJava] Fixed test failures Summary: The option bottommost_level_compaction was introduced lately. This option breaks the Java API behavior. To prevent the library from doing so we set that option to a fixed value in Java. In future we are going to remove that portion and replace the hardcoded options using a more flexible way. Fixed bug introduced by WriteBatchWithIndex Patch Lately icanadi changed the behavior of WriteBatchWithIndex. See commit: 821cff114e57efa67711c1c1c105aa02831a0d23 This commit solves problems introduced by above mentioned commit. Test Plan: make rocksdbjava make jtest Reviewers: adamretter, ankgup87, yhchiang Reviewed By: yhchiang Subscribers: igor, dhruba Differential Revision:"
1156,1156,13.0,0.9947999715805054,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Dont let flushes preempt compactions Summary: When we first started, max_background_flushes was 0 by default and compaction thread was executing flushes (since there was no flush thread). Then, we switched the default max_background_flushes to 1. However, we still support the case where there is no flush thread and flushes are done in compaction. This is making our code a bit more complicated. By not supporting this use-case we can make our code simpler. We have a special case that when you set max_background_flushes to 0, we schedule the flush to execute on the compaction thread. Test Plan: make check (there might be some unit tests that depend on this behavior) Reviewers: IslamAbdelRahman, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: ability to specify a compaction filter via the Java API/Support saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1157,1157,14.0,0.5232999920845032,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",The ability to specify a compaction filter via the Java API/
1158,1158,11.0,0.944100022315979,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Skip unsupported tests in ROCKSDB_LITE Summary: Skipping these tests in ROCKSDB_LITE since they are not supported json_document_test wal_manager_test ttl_test sst_dump_test deletefile_test compact_files_test prefix_test checkpoint_test Test Plan: json_document_test wal_manager_test ttl_test sst_dump_test deletefile_test compact_files_test prefix_test checkpoint_test Reviewers: igor, sdong, yhchiang, kradhakrishnan, anthony Reviewed By: anthony Subscribers: dhruba Differential Revision:"
1159,1159,10.0,0.7325000166893005,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Improved FileExists API Summary: Add new CheckFileExists method. Considered changing the FileExists api but didnt want to break anyones builds. Test Plan: unit tests Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: backupable_db_test_lite in ROCKSDB_LITE Summary: BackupableDB is not supported in ROCKSDB_LITE, blocking backupable_db_test_lite Test Plan: backupable_db_test Reviewers: sdong, igor, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: error handling in BackupEngine Summary: Couple of changes here: * NewBackupEngine() and NewReadOnlyBackupEngine() are now removed. They were deprecated since RocksDB 3.8. Changing these to new functions should be pretty straight-forward. As a followup, Ill fix all fbcode callsights * Instead of initializing backup engine in the constructor, we initialize it in a separate function now. That way, we can catch all errors and return appropriate status code. * We catch all errors during initializations and return them to the client properly. * Added new tests to backupable_db_test, to make sure that we cant open BackupEngine when there are Env errors. * Transitioned backupable_db_test to use BackupEngine rather than BackupableDB. From the two available APIs, judging by the current use-cases, it looks like BackupEngine API won. Its much more flexible since it doesnt require StackableDB. Test Plan: Added a new unit test to backupable_db_test Reviewers: yhchiang, sdong, AaronFeldman Reviewed By: AaronFeldman Subscribers: dhruba, leveldb Differential Revision: backup and restore in BackupEngineImpl Summary: Add a new field: BackupableDBOptions.max_background_copies. CreateNewBackup() and RestoreDBFromBackup() will use this number of threads to perform copies. If there is a backup rate limit, then max_background_copies must be 1. Update backupable_db_test.cc to test multi-threaded backup and restore. Update backupable_db_test.cc to test backups when the backup environment is not the same as the database environment. Test Plan: Run ./backupable_db_test Run valgrind ./backupable_db_test Run with TSAN and ASAN Reviewers: yhchiang, rven, anthony, sdong, igor Reviewed By: igor Subscribers: yhchiang, anthony, sdong, leveldb, dhruba Differential Revision:"
1160,1160,7.0,0.970300018787384,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Move rate_limiter, write buffering, most perf context instrumentation and most random kill out of Env Summary: We want to keep Env a think layer for better portability. Less platform dependent codes should be moved out of Env. In this patch, I create a wrapper of file readers and writers, and put rate limiting, write buffering, as well as most perf context instrumentation and random kill out of Env. It will make it easier to maintain multiple Env in the future. Test Plan: Run all existing unit tests. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1161,1161,15.0,0.6427000164985657,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Use malloc_usable_size() for accounting block cache size Summary: Currently, when we insert something into block cache, we say that the block cache capacity decreased by the size of the block. However, size of the block might be less than the actual memory used by this object. For example, 4.5KB block will actually use 8KB of memory. So even if we configure block cache to 10GB, our actually memory usage of block cache will be 20GB This problem showed up a lot in testing and just recently also showed up in MongoRocks production where we were using 30GB more memory than expected. This diff will fix the problem. Instead of counting the block size, we will count memory used by the block. That way, a block cache configured to be 10GB will actually use only 10GB of memory. Im using non-portable function and I couldnt find info on portability on Google. However, it seems to work on Linux, which will cover majority of our use-cases. Test Plan: 1. fill up mongo instance with 80GB of data 2. restart mongo with block cache size configured to 10GB 3. do a table scan in mongo 4. memory usage before the diff: 12GB. memory usage after the diff: 10.5GB Reviewers: sdong, MarkCallaghan, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
1162,1162,7.0,0.9783999919891357,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Block cuckoo table tests in ROCKSDB_LITE Summary: Cuckoo table is not supported in ROCKSDB_LITE, blocking its tests Test Plan: cuckoo_table_builder_test cuckoo_table_db_test cuckoo_table_reader_test Reviewers: sdong, igor, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: rate_limiter, write buffering, most perf context instrumentation and most random kill out of Env Summary: We want to keep Env a think layer for better portability. Less platform dependent codes should be moved out of Env. In this patch, I create a wrapper of file readers and writers, and put rate limiting, write buffering, as well as most perf context instrumentation and random kill out of Env. It will make it easier to maintain multiple Env in the future. Test Plan: Run all existing unit tests. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1163,1163,7.0,0.970300018787384,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Move rate_limiter, write buffering, most perf context instrumentation and most random kill out of Env Summary: We want to keep Env a think layer for better portability. Less platform dependent codes should be moved out of Env. In this patch, I create a wrapper of file readers and writers, and put rate limiting, write buffering, as well as most perf context instrumentation and random kill out of Env. It will make it easier to maintain multiple Env in the future. Test Plan: Run all existing unit tests. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1164,1164,7.0,0.6751000285148621,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Move rate_limiter, write buffering, most perf context instrumentation and most random kill out of Env Summary: We want to keep Env a think layer for better portability. Less platform dependent codes should be moved out of Env. In this patch, I create a wrapper of file readers and writers, and put rate limiting, write buffering, as well as most perf context instrumentation and random kill out of Env. It will make it easier to maintain multiple Env in the future. Test Plan: Run all existing unit tests. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: report time spent on reading index and bloom blocks Summary: Add a perf context counter to help users figure out time spent on reading indexes and bloom filter blocks. Test Plan: Will write a unit test Subscribers: leveldb, dhruba Differential Revision: malloc_usable_size() for accounting block cache size Summary: Currently, when we insert something into block cache, we say that the block cache capacity decreased by the size of the block. However, size of the block might be less than the actual memory used by this object. For example, 4.5KB block will actually use 8KB of memory. So even if we configure block cache to 10GB, our actually memory usage of block cache will be 20GB This problem showed up a lot in testing and just recently also showed up in MongoRocks production where we were using 30GB more memory than expected. This diff will fix the problem. Instead of counting the block size, we will count memory used by the block. That way, a block cache configured to be 10GB will actually use only 10GB of memory. Im using non-portable function and I couldnt find info on portability on Google. However, it seems to work on Linux, which will cover majority of our use-cases. Test Plan: 1. fill up mongo instance with 80GB of data 2. restart mongo with block cache size configured to 10GB 3. do a table scan in mongo 4. memory usage before the diff: 12GB. memory usage after the diff: 10.5GB Reviewers: sdong, MarkCallaghan, rven, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision:"
1165,1165,7.0,0.970300018787384,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Move rate_limiter, write buffering, most perf context instrumentation and most random kill out of Env Summary: We want to keep Env a think layer for better portability. Less platform dependent codes should be moved out of Env. In this patch, I create a wrapper of file readers and writers, and put rate limiting, write buffering, as well as most perf context instrumentation and random kill out of Env. It will make it easier to maintain multiple Env in the future. Test Plan: Run all existing unit tests. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1166,1166,7.0,0.9789000153541565,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Removing duplicate code Summary: While working on , I found duplicate code in the tests. This patch removes it. Test Plan: make clean all check Reviewers: igor, sdong, rven, anthony, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: rate_limiter, write buffering, most perf context instrumentation and most random kill out of Env Summary: We want to keep Env a think layer for better portability. Less platform dependent codes should be moved out of Env. In this patch, I create a wrapper of file readers and writers, and put rate limiting, write buffering, as well as most perf context instrumentation and random kill out of Env. It will make it easier to maintain multiple Env in the future. Test Plan: Run all existing unit tests. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1167,1167,13.0,0.9634000062942505,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Windows Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov"
1168,1168,11.0,0.9735999703407288,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Skip unsupported tests in ROCKSDB_LITE Summary: Skipping these tests in ROCKSDB_LITE since they are not supported json_document_test wal_manager_test ttl_test sst_dump_test deletefile_test compact_files_test prefix_test checkpoint_test Test Plan: json_document_test wal_manager_test ttl_test sst_dump_test deletefile_test compact_files_test prefix_test checkpoint_test Reviewers: igor, sdong, yhchiang, kradhakrishnan, anthony Reviewed By: anthony Subscribers: dhruba Differential Revision: Change] Improve EventListener::OnFlushCompleted interface Summary: EventListener::OnFlushCompleted() now passes a structure instead of a list of parameters. This minimizes the API change in the future. Test Plan: listener_test compact_files_test example/compact_files_example Reviewers: kradhakrishnan, sdong, IslamAbdelRahman, rven, igor Reviewed By: rven, igor Subscribers: IslamAbdelRahman, rven, dhruba, leveldb Differential Revision:"
1169,1169,7.0,0.9789000153541565,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Removing duplicate code Summary: While working on , I found duplicate code in the tests. This patch removes it. Test Plan: make clean all check Reviewers: igor, sdong, rven, anthony, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: rate_limiter, write buffering, most perf context instrumentation and most random kill out of Env Summary: We want to keep Env a think layer for better portability. Less platform dependent codes should be moved out of Env. In this patch, I create a wrapper of file readers and writers, and put rate limiting, write buffering, as well as most perf context instrumentation and random kill out of Env. It will make it easier to maintain multiple Env in the future. Test Plan: Run all existing unit tests. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1170,1170,7.0,0.7440999746322632,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Measure file read latency histogram per level Summary: In internal stats, remember read latency histogram, if statistics is enabled. It can be retrieved from DB::GetProperty() with ""rocksdb.dbstats"" property, if it is enabled. Test Plan: Manually run db_bench and prints out ""rocksdb.dbstats"" by hand and make sure it prints out as expected Reviewers: igor, IslamAbdelRahman, rven, kradhakrishnan, anthony, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, leveldb, dhruba Differential Revision: db mutex contention for write batch groups Summary: This diff allows a Writer to join the next write batch group without acquiring any locks. Waiting is performed via a per-Writer mutex, so all of the non-leader writers never need to acquire the db mutex. It is now possible to join a write batch group after the leader has been chosen but before the batch has been constructed. This diff doesnt increase parallelism, but reduces synchronization overheads. For some CPU-bound workloads (no WAL, RAM-sized working set) this can substantially reduce contention on the db mutex in a multi-threaded environment. With T=8 N=500000 in a CPU-bound scenario (see the test plan) this is good for a 33% perf win. Not all scenarios see such a win, but none show a loss. This code is slightly faster even for the single-threaded case (about 2% for the CPU-bound scenario below). Test Plan: 1. unit tests 2. COMPILE_WITH_TSAN=1 make check 3. stress high-contention scenarios with db_bench Reviewers: sdong, igor, rven, ljin, yhchiang Subscribers: dhruba Differential Revision: options.compaction_measure_io_stats to print write I/O stats in compactions Summary: Add options.compaction_measure_io_stats to print out / pass to listener accumulated time spent on write calls. Example outputs in info logs: 2015/08/12-16:27:59.463944 7fd428bff700 (Original Log Time 2015/08/12-16:27:59.463922) EVENT_LOG_v1 {""time_micros"": 1439422079463897, ""job"": 6, ""event"": ""compaction_finished"", ""output_level"": 1, ""num_output_files"": 4, ""total_output_size"": 6900525, ""num_input_records"": 111483, ""num_output_records"": 106877, ""file_write_nanos"": 15663206, ""file_range_sync_nanos"": 649588, ""file_fsync_nanos"": 349614797, ""file_prepare_write_nanos"": 1505812, ""lsm_state"": [2, 4, 0, 0, 0, 0, 0]} Add two more counters in iostats_context. Also add a parameter of db_bench. Test Plan: Add a unit test. Also manually verify LOG outputs in db_bench Subscribers: leveldb, dhruba Differential Revision: error statuses Summary: Based on feedback from spetrunia, we should better differentiate error statuses for transaction failures. Test Plan: unit tests Reviewers: rven, kradhakrishnan, spetrunia, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Transactions Summary: Initial implementation of Pessimistic Transactions. This diff contains the api changes discussed in D38913. This diff is pretty large, so let me know if people would prefer to meet up to discuss it. MyRocks folks: please take a look at the API in include/rocksdb/utilities/transaction[_db].h and let me know if you have any issues. Also, youll notice a couple of TODOs in the implementation of RollbackToSavePoint(). After chatting with Siying, Im going to send out a separate diff for an alternate implementation of this feature that implements the rollback inside of WriteBatch/WriteBatchWithIndex. We can then decide which route is preferable. Next, Im planning on doing some perf testing and then integrating this diff into MongoRocks for further testing. Test Plan: Unit tests, db_bench parallel testing. Reviewers: igor, rven, sdong, yhchiang, yoshinorim Reviewed By: sdong Subscribers: hermanlee4, maykov, spetrunia, leveldb, dhruba Differential Revision: type unique_ptr in LogWriterNumber::writer for Windows build break Summary: Visual Studio complains about deque<LogWriterNumber> because LogWriterNumber is non-copyable for its unique_ptr member writer. Move away from it, and do explit free. It is less safe but I cant think of a better way to unblock it. Test Plan: valgrind check test Reviewers: anthony, IslamAbdelRahman, kolmike, rven, yhchiang Reviewed By: yhchiang Subscribers: leveldb, dhruba Differential Revision: changes 3/3] method in DB to sync WAL without blocking writers Summary: Subj. We really need this feature. Previous diff D40899 has most of the changes to make this possible, this diff just adds the method. Test Plan: `make check`, the new test fails without this diff; ran with ASAN, TSAN and valgrind. Reviewers: igor, rven, IslamAbdelRahman, anthony, kradhakrishnan, tnovak, yhchiang, sdong Reviewed By: sdong Subscribers: MarkCallaghan, maykov, hermanlee4, yoshinorim, tnovak, dhruba Differential Revision: delete rate limiting Summary: Introduce DeleteScheduler that allow enforcing a rate limit on file deletion Instead of deleting files immediately, files are moved to trash directory and deleted in a background thread that apply sleep penalty between deletes if needed. I have updated PurgeObsoleteFiles and PurgeObsoleteWALFiles to use the delete_scheduler instead of env_->DeleteFile Test Plan: added delete_scheduler_test existing unit tests Reviewers: kradhakrishnan, anthony, rven, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: changes 2/3] write with sync=true syncs previous unsynced wals to prevent illegal data loss Summary: Ill just copy internal task summary here: "" This sequence will cause data loss in the middle after an sync write: non-sync write key 1 flush triggered, not yet scheduled sync write key 2 system crash After rebooting, users might see key 2 but not key 1, which violates the API of sync write. This can be reproduced using unit test FaultInjectionTest::DISABLED_WriteOptionSyncTest. One way to fix it is for a sync write, if there is outstanding unsynced log files, we need to syc them too. "" This diff should be considered together with the next diff D40905; in isolation this fix probably could be a little simpler. Test Plan: `make check`; added a test for that (DBTest.SyncingPreviousLogs) before noticing FaultInjectionTest.WriteOptionSyncTest (keeping both since mine asserts a bit more); both tests fail without this diff; for D40905 stacked on top of this diff, ran tests with ASAN, TSAN and valgrind Reviewers: rven, yhchiang, IslamAbdelRahman, anthony, kradhakrishnan, igor, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: rate_limiter, write buffering, most perf context instrumentation and most random kill out of Env Summary: We want to keep Env a think layer for better portability. Less platform dependent codes should be moved out of Env. In this patch, I create a wrapper of file readers and writers, and put rate limiting, write buffering, as well as most perf context instrumentation and random kill out of Env. It will make it easier to maintain multiple Env in the future. Test Plan: Run all existing unit tests. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: delete files in Trivial move of universal compaction Summary: Trvial move in universal compaction was failing when trying to move files from levels other than 0. This was because the DeleteFile while trivially moving, was only deleting files of level 0 which caused duplication of same file in different levels. This is fixed by passing the right level as argument in the call of DeleteFile while doing trivial move. Test Plan: ./db_test ran successfully with the new test cases. Reviewers: sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: WriteOptions::timeout_hint_us Summary: In one of our recent meetings, we discussed deprecating features that are not being actively used. One of those features, at least within Facebook, is timeout_hint. The feature is really nicely implemented, but if nobody needs it, we should remove it from our code-base (until we get a valid use-case). Some arguments: * Less code better icache hit rate, smaller builds, simpler code * The motivation for adding timeout_hint_us was to work-around RocksDBs stall issue. However, were currently addressing the stall issue itself (see recent work on stall write_rate), so we should never see sharp lock-ups in the future. * Nobody is using the feature within Facebooks code-base. Googling for `timeout_hint_us` also doesnt yield any users. Test Plan: make check Reviewers: anthony, kradhakrishnan, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, dhruba, leveldb Differential Revision: Fast CRC32 support information in DB LOG Summary: Print whether fast CRC32 is supported in DB info LOG Test Plan: Run db_bench and see it prints out correctly. Reviewers: yhchiang, anthony, kradhakrishnan, igor Reviewed By: igor Subscribers: MarkCallaghan, yoshinorim, leveldb, dhruba Differential Revision: trivial move in universal compaction Summary: This change enables trivial move if all the input files are non onverlapping while doing Universal Compaction. Test Plan: ./compaction_picker_test and db_test ran successfully with the new testcases. Reviewers: sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: leaking log::Writers Summary: Fixes valgrind errors in column_family_test. Test Plan: `make check`, `make valgrind_check` Reviewers: igor, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: changes 1/3] fixed unbounded wal growth in some workloads Summary: This fixes the following scenario weve hit: we reached max_total_wal_size, created a new wal and scheduled flushing all memtables corresponding to the old one, before the last of these flushes started its column family was dropped; the last background flush call was a no-op; no one removed the old wal from alive_logs_, hours have passed and no flushes happened even though lots of data was written; data is written to different column families, compactions are disabled; old column families are dropped before memtable grows big enough to trigger a flush; the old wal still sits in alive_logs_ preventing max_total_wal_size limit from kicking in, a few more hours pass and we run out disk space because of one huge .log file. Test Plan: `make check`; backported the new test, checked that it fails without this diff Reviewers: igor Reviewed By: igor Subscribers: dhruba Differential Revision: Port from Microsoft Summary: Make RocksDb build and run on Windows to be functionally complete and performant. All existing test cases run with no regressions. Performance numbers are in the pull-request. Test plan: make all of the existing unit tests pass, obtain perf numbers. Co-authored-by: Praveen Rao Co-authored-by: Sherlock Huang Co-authored-by: Alex Zinoviev Co-authored-by: Dmitri Smirnov flush check for shutdown Summary: Fixes task 7156865 where a compaction causes a hang in flush memtable if CancelAllBackgroundWork was called prior to it. Stack trace is in : We end up waiting for a flush which will never happen because there are no background threads. Test Plan: PreShutdownFlush Reviewers: sdong, igor Reviewed By: sdong, igor Subscribers: dhruba, leveldb Differential Revision: WAL recovery consistency levels Summary: The ""one size fits all"" approach with WAL recovery will only introduce inconvenience for our varied clients as we go forward. The current recovery is a bit heuristic. We introduce the following levels of consistency while replaying the WAL. 1. RecoverAfterRestart (kTolerateCorruptedTailRecords) This mocks the current recovery mode. 2. RecoverAfterCleanShutdown (kAbsoluteConsistency) This is ideal for unit test and cases where the store is shutdown cleanly. We tolerate no corruption or incomplete writes. 3. RecoverPointInTime (kPointInTimeRecovery) This is ideal when using devices with controller cache or file systems which can loose data on restart. We recover upto the point were is no corruption or incomplete write. 4. RecoverAfterDisaster (kSkipAnyCorruptRecord) This is ideal mode to recover data. We tolerate corruption and incomplete writes, and we hop over those sections that we cannot make sense of salvaging as many records as possible. Test Plan: (1) Run added unit test to cover all levels. (2) Run make check. Reviewers: leveldb, sdong, igor Subscribers: yoshinorim, dhruba Differential Revision: trivial move merge Summary: Fixing bad merge Test Plan: make check (this is not enough to verify the fix) Reviewers: igor, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: CompactRangeOptions for CompactRange Summary: This diff update DB::CompactRange to use RangeCompactionOptions instead of using multiple parameters Old CompactRange is still available but deprecated Test Plan: make all check make rocksdbjava USE_CLANG=1 make all OPT=-DROCKSDB_LITE make release Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: dhruba Differential Revision: up InstallSuperVersion Summary: We go to great lengths to make sure MaybeScheduleFlushOrCompaction() is called outside of write thread. But anyway, its still called in the mutex, so its not that much cheaper. This diff removes the ""optimization"" and cleans up the code a bit. Test Plan: make check Reviewers: rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: down writes by bytes written Summary: We slow down data into the database to the rate of options.delayed_write_rate (a new option) with this patch. The thread synchronization approach I take is to still synchronize write controller by DB mutex and GetDelay() is inside DB mutex. Try to minimize the frequency of getting time in GetDelay(). I verified it through db_bench and it seems to work hard_rate_limit is deprecated. options.delayed_write_rate is still not dynamically changeable. Need to work on it as a follow-up. Test Plan: Add new unit tests in db_test Reviewers: yhchiang, rven, kradhakrishnan, anthony, MarkCallaghan, igor Reviewed By: igor Subscribers: ikabiljo, leveldb, dhruba Differential Revision: largest sequence to FlushJobInfo Summary: Adding largest sequence number to FlushJobInfo and passing flushed file metadata to NotifyOnFlushCompleted which include alot of other values that we may want to expose in FlushJobInfo Test Plan: make check Reviewers: igor, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: TablePropertiesCollector::NeedCompact() to suggest DB to further compact output files Summary: It is experimental. Allow users to return from a call back function TablePropertiesCollector::NeedCompact(), based on the data in the file. It can be used to allow users to suggest DB to clear up delete tombstones faster. Test Plan: Add a unit test. Reviewers: igor, yhchiang, kradhakrishnan, rven Reviewed By: rven Subscribers: yoshinorim, MarkCallaghan, maykov, leveldb, dhruba Differential Revision: mutex in ReFitLevel Summary: I encountered an issue where the database hang, it looks like the mutex is not unlocked on return in ReFitLevel function Test Plan: make check Reviewers: yhchiang, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: L0 L1 trivial move on sorted data Summary: This diff updates the logic of how we do trivial move, now trivial move can run on any number of files in input level as long as they are not overlapping The conditions for trivial move have been updated Introduced conditions: Trivial move cannot happen if we have a compaction filter (except if the compaction is not manual) Input level files cannot be overlapping Removed conditions: Trivial move only run when the compaction is not manual Input level should can contain only 1 file More context on what tests failed because of Trivial move ``` DBTest.CompactionsGenerateMultipleFiles This test is expecting compaction on a file in L0 to generate multiple files in L1, this test will fail with trivial move because we end up with one file in L1 ``` ``` DBTest.NoSpaceCompactRange This test expect compaction to fail when we force environment to report running out of space, of course this is not valid in trivial move situation because trivial move does not need any extra space, and did not check for that ``` ``` DBTest.DropWrites Similar to DBTest.NoSpaceCompactRange ``` ``` DBTest.DeleteObsoleteFilesPendingOutputs This test expect that a file in L2 is deleted after its moved to L3, this is not valid with trivial move because although the file was moved it is now used by L3 ``` ``` CuckooTableDBTest.CompactionIntoMultipleFiles Same as DBTest.CompactionsGenerateMultipleFiles ``` This diff is based on a work by Test Plan: make check Reviewers: rven, sdong, igor Reviewed By: igor Subscribers: yhchiang, ott, march, dhruba, sdong Differential Revision: EventListener::OnTableFileDeletion() Summary: Add EventListener::OnTableFileDeletion(), which will be called when a table file is deleted. Test Plan: Extend three existing tests in db_test to verify the deleted files. Reviewers: rven, anthony, kradhakrishnan, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: EventListener::OnCompactionCompleted to return CompactionJobStats. Summary: Allow EventListener::OnCompactionCompleted to return CompactionJobStats, which contains useful information about a compaction. Example CompactionJobStats returned by OnCompactionCompleted(): smallest_output_key_prefix 05000000 largest_output_key_prefix 06990000 elapsed_time 42419 num_input_records 300 num_input_files 3 num_input_files_at_output_level 2 num_output_records 200 num_output_files 1 actual_bytes_input 167200 actual_bytes_output 110688 total_input_raw_key_bytes 5400 total_input_raw_value_bytes 300000 num_records_replaced 100 is_manual_compaction 1 Test Plan: Developed a mega test in db_test which covers 20 variables in CompactionJobStats. Reviewers: rven, igor, anthony, sdong Reviewed By: sdong Subscribers: tnovak, dhruba, leveldb Differential Revision: EventListener::OnTableFileCreated() Summary: Add EventListener::OnTableFileCreated(), which will be called when a table file is created. This patch is part of the EventLogger and EventListener integration. Test Plan: Augment existing test in db/listener_test.cc Reviewers: anthony, kradhakrishnan, rven, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: times in perf_context and iostats_context Summary: We occasionally get write stalls (>1s Write() calls) on HDD under read load. The following timers explain almost all of the stalls: perf_context.db_mutex_lock_nanos perf_context.db_condition_wait_nanos iostats_context.open_time iostats_context.allocate_time iostats_context.write_time iostats_context.range_sync_time iostats_context.logger_time In my experiments each of these occasionally takes >1s on write path under some workload. There are rare cases when Write() takes long but none of these takes long. Test Plan: Added code to our application to write the listed timings to log for slow writes. They usually add up to almost exactly the time Write() call took. Reviewers: rven, yhchiang, sdong Reviewed By: sdong Subscribers: march, dhruba, tnovak Differential Revision: DBImpl::notifying_events_ Summary: DBImpl::notifying_events_ is a internal counter in DBImpl which is used to prevent DB close when DB is notifying events. However, as the current events all rely on either compaction or flush which already have similar counters to prevent DB close, it is safe to remove notifying_events_. Test Plan: listener_test examples/compact_files_example Reviewers: igor, anthony, kradhakrishnan, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Transactions Summary: Optimistic transactions supporting begin/commit/rollback semantics. Currently relies on checking the memtable to determine if there are any collisions at commit time. Not yet implemented would be a way of enuring the memtable has some minimum amount of history so that we wont fail to commit when the memtable is empty. You should probably start with transaction.h to get an overview of what is currently supported. Test Plan: Added a new test, but still need to look into stress testing. Reviewers: yhchiang, igor, rven, sdong Reviewed By: sdong Subscribers: adamretter, MarkCallaghan, leveldb, dhruba Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: Change] Move listeners from ColumnFamilyOptions to DBOptions Summary: Move listeners from ColumnFamilyOptions to DBOptions Test Plan: listener_test compact_files_test Reviewers: rven, anthony, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: two bugs on logging file deletion. Summary: This patch fixes the following two bugs on logging file deletion. 1. Previously, file deletion failure was only logged in INFO_LEVEL. This patch changes it to ERROR_LEVEL and does some code clean. 2. EventLogger previously will always generate the same log on table file deletion even when file deletion is not successful. Now the resulting status of file deletion will also be logged. Test Plan: make all check Reviewers: sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: flushes to run in parallel with manual compaction Summary: As title. I spent some time thinking about it and I dont think there should be any issue with running manual compaction and flushes in parallel Test Plan: make check works Reviewers: rven, yhchiang, sdong Reviewed By: yhchiang, sdong Subscribers: dhruba, leveldb Differential Revision: skips levels 1 to base_level for dynamic level base size Summary: CompactRange() now is much more expensive for dynamic level base size as it goes through all the levels. Skip those not used levels between level 0 an base level. Test Plan: Run all unit tests Reviewers: yhchiang, rven, anthony, kradhakrishnan, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1171,1171,13.0,0.9871000051498413,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Dont let flushes preempt compactions Summary: When we first started, max_background_flushes was 0 by default and compaction thread was executing flushes (since there was no flush thread). Then, we switched the default max_background_flushes to 1. However, we still support the case where there is no flush thread and flushes are done in compaction. This is making our code a bit more complicated. By not supporting this use-case we can make our code simpler. We have a special case that when you set max_background_flushes to 0, we schedule the flush to execute on the compaction thread. Test Plan: make check (there might be some unit tests that depend on this behavior) Reviewers: IslamAbdelRahman, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: purge_redundant_kvs_while_flush Summary: This option is guarding the feature implemented 2 and a half years ago: D8991. The feature was enabled by default back then and has been running without issues. There is no reason why any client would turn this feature off. I found no reference in fbcode. Test Plan: none Reviewers: sdong, yhchiang, anthony, dhruba Reviewed By: dhruba Subscribers: dhruba, leveldb Differential Revision: the latest changes from github/master/Use CompactRangeOptions for CompactRange Summary: This diff update DB::CompactRange to use RangeCompactionOptions instead of using multiple parameters Old CompactRange is still available but deprecated Test Plan: make all check make rocksdbjava USE_CLANG=1 make all OPT=-DROCKSDB_LITE make release Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: dhruba Differential Revision: add support for WriteBatch SliceParts params/Support saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: extra bbto / noop slice transform/C api: human-readable statistics/"
1172,1172,10.0,0.5256999731063843,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fix when output level is 0 of universal compaction with trivial move Summary: Fix for universal compaction with trivial move, when the ouput level is 0. The tests where failing. Fixed by allowing normal compaction when output level is 0. Test Plan: modified test cases run successfully. Reviewers: sdong, yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: anthony, kradhakrishnan, leveldb, dhruba Differential Revision: fix. Summary: gcc-4.9-glibc-2.20 complains about uninitialized variable. db/compaction_picker.cc: In member function bool rocksdb::CompactionPicker::IsInputNonOverlapping(rocksdb::Compaction*): db/compaction_picker.cc:1174:17: error: prev.rocksdb::{anonymous}::InputFileInfo::f may be used uninitialized in this function [-Werror=maybe-uninitialized] InputFileInfo prev, curr, next; Test Plan: pmake on local environment Reviewers: sdong igor CC: Task ID: Blame Rev:/""make format"" against last 10 commits Summary: This helps Windows port to format their changes, as discussed. Might have formatted some other codes too becasue last 10 commits include more. Test Plan: Build it. Reviewers: anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: fail fix Summary: Build fail fix. Type cast issues. Test Plan: compiled Reviewers: sdong, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: trivial move in universal compaction Summary: This change enables trivial move if all the input files are non onverlapping while doing Universal Compaction. Test Plan: ./compaction_picker_test and db_test ran successfully with the new testcases. Reviewers: sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: let two L0->L1 compactions run in parallel Summary: With experimental feature SuggestCompactRange() we dont restrict running two L0->L1 compactions in parallel. This diff fixes this. Test Plan: added a unit test to reproduce the failure. fixed the unit test Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: a chance on a random file when choosing compaction Summary: When trying to compact entire database with SuggestCompactRange(), well first try the left-most files. This is pretty bad, because: 1) the left part of LSM tree will be overly compacted, but right part will not be touched 2) First compaction will pick up the left-most file. Second compaction will try to pick up next left-most, but this will not be possible, because theres a big chance that seconds file range on N+1 level is already being compacted. I observe both of those problems when running Mongo+RocksDB and trying to compact the DB to clean up tombstones. Im unable to clean them up :( This diff adds a bit of randomness into choosing a file. First, it chooses a file at random and tries to compact that one. This should solve both problems specified here. Test Plan: make check Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1173,1173,12.0,0.6744999885559082,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Report live data size estimate Summary: Fixes T6548822. Added a new function for estimating the size of the live data as proposed in the task. The value can be accessed through the property rocksdb.estimate-live-data-size. Test Plan: There are two unit tests in version_set_test and a simple test in db_test. make version_set_test && ./version_set_test; make db_test && ./db_test gtest_filter=GetProperty Reviewers: rven, igor, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: a bug of CompactionStats in multi-level universal compaction case Summary: Universal compaction can involves in multiple levels. However, the current implementation of bytes_readn and bytes_readnp1 (and some other stats with postfix `n` and `np1`) assumes compaction can only have two levels. This patch fixes this bug and redefines bytes_readn and bytes_readnp1: * bytes_readnp1: the number of bytes read in the compaction output level. * bytes_readn: the total number of bytes read minus bytes_readnp1 Test Plan: Add a test in compaction_job_stats_test Reviewers: igor, sdong, rven, anthony, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: dhruba, leveldb Differential Revision: saving history in memtable_list Summary: For transactions, we are using the memtables to validate that there are no write conflicts. But after flushing, we dont have any memtables, and transactions could fail to commit. So we want to someone keep around some extra history to use for conflict checking. In addition, we want to provide a way to increase the size of this history if too many transactions fail to commit. After chatting with people, it seems like everyone prefers just using Memtables to store this history (instead of a separate history structure). It seems like the best place for this is abstracted inside the memtable_list. I decide to create a separate list in MemtableListVersion as using the same list complicated the flush/installalflushresults logic too much. This diff adds a new parameter to control how much memtable history to keep around after flushing. However, it sounds like people arent too fond of adding new parameters. So I am making the default size of flushed+not-flushed memtables be set to max_write_buffers. This should not change the maximum amount of memory used, but make it more likely were using closer the the limit. (We are now postponing deleting flushed memtables until the max_write_buffer limit is reached). So while we might use more memory on average, we are still obeying the limit set (and you could argue its better to go ahead and use up memory now instead of waiting for a write stall to happen to test this limit). However, if people are opposed to this default behavior, we can easily set it to 0 and require this parameter be set in order to use transactions. Test Plan: Added a xfunc test to play around with setting different values of this parameter in all tests. Added testing in memtablelist_test and planning on adding more testing here. Reviewers: sdong, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: for db_bench and more IO stats Summary: See for the IO stats. I added ""Cumulative compaction:"" and ""Interval compaction:"" lines. The IO rates can be confusing. Rates fro per-level stats lines, Wr(MB/s) & Rd(MB/s), are computed using the duration of the compaction job. If the job reads 10MB, writes 9MB and the job (IO & merging) takes 1 second then the rates are 10MB/s for read and 9MB/s for writes. The IO rates in the Cumulative compaction line uses the total uptime. The IO rates in the Interval compaction line uses the interval uptime. So these Cumalative & Interval compaction IO rates cannot be compared to the per-level IO rates. But both forms of the rates are useful for debugging perf. Task ID: Blame Rev: Test Plan: run db_bench Revert Plan: Database Impact: Memcache Impact: Other Notes: EImportant: begin *PUBLIC* platform impact section Bugzilla: end platform impact Reviewers: igor Reviewed By: igor Subscribers: dhruba Differential Revision:"
1174,1174,4.0,0.9793000221252441,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Added JSON manifest dump option to ldb command Summary: Added a new flag to the ldb manifest_dump command that prints out the version edits as JSON objects for easier reading and parsing of information. Test Plan: **Sample usage: ** ``` ./ldb manifest_dump ``` **Sample output:** ``` {""EditNumber"": 0, ""Comparator"": ""leveldb.BytewiseComparator"", ""ColumnFamily"": 0} {""EditNumber"": 1, ""LogNumber"": 0, ""ColumnFamily"": 0} {""EditNumber"": 2, ""LogNumber"": 4, ""PrevLogNumber"": 0, ""NextFileNumber"": 7, ""LastSeq"": 35356, ""AddedFiles"": [{""Level"": 0, ""FileNumber"": 5, ""FileSize"": 1949284, ""SmallestIKey"": """", ""LargestIKey"": """"}], ""ColumnFamily"": 0} ... {""EditNumber"": 13, ""PrevLogNumber"": 0, ""NextFileNumber"": 36, ""LastSeq"": 290994, ""DeletedFiles"": [{""Level"": 0, ""FileNumber"": 17}, {""Level"": 0, ""FileNumber"": 20}, {""Level"": 0, ""FileNumber"": 22}, {""Level"": 0, ""FileNumber"": 24}, {""Level"": 1, ""FileNumber"": 13}, {""Level"": 1, ""FileNumber"": 14}, {""Level"": 1, ""FileNumber"": 15}, {""Level"": 1, ""FileNumber"": 18}], ""AddedFiles"": [{""Level"": 1, ""FileNumber"": 25, ""FileSize"": 2114340, ""SmallestIKey"": """", ""LargestIKey"": """"}, {""Level"": 1, ""FileNumber"": 26, ""FileSize"": 2115213, ""SmallestIKey"": """", ""LargestIKey"": """"}, {""Level"": 1, ""FileNumber"": 27, ""FileSize"": 2114807, ""SmallestIKey"": """", ""LargestIKey"": """"}, {""Level"": 1, ""FileNumber"": 30, ""FileSize"": 2115271, ""SmallestIKey"": """", ""LargestIKey"": """"}, {""Level"": 1, ""FileNumber"": 31, ""FileSize"": 2115165, ""SmallestIKey"": """", ""LargestIKey"": """"}, {""Level"": 1, ""FileNumber"": 32, ""FileSize"": 2114683, ""SmallestIKey"": """", ""LargestIKey"": """"}, {""Level"": 1, ""FileNumber"": 35, ""FileSize"": 1757512, ""SmallestIKey"": """", ""LargestIKey"": """"}], ""ColumnFamily"": 0} ... ``` Reviewers: sdong, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba Differential Revision:"
1175,1175,7.0,0.8500000238418579,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Add function GetInfoLogList() Summary: The list of info log files of a db can be obtained using the new function. Test Plan: New test in db_test.cc passed. Reviewers: yhchiang, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: IslamAbdelRahman, leveldb, dhruba Differential Revision: Tests To Enable Subcompactions Summary: Updated DBTest DBCompactionTest and CompactionJobStatsTest to run compaction-related tests once with subcompactions enabled and once disabled using the TEST_P test type in the Google Test suite. Test Plan: ./db_test ./db_compaction-test ./compaction_job_stats_test Reviewers: sdong, igor, anthony, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: delete rate limiting Summary: Introduce DeleteScheduler that allow enforcing a rate limit on file deletion Instead of deleting files immediately, files are moved to trash directory and deleted in a background thread that apply sleep penalty between deletes if needed. I have updated PurgeObsoleteFiles and PurgeObsoleteWALFiles to use the delete_scheduler instead of env_->DeleteFile Test Plan: added delete_scheduler_test existing unit tests Reviewers: kradhakrishnan, anthony, rven, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: WriteOptions::timeout_hint_us Summary: In one of our recent meetings, we discussed deprecating features that are not being actively used. One of those features, at least within Facebook, is timeout_hint. The feature is really nicely implemented, but if nobody needs it, we should remove it from our code-base (until we get a valid use-case). Some arguments: * Less code better icache hit rate, smaller builds, simpler code * The motivation for adding timeout_hint_us was to work-around RocksDBs stall issue. However, were currently addressing the stall issue itself (see recent work on stall write_rate), so we should never see sharp lock-ups in the future. * Nobody is using the feature within Facebooks code-base. Googling for `timeout_hint_us` also doesnt yield any users. Test Plan: make check Reviewers: anthony, kradhakrishnan, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, dhruba, leveldb Differential Revision: global static functions in db_test_util to DBTestBase Summary: Move global static functions in db_test_util to DBTestBase. This is to prevent unused function warning when decoupling db_test.cc into multiple files. Test Plan: db_test Reviewers: igor, sdong, anthony, IslamAbdelRahman, kradhakrishnan Reviewed By: kradhakrishnan Subscribers: dhruba Differential Revision: a noisy unit test. Summary: The t/DBTest.DropWrites test still fails under certain gcc version in release unit test. I unfortunately cannot repro the failure (since the compilers have mapped library which I am not able to map to correctly). I am suspecting the clock skew. Test Plan: Run make check Reviewers: CC: sdong igore Task ID: Blame Rev:/Revert two diffs related to DBIter::FindPrevUserKey() Summary: This diff reverts the following two previous diffs related to DBIter::FindPrevUserKey(), which makes db_stress unstable. We should bake a better fix for this. * ""Fix a comparison in DBIter::FindPrevUserKey()"" ec70fea4c4025351190eba7a02bd09bb5f083790. * ""Fixed endless loop in DBIter::FindPrevUserKey()"" acee2b08a2d37154b8f9e2dc74b1966202c15ec5. Test Plan: db_stress Reviewers: anthony, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: timeout for drop writes. Summary: We have a race in the way test works. We avoided the race by adding the wait to the counter. I thought 1s was eternity, but that is not true in some scenarios. Increasing the timeout to 10s and adding warnings. Also, adding nosleep to avoid the case where the wakeup thread is waiting behind the sleeping thread for scheduling. Test Plan: Run make check Reviewers: siying igorcanadi CC: Task ID: Blame Rev:/Fix a comparison in DBIter::FindPrevUserKey() Summary: When seek target is a merge key (`kTypeMerge`), `DBIter::FindNextUserEntry()` advances the underlying iterator _past_ the current key (`saved_key_`); see `MergeValuesNewToOld()`. However, `FindPrevUserKey()` assumes that `iter_` points to an entry with the same user key as `saved_key_`. As a result, `it->Seek(key) && it->Prev()` can cause the iterator to be positioned at the _next_, instead of the previous, entry (new test, written by reproduces the bug). This diff changes `FindPrevUserKey()` to also skip keys that are _greater_ than `saved_key_`. Test Plan: db_test Reviewers: igor, sdong Reviewed By: sdong Subscribers: leveldb, dhruba, lovro Differential Revision: race in unit test. Summary: Avoid falling victim to race condition. Test Plan: Run the unit test Reviewers: sdong igor CC: Task ID: Blame Rev:/Implement a table-level row cache Summary: Implementation of a table-level row cache. It only caches point queries done through the `DB::Get` interface, queries done through the `Iterator` interface will completely skip the cache. Supports snapshots and merge operations. Test Plan: Ran `make valgrind_check commit-prereq` Reviewers: igor, philipp, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: WAL recovery consistency levels Summary: The ""one size fits all"" approach with WAL recovery will only introduce inconvenience for our varied clients as we go forward. The current recovery is a bit heuristic. We introduce the following levels of consistency while replaying the WAL. 1. RecoverAfterRestart (kTolerateCorruptedTailRecords) This mocks the current recovery mode. 2. RecoverAfterCleanShutdown (kAbsoluteConsistency) This is ideal for unit test and cases where the store is shutdown cleanly. We tolerate no corruption or incomplete writes. 3. RecoverPointInTime (kPointInTimeRecovery) This is ideal when using devices with controller cache or file systems which can loose data on restart. We recover upto the point were is no corruption or incomplete write. 4. RecoverAfterDisaster (kSkipAnyCorruptRecord) This is ideal mode to recover data. We tolerate corruption and incomplete writes, and we hop over those sections that we cannot make sense of salvaging as many records as possible. Test Plan: (1) Run added unit test to cover all levels. (2) Run make check. Reviewers: leveldb, sdong, igor Subscribers: yoshinorim, dhruba Differential Revision: CompactRangeOptions for CompactRange Summary: This diff update DB::CompactRange to use RangeCompactionOptions instead of using multiple parameters Old CompactRange is still available but deprecated Test Plan: make all check make rocksdbjava USE_CLANG=1 make all OPT=-DROCKSDB_LITE make release Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: dhruba Differential Revision: down writes by bytes written Summary: We slow down data into the database to the rate of options.delayed_write_rate (a new option) with this patch. The thread synchronization approach I take is to still synchronize write controller by DB mutex and GetDelay() is inside DB mutex. Try to minimize the frequency of getting time in GetDelay(). I verified it through db_bench and it seems to work hard_rate_limit is deprecated. options.delayed_write_rate is still not dynamically changeable. Need to work on it as a follow-up. Test Plan: Add new unit tests in db_test Reviewers: yhchiang, rven, kradhakrishnan, anthony, MarkCallaghan, igor Reviewed By: igor Subscribers: ikabiljo, leveldb, dhruba Differential Revision: EventListener::OnTableFileDeletion() Summary: Add EventListener::OnTableFileDeletion(), which will be called when a table file is deleted. Test Plan: Extend three existing tests in db_test to verify the deleted files. Reviewers: rven, anthony, kradhakrishnan, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: a stats counter for DB_WRITE back which was mistakenly removed. Summary: Add a stats counter for DB_WRITE back which was mistakenly removed. Test Plan: augment GroupCommitTest Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: users to migrate to options.level_compaction_dynamic_level_bytes=true using CompactRange() Summary: In DB::CompactRange(), change parameter ""reduce_level"" to ""change_level"". Users can compact all data to the last level if needed. By doing it, users can migrate the DB to options.level_compaction_dynamic_level_bytes=true. Test Plan: Add a unit test for it. Reviewers: yhchiang, anthony, kradhakrishnan, igor, rven Reviewed By: rven Subscribers: leveldb, dhruba Differential Revision: API Change] Make DB::GetDbIdentity() be const function. Summary: Make DB::GetDbIdentity() be const function. Test Plan: make db_test Reviewers: igor, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: flushes to run in parallel with manual compaction Summary: As title. I spent some time thinking about it and I dont think there should be any issue with running manual compaction and flushes in parallel Test Plan: make check works Reviewers: rven, yhchiang, sdong Reviewed By: yhchiang, sdong Subscribers: dhruba, leveldb Differential Revision: skips levels 1 to base_level for dynamic level base size Summary: CompactRange() now is much more expensive for dynamic level base size as it goes through all the levels. Skip those not used levels between level 0 an base level. Test Plan: Run all unit tests Reviewers: yhchiang, rven, anthony, kradhakrishnan, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1176,1176,2.0,0.9908999800682068,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Deprecate WriteOptions::timeout_hint_us Summary: In one of our recent meetings, we discussed deprecating features that are not being actively used. One of those features, at least within Facebook, is timeout_hint. The feature is really nicely implemented, but if nobody needs it, we should remove it from our code-base (until we get a valid use-case). Some arguments: * Less code better icache hit rate, smaller builds, simpler code * The motivation for adding timeout_hint_us was to work-around RocksDBs stall issue. However, were currently addressing the stall issue itself (see recent work on stall write_rate), so we should never see sharp lock-ups in the future. * Nobody is using the feature within Facebooks code-base. Googling for `timeout_hint_us` also doesnt yield any users. Test Plan: make check Reviewers: anthony, kradhakrishnan, sdong, yhchiang Reviewed By: yhchiang Subscribers: sdong, dhruba, leveldb Differential Revision: Env::GetThreadID(), which returns the ID of the current thread. Summary: Add Env::GetThreadID(), which returns the ID of the current thread. In addition, make GetThreadList() and InfoLog use same unique ID for the same thread. Test Plan: db_test listener_test Reviewers: igor, rven, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: EventListener::OnTableFileCreated() Summary: Add EventListener::OnTableFileCreated(), which will be called when a table file is created. This patch is part of the EventLogger and EventListener integration. Test Plan: Augment existing test in db/listener_test.cc Reviewers: anthony, kradhakrishnan, rven, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1177,1177,7.0,0.9133999943733215,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Measure file read latency histogram per level Summary: In internal stats, remember read latency histogram, if statistics is enabled. It can be retrieved from DB::GetProperty() with ""rocksdb.dbstats"" property, if it is enabled. Test Plan: Manually run db_bench and prints out ""rocksdb.dbstats"" by hand and make sure it prints out as expected Reviewers: igor, IslamAbdelRahman, rven, kradhakrishnan, anthony, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, leveldb, dhruba Differential Revision: LoadTableHandlers Summary: Add a new option that all LoadTableHandlers to use multiple threads to load files on DB Open and Recover Test Plan: make check COMPILE_WITH_TSAN=1 make check DISABLE_JEMALLOC=1 make all valgrind_check (still running) Reviewers: yhchiang, anthony, rven, kradhakrishnan, igor, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: DBOptions::skip_sats_update_on_db_open Summary: UpdateAccumulatedStats() is used to optimize compaction decision esp. when the number of deletion entries are high, but this function can slowdown DBOpen esp. in disk environment. This patch adds DBOptions::skip_sats_update_on_db_open, which skips UpdateAccumulatedStats() in DB::Open() time when its set to true. Test Plan: Add DBCompactionTest.SkipStatsUpdateTest Reviewers: igor, anthony, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: tnovak, dhruba, leveldb Differential Revision: L0-L1 Compaction: Restructure Compaction Job Summary: As of now compactions involving files from Level 0 and Level 1 are single threaded because the files in L0, although sorted, are not range partitioned like the other levels. This means that during L0-L1 compaction each file from L1 needs to be merged with potentially all the files from L0. This attempt to parallelize the L0-L1 compaction assigns a thread and a corresponding iterator to each L1 file that then considers only the key range found in that L1 file and only the L0 files that have those keys (and only the specific portion of those L0 files in which those keys are found). In this way the overlap is minimized and potentially eliminated between different iterators focusing on the same files. The first step is to restructure the compaction logic to break L0-L1 compactions into multiple, smaller, sequential compactions. Eventually each of these smaller jobs will be run simultaneously. Areas to pay extra attention to are Correct aggregation of compaction job statistics across multiple threads Proper opening/closing of output files (make sure each threads is unique) Keys that span multiple L1 files Skewed distributions of keys within L0 files Test Plan: Make and run db_test (newer version has separate compaction tests) and compaction_job_stats_test Reviewers: igor, noetzli, anthony, sdong, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, dhruba, leveldb Differential Revision: rate_limiter, write buffering, most perf context instrumentation and most random kill out of Env Summary: We want to keep Env a think layer for better portability. Less platform dependent codes should be moved out of Env. In this patch, I create a wrapper of file readers and writers, and put rate limiting, write buffering, as well as most perf context instrumentation and random kill out of Env. It will make it easier to maintain multiple Env in the future. Test Plan: Run all existing unit tests. Reviewers: anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: LevelFileIteratorState and LevelFileNumIterator from DB iterators arena Summary: Try to allocate LevelFileIteratorState and LevelFileNumIterator from DB iterators arena, instead of calling malloc and free. Test Plan: valgrind check Reviewers: rven, yhchiang, anthony, kradhakrishnan, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: L0 L1 trivial move on sorted data Summary: This diff updates the logic of how we do trivial move, now trivial move can run on any number of files in input level as long as they are not overlapping The conditions for trivial move have been updated Introduced conditions: Trivial move cannot happen if we have a compaction filter (except if the compaction is not manual) Input level files cannot be overlapping Removed conditions: Trivial move only run when the compaction is not manual Input level should can contain only 1 file More context on what tests failed because of Trivial move ``` DBTest.CompactionsGenerateMultipleFiles This test is expecting compaction on a file in L0 to generate multiple files in L1, this test will fail with trivial move because we end up with one file in L1 ``` ``` DBTest.NoSpaceCompactRange This test expect compaction to fail when we force environment to report running out of space, of course this is not valid in trivial move situation because trivial move does not need any extra space, and did not check for that ``` ``` DBTest.DropWrites Similar to DBTest.NoSpaceCompactRange ``` ``` DBTest.DeleteObsoleteFilesPendingOutputs This test expect that a file in L2 is deleted after its moved to L3, this is not valid with trivial move because although the file was moved it is now used by L3 ``` ``` CuckooTableDBTest.CompactionIntoMultipleFiles Same as DBTest.CompactionsGenerateMultipleFiles ``` This diff is based on a work by Test Plan: make check Reviewers: rven, sdong, igor Reviewed By: igor Subscribers: yhchiang, ott, march, dhruba, sdong Differential Revision:"
1178,1178,3.0,0.5249999761581421,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",C: add MultiGet support/
1179,1179,10.0,0.8881999850273132,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Support for SingleDelete() Summary: This patch fixes It introduces SingleDelete as a new database operation. This operation can be used to delete keys that were never overwritten (no put following another put of the same key). If an overwritten key is single deleted the behavior is undefined. Single deletion of a non-existent key has no effect but multiple consecutive single deletions are not allowed (see limitations). In contrast to the conventional Delete() operation, the deletion entry is removed along with the value when the two are lined up in a compaction. Note: The semantics are similar to prototype that allowed to have this behavior on the granularity of a column family ( ). This new patch, however, is more aggressive when it comes to removing tombstones: It removes the SingleDelete together with the value whenever there is no snapshot between them while the older patch only did this when the sequence number of the deletion was older than the earliest snapshot. Most of the complex additions are in the Compaction Iterator, all other changes should be relatively straightforward. The patch also includes basic support for single deletions in db_stress and db_bench. Limitations: Not compatible with cuckoo hash tables Single deletions cannot be used in combination with merges and normal deletions on the same key (other keys are not affected by this) Consecutive single deletions are currently not allowed (and older version of this patch supported this so it could be resurrected if needed) Test Plan: make all check Reviewers: yhchiang, sdong, rven, anthony, yoshinorim, igor Reviewed By: igor Subscribers: maykov, dhruba, leveldb Differential Revision: num_subcompactions to the more accurate max_subcompactions Summary: Up until this point we had DbOptions.num_subcompactions, but it is semantically more correct to call this max_subcompactions since we will schedule *up to* DbOptions.max_subcompactions smaller compactions at a time during a compaction job. I also added a option to db_bench Test Plan: make all make check Reviewers: sdong, igor, anthony, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: L0-L1 Compaction Prep]: Giving Subcompactions Their Own State Summary: In prepration for running multiple threads at the same time during a compaction job, this patch assigns each subcompaction its own state (instead of sharing the one global CompactionState). Each subcompaction then uses this state to update its statistics, keep track of its snapshots, etc. during the course of execution. Then at the end of all the executions the statistics are aggregated across the subcompactions so that the final result is the same as if only one larger compaction had run. Test Plan: ./db_test ./db_compaction_test ./compaction_job_test Reviewers: sdong, anthony, igor, noetzli, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, dhruba, leveldb Differential Revision:"
1180,1180,2.0,0.9587000012397766,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Changed num_subcompactions to the more accurate max_subcompactions Summary: Up until this point we had DbOptions.num_subcompactions, but it is semantically more correct to call this max_subcompactions since we will schedule *up to* DbOptions.max_subcompactions smaller compactions at a time during a compaction job. I also added a option to db_bench Test Plan: make all make check Reviewers: sdong, igor, anthony, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision:"
1181,1181,1.0,0.9886999726295471,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","env: add ReuseWritableFile Add an environment method to reuse an existing file. Provide a generic implementation that does a simple rename + open (writeable), and also a posix variant that is more careful about error handling (if we fail to open, do not rename, etc.). Signed-off-by: Sage Weil boolean variable to guard fallocate() calls Summary: Added boolean variable to guard fallocate() calls. Set to false to prevent space leaks when tests fail. Test Plan: Compliles Set to false and ran log device tests Reviewers: sdong, lovro, igor Reviewed By: igor Subscribers: dhruba Differential Revision: Use fallocate on LOG FILESS Summary: Use fallocate on LOG FILES to Test Plan: make check + with strace=== ~/rocksdb] strace trace=fallocate ./ldb scan fallocate(3, 01, 0, 4194304) 0 Reviewers: sdong, anthony, IslamAbdelRahman, kradhakrishnan, lgalanis, rven, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: reads should not return error if reading past file Summary: Currently, mmap returns IOError when user tries to read data past the end of the file. This diff changes the behavior. Now, we return just the bytes that we can, and report the size we returned via a Slice result. This is consistent with non-mmap behavior and also pread() system call. This diff is taken out of D45123. Test Plan: make check Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1182,1182,10.0,0.9277999997138977,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Adding new table properties Summary: This diff introduce new table properties that will be written for block based tables These properties are comparator name merge operator name property collectors names Test Plan: Added a new unit test to verify that these tests are written/read correctly Running all other tests right now (wont land until all tests finish) Reviewers: rven, kradhakrishnan, igor, sdong, anthony, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: for SingleDelete() Summary: This patch fixes It introduces SingleDelete as a new database operation. This operation can be used to delete keys that were never overwritten (no put following another put of the same key). If an overwritten key is single deleted the behavior is undefined. Single deletion of a non-existent key has no effect but multiple consecutive single deletions are not allowed (see limitations). In contrast to the conventional Delete() operation, the deletion entry is removed along with the value when the two are lined up in a compaction. Note: The semantics are similar to prototype that allowed to have this behavior on the granularity of a column family ( ). This new patch, however, is more aggressive when it comes to removing tombstones: It removes the SingleDelete together with the value whenever there is no snapshot between them while the older patch only did this when the sequence number of the deletion was older than the earliest snapshot. Most of the complex additions are in the Compaction Iterator, all other changes should be relatively straightforward. The patch also includes basic support for single deletions in db_stress and db_bench. Limitations: Not compatible with cuckoo hash tables Single deletions cannot be used in combination with merges and normal deletions on the same key (other keys are not affected by this) Consecutive single deletions are currently not allowed (and older version of this patch supported this so it could be resurrected if needed) Test Plan: make all check Reviewers: yhchiang, sdong, rven, anthony, yoshinorim, igor Reviewed By: igor Subscribers: maykov, dhruba, leveldb Differential Revision:"
1183,1183,10.0,0.4223000109195709,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Clean and expose CreateLoggerFromOptions Summary: CreateLoggerFromOptions have some parameters like db_log_dir and env, these parameters are redundant since they already exist in DBOptions this patch remove the redundant parameters and expose CreateLoggerFromOptions to users Test Plan: make check Reviewers: igor, anthony, yhchiang, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba, hermanlee4 Differential Revision: for informing backup downloading added Summary: In case of huge db backup infromation about progress of downloading would help. New callback parameter in CreateNewBackup() function will trigger whenever a some amount of data downloaded. Task: 8057631 Test Plan: ProgressCallbackDuringBackup test that cover new functionality added to BackupableDBTest tests. other test succeed as well. Reviewers: Guenena, benj, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: the need for LATEST_BACKUP in BackupEngine Summary: In the first implementation of BackupEngine, LATEST_BACKUP was the commit point. The backup became committed after the write to LATEST_BACKUP completed. However, we can avoid the need for LATEST_BACKUP. Instead of write to LATEST_BACKUP, the commit point can be the rename from `meta/<backup_id>.tmp` to `meta/<backup_id>`. Once we see that there exists a file `meta/<backup_id>` (without tmp), we can assume that backup is valid. In this diff, we still write out the file LATEST_BACKUP. We need to do this so that we can maintain backward compatibility. However, the new version doesnt depend on this file anymore. We get the latest backup by `ls`-ing `meta` directory. This diff depends on D41925 Test Plan: Adjusted backupable_db_test to this new behavior Reviewers: benj, yhchiang, sdong, AaronFeldman Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1184,1184,1.0,0.9635000228881836,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Pass column family ID to table property collector Summary: Pass column family ID through TablePropertiesCollectorFactory::CreateTablePropertiesCollector() so that users can identify which column family this file is for and handle it differently. Test Plan: Add unit test scenarios in tests related to table properties collectors to verify the information passed in is correct. Reviewers: rven, yhchiang, anthony, kradhakrishnan, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, leveldb, dhruba Differential Revision:"
1185,1185,10.0,0.9355000257492065,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Deferred snapshot creation in transactions Summary: Support for Transaction::CreateSnapshotOnNextOperation(). This is to fix a write-conflict race-condition that Yoshinori was running into when testing MyRocks with LinkBench. Test Plan: New tests Reviewers: yhchiang, spetrunia, rven, igor, yoshinorim, sdong Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: accidental object copy in transactions Summary: Should have used auto& instead of auto. Also needed to change the code a bit due to const correctness. Test Plan: unit tests Reviewers: sdong, igor, yoshinorim, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: Release Locks when rolling back to a savepoint Summary: Transaction::RollbackToSavePoint() will now release any locks that were taken since the previous SavePoint. To do this cleanly, I moved tracked_keys_ management into TransactionBase. Test Plan: New Transaction test. Reviewers: igor, rven, sdong Reviewed By: sdong Subscribers: dhruba, spetrunia, leveldb Differential Revision: Custom Locking API Summary: Prototype of API to allow MyRocks to override default Mutex/CondVar used by transactions with their own implementations. They would simply need to pass their own implementations of Mutex/CondVar to the templated TransactionDB::Open(). Default implementation of TransactionDBMutex/TransactionDBCondVar provided (but the code is not currently changed to use this). Let me know if this API makes sense or if it should be changed Test Plan: n/a Reviewers: yhchiang, rven, igor, sdong, spetrunia Reviewed By: spetrunia Subscribers: maykov, dhruba, leveldb Differential Revision: base class for transactions Summary: As I keep adding new features to transactions, I keep creating more duplicate code. This diff cleans this up by creating a base implementation class for Transaction and OptimisticTransaction to inherit from. The code in TransactionBase.h/.cc is all just copied from elsewhere. The only entertaining part of this class worth looking at is the virtual TryLock method which allows OptimisticTransactions and Transactions to share the same common code for Put/Get/etc. The rest of this diff is mostly red and easy on the eyes. Test Plan: No functionality change. existing tests pass. Reviewers: sdong, jkedgar, rven, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1186,1186,10.0,0.7382000088691711,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","PlainTableReader to support non-mmap mode Summary: PlainTableReader now only allows mmap-mode. Add the support to non-mmap mode for more flexibility. Refactor the codes to move all logic of reading data to PlainTableKeyDecoder, and consolidate the calls to Read() call and ReadVarint32() call. Implement the calls for both of mmap and non-mmap case seperately. For non-mmap mode, make copy of keys in several places when we need to move the buffer after reading the keys. Test Plan: Add the mode of non-mmap case in plain_table_db_test. Run it in valgrind mode too. Subscribers: leveldb, dhruba Differential Revision:"
1187,1187,10.0,0.6044999957084656,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Adding new table properties Summary: This diff introduce new table properties that will be written for block based tables These properties are comparator name merge operator name property collectors names Test Plan: Added a new unit test to verify that these tests are written/read correctly Running all other tests right now (wont land until all tests finish) Reviewers: rven, kradhakrishnan, igor, sdong, anthony, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: Option to Skip Flushing in TableBuilder/Pass column family ID to table property collector Summary: Pass column family ID through TablePropertiesCollectorFactory::CreateTablePropertiesCollector() so that users can identify which column family this file is for and handle it differently. Test Plan: Add unit test scenarios in tests related to table properties collectors to verify the information passed in is correct. Reviewers: rven, yhchiang, anthony, kradhakrishnan, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, leveldb, dhruba Differential Revision: ZSTD (not final format) compression type Summary: Add ZSTD compression type. The same way as adding LZ4. Test Plan: run all tests. Generate files in db_bench. Make sure reads succeed. But the SST files cannot be opened in older versions. Also some other adhoc tests. Reviewers: rven, anthony, IslamAbdelRahman, kradhakrishnan, igor Reviewed By: igor Subscribers: MarkCallaghan, maykov, yoshinorim, leveldb, dhruba Differential Revision:"
1188,1188,1.0,0.9697999954223633,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Fix MockTable ID storage On Windows two tests fail that use MockTable: flush_job_test and compaction_job_test with the following message: compaction_job_test_je.exe : Assertion failed: result.size() 4, file c:\dev\rocksdb\rocksdb\table\mock_table.cc, line 110 Investigation reveals that this failure occurs when a 4 byte ID written to a beginning of the physically open file (main contents remains in a in-memory map) can not be read back. The reason for the failure is that the ID is written directly to a WritableFile bypassing WritableFileWriter. The side effect of that is that pending_sync_ never becomes true so the file is never flushed, however, the direct cause of the failure is that the filesize_ member of the WritableFileWriter remains zero. At Close() the file is truncated to that size and the file becomes empty so the ID can not be read back./Pass column family ID to table property collector Summary: Pass column family ID through TablePropertiesCollectorFactory::CreateTablePropertiesCollector() so that users can identify which column family this file is for and handle it differently. Test Plan: Add unit test scenarios in tests related to table properties collectors to verify the information passed in is correct. Reviewers: rven, yhchiang, anthony, kradhakrishnan, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, leveldb, dhruba Differential Revision: NewTableReader to accept TableReaderOptions Summary: Refactoring NewTableReader to accept TableReaderOptions This will make it easier to add new options in the future, for example in this diff Test Plan: run existing tests Reviewers: igor, yhchiang, anthony, rven, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: maps with Comparator for sorting, other cleanup Summary: This diff is a collection of cleanups that were initially part of D43179. Additionally it adds a unified way of defining key-value maps that use a Comparator for sorting (this was previously implemented in four different places). Test Plan: make clean check all Reviewers: rven, anthony, yhchiang, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1189,1189,6.0,0.9872000217437744,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Seperate InternalIterator from Iterator Summary: Separate a new class InternalIterator from class Iterator, when the look-up is done internally, which also means they operate on key with sequence ID and type. This change will enable potential future optimizations but for now InternalIterators functions are still the same as Iterators. At the same time, separate the cleanup function to a separate class and let both of InternalIterator and Iterator inherit from it. Test Plan: Run all existing tests. Reviewers: igor, yhchiang, anthony, kradhakrishnan, IslamAbdelRahman, rven Reviewed By: rven Subscribers: leveldb, dhruba Differential Revision: read/written from cache statistics Summary: Add 2 new counters BLOCK_CACHE_BYTES_WRITE, BLOCK_CACHE_BYTES_READ to keep track of how many bytes were written to the cache and how many bytes that we read from cache Test Plan: make check Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: dhruba Differential Revision: bloom filter cache misses Summary: This optimizes the case when (cache_index_and_filter_blocks=1) and bloom filter is not present in the cache. Previously we did: 1. Read meta block from file 2. Read the filter position from the meta block 3. Read the filter Now, we pre-load the filter position on Table::Open(), so we can skip steps (1) and (2) on bloom filter cache miss. Instead of 2 IOs, we do only 1. Test Plan: make check Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1190,1190,10.0,0.988099992275238,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fix the bug of using freed memory introduced by recent plain table reader patch Summary: Recent patch introduced a bug that if non-mmap mode is used, in prefix encoding case, there is a resizing of cur_key_ within the same prefix, we still read prefix from the released buffer. It fails ASAN tests and this commit fixes it. Test Plan: Run the ASAN tests for the failing test case. Reviewers: IslamAbdelRahman, yhchiang, anthony, igor, kradhakrishnan, rven Subscribers: leveldb, dhruba Differential Revision: to support non-mmap mode Summary: PlainTableReader now only allows mmap-mode. Add the support to non-mmap mode for more flexibility. Refactor the codes to move all logic of reading data to PlainTableKeyDecoder, and consolidate the calls to Read() call and ReadVarint32() call. Implement the calls for both of mmap and non-mmap case seperately. For non-mmap mode, make copy of keys in several places when we need to move the buffer after reading the keys. Test Plan: Add the mode of non-mmap case in plain_table_db_test. Run it in valgrind mode too. Subscribers: leveldb, dhruba Differential Revision:"
1191,1191,6.0,0.7132999897003174,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Seperate InternalIterator from Iterator Summary: Separate a new class InternalIterator from class Iterator, when the look-up is done internally, which also means they operate on key with sequence ID and type. This change will enable potential future optimizations but for now InternalIterators functions are still the same as Iterators. At the same time, separate the cleanup function to a separate class and let both of InternalIterator and Iterator inherit from it. Test Plan: Run all existing tests. Reviewers: igor, yhchiang, anthony, kradhakrishnan, IslamAbdelRahman, rven Reviewed By: rven Subscribers: leveldb, dhruba Differential Revision:"
1192,1192,10.0,0.9919000267982483,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Adding new table properties Summary: This diff introduce new table properties that will be written for block based tables These properties are comparator name merge operator name property collectors names Test Plan: Added a new unit test to verify that these tests are written/read correctly Running all other tests right now (wont land until all tests finish) Reviewers: rven, kradhakrishnan, igor, sdong, anthony, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: read/written from cache statistics Summary: Add 2 new counters BLOCK_CACHE_BYTES_WRITE, BLOCK_CACHE_BYTES_READ to keep track of how many bytes were written to the cache and how many bytes that we read from cache Test Plan: make check Reviewers: sdong, yhchiang, igor Reviewed By: igor Subscribers: dhruba Differential Revision: NewTableReader to accept TableReaderOptions Summary: Refactoring NewTableReader to accept TableReaderOptions This will make it easier to add new options in the future, for example in this diff Test Plan: run existing tests Reviewers: igor, yhchiang, anthony, rven, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: bloom filter cache misses Summary: This optimizes the case when (cache_index_and_filter_blocks=1) and bloom filter is not present in the cache. Previously we did: 1. Read meta block from file 2. Read the filter position from the meta block 3. Read the filter Now, we pre-load the filter position on Table::Open(), so we can skip steps (1) and (2) on bloom filter cache miss. Instead of 2 IOs, we do only 1. Test Plan: make check Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: maps with Comparator for sorting, other cleanup Summary: This diff is a collection of cleanups that were initially part of D43179. Additionally it adds a unified way of defining key-value maps that use a Comparator for sorting (this was previously implemented in four different places). Test Plan: make clean check all Reviewers: rven, anthony, yhchiang, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1193,1193,1.0,0.9635000228881836,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Pass column family ID to table property collector Summary: Pass column family ID through TablePropertiesCollectorFactory::CreateTablePropertiesCollector() so that users can identify which column family this file is for and handle it differently. Test Plan: Add unit test scenarios in tests related to table properties collectors to verify the information passed in is correct. Reviewers: rven, yhchiang, anthony, kradhakrishnan, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, leveldb, dhruba Differential Revision:"
1194,1194,10.0,0.4156999886035919,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Dont merge WriteBatch-es if WAL is disabled Summary: Theres no need for WriteImpl to flatten the write batch group into a single WriteBatch if the WAL is disabled. This diff moves the flattening into the WAL step, and skips flattening entirely if it isnt needed. Its good for about 5% speedup on a multi-threaded workload with no WAL. This diff also adds clarifying comments about the chance for partial failure of WriteBatchInternal::InsertInto, and always sets bg_error_ if the memtable state diverges from the logged state or if a WriteBatch succeeds only partially. Benchmark for speedup: db_bench Test Plan: asserts + make check Reviewers: sdong, igor Reviewed By: igor Subscribers: dhruba Differential Revision: RocksDB to persist Options file. Summary: This patch allows rocksdb to persist options into a file on DB::Open, SetOptions, and Create / Drop ColumnFamily. Options files are created under the same directory as the rocksdb instance. In addition, this patch also adds a fail_if_missing_options_file in DBOptions that makes any function call return non-ok status when it is not able to persist options properly. // If true, then DB::Open / CreateColumnFamily / DropColumnFamily // / SetOptions will fail if options file is not detected or properly // persisted. // // DEFAULT: false bool fail_if_missing_options_file; Options file names are formatted as OPTIONS-<number>, and RocksDB will always keep the latest two options files. Test Plan: Add options_file_test. options_test column_family_test Reviewers: igor, IslamAbdelRahman, sdong, anthony Reviewed By: anthony Subscribers: dhruba Differential Revision: GetAggregatedIntProperty(): returns the aggregated value from all CFs Summary: This patch adds GetAggregatedIntProperty() that returns the aggregated value from all CFs Test Plan: Added a test in db_test Reviewers: igor, sdong, anthony, IslamAbdelRahman, rven Reviewed By: rven Subscribers: rven, dhruba, leveldb Differential Revision: multiple batches in single log record allow app to return a new batch + allow app to return corrupted record status/db_impl: recycle log files If log recycling is enabled, put old WAL files on a recycle queue instead of deleting them. When we need a new log file, take a recycled file off the list if one is available. Signed-off-by: Sage Weil for GetPropertiesOfTablesInRange Summary: In MyRocks, it is sometimes important to get propeties only for the subset of the database. This diff implements the API in RocksDB. Test Plan: ran the GetPropertiesOfTablesInRange Reviewers: rven, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: log filter to inspect and filter log records on recovery/Pass column family ID to table property collector Summary: Pass column family ID through TablePropertiesCollectorFactory::CreateTablePropertiesCollector() so that users can identify which column family this file is for and handle it differently. Test Plan: Add unit test scenarios in tests related to table properties collectors to verify the information passed in is correct. Reviewers: rven, yhchiang, anthony, kradhakrishnan, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, leveldb, dhruba Differential Revision: APIs PauseBackgroundWork() and ContinueBackgroundWork() Summary: To support a new MongoDB capability, we need to make sure that we dont do any IO for a short period of time. For background, see: * * To implement that, I add a new API calls PauseBackgroundWork() and ContinueBackgroundWork() which reuse the capability we already have in place for RefitLevel() function. Test Plan: Added a new test in db_test. Made sure that test fails when PauseBackgroundWork() is commented out. Reviewers: IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: experimental DB::AddFile() to plug sst files into empty DB Summary: This is an initial version of bulk load feature This diff allow us to create sst files, and then bulk load them later, right now the restrictions for loading an sst file are (1) Memtables are empty (2) Added sst files have sequence number 0, and existing values in database have sequence number 0 (3) Added sst files values are not overlapping Test Plan: unit testing Reviewers: igor, ott, sdong Reviewed By: sdong Subscribers: leveldb, ott, dhruba Differential Revision: stats WAL file synced to match meaning of the stats of the same name Summary: changed WAL sync bytes to extra fsync. This change does the same for internal stats. Test Plan: Run all existing unit tests and verify results in db_bench. Reviewers: anthony, rven, igor, MarkCallaghan, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: leveldb, dhruba Differential Revision: not flag error if file to be deleted does not exist Summary: Some users have observed errors in the log file when the log file or sst file is already deleted. Test Plan: Make sure that the errors do not appear for already deleted files. Reviewers: sdong Reviewed By: sdong Subscribers: anthony, kradhakrishnan, yhchiang, rven, igor, IslamAbdelRahman, dhruba, leveldb Differential Revision: the log level of DB start-up log from Warn to Header. Summary: Change the log level of DB start-up log from Warn to Header. Test Plan: db_bench and observe the LOG header Reviewers: igor, anthony, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: shouldnt release mutex between getting min_pending_output and scanning files Summary: Releasing mutex between getting min_pending_output and scanning files may cause min_pending_output to be max but some non-final files are found in file scanning, ending up with deleting wrong files. As a recent regression, mutex can be released while waiting for log sync. We move it to after file scanning. Test Plan: Run all existing tests. Dont think it is easy to write a unit test. Maybe we should find a way to assert lock not released so that we can have some test verification for similar cases. Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, kolmike, rven Reviewed By: rven Subscribers: leveldb, dhruba Differential Revision: deadlock in WAL sync Summary: MarkLogsSynced() was doing `logs_.erase(it++);`. The standard is saying: ``` all iterators and references are invalidated, unless the erased members are at an end (front or back) of the deque (in which case only iterators and references to the erased members are invalidated) ``` Because `it` is an iterator to the first element of the container, it is invalidated, only one iteration is executed and `log.getting_synced false;` is not being done, so `while (logs_.front().getting_synced)` in `WriteImpl()` is not terminating. Test Plan: make db_bench && ./db_bench Reviewers: igor, rven, IslamAbdelRahman, anthony, kradhakrishnan, yhchiang, sdong, tnovak Reviewed By: tnovak Subscribers: kolmike, dhruba, leveldb Differential Revision: race condition in DBTest.DynamicMemtableOptions Summary: This patch fixes a race condition in DBTEst.DynamicMemtableOptions. In rare cases, it was possible that the main thread would fill up both memtables before the flush job acquired its work. Then, the flush job was flushing both memtables together, producing only one L0 file while the test expected two. Now, the test waits for flushes to finish earlier, to make sure that the memtables are flushed in separate flush jobs. Test Plan: Insert ""usleep(10000);"" after ""IOSTATS_SET_THREAD_POOL_ID(Env::Priority::HIGH);"" in BGWorkFlush() to make the issue more likely. Then test with: make db_test && time while ./db_test do true; done Reviewers: rven, sdong, yhchiang, anthony, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: purging during flush Summary: Currently, we only purge duplicate keys and deletions during flush if `earliest_seqno_in_memtable newest_snapshot`. This means that the newest snapshot happened before we first created the memtable. This is almost never true for MyRocks and MongoRocks. This patch makes purging during flush able to understand snapshots. The main logic is copied from compaction_job.cc, although the logic over there is much more complicated and extensive. However, we should try to merge the common functionality at some point. I need this patch to implement no_overwrite_i_promise functionality for flush. Well also need this to support SingleDelete() during Flush(). requested the feature. Test Plan: make check I had to adjust some unit tests to understand this new behavior Reviewers: yhchiang, yoshinorim, anthony, sdong, noetzli Reviewed By: noetzli Subscribers: yoshinorim, dhruba, leveldb Differential Revision: limit deletes issued by DestroyDB Summary: Update DestroyDB so that all SST files in the first path id go through DeleteScheduler instead of being deleted immediately Test Plan: added a unittest Reviewers: igor, yhchiang, anthony, kradhakrishnan, rven, sdong Reviewed By: sdong Subscribers: jeanxu2012, dhruba Differential Revision: L0-L1 Compaction Prep]: Giving Subcompactions Their Own State Summary: In prepration for running multiple threads at the same time during a compaction job, this patch assigns each subcompaction its own state (instead of sharing the one global CompactionState). Each subcompaction then uses this state to update its statistics, keep track of its snapshots, etc. during the course of execution. Then at the end of all the executions the statistics are aggregated across the subcompactions so that the final result is the same as if only one larger compaction had run. Test Plan: ./db_test ./db_compaction_test ./compaction_job_test Reviewers: sdong, anthony, igor, noetzli, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, dhruba, leveldb Differential Revision:"
1195,1195,7.0,0.9911999702453613,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Make sure that CompactFiles does not run two parallel Level 0 compactions Summary: Since level 0 files can overlap, two level 0 compactions cannot run in parallel. Compact files needs to check this before running a compaction. Test Plan: CompactFilesTest.L0ConflictsFiles Reviewers: igor, IslamAbdelRahman, anthony, sdong, yhchiang Reviewed By: yhchiang Subscribers: dhruba, leveldb Differential Revision: a mode to always pick the oldest file to compact for each level Summary: Add options.compaction_pri, which specifies the policy about which file to compact first. kCompactionPriByLargestSeq will compact oldest files first. Verified the behavior in db_bench but did not write unit tests yet. Also need to make it settable through option string and dynamically changeable. Test Plan: Will write unit tests Reviewers: igor, rven, anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, MarkCallaghan Reviewed By: yhchiang Subscribers: leveldb, dhruba Differential Revision: condition for bottommost level during compaction Summary: The diff modifies the condition checked to determine the bottommost level during compaction. Previously, absence of files in higher levels alone was used as the condition. Now, the function additionally evaluates if the higher levels have files which have non-overlapping key ranges, then the level can be safely considered as the bottommost level. Test Plan: Unit test cases added and passing. However, unit tests of universal compaction are failing as a result of the changes made in this diff. Need to understand why that is happening. Reviewers: igor Subscribers: dhruba, sdong, lgalanis, meyering Differential Revision:"
1196,1196,10.0,0.9952999949455261,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Allow GetProperty to report the number of currently running flushes / compactions. Summary: Add rocksdb.num-running-compactions and rocksdb.num-running-flushes to GetIntProperty() that reports the number of currently running compactions / flushes. Test Plan: augmented existing tests in db_test Reviewers: igor, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: counters for L0 stall while L0-L1 compaction is taking place Summary: Although there are currently counters to keep track of the stall caused by having too many L0 files, there is no distinction as to whether when that stall occurs either (A) L0-L1 compaction is taking place to try and mitigate it, or (B) no L0-L1 compaction has been scheduled at the moment. This diff adds a counter for (A) so that the nature of L0 stalls can be better understood. Test Plan: make all && make check Reviewers: sdong, igor, anthony, noetzli, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, dhruba Differential Revision: maps with Comparator for sorting, other cleanup Summary: This diff is a collection of cleanups that were initially part of D43179. Additionally it adds a unified way of defining key-value maps that use a Comparator for sorting (this was previously implemented in four different places). Test Plan: make clean check all Reviewers: rven, anthony, yhchiang, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: per-level aggregated table properties via GetProperty() Summary: This patch adds ""rocksdb.aggregated-table-properties"" and ""rocksdb.aggregated-table-properties-at-levelN"", the former returns the aggreated table properties of a column family, while the later returns the aggregated table properties of the specified level N. Test Plan: Added tests in db_test Reviewers: igor, sdong, IslamAbdelRahman, anthony Reviewed By: anthony Subscribers: dhruba, leveldb Differential Revision: a counter about estimated pending compaction bytes Summary: Add a counter of estimated bytes the DB needs to compact for all the compactions to finish. Expose it as a DB Property. In the future, we can use threshold of this counter to replace soft rate limit and hard rate limit. A single threshold of estimated compaction debt in bytes will be easier for users to reason about when should slow down and stopping than more abstract soft and hard rate limits. Test Plan: Add unit tests Reviewers: IslamAbdelRahman, yhchiang, rven, kradhakrishnan, anthony, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: SST files size DB Property Summary: Add a new DB property that calculate the total size of files used by all RocksDB Versions Test Plan: Unittests for the new property Reviewers: igor, yhchiang, anthony, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: GetIntProperty(""rocksdb.size-all-mem-tables"") Summary: Currently, GetIntProperty(""rocksdb.cur-size-all-mem-tables"") only returns the memory usage by those memtables which have not yet been flushed. This patch introduces GetIntProperty(""rocksdb.size-all-mem-tables""), which includes the memory usage by all the memtables, includes those have been flushed but pinned by iterators. Test Plan: Added a test in db_test Reviewers: igor, anthony, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1197,1197,7.0,0.7071999907493591,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","New Manifest format to allow customized fields in NewFile. Summary: With this commit, we add a new format in manifest when adding a new file. Now path ID and need-compaction hint are first two customized fields. Test Plan: Add a test case in version_edit_test to verify the encoding and decoding logic. Add a unit test in db_test to verify need compaction is persistent after DB restarting. Reviewers: kradhakrishnan, anthony, IslamAbdelRahman, yhchiang, rven, igor Reviewed By: igor Subscribers: javigon, leveldb, dhruba Differential Revision:"
1198,1198,10.0,0.9316999912261963,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Support for SingleDelete() Summary: This patch fixes It introduces SingleDelete as a new database operation. This operation can be used to delete keys that were never overwritten (no put following another put of the same key). If an overwritten key is single deleted the behavior is undefined. Single deletion of a non-existent key has no effect but multiple consecutive single deletions are not allowed (see limitations). In contrast to the conventional Delete() operation, the deletion entry is removed along with the value when the two are lined up in a compaction. Note: The semantics are similar to prototype that allowed to have this behavior on the granularity of a column family ( ). This new patch, however, is more aggressive when it comes to removing tombstones: It removes the SingleDelete together with the value whenever there is no snapshot between them while the older patch only did this when the sequence number of the deletion was older than the earliest snapshot. Most of the complex additions are in the Compaction Iterator, all other changes should be relatively straightforward. The patch also includes basic support for single deletions in db_stress and db_bench. Limitations: Not compatible with cuckoo hash tables Single deletions cannot be used in combination with merges and normal deletions on the same key (other keys are not affected by this) Consecutive single deletions are currently not allowed (and older version of this patch supported this so it could be resurrected if needed) Test Plan: make all check Reviewers: yhchiang, sdong, rven, anthony, yoshinorim, igor Reviewed By: igor Subscribers: maykov, dhruba, leveldb Differential Revision:"
1199,1199,10.0,0.5263000130653381,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Initialize variable to avoid warning Summary: RocksDB debug version failed to build under gcc-4.8.1 on sandcastle with the following error: ``` db/db_compaction_filter_test.cc:570:33: error: snapshot may be used uninitialized in this function [-Werror=maybe-uninitialized] ``` Test Plan: make db_compaction_filter_test && ./db_compaction_filter_test Reviewers: rven, anthony, yhchiang, aekmekji, igor, sdong Reviewed By: igor, sdong Subscribers: dhruba, leveldb Differential Revision: bug in compaction iterator Summary: During the refactoring, the condition that makes sure that compaction filters are only applied to records newer than the latest snapshot got butchered. This patch fixes the condition and adds a test case. Test Plan: make db_compaction_filter_test && ./db_compaction_filter_test Reviewers: rven, anthony, yhchiang, sdong, aekmekji, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision:"
1200,1200,10.0,0.9973000288009644,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","make field order match initialization order/Handle multiple batches in single log record allow app to return a new batch + allow app to return corrupted record status/Implementation for GetPropertiesOfTablesInRange Summary: In MyRocks, it is sometimes important to get propeties only for the subset of the database. This diff implements the API in RocksDB. Test Plan: ran the GetPropertiesOfTablesInRange Reviewers: rven, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: format specifiers/Adding log filter to inspect and filter log records on recovery/Pass column family ID to table property collector Summary: Pass column family ID through TablePropertiesCollectorFactory::CreateTablePropertiesCollector() so that users can identify which column family this file is for and handle it differently. Test Plan: Add unit test scenarios in tests related to table properties collectors to verify the information passed in is correct. Reviewers: rven, yhchiang, anthony, kradhakrishnan, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, leveldb, dhruba Differential Revision: DBTest.AggregatedTableProperties more deterministic Summary: Now based on environment, DBTest.AggregatedTableProperties has a possibility of issuing a L0->L1 compaction after reopening and the results are not what we expected. We tune the L0 compaction trigger to make it less likely to happen. Test Plan: I cant repro the failure but I think the change is better. Just run the test and make sure it passes. Reviewers: kradhakrishnan, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: hit/miss stats for SST and memtable Summary: hit and miss bloom filter stats for memtable and SST stats added to perf_context struct key matches and prefix matches combined into one stat Test Plan: unit test veryfing the functionality added, see BloomStatsTest in db_test.cc for details Reviewers: yhchiang, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: for LevelDB SST with .ldb suffix Summary: Handle SST files with both "".sst"" and "".ldb"" suffix. This enables user to migrate from leveldb to rocksdb. Test Plan: Added unit test with DB operating on SSTs with names schema. See db/dc_test.cc:SSTsWithLdbSuffixHandling for details Reviewers: yhchiang, sdong, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: APIs PauseBackgroundWork() and ContinueBackgroundWork() Summary: To support a new MongoDB capability, we need to make sure that we dont do any IO for a short period of time. For background, see: * * To implement that, I add a new API calls PauseBackgroundWork() and ContinueBackgroundWork() which reuse the capability we already have in place for RefitLevel() function. Test Plan: Added a new test in db_test. Made sure that test fails when PauseBackgroundWork() is commented out. Reviewers: IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: options.hard_pending_compaction_bytes_limit to stop writes if compaction lagging behind Summary: Add an option to stop writes if compaction lefts behind. If estimated pending compaction bytes is more than threshold specified by options.hard_pending_compaction_bytes_liimt, writes will stop until compactions are cleared to under the threshold. Test Plan: Add unit test DBTest.HardLimit Reviewers: rven, kradhakrishnan, anthony, IslamAbdelRahman, yhchiang, igor Reviewed By: igor Subscribers: MarkCallaghan, leveldb, dhruba Differential Revision: DBTest.ReadLatencyHistogramByLevel more robust Summary: DBTest.ReadLatencyHistogramByLevel was not written as expected. After writes, reads arent guaranteed to hit data written. It was not expected. Fix it. Test Plan: Run the test multiple times Reviewers: IslamAbdelRahman, rven, anthony, kradhakrishnan, yhchiang, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: fix: table readers created by TableCache::Get() doesnt have latency histogram reported Summary: TableCache::Get() puts parameters in the wrong places so that table readers created by Get() will not have the histogram updated. Test Plan: Will write a unit test for that. Subscribers: leveldb, dhruba Differential Revision: usage to be calculated using malloc_usable_size() Summary: malloc_usable_size() gets a better estimation of memory usage. It is already used to calculate block cache memory usage. Use it in arena too. Test Plan: Run all unit tests Reviewers: anthony, kradhakrishnan, rven, IslamAbdelRahman, yhchiang Reviewed By: yhchiang Subscribers: leveldb, dhruba Differential Revision: num_subcompactions to the more accurate max_subcompactions Summary: Up until this point we had DbOptions.num_subcompactions, but it is semantically more correct to call this max_subcompactions since we will schedule *up to* DbOptions.max_subcompactions smaller compactions at a time during a compaction job. I also added a option to db_bench Test Plan: make all make check Reviewers: sdong, igor, anthony, yhchiang Reviewed By: yhchiang Subscribers: dhruba Differential Revision: a counter about estimated pending compaction bytes Summary: Add a counter of estimated bytes the DB needs to compact for all the compactions to finish. Expose it as a DB Property. In the future, we can use threshold of this counter to replace soft rate limit and hard rate limit. A single threshold of estimated compaction debt in bytes will be easier for users to reason about when should slow down and stopping than more abstract soft and hard rate limits. Test Plan: Add unit tests Reviewers: IslamAbdelRahman, yhchiang, rven, kradhakrishnan, anthony, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: GetIntProperty(""rocksdb.size-all-mem-tables"") Summary: Currently, GetIntProperty(""rocksdb.cur-size-all-mem-tables"") only returns the memory usage by those memtables which have not yet been flushed. This patch introduces GetIntProperty(""rocksdb.size-all-mem-tables""), which includes the memory usage by all the memtables, includes those have been flushed but pinned by iterators. Test Plan: Added a test in db_test Reviewers: igor, anthony, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1201,1201,10.0,0.5773000121116638,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","log_reader: pass log_number and optional info_log to ctor We will need the log number to validate the recycle-style CRCs. The log is helpful for debugging, but optional, as not all callers have it. Signed-off-by: Sage Weil pass log number and whether recycling is enabled to ctor When we recycle log files, we need to mix the log number into the CRC for each record. Note that for logs that dont get recycled (like the manifest), we always pass a log_number of 0 and false. Signed-off-by: Sage Weil for GetPropertiesOfTablesInRange Summary: In MyRocks, it is sometimes important to get propeties only for the subset of the database. This diff implements the API in RocksDB. Test Plan: ran the GetPropertiesOfTablesInRange Reviewers: rven, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: more kill points Summary: Add kill points in: 1. after creating a file 2. before writing a manifest record 3. before syncing manifest 4. before creating a new current file 5. after creating a new current file Test Plan: Run all current tests. Reviewers: yhchiang, igor, anthony, IslamAbdelRahman, rven, kradhakrishnan Reviewed By: kradhakrishnan Subscribers: leveldb, dhruba Differential Revision: InternalIterator from Iterator Summary: Separate a new class InternalIterator from class Iterator, when the look-up is done internally, which also means they operate on key with sequence ID and type. This change will enable potential future optimizations but for now InternalIterators functions are still the same as Iterators. At the same time, separate the cleanup function to a separate class and let both of InternalIterator and Iterator inherit from it. Test Plan: Run all existing tests. Reviewers: igor, yhchiang, anthony, kradhakrishnan, IslamAbdelRahman, rven Reviewed By: rven Subscribers: leveldb, dhruba Differential Revision: a mode to always pick the oldest file to compact for each level Summary: Add options.compaction_pri, which specifies the policy about which file to compact first. kCompactionPriByLargestSeq will compact oldest files first. Verified the behavior in db_bench but did not write unit tests yet. Also need to make it settable through option string and dynamically changeable. Test Plan: Will write unit tests Reviewers: igor, rven, anthony, kradhakrishnan, IslamAbdelRahman, yhchiang, MarkCallaghan Reviewed By: yhchiang Subscribers: leveldb, dhruba Differential Revision: should fail if the column family has been dropped Summary: This patch finally fixes the ColumnFamilyTest.ReadDroppedColumnFamily test. The test has been failing very sporadically and it was hard to repro. However, I managed to write a new tests that reproes the failure deterministically. Heres what happens: 1. We start the flush for the column family 2. We check if the column family was dropped here: 3. This check goes through, ends up in InstallMemtableFlushResults() and it goes into LogAndApply() 4. At about this time, we start dropping the column family. Dropping the column family process gets to LogAndApply() at about the same time as LogAndApply() from flush process 5. Drop column family goes through LogAndApply() first, marking the column family as dropped. 6. Flush process gets woken up and gets a chance to write to the MANIFEST. However, this is where it gets stuck: 7. We see that the column family was dropped, so there is no need to write to the MANIFEST. We return OK. 8. Flush gets OK back from LogAndApply() and it deletes the memtable, thinking that the data is now safely persisted to sst file. The fix is pretty simple. Instead of OK, we return ShutdownInProgress. This is not really true, but we have been using this status code to also mean ""this operation was canceled because the column family has been dropped"". The fix is only one LOC. All other code is related to tests. I added a new test that reproes the failure. I also moved SleepingBackgroundTask to util/testutil.h (because I needed it in column_family_test for my new test). Theres plenty of other places where we reimplement SleepingBackgroundTask, but Ill address that in a separate commit. Test Plan: 1. new test 2. make check 3. Make sure the ColumnFamilyTest.ReadDroppedColumnFamily doesnt fail on Travis: Reviewers: yhchiang, anthony, IslamAbdelRahman, kradhakrishnan, rven, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: boundaries of subcompactions Summary: Up to this point, the subcompactions that make up a compaction job have been divided based on the key range of the L1 files, and each subcompaction has handled the key range of only one file. However DBOption.max_subcompactions allows the user to designate how many subcompactions at most to perform. This patch updates the CompactionJob::GetSubcompactionBoundaries() to determine these divisions accordingly based on that option and other input/system factors. The current approach orders the starting and/or ending keys of certain compaction input files and then generates a histogram to approximate the size covered by the key range between each consecutive pair of keys. Then it groups these ranges into groups so that the sizes are approximately equal to one another. The approach has also been adapted to work for universal compaction as well instead of just for level-based compaction as it was before. These subcompactions are then executed in parallel by locally spawning threads, one for each. The results are then aggregated and the compaction completed. Test Plan: make all && make check Reviewers: yhchiang, anthony, igor, noetzli, sdong Reviewed By: sdong Subscribers: MarkCallaghan, dhruba, leveldb Differential Revision: a counter about estimated pending compaction bytes Summary: Add a counter of estimated bytes the DB needs to compact for all the compactions to finish. Expose it as a DB Property. In the future, we can use threshold of this counter to replace soft rate limit and hard rate limit. A single threshold of estimated compaction debt in bytes will be easier for users to reason about when should slow down and stopping than more abstract soft and hard rate limits. Test Plan: Add unit tests Reviewers: IslamAbdelRahman, yhchiang, rven, kradhakrishnan, anthony, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1202,1202,2.0,0.5976999998092651,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Refactor to support file_reader_writer on Windows. Summary. A change Has attempted to move common functionality out of platform dependent code to a new facility called file_reader_writer. This includes: perf counters Buffering RateLimiting However, the change did not attempt to refactor Windows code. To mitigate, we introduce new quering interfaces such as UseOSBuffer(), GetRequiredBufferAlignment() and ReaderWriterForward() for pure forwarding where required. Introduce WritableFile got a new method Truncate(). This is to communicate to the file as to how much data it has on close. When space is pre-allocated on Linux it is filled with zeros implicitly, no such thing exist on Windows so we must truncate file on close. When operating in unbuffered mode the last page is filled with zeros but we still want to truncate. Previously, Close() would take care of it but now buffer management is shifted to the wrappers and the file has no idea about the file true size. This means that Close() on the wrapper level must always include Truncate() as well as wrapper __dtor should call Close() and against double Close(). Move buffered/unbuffered write logic to the wrapper. Utilize Aligned buffer class. Adjust tests and implement Truncate() where necessary. Come up with reasonable defaults for new virtual interfaces. Forward calls for RandomAccessReadAhead class to avoid double buffering and locking (double locking in unbuffered mode on WIndows)./Make WinEnv::NowMicros return system time Previous change for the function made use of the QueryPerformanceCounter to return microseconds values that do not repeat as std::chrono::system_clock returned values that made auto_roll_logger_test fail. The interface documentation does not state that we need to return system time describing the return value as a number of microsecs since some moment in time. However, because on Linux it is implemented using gettimeofday various pieces of code (such as GenericRateLimiter) took advantage of that and make use of NowMicros() as a system timestamp. Thus the previous change broke rate_limiter_test on Windows. In addition, the interface name NowMicros() suggests that it is actually a timestamp so people use it as such. This change makes use of the new system call on Windows that returns system time with required precision. This change preserves the fix for auto_roll_logger_test and fixes rate_limiter_test. Note that DBTest.RateLimitingTest still fails due to a separately reported issue./Remove usage of C runtime API that has file handle limitation/"
1203,1203,14.0,0.43779999017715454,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Add in stress test and run it in crash_test Summary: Add an option of in stress test and cover it in crash test Test Plan: Run crash test and make sure three combinations of the two options show up randomly. Reviewers: IslamAbdelRahman, yhchiang, andrewkr, anthony, kradhakrishnan Reviewed By: kradhakrishnan Subscribers: leveldb, dhruba Differential Revision: MS compiler warning c4244. Mostly due to the fact that there are differences in sizes of int,long on 64 bit systems vs GNU./"
1204,1204,14.0,0.972100019454956,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Fix a race condition in persisting options Summary: This patch fix a race condition in persisting options which will cause a crash when: * Thread A obtain cf options and start to persist options based on that cf options. * Thread B kicks in and finish DropColumnFamily and delete cf_handle. * Thread A wakes up and tries to finish the persisting options and crashes. Test Plan: Add a test in column_family_test that can reproduce the crash Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1205,1205,13.0,0.9934999942779541,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Fixed a dependency issue of ThreadLocalPtr Summary: When a child thread that uses ThreadLocalPtr, ThreadLocalPtr::OnThreadExit will be called when that child thread is destroyed. However, OnThreadExit will try to access a static singleton of ThreadLocalPtr, which will be destroyed when the main thread exit. As a result, when a child thread that uses ThreadLocalPtr exits AFTER the main thread exits, illegal memory access will occur. This diff includes a test that reproduce this legacy bug. AddressSanitizer: heap-use-after-free on address 0x608000007fa0 at pc 0x959b79 bp 0x7f5fa7426b60 sp 0x7f5fa7426b58 READ of size 8 at 0x608000007fa0 thread T1 This patch fix this issue by having the thread local mutex never be deleted (but will leak small piece of memory at the end.) The patch also describe a better solution (thread_local) in the comment that requires gcc 4.8.1 and in latest clang as a future work once we agree to move toward gcc 4.8. Test Plan: COMPILE_WITH_ASAN=1 make thread_local_test ./thread_local_test Reviewers: anthony, hermanlee4, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: the destruction order of PosixEnv and ThreadLocalPtr Summary: By default, RocksDB initializes the singletons of ThreadLocalPtr first, then initializes PosixEnv via static initializer. Destructor terminates objects in reverse order, so terminating PosixEnv (calling pthread_mutex_lock), then ThreadLocal (calling pthread_mutex_destroy). However, in certain case, application might initialize PosixEnv first, then ThreadLocalPtr. This will cause core dump at the end of the program (eg. This patch fix this issue by ensuring the destruction order by moving the global static singletons to function static singletons. Since function static singletons are initialized when the function is first called, this property allows us invoke to enforce the construction of the static PosixEnv and the singletons of ThreadLocalPtr by calling the function where the ThreadLocalPtr singletons belongs right before we initialize the static PosixEnv. Test Plan: Verified in the MyRocks. Reviewers: yoshinorim, IslamAbdelRahman, rven, kradhakrishnan, anthony, sdong, MarkCallaghan Reviewed By: anthony Subscribers: dhruba, leveldb Differential Revision:"
1206,1206,18.0,0.864300012588501,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Fix a memory leak of Slice objects from org.rocksdb.WBWIRocksIterator#entry1/Improve the speed and synchronization around the construction of Java/JNI objects/
1207,1207,18.0,0.9136000275611877,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Remove unnessecary java.util.List expense in JNI/Pass by pointer from/to Java from JNI not by object/Improve the speed and synchronization around the construction of Java/JNI objects/Adding support for Windows JNI build/
1208,1208,18.0,0.84170001745224,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Pass by pointer from/to Java from JNI not by object/Improve the speed and synchronization around the construction of Java/JNI objects/
1209,1209,18.0,0.84170001745224,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Pass by pointer from/to Java from JNI not by object/Improve the speed and synchronization around the construction of Java/JNI objects/
1210,1210,18.0,0.864300012588501,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Remove unnessecary java.util.List expense in JNI/Improve the speed and synchronization around the construction of Java/JNI objects/
1211,1211,18.0,0.84170001745224,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Pass by pointer from/to Java from JNI not by object/Improve the speed and synchronization around the construction of Java/JNI objects/
1212,1212,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1213,1213,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1214,1214,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1215,1215,18.0,0.84170001745224,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Pass by pointer from/to Java from JNI not by object/Improve the speed and synchronization around the construction of Java/JNI objects/
1216,1216,18.0,0.84170001745224,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Pass by pointer from/to Java from JNI not by object/Improve the speed and synchronization around the construction of Java/JNI objects/
1217,1217,18.0,0.864300012588501,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Fix a memory leak of Slice objects from org.rocksdb.WBWIRocksIterator#entry1/Improve the speed and synchronization around the construction of Java/JNI objects/
1218,1218,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1219,1219,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1220,1220,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1221,1221,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1222,1222,18.0,0.864300012588501,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Fix a memory leak of Slice objects from org.rocksdb.WBWIRocksIterator#entry1/Improve the speed and synchronization around the construction of Java/JNI objects/
1223,1223,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1224,1224,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1225,1225,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1226,1226,18.0,0.864300012588501,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Fix a memory leak of Slice objects from org.rocksdb.WBWIRocksIterator#entry1/Improve the speed and synchronization around the construction of Java/JNI objects/
1227,1227,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1228,1228,18.0,0.8100000023841858,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Improve the speed and synchronization around the construction of Java/JNI objects/
1229,1229,10.0,0.49939998984336853,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fix issue Summary: See a bug report here: The fix is to not check the shared/ directory if share_table_files is false. We could also check FileExists() before GetChildren(), but that will add extra latency when Env is Hdfs :( Test Plan: added a unit test Reviewers: rven, sdong, IslamAbdelRahman, yhchiang, anthony Reviewed By: anthony Subscribers: dhruba, leveldb Differential Revision: to have a mode to use file size in file name Summary: Getting file size from all the backup files can take a long time. In some cases, the sizes are available in file names. We allow a mode to get those sizes from file name. Test Plan: Make some unit tests in backupable_db_test to run in such a mode. Make sure RocksDB Lite builds too. Reviewers: IslamAbdelRahman, rven, yhchiang, kradhakrishnan, anthony, igor Reviewed By: igor Subscribers: muthu, asameet, leveldb, dhruba Differential Revision:"
1230,1230,8.0,0.6489999890327454,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Transaction::UndoGetForUpdate Summary: MyRocks wants to be able to un-lock a key that was just locked by GetForUpdate(). To do this safely, I am now keeping track of the number of reads(for update) and writes for each key in a transaction. UndoGetForUpdate() will only unlock a key if it hasnt been written and the read count reaches 0. Test Plan: more unit tests Reviewers: igor, rven, yhchiang, spetrunia, sdong Reviewed By: spetrunia, sdong Subscribers: spetrunia, dhruba, leveldb Differential Revision: data race from expirable transactions Summary: Doing inline checking of transaction expiration instead of using a callback. Test Plan: To be added Reviewers: anthony Reviewed By: anthony Subscribers: leveldb, dhruba Differential Revision:"
1231,1231,7.0,0.9136000275611877,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Add BlockBasedTableOptions::index_block_restart_interval Summary: Add a new option to BlockBasedTableOptions that will allow us to change the restart interval for the index block Test Plan: unit tests Reviewers: yhchiang, anthony, andrewkr, sdong Reviewed By: sdong Subscribers: march, dhruba Differential Revision:"
1232,1232,10.0,0.9587000012397766,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Disallow SstFileWriter from creating empty sst files Summary: SstFileWriter may create an sst file with no entries Right now this will fail when being ingested using DB::AddFile() saying that the keys are corrupted Test Plan: make check Reviewers: yhchiang, rven, anthony, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision:"
1233,1233,13.0,0.9855999946594238,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Skip bottom-level filter block caching when hit-optimized Summary: When Get() or NewIterator() trigger file loads, skip caching the filter block if (1) optimize_filters_for_hits is set and (2) the file is on the bottommost level. Also skip checking filters under the same conditions, which means that for a preloaded file or a file that was trivially-moved to the bottom level, its filter block will eventually expire from the cache. added parameters/instance variables in various places in order to propagate the config (""skip_filters"") from version_set to block_based_table_reader in BlockBasedTable::Rep, this optimization prevents filter from being loaded when the file is opened simply by setting filter_policy nullptr in BlockBasedTable::Get/BlockBasedTable::NewIterator, this optimization prevents filter from being used (even if it was loaded already) by setting filter nullptr Test Plan: updated unit test: $ ./db_test will also run make check Reviewers: sdong, igor, paultuckfield, anthony, rven, kradhakrishnan, IslamAbdelRahman, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision:"
1234,1234,13.0,0.991100013256073,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Allows Get and MultiGet to read directly from SST files. Summary: Add kSstFileTier to ReadTier, which allows Get and MultiGet to read only directly from SST files and skip mem-tables. kSstFileTier 0x2 // data in SST files. // Note that this ReadTier currently only supports // Get and MultiGet and does not support iterators. Test Plan: add new test in db_test. Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: igor, dhruba, leveldb Differential Revision: issue in Iterator::Seek when using Block based filter block with prefix_extractor Summary: Similar to D53385 we need to check InDomain before checking the filter block. Test Plan: unit tests Reviewers: yhchiang, rven, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: bottom-level filter block caching when hit-optimized Summary: When Get() or NewIterator() trigger file loads, skip caching the filter block if (1) optimize_filters_for_hits is set and (2) the file is on the bottommost level. Also skip checking filters under the same conditions, which means that for a preloaded file or a file that was trivially-moved to the bottom level, its filter block will eventually expire from the cache. added parameters/instance variables in various places in order to propagate the config (""skip_filters"") from version_set to block_based_table_reader in BlockBasedTable::Rep, this optimization prevents filter from being loaded when the file is opened simply by setting filter_policy nullptr in BlockBasedTable::Get/BlockBasedTable::NewIterator, this optimization prevents filter from being used (even if it was loaded already) by setting filter nullptr Test Plan: updated unit test: $ ./db_test will also run make check Reviewers: sdong, igor, paultuckfield, anthony, rven, kradhakrishnan, IslamAbdelRahman, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision:"
1235,1235,7.0,0.8173999786376953,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Fixed the bug when both whole_key_filtering and prefix_extractor are set. Summary: When both whole_key_filtering and prefix_extractor are set, RocksDB will mistakenly encode prefix + whole key into the database instead of simply whole key when BlockBasedTable is used. This patch fixes this bug. Test Plan: Add a test in table_test Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: index seeking in BlockTableReader::PrefixMayMatch. PrefixMayMatch previously seeked in the prefix index using an internal key with a sequence number of 0. This would cause the prefix index seek to fall off the end if the last key in the index had a user-key greater than or equal to the key being looked for. Falling off the end of the index in turn results in PrefixMayMatch returning false if the index is in memory./support for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: ReadOptions::pin_data (support zero copy for keys) Summary: This patch update the Iterator API to introduce new functions that allow users to keep the Slices returned by key() valid as long as the Iterator is not deleted ReadOptions::pin_data : If true keep loaded blocks in memory as long as the iterator is not deleted Iterator::IsKeyPinned() : If true, this mean that the Slice returned by key() is valid as long as the iterator is not deleted Also add a new option BlockBasedTableOptions::use_delta_encoding to allow users to disable delta_encoding if needed. Benchmark results (using ``` // $ du /home/tec/local/normal.4K.Snappy/db10077 // 6.1G /home/tec/local/normal.4K.Snappy/db10077 // $ du /home/tec/local/zero.8K.LZ4/db10077 // 6.4G /home/tec/local/zero.8K.LZ4/db10077 // Benchmarks for shard db10077 // _build/opt/rocks/benchmark/rocks_copy_benchmark \ // \ // // First run // // rocks/benchmark/RocksCopyBenchmark.cpp relative time/iter iters/s // // BM_StringCopy 1.73s 576.97m // BM_StringPiece 103.74% 1.67s 598.55m // // Match rate : 1000000 / 1000000 // Second run // // rocks/benchmark/RocksCopyBenchmark.cpp relative time/iter iters/s // // BM_StringCopy 611.99ms 1.63 // BM_StringPiece 203.76% 300.35ms 3.33 // // Match rate : 1000000 / 1000000 ``` Test Plan: Unit tests Reviewers: sdong, igor, anthony, yhchiang, rven Reviewed By: rven Subscribers: dhruba, lovro, adsharma Differential Revision:"
1236,1236,7.0,0.8763999938964844,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Allows Get and MultiGet to read directly from SST files. Summary: Add kSstFileTier to ReadTier, which allows Get and MultiGet to read only directly from SST files and skip mem-tables. kSstFileTier 0x2 // data in SST files. // Note that this ReadTier currently only supports // Get and MultiGet and does not support iterators. Test Plan: add new test in db_test. Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: igor, dhruba, leveldb Differential Revision: perf of Pessimistic Transaction expirations (and optimistic transactions) Summary: copy from task 8196669: 1) Optimistic transactions do not support batching writes from different threads. 2) Pessimistic transactions do not support batching writes if an expiration time is set. In these 2 cases, we currently do not do any write batching in DBImpl::WriteImpl() because there is a WriteCallback that could decide at the last minute to abort the write. But we could support batching write operations with callbacks if we make sure to process the callbacks correctly. To do this, we would first need to modify write_thread.cc to stop preventing writes with callbacks from being batched together. Then we would need to change DBImpl::WriteImpl() to call all WriteCallbacks in a batch, only write the batches that succeed, and correctly set the state of each batchs WriteThread::Writer. Test Plan: Added test WriteWithCallbackTest to write_callback_test.cc which creates multiple client threads and verifies that writes are batched and executed properly. Reviewers: hermanlee4, anthony, ngbronson Subscribers: leveldb, dhruba Differential Revision: duplicated property constants Summary: Before this diff, there were duplicated constants to refer to properties (user- facing API had strings and InternalStats had an enum). I noticed these were inconsistent in terms of which constants are provided, names of constants, and documentation of constants. Overall it seemed annoying/error-prone to maintain these duplicated constants. So, this diff gets rid of InternalStatss constants and replaces them with a map keyed on the user-facing constant. The value in that map contains a function pointer to get the property value, so we dont need to do string matching while holding db->mutex_. This approach has a side benefit of making many small handler functions rather than a giant switch-statement. Test Plan: db_properties_test passes, running ""make commit-prereq Reviewers: sdong, yhchiang, kradhakrishnan, IslamAbdelRahman, rven, anthony Reviewed By: anthony Subscribers: dhruba, leveldb Differential Revision: for with batching Summary: Concurrent memtable adds were incorrectly computing the last sequence number for a write batch group when the write batches were not solitary. This is the cause of Test Plan: 1. unit tests 2. new unit test 3. parallel db_bench stress tests with batch size of 10 and asserts enabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: IslamAbdelRahman, MarkCallaghan, dhruba Differential Revision: SstFileManager (component tracking all SST file in DBs and control the deletion rate) Summary: Add a new class SstFileTracker that will be notified whenever a DB add/delete/move and sst file, it will also replace DeleteScheduler SstFileTracker can be used later to abort writes when we exceed a specific size Test Plan: unit tests Reviewers: rven, anthony, yhchiang, sdong Reviewed By: sdong Subscribers: igor, lovro, march, dhruba Differential Revision: Mark files to be deleted as being compacted before applying change Summary: While running the myrocks regression suite, I found that while dropping a table soon after inserting rows into it resulted in an assertion failure in CheckConsistencyForDeletes for not finding a file which was recently added or moved. Marking the files to be deleted as being compacted before calling LogAndApplyChange fixed the assertion failures. Test Plan: DBCompactionTest.DeleteFileRange Reviewers: IslamAbdelRahman, anthony, yhchiang, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba, yoshinorim, leveldb Differential Revision: files in given key range Summary: This is an initial diff for providing the ability to delete files which are completely within a given range of keys. Test Plan: DBCompactionTest.DeleteRange Reviewers: IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: Visual Studio Warning C4351 Currently Windows build is broken because of Warning C4351. Disable the warning before figuring out the right way to fix it./Fix CLANG errors introduced by 7d87f02799bd0a8fd36df24fab5baa4968615c86 Summary: Fix some CLANG errors introduced in 7d87f02799bd0a8fd36df24fab5baa4968615c86 Test Plan: Build with both of CLANG and gcc Reviewers: rven, yhchiang, kradhakrishnan, anthony, IslamAbdelRahman, ngbronson Subscribers: leveldb, dhruba Differential Revision: for concurrent adds to memtable Summary: This diff adds support for concurrent adds to the skiplist memtable implementations. Memory allocation is made thread-safe by the addition of a spinlock, with small per-core buffers to avoid contention. Concurrent memtable writes are made via an additional method and dont impose a performance overhead on the non-concurrent case, so parallelism can be selected on a per-batch basis. Write thread synchronization is an increasing bottleneck for higher levels of concurrency, so this diff adds (default off). This feature causes threads joining a write batch group to spin for a short time (default 100 usec) using sched_yield, rather than going to sleep on a mutex. If the timing of the yield calls indicates that another thread has actually run during the yield then spinning is avoided. This option improves performance for concurrent situations even without parallel adds, although it has the potential to increase CPU usage (and the heuristic adaptation is not yet mature). Parallel writes are not currently compatible with inplace updates, update callbacks, or delete filtering. Enable it with (and Parallel memtable writes are performance neutral when there is no actual parallelism, and in my experiments (SSD server-class Linux and varying contention and key sizes for fillrandom) they are always a performance win when there is more than one thread. Statistics are updated earlier in the write path, dropping the number of DB mutex acquisitions from 2 to 1 for almost all cases. This diff was motivated and inspired by Yahoos cLSM work. It is more conservative than cLSM: RocksDBs write batch group leader role is preserved (along with all of the existing flush and write throttling logic) and concurrent writers are blocked until all memtable insertions have completed and the sequence number has been advanced, to preserve linearizability. My test config is ""db_bench on a two-socket Xeon E5-2660 2.2Ghz with lots of memory and an SSD hard drive. With 1 thread I get ~440Kops/sec. Peak performance for 1 socket (numactl is slightly more than 1Mops/sec, at 16 threads. Peak performance across both sockets happens at 30 threads, and is ~900Kops/sec, although with fewer threads there is less performance loss when the system has background work. Test Plan: 1. concurrent stress tests for InlineSkipList and DynamicBloom 2. make clean; make check 3. make clean; DISABLE_JEMALLOC=1 make valgrind_check; valgrind db_bench 4. make clean; COMPILE_WITH_TSAN=1 make all check; db_bench 5. make clean; COMPILE_WITH_ASAN=1 make all check; db_bench 6. make clean; OPT=-DROCKSDB_LITE make check 7. verify no perf regressions when disabled Reviewers: igor, sdong Reviewed By: sdong Subscribers: MarkCallaghan, IslamAbdelRahman, anthony, yhchiang, rven, sdong, guyg8, kradhakrishnan, dhruba Differential Revision: call to install superversion and schedule work in enableautocompactions Summary: This patch fixes There is a recent change in rocksdb to disable auto compactions on startup: However, there is a small timing window where a column family needs to be compacted and schedules a compaction, but the scheduled compaction fails when it checks the disable_auto_compactions setting. The expectation is once the application is ready, it will call EnableAutoCompactions() to allow new compactions to go through. However, if the Column family is stalled because L0 is full, and no writes can go through, it is possible the column family may never have a new compaction request get scheduled. EnableAutoCompaction() should probably schedule an new flush and compaction event when it resets disable_auto_compaction. Using InstallSuperVersionAndScheduleWork, we call SchedulePendingFlush, SchedulePendingCompaction, as well as MaybeScheduleFlushOrcompaction on all the column families to avoid the situation above. This is still a first pass for feedback. Could also just call SchedePendingFlush and SchedulePendingCompaction directly. Test Plan: Run on Asan build cd _build-5.6-ASan/ && ./mysql-test/mtr rocksdb_rpl.rpl_rocksdb_stress_crash Ensure that it no longer hangs during the test. Reviewers: hermanlee4, yhchiang, anthony Reviewed By: anthony Subscribers: leveldb, yhchiang, dhruba Differential Revision: minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: MS compiler warning c4244. Mostly due to the fact that there are differences in sizes of int,long on 64 bit systems vs GNU./Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision: the fix for a race condition in persisting options Summary: This patch fix a race condition in persisting options which will cause a crash when: * Thread A obtain cf options and start to persist options based on that cf options. * Thread B kicks in and finish DropColumnFamily and delete cf_handle. * Thread A wakes up and tries to finish the persisting options and crashes. Test Plan: Add a test in column_family_test that can reproduce the crash Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision: marking snapshots for write-conflict checking Take 2 Summary: D51183 was reverted due to breaking the LITE build. This diff is the same as D51183 but with a fix for the LITE BUILD(D51693) Test Plan: run all unit tests Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: ""Support marking snapshots for write-conflict checking"" This reverts commit ec704aafdcfa997330e9c92736d15e17025e3399 for it broke RocksDB LITE build./Revert ""Fix a race condition in persisting options"" This reverts commit 2fa3ed5180340e485a1caf6fa71cc400ea599278. It breaks RocksDB lite build/Fix a race condition in persisting options Summary: This patch fix a race condition in persisting options which will cause a crash when: * Thread A obtain cf options and start to persist options based on that cf options. * Thread B kicks in and finish DropColumnFamily and delete cf_handle. * Thread A wakes up and tries to finish the persisting options and crashes. Test Plan: Add a test in column_family_test that can reproduce the crash Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: public api to schedule flush/compaction, code to prevent race with db::open Summary: Fixes T8781168. Added a new function EnableAutoCompactions in db.h to be publicly avialable. This allows compaction to be re-enabled after disabling it via SetOptions Refactored code to set the dbptr earlier on in TransactionDB::Open and DB::Open Temporarily disable auto_compaction in TransactionDB::Open until dbptr is set to prevent race condition. Test Plan: Ran make all check verified fix on myrocks side: was able to reproduce the seg fault with ../tools/mysqltest.sh rocksdb.drop_table method was to manually sleep the thread after DB::Open but before TransactionDB ptr was assigned in transaction_db_impl.cc: DB::Open(db_options, dbname, column_families_copy, handles, &db); clock_t goal (60000 * 10) + clock(); while (goal > clock()); ...dbptr(aka rdb) gets assigned below verified my changes fixed the issue. Also added unit test ToggleAutoCompaction in transaction_test.cc Reviewers: hermanlee4, anthony Reviewed By: anthony Subscribers: alex, dhruba Differential Revision: to only flush the column family with the largest memtable while option.db_write_buffer_size is hit Summary: When option.db_write_buffer_size is hit, we currently flush all column families. Move to flush the column family with the largest active memt table instead. In this way, we can avoid too many small files in some cases. Test Plan: Modify test DBTest.SharedWriteBuffer to work with the updated behavior Reviewers: kradhakrishnan, yhchiang, rven, anthony, IslamAbdelRahman, igor Reviewed By: igor Subscribers: march, leveldb, dhruba Differential Revision:"
1237,1237,10.0,0.9814000129699707,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add SstFileManager (component tracking all SST file in DBs and control the deletion rate) Summary: Add a new class SstFileTracker that will be notified whenever a DB add/delete/move and sst file, it will also replace DeleteScheduler SstFileTracker can be used later to abort writes when we exceed a specific size Test Plan: unit tests Reviewers: rven, anthony, yhchiang, sdong Reviewed By: sdong Subscribers: igor, lovro, march, dhruba Differential Revision: DBTest.Randomized Summary: Break down DBTest.Randomized to multiple gtest tests based on config type Test Plan: Run the test and all tests. Make sure configurations are correctly set Reviewers: yhchiang, IslamAbdelRahman, rven, kradhakrishnan, andrewkr, anthony Reviewed By: anthony Subscribers: leveldb, dhruba Differential Revision:"
1238,1238,7.0,0.9954000115394592,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Not scheduling more L1->L2 compaction if L0->L1 is pending with higher priority Summary: When L0->L1 is pending, there may be one L1->L2 compaction going on which prevents the L0->L1 compaction from happening. If L1 needs more data to be moved to L2, then we may continue scheduling more L1->L2 compactions. The end result may be that L0->L1 compaction will not happen until L1 size drops to below target size. We can reduce the stalling because of number of L0 files by stopping schedling new L1->L2 compaction when L0s score is higher than L1. Test Plan: Run all existing tests. Reviewers: yhchiang, MarkCallaghan, rven, anthony, IslamAbdelRahman, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision: compaction reason in CompactionListener Summary: Add CompactionReason to CompactionJobInfo This will allow users to understand why compaction started which will help options tuning Test Plan: added new tests make check Reviewers: yhchiang, anthony, kradhakrishnan, sdong, rven Reviewed By: rven Subscribers: dhruba Differential Revision: typo: sr to picking_sr/Fix minor bugs in delete operator, snprintf, and size_t usage Summary: List of changes: 1) Fix the snprintf() usage in cases where wrong variable was used to determine the output buffer size. 2) Remove unnecessary checks before calling delete operator. 3) Increase code correctness by using size_t type when getting vectors size. 4) Unify the coding style by removing namespace::std usage at the top of the file to confirm to the majority usage. 5) Fix various lint errors pointed out by arc lint. Test Plan: Code review and build: git diff make clean make 32 commit-prereq arc lint Reviewers: kradhakrishnan, sdong, rven, anthony, yhchiang, igor Reviewed By: igor Subscribers: dhruba, leveldb Differential Revision: manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision: avoid to form compactions if there is no file Summary: Currently RocksDB may break in lines like this: for (size_t i sorted_runs.size() 1; i >= first_index_after; i--) { if options.level0_file_num_compaction_trigger=0. Fix it by not executing the logic of picking compactions if there is no file (sorted_runs.size() 0). Also internally set options.level0_file_num_compaction_trigger=1 if users give a 0. 0 is a value makes no sense in RocksDB. Test Plan: Run all tests. Will add a unit test too. Reviewers: yhchiang, IslamAbdelRahman, anthony, kradhakrishnan, rven Reviewed By: rven Subscribers: leveldb, dhruba Differential Revision:"
1239,1239,10.0,0.636900007724762,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Eliminate duplicated property constants Summary: Before this diff, there were duplicated constants to refer to properties (user- facing API had strings and InternalStats had an enum). I noticed these were inconsistent in terms of which constants are provided, names of constants, and documentation of constants. Overall it seemed annoying/error-prone to maintain these duplicated constants. So, this diff gets rid of InternalStatss constants and replaces them with a map keyed on the user-facing constant. The value in that map contains a function pointer to get the property value, so we dont need to do string matching while holding db->mutex_. This approach has a side benefit of making many small handler functions rather than a giant switch-statement. Test Plan: db_properties_test passes, running ""make commit-prereq Reviewers: sdong, yhchiang, kradhakrishnan, IslamAbdelRahman, rven, anthony Reviewed By: anthony Subscribers: dhruba, leveldb Differential Revision: property-related variable names Summary: I noticed these names were quite confusing while updating GetProperty documentation. Test Plan: running ""make commit-prereq Reviewers: yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: named constants for remaining properties Summary: There were just these two properties that didnt have any named constant. Test Plan: build and below test $ ./db_properties_test Reviewers: yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: options.soft_rate_limit and add options.soft_pending_compaction_bytes_limit Summary: Deprecate options.soft_rate_limit, which is hard to tune, with options.soft_pending_compaction_bytes_limit, which would trigger the slowdown if estimated pending compaction bytes exceeds the threshold. The hope is to make it more striaght-forward to tune. Test Plan: Modify DBTest.SoftLimit to cover options.soft_pending_compaction_bytes_limit instead; run all unit tests. Reviewers: IslamAbdelRahman, yhchiang, rven, kradhakrishnan, igor, anthony Reviewed By: anthony Subscribers: leveldb, dhruba Differential Revision:"
1240,1240,16.0,0.944100022315979,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","compaction assertion triggering test fix for sequence zeroing assertion trip/Have a way for compaction filter to ignore snapshots Summary: Provide an API for compaction filter to specify that it needs to be applied even if there are snapshots. Test Plan: DBTestCompactionFilter.CompactionFilterIgnoreSnapshot Reviewers: yhchiang, IslamAbdelRahman, sdong, anthony Reviewed By: anthony Subscribers: yoshinorim, dhruba, leveldb Differential Revision:"
1241,1241,7.0,0.5030999779701233,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","When slowdown is triggered, reduce the write rate Summary: Its usually hard for users to set a value of options.delayed_write_rate. With this diff, after slowdown condition triggers, we greedily reduce write rate if estimated pending compaction bytes increase. If estimated compaction pending bytes drop, we increase the write rate. Test Plan: Add a unit test Test with db_bench setting: TEST_TMPDIR=/dev/shm/ ./db_bench and make sure without the commit, write stop will happen, but with the commit, it will not happen. Reviewers: igor, anthony, rven, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision: clang build Summary: Missed this in because I didnt wait for make commit-prereq to finish Test Plan: make clean && USE_CLANG=1 make all Reviewers: IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: bottom-level filter block caching when hit-optimized Summary: When Get() or NewIterator() trigger file loads, skip caching the filter block if (1) optimize_filters_for_hits is set and (2) the file is on the bottommost level. Also skip checking filters under the same conditions, which means that for a preloaded file or a file that was trivially-moved to the bottom level, its filter block will eventually expire from the cache. added parameters/instance variables in various places in order to propagate the config (""skip_filters"") from version_set to block_based_table_reader in BlockBasedTable::Rep, this optimization prevents filter from being loaded when the file is opened simply by setting filter_policy nullptr in BlockBasedTable::Get/BlockBasedTable::NewIterator, this optimization prevents filter from being used (even if it was loaded already) by setting filter nullptr Test Plan: updated unit test: $ ./db_test will also run make check Reviewers: sdong, igor, paultuckfield, anthony, rven, kradhakrishnan, IslamAbdelRahman, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision: ReadOptions::pin_data (support zero copy for keys) Summary: This patch update the Iterator API to introduce new functions that allow users to keep the Slices returned by key() valid as long as the Iterator is not deleted ReadOptions::pin_data : If true keep loaded blocks in memory as long as the iterator is not deleted Iterator::IsKeyPinned() : If true, this mean that the Slice returned by key() is valid as long as the iterator is not deleted Also add a new option BlockBasedTableOptions::use_delta_encoding to allow users to disable delta_encoding if needed. Benchmark results (using ``` // $ du /home/tec/local/normal.4K.Snappy/db10077 // 6.1G /home/tec/local/normal.4K.Snappy/db10077 // $ du /home/tec/local/zero.8K.LZ4/db10077 // 6.4G /home/tec/local/zero.8K.LZ4/db10077 // Benchmarks for shard db10077 // _build/opt/rocks/benchmark/rocks_copy_benchmark \ // \ // // First run // // rocks/benchmark/RocksCopyBenchmark.cpp relative time/iter iters/s // // BM_StringCopy 1.73s 576.97m // BM_StringPiece 103.74% 1.67s 598.55m // // Match rate : 1000000 / 1000000 // Second run // // rocks/benchmark/RocksCopyBenchmark.cpp relative time/iter iters/s // // BM_StringCopy 611.99ms 1.63 // BM_StringPiece 203.76% 300.35ms 3.33 // // Match rate : 1000000 / 1000000 ``` Test Plan: Unit tests Reviewers: sdong, igor, anthony, yhchiang, rven Reviewed By: rven Subscribers: dhruba, lovro, adsharma Differential Revision:"
1242,1242,7.0,0.9269000291824341,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Report compaction reason in CompactionListener Summary: Add CompactionReason to CompactionJobInfo This will allow users to understand why compaction started which will help options tuning Test Plan: added new tests make check Reviewers: yhchiang, anthony, kradhakrishnan, sdong, rven Reviewed By: rven Subscribers: dhruba Differential Revision:"
1243,1243,7.0,0.7279999852180481,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Skip filters for last L0 file if hit-optimized Summary: Following up on D53493, we can still enable the filter-skipping optimization for last file in L0. Its correct to assume the key will be present in the last L0 file when were hit-optimized and L0 is deepest. The FilePicker encapsulates the state for traversing each levels files, so I needed to make it expose whether the returned file is last in its level. Test Plan: verified below test fails before this patch and passes afterwards. The change to how the test memtable is populated is needed so file 1 has keys (0, 30, 60), file 2 has keys (10, 40, 70), etc. $ ./db_universal_compaction_test Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: not skip bloom filter for L0 during the query. Summary: Its a regression bug caused by e089db40f9c8f2a8af466377ed0f6fd8a3c26456. With the change, if options.optimize_filters_for_hits=true and there are only L0 files (like single level universal compaction), we skip all the files in L0, which is more than necessary. Fix it by always trying to query bloom filter for files in level 0. Test Plan: Add a unit test for it. Reviewers: anthony, rven, yhchiang, IslamAbdelRahman, kradhakrishnan, andrewkr Reviewed By: andrewkr Subscribers: leveldb, dhruba Differential Revision: Fixed a crash in LogAndApply() when CF creation failed Summary: That line used to dereference `column_family_data`, which is nullptr if were creating a column family. Test Plan: `make check` Reviewers: sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: bottom-level filter block caching when hit-optimized Summary: When Get() or NewIterator() trigger file loads, skip caching the filter block if (1) optimize_filters_for_hits is set and (2) the file is on the bottommost level. Also skip checking filters under the same conditions, which means that for a preloaded file or a file that was trivially-moved to the bottom level, its filter block will eventually expire from the cache. added parameters/instance variables in various places in order to propagate the config (""skip_filters"") from version_set to block_based_table_reader in BlockBasedTable::Rep, this optimization prevents filter from being loaded when the file is opened simply by setting filter_policy nullptr in BlockBasedTable::Get/BlockBasedTable::NewIterator, this optimization prevents filter from being used (even if it was loaded already) by setting filter nullptr Test Plan: updated unit test: $ ./db_test will also run make check Reviewers: sdong, igor, paultuckfield, anthony, rven, kradhakrishnan, IslamAbdelRahman, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision: rebase issues and new code warnings./Enable MS compiler warning c4244. Mostly due to the fact that there are differences in sizes of int,long on 64 bit systems vs GNU./Enable MS compiler warning c4244. Mostly due to the fact that there are differences in sizes of int,long on 64 bit systems vs GNU./Use SST files for Transaction conflict detection Summary: Currently, transactions can fail even if there is no actual write conflict. This is due to relying on only the memtables to check for write-conflicts. Users have to tune memtable settings to try to avoid this, but its hard to figure out exactly how to tune these settings. With this diff, TransactionDB will use both memtables and SST files to determine if there are any write conflicts. This relies on the fact that BlockBasedTable stores sequence numbers for all writes that happen after any open snapshot. Also, D50295 is needed to prevent SingleDelete from disappearing writes (the TODOs in this test code will be fixed once the other diff is approved and merged). Note that Optimistic transactions will still rely on tuning memtable settings as we do not want to read from SST while on the write thread. Also, memtable settings can still be used to reduce how often TransactionDB needs to read SST files. Test Plan: unit tests, db bench Reviewers: rven, yhchiang, kradhakrishnan, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb, yoshinorim Differential Revision: new compaction picking priority that optimizes for write amplification for random updates. Summary: Introduce a compaction picking priority that picks files who contains the oldest rows to compact. This is a mode that slightly improves write amplification for random update cases. Test Plan: Add a unit test and run it in valgrind too. Reviewers: yhchiang, anthony, IslamAbdelRahman, rven, kradhakrishnan, MarkCallaghan, igor Reviewed By: igor Subscribers: leveldb, dhruba Differential Revision:"
1244,1244,7.0,0.704800009727478,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Env function for bulk metadata retrieval Summary: Added this new function, which returns filename, size, and modified timestamp for each file in the provided directory. The default implementation retrieves the metadata sequentially using existing functions. In the next diff Ill make HdfsEnv override this function to use libhdfss bulk get function. This wont work on windows due to the path separator. Test Plan: new unit test $ ./env_test Reviewers: yhchiang, sdong Reviewed By: sdong Subscribers: IslamAbdelRahman, dhruba, leveldb Differential Revision: use of GetSystemTimePreciseAsFileTime dynamic to not break compatibility with Windows 7. The issue with rotated logs was fixed other way./Running manual compactions in parallel with other automatic or manual compactions in restricted cases Summary: This diff provides a framework for doing manual compactions in parallel with other compactions. We now have a deque of manual compactions. We also pass manual compactions as an argument from RunManualCompactions down to BackgroundCompactions, so that RunManualCompactions can be reentrant. Parallelism is controlled by the two routines ConflictingManualCompaction to allow/disallow new parallel/manual compactions based on already existing ManualCompactions. In this diff, by default manual compactions still have to run exclusive of other compactions. However, by setting the compaction option, exclusive_manual_compaction to false, it is possible to run other compactions in parallel with a manual compaction. However, we are still restricted to one manual compaction per column family at a time. All of these restrictions will be relaxed in future diffs. I will be adding more tests later. Test Plan: Rocksdb regression + new tests + valgrind Reviewers: igor, anthony, IslamAbdelRahman, kradhakrishnan, yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, dhruba, leveldb Differential Revision:"
1245,1245,7.0,0.9891999959945679,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[ldb] Templatize the Selector Summary: So a customized ldb tool can pass its own Selector. Such a selector is expected to call LDBCommand::SelectCommand and then add some of its own customized commands Test Plan: make ldb Reviewers: sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba Differential Revision: option for compression dictionary size Summary: Expose the option so its easy to run offline tests of compression dictionary feature. Test Plan: verified compression dictionary is loaded into lz4 for below command: $ ./ldb compact Reviewers: IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: Export ldb_cmd*.h Summary: This is needed so that rocksdb users can add more commands to the included ldb tool by adding more custom commands. Test Plan: make ldb Reviewers: sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba Differential Revision: support new WAL records/Introduce XPRESS compresssion on Windows. (#1081) Comparable with Snappy on comp ratio. Implemented using Windows API, does not require external package. Avaiable since Windows 8 and server 2012. Use with CMake to enable./Expose RepairDB as ldb command Summary: This will make it easier for admins and devs to use RepairDB. Test Plan: Tried deleting the manifest and verified it recovers: $ ldb put ok ok $ rm /tmp/test_db/MANIFEST-000001 $ ./ldb repair $ ldb get ok ok Reviewers: yhchiang, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision:"
1246,1246,13.0,0.9075000286102295,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Enable testing CompactFiles in db_stress Summary: Enable testing CompactFiles in db_stress by adding flag test_compact_files to db_stress. Test Plan: ./db_stress ./db_stress Sample output (note that its normal to have some CompactFiles() failed): Stress Test : 491.891 micros/op 65054 ops/sec : Wrote 21.98 MB (0.45 MB/sec) (45% of 3200352 ops) : Wrote 1440728 times : Deleted 441616 times : Single deleted 38181 times : 319251 read and 19025 found the key : Prefix scanned 640520 times : Iterator size sum is 9691415 : Iterated 319704 times : Got errors 0 times : 1323 CompactFiles() succeed : 32 CompactFiles() failed 2016/04/11-15:50:58 Verification successful Reviewers: sdong, IslamAbdelRahman, kradhakrishnan, yiwu, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1247,1247,3.0,0.5670999884605408,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Improve sst_dump help message Summary: Current Message ``` sst_dump [--command=check|scan|none|raw] [--verify_checksum] [--output_hex] [--input_key_hex] [--from=<user_key>] [--to=<user_key>] [--read_num=NUM] [--show_properties] [--show_compression_sizes] [--show_compression_sizes [--set_block_size=<block_size>]] ``` New message ``` sst_dump [--command=check|scan|raw] Path to SST file or directory containing SST files check: Iterate over entries in files but dont print anything except if an error is encounterd (default command) scan: Iterate over entries in files and print them to screen raw: Dump all the table contents to Can be combined with scan command to print the keys and values in Hex Key to start reading from when executing check|scan Key to stop reading at when executing check|scan Maximum number of entries to read when executing check|scan Verify file checksum when executing check|scan Can be combined with and to indicate that these values are encoded in Hex Print table properties after iterating over the file Independent command that will recreate the SST file using 16K block size with different compressions and report the size of the file using such compression Can be combined with to set the block size that will be used when trying different compression algorithms ``` Test Plan: none Reviewers: yhchiang, andrewkr, kradhakrishnan, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
1248,1248,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Fix formatting identified by `arc lint`/
1249,1249,16.0,0.5249999761581421,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Add support for PauseBackgroundWork and ContinueBackgroundWork to the Java API (#1087) Closes
1250,1250,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Fix formatting identified by `arc lint`/
1251,1251,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Fix formatting identified by `arc lint`/
1252,1252,7.0,0.9472000002861023,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Revert ""Fix failing Java unit test."" This reverts commit d7ae42b0f89fd25d8aaed28703059889d145596e. This is reverted as auto buld failure. This commit itself doesnt have any problem. Reverting as it depends on the commit to revert./Fix failing Java unit test. Test Plan: sent diff to sdong, passes :) Reviewers: sdong Reviewed By: sdong Subscribers: leveldb, andrewkr, dhruba Differential Revision:"
1253,1253,4.0,0.8416000008583069,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Fix the javadoc and the formatting of the base-classes for objects with native references/
1254,1254,4.0,0.8416000008583069,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Fix the javadoc and the formatting of the base-classes for objects with native references/
1255,1255,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Deprecate org.rocksdb.AbstractNativeReference#dispose() and implement java.lang.AutoCloseable/
1256,1256,4.0,0.8416000008583069,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Fix formatting identified by `arc lint`/Fix the javadoc and the formatting of the base-classes for objects with native references/
1257,1257,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Fix formatting identified by `arc lint`/
1258,1258,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Fix formatting identified by `arc lint`/
1259,1259,0.0,0.05000000074505806,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index",Fix formatting identified by `arc lint`/
1260,1260,11.0,0.3862999975681305,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Isolate db env and backup Env in unit tests Summary: Used ChrootEnv so the database and backup Envs are isolated in the filesystem. Removed DifferentEnvs test since now every test uses different Envs Depends on D57543 Test Plan: ran backupable_db_test verified backupable_db_test now catches the bug when D57159 is backed out (this bug previously passed through the test cases, which motivated this change) Reviewers: sdong, lightmark, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba, leveldb Differential Revision: BackupableDBTest Summary: Fix BackupableDBTest.NoDoubleCopy and BackupableDBTest.DifferentEnvs by mocking the db files in db_env instead of backup_env_ Test Plan: make check Reviewers: sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: backupable_db_test test cases that cant run by itself Summary: Several of backupable_db_test fails if running standalone, because of directory missing. Fix it by: (1) garbage collector skips shared directory if it doesnt exit (2) BackupableDBTest.Issue921Test to create the parent directory of the backup directory fist. Test Plan: Run the tests individually and make sure they pass Subscribers: leveldb, andrewkr, dhruba Differential Revision: backup can store optional application specific metadata Summary: Rocksdb backup engine maintains metadata about backups in separate files. But, there was no way to add extra application specific data to it. Adding support for that. In some use cases, applications decide to restore a backup based on some metadata. This will help those cases to cheaply decide whether to restore or not. Test Plan: Added a unit test. Existing ones are passing Sample meta file for BinaryMetadata test- ``` 1459454043 0 metadata 6162630A64656600676869 2 private/1/MANIFEST-000001 crc32 1184723444 private/1/CURRENT crc32 3505765120 ``` Reviewers: sdong, ldemailly, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, ldemailly Differential Revision: stale manifests outside of full purge Summary: Keep track of obsolete manifests in VersionSet Updated FindObsoleteFiles() to put obsolete manifests in the JobContext for later use by PurgeObsoleteFiles() Added test case that verifies a stale manifest is deleted by a non-full purge Test Plan: $ ./backupable_db_test Reviewers: IslamAbdelRahman, yoshinorim, sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision: file attributes in bulk for VerifyBackup and CreateNewBackup Summary: For VerifyBackup(), backup files can be spread across ""shared/"", ""shared_checksum/"", and ""private/"" subdirectories, so we have to bulk get all three. For CreateNewBackup(), we make two separate bulk calls: one for the data files and one for WAL files. There is also a new helper function, ExtendPathnameToSizeBytes(), that translates the file attributes vector to a map. I decided to leave GetChildrenFileAttributes()s (from D53781) return type as vector to keep it consistent with GetChildren(). Depends on D53781. Test Plan: verified relevant unit tests $ ./backupable_db_test Reviewers: IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: concurrent manifest update and backup creation Summary: Fixed two related race conditions in backup creation. (1) CreateNewBackup() uses DB::DisableFileDeletions() to prevent table files from being deleted while it is copying; however, the MANIFEST file could still rotate during this time. The fix is to stop deleting the old manifest in the rotation logic. It will be deleted safely later when PurgeObsoleteFiles() runs (can only happen when file deletions are enabled). (2) CreateNewBackup() did not account for the CURRENT file being mutable. This is significant because the files returned by GetLiveFiles() contain a particular manifest filename, but the manifest to which CURRENT refers can change at any time. This causes problems when CURRENT changes between the call to GetLiveFiles() and when its copied to the backup directory. To workaround this, I manually forge a CURRENT file referring to the manifest filename returned in GetLiveFiles(). (2) also applies to the checkpointing code, so let me know if this approach is good and Ill make the same change there. Test Plan: new test for roll manifest during backup creation. running the test before this change: $ ./backupable_db_test ... IO error: /tmp/rocksdbtest-9383/backupable_db/MANIFEST-000001: No such file or directory running the test after this change: $ ./backupable_db_test ... [ RUN ] BackupableDBTest.ChangeManifestDuringBackupCreation [ OK ] BackupableDBTest.ChangeManifestDuringBackupCreation (2836 ms) Reviewers: IslamAbdelRahman, anthony, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
1261,1261,15.0,0.7978000044822693,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","[rocksdb] Two Phase Transaction Summary: Two Phase Commit addition to RocksDB. See wiki: Quip: Depends on: WriteBatch modification: Memtable Log Referencing and Prepared Batch Recovery: Test Plan: SimpleTwoPhaseTransactionTest PersistentTwoPhaseTransactionTest. TwoPhaseRollbackTest TwoPhaseMultiThreadTest TwoPhaseLogRollingTest TwoPhaseEmptyWriteTest TwoPhaseExpirationTest Reviewers: IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: leveldb, hermanlee4, andrewkr, vasilep, dhruba, santoshb Differential Revision:"
1262,1262,1.0,0.9943000078201294,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Store SST file compression algorithm as a TableProperty Summary: Store SST file compression algorithm as a TableProperty. Test Plan: Modified and ran the table_test UT that checks for TableProperties Reviewers: IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: lgalanis, andrewkr, dhruba, IslamAbdelRahman Differential Revision: dictionary compression using reference block Summary: This adds a new metablock containing a shared dictionary that is used to compress all data blocks in the SST file. The size of the shared dictionary is configurable in CompressionOptions and defaults to 0. Its currently only used for zlib/lz4/lz4hc, but the block will be stored in the SST regardless of the compression type if the user chooses a nonzero dictionary size. During compaction, computes the dictionary by randomly sampling the first output file in each subcompaction. It pre-computes the intervals to sample by assuming the output file will have the maximum allowable length. In case the file is smaller, some of the pre-computed sampling intervals can be beyond end-of-file, in which case we skip over those samples and the dictionary will be a bit smaller. After the dictionary is generated using the first file in a subcompaction, it is loaded into the compression library before writing each block in each subsequent file of that subcompaction. On the read path, gets the dictionary from the metablock, if it exists. Then, loads that dictionary into the compression library before reading each block. Test Plan: new unit test Reviewers: yhchiang, IslamAbdelRahman, cyan, sdong Reviewed By: sdong Subscribers: andrewkr, yoshinorim, kradhakrishnan, dhruba, leveldb Differential Revision: comparator, merge operator, property collectors to SST file properties (again) Summary: This is the original diff that I have landed and reverted and now I want to land again For old SST files we will show ``` comparator name: N/A merge operator name: N/A property collectors names: N/A ``` For new SST files with no merge operator name and with no property collectors ``` comparator name: leveldb.BytewiseComparator merge operator name: nullptr property collectors names: [] ``` for new SST files with these properties ``` comparator name: leveldb.BytewiseComparator merge operator name: UInt64AddOperator property collectors names: [DummyPropertiesCollector1,DummyPropertiesCollector2] ``` Test Plan: unittests Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: column family name in SST file Summary: Added the column family name to the properties block. This property is omitted only if the property is unavailable, such as when RepairDB() writes SST files. In a next diff, I will change RepairDB to use this new property for deciding to which column family an existing SST file belongs. If this property is missing, it will add it to the ""unknown"" column family (same as its existing behavior). Test Plan: New unit test: $ ./db_table_properties_test Reviewers: IslamAbdelRahman, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1263,1263,1.0,0.48649999499320984,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Shared dictionary compression using reference block Summary: This adds a new metablock containing a shared dictionary that is used to compress all data blocks in the SST file. The size of the shared dictionary is configurable in CompressionOptions and defaults to 0. Its currently only used for zlib/lz4/lz4hc, but the block will be stored in the SST regardless of the compression type if the user chooses a nonzero dictionary size. During compaction, computes the dictionary by randomly sampling the first output file in each subcompaction. It pre-computes the intervals to sample by assuming the output file will have the maximum allowable length. In case the file is smaller, some of the pre-computed sampling intervals can be beyond end-of-file, in which case we skip over those samples and the dictionary will be a bit smaller. After the dictionary is generated using the first file in a subcompaction, it is loaded into the compression library before writing each block in each subsequent file of that subcompaction. On the read path, gets the dictionary from the metablock, if it exists. Then, loads that dictionary into the compression library before reading each block. Test Plan: new unit test Reviewers: yhchiang, IslamAbdelRahman, cyan, sdong Reviewed By: sdong Subscribers: andrewkr, yoshinorim, kradhakrishnan, dhruba, leveldb Differential Revision: to skip index checking if we cant find a filter block. Summary: In the case where we cant find a filter block, there is not much benefit of doing the binary search and see whether the index key has the prefix. With the change, we blindly return true if we cant get the filter. It also fixes missing row cases for reverse comparator with full bloom. Test Plan: Add a test case that used to fail. Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: kradhakrishnan, yiwu, hermanlee4, yoshinorim, leveldb, andrewkr, dhruba Differential Revision: no need to find data block after full bloom checking Summary: Full block checking should be a good enough indication of prefix existance. No need to further check data block. This also fixes wrong results when using prefix bloom and reverse bitwise comparator. Test Plan: Will add a unit test. Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: hermanlee4, yoshinorim, yiwu, kradhakrishnan, leveldb, andrewkr, dhruba Differential Revision: pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision: Reader should not be reused after DB restart Summary: In block based table reader, wow we put index reader to block cache, which can be retrieved after DB restart. However, index reader may reference internal comparator, which can be destroyed after DB restarts, causing problems. Fix it by making cache key identical per table reader. Test Plan: Add a new test which failed with out the commit but now pass. Reviewers: IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: maro, yhchiang, kradhakrishnan, leveldb, andrewkr, dhruba Differential Revision: to have an option to fail Cache::Insert() when full Summary: Cache to have an option to fail Cache::Insert() when full. Update call sites to check status and handle error. I totally have no idea whats correct behavior of all the call sites when they encounter error. Please let me know if you see something wrong or more unit test is needed. Test Plan: make check see tests pass. Reviewers: anthony, yhchiang, andrewkr, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1264,1264,1.0,0.7533000111579895,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Shared dictionary compression using reference block Summary: This adds a new metablock containing a shared dictionary that is used to compress all data blocks in the SST file. The size of the shared dictionary is configurable in CompressionOptions and defaults to 0. Its currently only used for zlib/lz4/lz4hc, but the block will be stored in the SST regardless of the compression type if the user chooses a nonzero dictionary size. During compaction, computes the dictionary by randomly sampling the first output file in each subcompaction. It pre-computes the intervals to sample by assuming the output file will have the maximum allowable length. In case the file is smaller, some of the pre-computed sampling intervals can be beyond end-of-file, in which case we skip over those samples and the dictionary will be a bit smaller. After the dictionary is generated using the first file in a subcompaction, it is loaded into the compression library before writing each block in each subsequent file of that subcompaction. On the read path, gets the dictionary from the metablock, if it exists. Then, loads that dictionary into the compression library before reading each block. Test Plan: new unit test Reviewers: yhchiang, IslamAbdelRahman, cyan, sdong Reviewed By: sdong Subscribers: andrewkr, yoshinorim, kradhakrishnan, dhruba, leveldb Differential Revision: XPRESS compresssion on Windows. (#1081) Comparable with Snappy on comp ratio. Implemented using Windows API, does not require external package. Avaiable since Windows 8 and server 2012. Use with CMake to enable./"
1265,1265,1.0,0.992900013923645,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Shared dictionary compression using reference block Summary: This adds a new metablock containing a shared dictionary that is used to compress all data blocks in the SST file. The size of the shared dictionary is configurable in CompressionOptions and defaults to 0. Its currently only used for zlib/lz4/lz4hc, but the block will be stored in the SST regardless of the compression type if the user chooses a nonzero dictionary size. During compaction, computes the dictionary by randomly sampling the first output file in each subcompaction. It pre-computes the intervals to sample by assuming the output file will have the maximum allowable length. In case the file is smaller, some of the pre-computed sampling intervals can be beyond end-of-file, in which case we skip over those samples and the dictionary will be a bit smaller. After the dictionary is generated using the first file in a subcompaction, it is loaded into the compression library before writing each block in each subsequent file of that subcompaction. On the read path, gets the dictionary from the metablock, if it exists. Then, loads that dictionary into the compression library before reading each block. Test Plan: new unit test Reviewers: yhchiang, IslamAbdelRahman, cyan, sdong Reviewed By: sdong Subscribers: andrewkr, yoshinorim, kradhakrishnan, dhruba, leveldb Differential Revision: comparator, merge operator, property collectors to SST file properties (again) Summary: This is the original diff that I have landed and reverted and now I want to land again For old SST files we will show ``` comparator name: N/A merge operator name: N/A property collectors names: N/A ``` For new SST files with no merge operator name and with no property collectors ``` comparator name: leveldb.BytewiseComparator merge operator name: nullptr property collectors names: [] ``` for new SST files with these properties ``` comparator name: leveldb.BytewiseComparator merge operator name: UInt64AddOperator property collectors names: [DummyPropertiesCollector1,DummyPropertiesCollector2] ``` Test Plan: unittests Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: ""Revert ""Fixed the bug when both whole_key_filtering and prefix_extractor are set."""" Summary: This reverts commit 73c31377bbcd300061245138dbaf782fedada9ba, which mistakenly reverts 73c31377bbcd300061245138dbaf782fedada9ba that fixes a bug when both whole_key_filtering and prefix_extractor are set Test Plan: revert the patch Reviewers: anthony, IslamAbdelRahman, rven, kradhakrishnan, sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision:"
1266,1266,7.0,0.6273000240325928,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","TransactionLogIterator sequence gap fix Summary: DBTestXactLogIterator.TransactionLogIterator was failing due the sequence gaps. This was caused by an off-by-one error when calculating the new sequence number after recovering from logs. Test Plan: db_log_iter_test Reviewers: andrewkr Subscribers: andrewkr, hermanlee4, dhruba, IslamAbdelRahman Differential Revision: Recovery path sequence miscount fix Summary: Consider the following WAL with 4 batch entries prefixed with their sequence at time of memtable insert. [1: BEGIN_PREPARE, PUT, PUT, PUT, PUT, END_PREPARE(a)] [1: BEGIN_PREPARE, PUT, PUT, PUT, PUT, END_PREPARE(b)] [4: COMMIT(a)] [7: COMMIT(b)] The first two batches do not consume any sequence numbers so are both prefixed with seq=1. For 2pc commit, memtable insertion takes place before COMMIT batch is written to WAL. We can see that sequence number consumption takes place between WAL entries giving us the seemingly sparse sequence prefix for WAL entries. This is a valid WAL. Because with 2PC markers one WriteBatch points to another batch containing its inserts a writebatch can consume more or less sequence numbers than the number of sequence consuming entries that it contains. We can see that, given the entries in the WAL, 6 sequence ids were consumed. Yet on recovery the maximum sequence consumed would be 7 + 3 (the number of sequence numbers consumed by COMMIT(b)) So, now upon recovery we must track the actual consumption of sequence numbers. In the provided scenario there will be no sequence gaps, but it is possible to produce a sequence gap. This should not be a problem though. correct? Test Plan: provided test. Reviewers: sdong Subscribers: andrewkr, leveldb, dhruba, hermanlee4 Differential Revision: Memtable Log Referencing and Prepared Batch Recovery Summary: This diff is built on top of WriteBatch modification: and adds the required functionality to rocksdb core necessary for rocksdb to support 2PC. modfication of DBImpl::WriteImpl() added two arguments *uint64_t log_used nullptr, uint64_t log_ref 0; *log_used is an output argument which will return the log number which the incoming batch was inserted into, 0 if no WAL insert took place. log_ref is a supplied log_number which all memtables inserted into will reference after the batch insert takes place. This number will reside in FindMinPrepLogReferencedByMemTable() until all Memtables insertinto have flushed. Recovery/writepath is now aware of prepared batches and commit and rollback markers. Test Plan: There is currently no test on this diff. All testing of this functionality takes place in the Transaction layer/diff but I will add some testing. Reviewers: IslamAbdelRahman, sdong Subscribers: leveldb, santoshb, andrewkr, vasilep, dhruba, hermanlee4 Differential Revision: enable jemalloc status printing if USE_CLANG=1 Summary: Warning is printed out with USE_CLANG=1 when including jemalloc.h. Disable it in that case. Test Plan: Run db_bench with USE_CLANG=1 and not. Make sure they can all build and jemalloc status is printed out in the case where USE_CLANG is not set. Reviewers: andrewkr, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision: memory allocation counters Summary: Introduced option to dump malloc statistics using new option flag. Added new command line option to db_bench tool to enable this funtionality. Also extended build to support environments with/without jemalloc. Test Plan: 1) Build rocksdb using `make` command. Launch the following command `./db_bench end verified that jemalloc dump is present in LOG file. 2) Build rocksdb using `DISABLE_JEMALLOC=1 make db_bench and ran the same db_bench tool and found the following message in LOG file: ""Please compile with jemalloc to enable malloc dump"". 3) Also built rocksdb using `make` command on MacOS to verify behavior in non-FB environment. Also to debug build configuration change temporary changed AM_DEFAULT_VERBOSITY 1 in Makefile to see compiler and build tools output. For case 1) was present in compiler command line. For both 2) and 3) this flag was not present. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: options.compaction_measure_io_stats to options.report_bg_io_stats and include flush too. Summary: It is useful to print out IO stats in flush jobs too. Extend options.compaction_measure_io_stats to flush jobs and raname it. Test Plan: Try db_bench and see the stats are printed out. Reviewers: yhchiang Reviewed By: yhchiang Subscribers: kradhakrishnan, yiwu, IslamAbdelRahman, leveldb, andrewkr, dhruba Differential Revision: sure that if use_mmap_reads is on use_os_buffer is also on Summary: The code assumes that if use_mmap_reads is on then use_os_buffer is also on. This make sense as by using memory mapped files for reading you are expecting the OS to cache what it needs. Add code to make sure the user does not turn off use_os_buffer when they turn on use_mmap_reads Test Plan: New test: DBTest.MMapAndBufferOptions Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: log numbers for column family to wal_filter, and provide log number in the record callback/Add Iterator Property rocksdb.iterator.version_number Summary: We want to provide a way to detect whether an iterator is stale and needs to be recreated. Add a iterator property to return version number. Test Plan: Add two unit tests for it. Reviewers: IslamAbdelRahman, yhchiang, anthony, kradhakrishnan, andrewkr Reviewed By: andrewkr Subscribers: leveldb, dhruba Differential Revision: assert failure when DBImpl::SyncWAL() conflicts with log rolling Summary: DBImpl::SyncWAL() releases db mutex before calling DBImpl::MarkLogsSynced(), while inside DBImpl::MarkLogsSynced() we assert there is none or one outstanding log file. However, a memtable switch can happen in between and causing two or outstanding logs there, failing the assert. The diff adds a unit test that repros the issue and fix the assert so that the unit test passes. Test Plan: Run the new tests. Reviewers: anthony, kolmike, yhchiang, IslamAbdelRahman, kradhakrishnan, andrewkr Reviewed By: andrewkr Subscribers: leveldb, dhruba Differential Revision: SstFileManager::SetMaxAllowedSpaceUsage() to cap disk space usage Summary: Introude SstFileManager::SetMaxAllowedSpaceUsage() that can be used to limit the maximum space usage allowed for RocksDB. When this limit is exceeded WriteImpl() will fail and return Status::Aborted() Test Plan: unit testing Reviewers: yhchiang, anthony, andrewkr, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision:"
1267,1267,1.0,0.9934999942779541,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Shared dictionary compression using reference block Summary: This adds a new metablock containing a shared dictionary that is used to compress all data blocks in the SST file. The size of the shared dictionary is configurable in CompressionOptions and defaults to 0. Its currently only used for zlib/lz4/lz4hc, but the block will be stored in the SST regardless of the compression type if the user chooses a nonzero dictionary size. During compaction, computes the dictionary by randomly sampling the first output file in each subcompaction. It pre-computes the intervals to sample by assuming the output file will have the maximum allowable length. In case the file is smaller, some of the pre-computed sampling intervals can be beyond end-of-file, in which case we skip over those samples and the dictionary will be a bit smaller. After the dictionary is generated using the first file in a subcompaction, it is loaded into the compression library before writing each block in each subsequent file of that subcompaction. On the read path, gets the dictionary from the metablock, if it exists. Then, loads that dictionary into the compression library before reading each block. Test Plan: new unit test Reviewers: yhchiang, IslamAbdelRahman, cyan, sdong Reviewed By: sdong Subscribers: andrewkr, yoshinorim, kradhakrishnan, dhruba, leveldb Differential Revision: to allow C libraries to create mem env (#1066)/Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
1268,1268,7.0,0.685699999332428,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Add bottommost_compression option Summary: Add a new option that can be used to set a specific compression algorithm for bottommost level. This option will only affect levels larger than base level. I have also updated CompactionJobInfo to include the compression algorithm used in compaction Test Plan: added new unittest existing unittests Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: lightmark, andrewkr, dhruba, yoshinorim Differential Revision: always needs to be removed from level0_compactions_in_progress_ for universal compaction Summary: We always put compaction to level0_compactions_in_progress_ for universal compaction, so we should also remove it. The bug causes assert failure when running manual compaction. Test Plan: TEST_TMPDIR=/dev/shm/ ./db_bench always fails on my host. After the fix, it doesnt fail any more. Reviewers: IslamAbdelRahman, andrewkr, kradhakrishnan, yhchiang Reviewed By: yhchiang Subscribers: leveldb, dhruba Differential Revision:"
1269,1269,7.0,0.9735999703407288,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Change Property name from ""rocksdb.current_version_number"" to ""rocksdb.current-super-version-number"" Summary: I realized I again is wrong about the naming convention. Let me change it to the correct one. Test Plan: Run unit tests. Reviewers: IslamAbdelRahman, kradhakrishnan, yhchiang, andrewkr Reviewed By: andrewkr Subscribers: leveldb, dhruba Differential Revision: DB Property ""rocksdb.current_version_number"" Summary: Add a DB Property ""rocksdb.current_version_number"" for users to monitor version changes and stale iterators. Test Plan: Add a unit test. Reviewers: andrewkr, yhchiang, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision:"
1270,1270,18.0,0.949999988079071,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Change some RocksDB default options Summary: Change some RocksDB default options to make it more friendly to server workloads. Test Plan: Run all existing tests Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: sumeet, muthu, benj, MarkCallaghan, igor, leveldb, andrewkr, dhruba Differential Revision:"
1271,1271,13.0,0.9472000002861023,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Add Iterator Property rocksdb.iterator.version_number Summary: We want to provide a way to detect whether an iterator is stale and needs to be recreated. Add a iterator property to return version number. Test Plan: Add two unit tests for it. Reviewers: IslamAbdelRahman, yhchiang, anthony, kradhakrishnan, andrewkr Reviewed By: andrewkr Subscribers: leveldb, dhruba Differential Revision:"
1272,1272,10.0,0.9927999973297119,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Estimate pending compaction bytes more accurately Summary: Currently we estimate bytes needed for compaction by assuming fanout value to be level multiplier. It overestimates when size of a level exceeds the target by large. We estimate by the ratio of actual sizes in levels instead. Test Plan: Fix existing test cases and add a new one. Reviewers: IslamAbdelRahman, igor, yhchiang Reviewed By: yhchiang Subscribers: MarkCallaghan, leveldb, andrewkr, dhruba Differential Revision: XPRESS compresssion on Windows. (#1081) Comparable with Snappy on comp ratio. Implemented using Windows API, does not require external package. Avaiable since Windows 8 and server 2012. Use with CMake to enable./Split db_test.cc Summary: Split db_test.cc into several files. Moving several helper functions into DBTestBase. Test Plan: make check Reviewers: sdong, yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: dhruba, andrewkr, kradhakrishnan, yhchiang, leveldb, sdong Differential Revision: log numbers for column family to wal_filter, and provide log number in the record callback/Reset block cache in failing unit test. Test Plan: make check OPT=-g, on both /tmp and /dev/shm Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
1273,1273,10.0,0.954800009727478,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Make EventListenerTest.CompactionReasonLevel more deterministic Summary: In this test some times automatic compactions do everything and Manual compaction become a no-op. Update the test to make sure manual compaction is not a no-op Test Plan: run the test Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
1274,1274,0.0,0.5281000137329102,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","No need to limit to 20 files in UpdateAccumulatedStats() if options.max_open_files=-1 Summary: There is a hardcoded constraint in our statistics collection that prevents reading properties from more than 20 SST files. This means our statistics will be very inaccurate for databases with > 20 files since additional files are just ignored. The purpose of constraining the number of files used is to bound the I/O performed during statistics collection, since these statistics need to be recomputed every time the database reopened. However, this constraint doesnt take into account the case where option ""max_open_files"" is In that case, all the file metadata has already been read, so MaybeInitializeFileMetaData() wont incur any I/O cost. so this diff gets rid of the 20-file constraint in case max_open_files Test Plan: write into unit test db/db_properties_test.cc ""ValidateSampleNumber"". We generate 20 files with 2 rows and 10 files with 1 row. If max_open_files the `rocksdb.estimate-num-keys` should be (10*1 + 10*2)/20 * 30 45. Otherwise, it should be the ground truth, 50. {F1089153} Reviewers: andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, leveldb Differential Revision: pin_l0_filter_and_index_blocks_in_cache feature and related fixes. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. Test Plan: export TEST_TMPDIR=/dev/shm/ && DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check is OK. I didnt run the Java tests, I dont have Java set up on my devserver. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: data race issue when sub-compaction is used in CompactionJob Summary: When subcompaction is used, all subcompactions share the same Compaction pointer in CompactionJob while each subcompaction all keeps their mutable stats in SubcompactionState. However, therere still some mutable part that is currently store in the shared Compaction pointer. This patch makes two changes: 1. Make the shared Compaction pointer const so that it can never be modified during the compaction. 2. Move necessary states from Compaction to SubcompactionState. 3. Make functions of Compaction const if the function does not modify its internal state. Test Plan: rocksdb and MyRocks test Reviewers: sdong, kradhakrishnan, andrewkr, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba, yoshinorim, gunnarku, leveldb Differential Revision: ""Adding pin_l0_filter_and_index_blocks_in_cache feature."" This reverts commit 522de4f59e6314698286cf29d8a325a284d81778. It has bug of index block cleaning up./Adding pin_l0_filter_and_index_blocks_in_cache feature. Summary: When a block based table file is opened, if prefetch_index_and_filter is true, it will prefetch the index and filter blocks, putting them into the block cache. What this feature adds: when a L0 block based table file is opened, if pin_l0_filter_and_index_blocks_in_cache is true in the options (and prefetch_index_and_filter is true), then the filter and index blocks arent released back to the block cache at the end of BlockBasedTableReader::Open(). Instead the table reader takes ownership of them, hence pinning them, ie. the LRU cache will never push them out. Meanwhile in the table reader, further accesses will not hit the block cache, thus avoiding lock contention. When the table reader is destroyed, it releases the pinned blocks (if there were any). This has to happen before the cache is destroyed, so I had to introduce a TableReader::Close(), to guarantee the order of destruction. Test Plan: Added two unit tests for this. Existing unit tests run fine (default is pin_l0_filter_and_index_blocks_in_cache=false). DISABLE_JEMALLOC=1 OPT=-g make all valgrind_check Mac: OK. Linux: with D55287 patched in its OK. Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision: stale manifests outside of full purge Summary: Keep track of obsolete manifests in VersionSet Updated FindObsoleteFiles() to put obsolete manifests in the JobContext for later use by PurgeObsoleteFiles() Added test case that verifies a stale manifest is deleted by a non-full purge Test Plan: $ ./backupable_db_test Reviewers: IslamAbdelRahman, yoshinorim, sdong Reviewed By: sdong Subscribers: andrewkr, leveldb, dhruba Differential Revision:"
1275,1275,2.0,0.8100000023841858,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",Use async file handle for better parallelism (#1049)/
1276,1276,7.0,0.9925000071525574,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","ldb restore subcommand Summary: Added a new subcommand, ldb restore, that restores from backup Made backup_env_uri optional (also for ldb backup) because it can use db_env when backup_env isnt provided Test Plan: verify backup and restore commands work: $ ./ldb backup 1 $ ./ldb restore 1 Reviewers: sdong, wanning Reviewed By: wanning Subscribers: andrewkr, dhruba, leveldb Differential Revision: backup support Summary: add backup support for ldb tool, and use it to run load test for backup on two HDFS envs Test Plan: first generate some db, then compile against load test in fbcode, run load_test path> backup of backup env> directory> of thread> Reviewers: andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, leveldb Differential Revision: to move from template to function wrapper Summary: Build failure with some compiler setting with tools/reduce_levels_test.cc:97: undefined reference to `rocksdb::LDBCommand* rocksdb::LDBCommand::InitFromCmdLineArgs<rocksdb::LDBCommand* (*)(std::string const&, std::vector<std::string, std::allocator<std::string> > const&, std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, std::vector<std::string, std::allocator<std::string> > const&)>(std::vector<std::string, std::allocator<std::string> > const&, rocksdb::Options const&, rocksdb::LDBOptions const&, std::vector<rocksdb::ColumnFamilyDescriptor, std::allocator<rocksdb::ColumnFamilyDescriptor> > const*, rocksdb::LDBCommand* (*)(std::string const&, std::vector<std::string, std::allocator<std::string> > const&, std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, std::vector<std::string, std::allocator<std::string> > const&)) Fix it by changing to function pointer instead Test Plan: Run all existing tests Reviewers: andrewkr, kradhakrishnan, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: adsharma, lightmark, yiwu, leveldb, andrewkr, dhruba Differential Revision: use of using namespace std. Also remove a number of ADL references to std functions. Summary: Reduce use of argument-dependent name lookup in RocksDB. Test Plan: make check passed. Reviewers: andrewkr Reviewed By: andrewkr Subscribers: leveldb, andrewkr, dhruba Differential Revision:"
1277,1277,19.0,0.9904000163078308,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","db_stress shouldnt assert file size 0 if file creation fails Summary: OnTableFileCreated() now is also called when the file creaion fails. In that case, we shouldnt assert the file size is not 0. Test Plan: Run crash test Reviewers: yiwu, andrewkr, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: IslamAbdelRahman, leveldb, andrewkr, dhruba Differential Revision: FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision:"
1278,1278,1.0,0.9839000105857849,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Added ""number of merge operands"" to statistics in ssts. Summary: A couple of notes from the diff: The namespace block I added at the top of table_properties_collector.cc was in reaction to an issue i was having with PutVarint64 and reusing the ""val"" string. Im not sure this is the cleanest way of doing this, but abstracting this out at least results in the correct behavior. I chose ""rocksdb.merge.operands"" as the property name. I am open to suggestions for better names. The change to sst_dump_tool.cc seems a bit inelegant to me. Is there a better way to do the if-else block? Test Plan: I added a test case in table_properties_collector_test.cc. It adds two merge operands and checks to make sure that both of them are reflected by GetMergeOperands. It also checks to make sure the wasPropertyPresent bool is properly set in the method. Running both of these tests should pass: ./table_properties_collector_test ./sst_dump_test Reviewers: IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba Differential Revision:"
1279,1279,4.0,0.9817000031471252,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Direct IO capability for RocksDB Summary: This patch adds direct IO capability to RocksDB Env. The direct IO capability is required for persistent cache since NVM is best accessed as 4K direct IO. SSDs can leverage direct IO for reading. Direct IO requires the offset and size be sector size aligned, and memory to be kernel page aligned. Since neither RocksDB/Persistent read cache data layout is aligned to sector size, the code can accommodate reading unaligned IO size (or unaligned memory) at the cost of an alloc/copy. The write code path expects the size and memory to be aligned. Test Plan: Run RocksDB unit tests Reviewers: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1280,1280,7.0,0.5741000175476074,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Fix Java Break Related to memtable bloom bits to size ratio change Summary: Need to change several more places for the change to fix Java tests Test Plan: make jtest under java, run ""make db_bench"" Reviewers: yhchiang, andrewkr, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision:"
1281,1281,3.0,0.975600004196167,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",Added SetOptions support to RocksJava (#1243) * [refactor] Split Java ColumnFamilyOptions into mutable and immutable and implement any missing immutable options * [feature] Implement RocksDB#setOptions/fixes 1220: rocksjni build fails on Windows due to variable-size array declaration (#1223) * fixes 1220: rocksjni build fails on Windows due to variable-size array declaration using new keyword to create variable-size arrays in order to satisfy most of the compilers * fixes 1220: rocksjni build fails on Windows due to variable-size array declaration using unique_ptr keyword to create variable-size arrays in order to satisfy most of the compilers/
1282,1282,3.0,0.9603999853134155,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",fixes 1220: rocksjni build fails on Windows due to variable-size array declaration (#1223) * fixes 1220: rocksjni build fails on Windows due to variable-size array declaration using new keyword to create variable-size arrays in order to satisfy most of the compilers * fixes 1220: rocksjni build fails on Windows due to variable-size array declaration using unique_ptr keyword to create variable-size arrays in order to satisfy most of the compilers/
1283,1283,3.0,0.6743000149726868,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Added further Java API options for controlling concurrent writes/Exposed further Java API options for controlling compaction/Added SetOptions support to RocksJava (#1243) * [refactor] Split Java ColumnFamilyOptions into mutable and immutable and implement any missing immutable options * [feature] Implement RocksDB#setOptions/[Fix java build] Stop using non standard std::make_unique Summary: std::make_unique is not standard and not always available, remove it Test Plan: Run ""make clean jclean rocksdbjava jtest on my mac Reviewers: yhchiang, yiwu, sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: 1220: rocksjni build fails on Windows due to variable-size array declaration (#1223) * fixes 1220: rocksjni build fails on Windows due to variable-size array declaration using new keyword to create variable-size arrays in order to satisfy most of the compilers * fixes 1220: rocksjni build fails on Windows due to variable-size array declaration using unique_ptr keyword to create variable-size arrays in order to satisfy most of the compilers/"
1284,1284,7.0,0.944100022315979,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","memtable_prefix_bloom_bits memtable_prefix_bloom_bits_ratio and deprecate memtable_prefix_bloom_probes Summary: memtable_prefix_bloom_probes is not a critical option. Remove it to reduce number of options. Its easier for users to make mistakes with memtable_prefix_bloom_bits, turn it to memtable_prefix_bloom_bits_ratio Test Plan: Run all existing tests Reviewers: yhchiang, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: gunnarku, yoshinorim, MarkCallaghan, leveldb, andrewkr, dhruba Differential Revision:"
1285,1285,19.0,0.9905999898910522,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Only cache level 0 indexes and filter when opening table reader Summary: In T8216281 we decided to disable prefetching the index and filter during opening table handlers during startup (max_open_files Test Plan: Rely on `IndexAndFilterBlocksOfNewTableAddedToCache` to guarantee L0 indexes and filters are still cached and change `PinL0IndexAndFilterBlocksTest` to make sure other levels are not cached (maybe add one more test to test we dont cache other levels?) Reviewers: sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision:"
1286,1286,2.0,0.9660999774932861,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Backup Options Summary: Backup options file to private directory Test Plan: backupable_db_test.cc, BackupOptions Modify DB options by calling OpenDB for 3 times. Check the latest options file is in the right place. Also check no redundent files are backuped. Reviewers: andrewkr Reviewed By: andrewkr Subscribers: leveldb, dhruba, andrewkr Differential Revision:"
1287,1287,7.0,0.84170001745224,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","support stackableDB as the baseDB of transactionDB Summary: make transactionDB working with StackableDB Test Plan: make all check Reviewers: andrewkr, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1288,1288,19.0,0.9869999885559082,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Introduce FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision:"
1289,1289,5.0,0.9829999804496765,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","New Statistics to track Compression/Decompression (#1197) * Added new statistics and refactored to allow ioptions to be passed around as required to access environment and statistics pointers (and, as a convenient side effect, info_log pointer). * Prevent incrementing compression counter when compression is turned off in options. * Prevent incrementing compression counter when compression is turned off in options. * Added two more supported compression types to test code in db_test.cc * Prevent incrementing compression counter when compression is turned off in options. * Added new StatsLevel that excludes compression timing. * Fixed casting error in coding.h * Fixed CompressionStatsTest for new StatsLevel. * Removed unused variable that was breaking the Linux build/Add a check mode to verify compressed block can be decompressed back Summary: Try to decompress compressed blocks when a special flag is set. assert and crash in debug builds if we cant decompress the just-compressed input. Test Plan: Run unit-tests. Reviewers: dhruba, andrewkr, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba Differential Revision:"
1290,1290,5.0,0.9682999849319458,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","New Statistics to track Compression/Decompression (#1197) * Added new statistics and refactored to allow ioptions to be passed around as required to access environment and statistics pointers (and, as a convenient side effect, info_log pointer). * Prevent incrementing compression counter when compression is turned off in options. * Prevent incrementing compression counter when compression is turned off in options. * Added two more supported compression types to test code in db_test.cc * Prevent incrementing compression counter when compression is turned off in options. * Added new StatsLevel that excludes compression timing. * Fixed casting error in coding.h * Fixed CompressionStatsTest for new StatsLevel. * Removed unused variable that was breaking the Linux build/"
1291,1291,12.0,0.9682999849319458,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Only cache level 0 indexes and filter when opening table reader Summary: In T8216281 we decided to disable prefetching the index and filter during opening table handlers during startup (max_open_files Test Plan: Rely on `IndexAndFilterBlocksOfNewTableAddedToCache` to guarantee L0 indexes and filters are still cached and change `PinL0IndexAndFilterBlocksTest` to make sure other levels are not cached (maybe add one more test to test we dont cache other levels?) Reviewers: sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision:"
1292,1292,13.0,0.5975000262260437,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Only cache level 0 indexes and filter when opening table reader Summary: In T8216281 we decided to disable prefetching the index and filter during opening table handlers during startup (max_open_files Test Plan: Rely on `IndexAndFilterBlocksOfNewTableAddedToCache` to guarantee L0 indexes and filters are still cached and change `PinL0IndexAndFilterBlocksTest` to make sure other levels are not cached (maybe add one more test to test we dont cache other levels?) Reviewers: sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: Statistics to track Compression/Decompression (#1197) * Added new statistics and refactored to allow ioptions to be passed around as required to access environment and statistics pointers (and, as a convenient side effect, info_log pointer). * Prevent incrementing compression counter when compression is turned off in options. * Prevent incrementing compression counter when compression is turned off in options. * Added two more supported compression types to test code in db_test.cc * Prevent incrementing compression counter when compression is turned off in options. * Added new StatsLevel that excludes compression timing. * Fixed casting error in coding.h * Fixed CompressionStatsTest for new StatsLevel. * Removed unused variable that was breaking the Linux build/Fix clang analyzer errors Summary: Fixing erros reported by clang static analyzer. * Removing some unused variables. * Adding assertions to fix false positives reported by clang analyzer. * Adding `__clang_analyzer__` macro to suppress false positive warnings. Test Plan: USE_CLANG=1 OPT=-g make analyze Reviewers: andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: statistics field to show total size of index and filter blocks in block cache Summary: With `table_options.cache_index_and_filter_blocks true`, index and filter blocks are stored in block cache. Then people are curious how much of the block cache total size is used by indexes and bloom filters. It will be nice we have a way to report that. It can help people tune performance and plan for optimized hardware setting. We add several enum values for db Statistics. BLOCK_CACHE_INDEX/FILTER_BYTES_INSERT BLOCK_CACHE_INDEX/FILTER_BYTES_ERASE current INDEX/FILTER total block size in bytes. Test Plan: write a test case called `DBBlockCacheTest.IndexAndFilterBlocksStats`. The result is: ``` ~/local/rocksdb] make db_block_cache_test && ./db_block_cache_test Makefile:101: Warning: Compiling in debug mode. Dont use the resulting binary in production GEN util/build_version.cc make: `db_block_cache_test is up to date. Note: Google Test filter DBBlockCacheTest.IndexAndFilterBlocksStats [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from DBBlockCacheTest [ RUN ] DBBlockCacheTest.IndexAndFilterBlocksStats [ OK ] DBBlockCacheTest.IndexAndFilterBlocksStats (689 ms) [----------] 1 test from DBBlockCacheTest (689 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (689 ms total) [ PASSED ] 1 test. ``` Reviewers: IslamAbdelRahman, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: BlockBasedTableOptions.hash_index_allow_collision=false. Summary: Deprecate this one option and delete code and tests that are now superfluous. Test Plan: all tests pass Reviewers: igor, yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: msalib, leveldb, andrewkr, dhruba Differential Revision: PersistentCache abstraction Summary: Added a new abstraction to cache page to RocksDB designed for the read cache use. RocksDB current block cache is more of an object cache. For the persistent read cache project, what we need is a page cache equivalent. This changes adds a cache abstraction to RocksDB to cache pages called PersistentCache. PersistentCache can cache uncompressed pages or raw pages (content as in filesystem). The user can choose to operate PersistentCache either in COMPRESSED or UNCOMPRESSED mode. Blame Rev: Test Plan: Run unit tests Reviewers: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1293,1293,5.0,0.9682999849319458,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","New Statistics to track Compression/Decompression (#1197) * Added new statistics and refactored to allow ioptions to be passed around as required to access environment and statistics pointers (and, as a convenient side effect, info_log pointer). * Prevent incrementing compression counter when compression is turned off in options. * Prevent incrementing compression counter when compression is turned off in options. * Added two more supported compression types to test code in db_test.cc * Prevent incrementing compression counter when compression is turned off in options. * Added new StatsLevel that excludes compression timing. * Fixed casting error in coding.h * Fixed CompressionStatsTest for new StatsLevel. * Removed unused variable that was breaking the Linux build/"
1294,1294,5.0,0.9682999849319458,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","New Statistics to track Compression/Decompression (#1197) * Added new statistics and refactored to allow ioptions to be passed around as required to access environment and statistics pointers (and, as a convenient side effect, info_log pointer). * Prevent incrementing compression counter when compression is turned off in options. * Prevent incrementing compression counter when compression is turned off in options. * Added two more supported compression types to test code in db_test.cc * Prevent incrementing compression counter when compression is turned off in options. * Added new StatsLevel that excludes compression timing. * Fixed casting error in coding.h * Fixed CompressionStatsTest for new StatsLevel. * Removed unused variable that was breaking the Linux build/"
1295,1295,13.0,0.54830002784729,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","New Statistics to track Compression/Decompression (#1197) * Added new statistics and refactored to allow ioptions to be passed around as required to access environment and statistics pointers (and, as a convenient side effect, info_log pointer). * Prevent incrementing compression counter when compression is turned off in options. * Prevent incrementing compression counter when compression is turned off in options. * Added two more supported compression types to test code in db_test.cc * Prevent incrementing compression counter when compression is turned off in options. * Added new StatsLevel that excludes compression timing. * Fixed casting error in coding.h * Fixed CompressionStatsTest for new StatsLevel. * Removed unused variable that was breaking the Linux build/Add a check mode to verify compressed block can be decompressed back Summary: Try to decompress compressed blocks when a special flag is set. assert and crash in debug builds if we cant decompress the just-compressed input. Test Plan: Run unit-tests. Reviewers: dhruba, andrewkr, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba Differential Revision: PersistentCache abstraction Summary: Added a new abstraction to cache page to RocksDB designed for the read cache use. RocksDB current block cache is more of an object cache. For the persistent read cache project, what we need is a page cache equivalent. This changes adds a cache abstraction to RocksDB to cache pages called PersistentCache. PersistentCache can cache uncompressed pages or raw pages (content as in filesystem). The user can choose to operate PersistentCache either in COMPRESSED or UNCOMPRESSED mode. Blame Rev: Test Plan: Run unit tests Reviewers: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1296,1296,19.0,0.9869999885559082,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Introduce FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision:"
1297,1297,5.0,0.9682999849319458,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","New Statistics to track Compression/Decompression (#1197) * Added new statistics and refactored to allow ioptions to be passed around as required to access environment and statistics pointers (and, as a convenient side effect, info_log pointer). * Prevent incrementing compression counter when compression is turned off in options. * Prevent incrementing compression counter when compression is turned off in options. * Added two more supported compression types to test code in db_test.cc * Prevent incrementing compression counter when compression is turned off in options. * Added new StatsLevel that excludes compression timing. * Fixed casting error in coding.h * Fixed CompressionStatsTest for new StatsLevel. * Removed unused variable that was breaking the Linux build/"
1298,1298,10.0,0.5284000039100647,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Ignore stale logs while restarting DBs Summary: Stale log files can be deleted out of order. This can happen for various reasons. One of the reason is that no data is ever inserted to a column family and we have an optimization to update its log number, but not all the old log files are cleaned up (the case shown in the unit tests added). It can also happen when we simply delete multiple log files out of order. This causes data corruption because we simply increase seqID after processing the next row and we may end up with writing data with smaller seqID than what is already flushed to memtables. In DB recovery, for the oldest files we are replaying, if there it contains no data for any column family, we ignore the sequence IDs in the file. Test Plan: Add two unit tests that fail without the fix. Reviewers: IslamAbdelRahman, igor, yiwu Reviewed By: yiwu Subscribers: hermanlee4, yoshinorim, leveldb, andrewkr, dhruba Differential Revision: to make sure log file synced before flushing memtable of one column family Summary: Multiput atomiciy is broken across multiple column families if we dont sync WAL before flushing one column family. The WAL file may contain a write batch containing writes to a key to the CF to be flushed and a key to other CF. If we dont sync WAL before flushing, if machine crashes after flushing, the write batch will only be partial recovered. Data to other CFs are lost. Test Plan: Add a new unit test which will fail without the diff. Reviewers: yhchiang, IslamAbdelRahman, igor, yiwu Reviewed By: yiwu Subscribers: yiwu, leveldb, andrewkr, dhruba Differential Revision: More Logging to track total_log_size Summary: We saw instances where total_log_size is off the real value, but Im not able to reproduce it. Add more logging to help debugging when it happens again. Test Plan: Run the unit test and see the logging. Reviewers: andrewkr, yhchiang, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision: options.write_buffer_manager: control total memtable size across DB instances Summary: Add option write_buffer_manager to help users control total memory spent on memtables across multiple DB instances. Test Plan: Add a new unit test. Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: adela, benj, sumeet, muthu, leveldb, andrewkr, dhruba Differential Revision: a read option to enable background purge when cleaning up iterators Summary: Add a read option `background_purge_on_iterator_cleanup` to avoid deleting files in foreground when destroying iterators. Instead, a job is scheduled in high priority queue and would be executed in a separate background thread. Test Plan: Add a variant of PurgeObsoleteFileTest. Turn on background purge option in the new test, and use sleeping task to ensure files are deleted in background. Reviewers: IslamAbdelRahman, sdong Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba Differential Revision: DB::AddFile() to ingest the file to the lowest possible level Summary: DB::AddFile() right now always add the ingested file to L0 update the logic to add the file to the lowest possible level Test Plan: unit tests Reviewers: jkedgar, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, yoshinorim Differential Revision: filter_deletes Summary: filter_deltes is not a frequently used feature. Remove it. Test Plan: Run all test suites. Reviewers: igor, yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision: option to not flush memtable on open() Summary: Add option to not flush memtable on open() In case the option is enabled, dont delete existing log files by not updating log numbers to MANIFEST. Will still flush if we need to (e.g. memtable full in the middle). In that case we also flush final memtable. If wal_recovery_mode kPointInTimeRecovery, do not halt immediately after encounter corruption. Instead, check if seq id of next log file is last_log_sequence + 1. In that case we continue recovery. Test Plan: See unit test. Reviewers: dhruba, horuff, sdong Reviewed By: sdong Subscribers: benj, yhchiang, andrewkr, dhruba, leveldb Differential Revision: Options Summary: Backup options file to private directory Test Plan: backupable_db_test.cc, BackupOptions Modify DB options by calling OpenDB for 3 times. Check the latest options file is in the right place. Also check no redundent files are backuped. Reviewers: andrewkr Reviewed By: andrewkr Subscribers: leveldb, dhruba, andrewkr Differential Revision: race condition in SwitchMemtable Summary: MemTableList::current_ could be written by background flush thread and simultaneously read in the user thread (NumNotFlushed() is used in SwitchMemtable()). Use the lock to prevent this case. Found the error from tsan. Related: D58833 Test Plan: $ OPT=-g COMPILE_WITH_TSAN=1 make db_test $ TEST_TMPDIR=/dev/shm/rocksdb ./db_test Reviewers: lightmark, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: a callback for when memtable is moved to immutable (#1137) * Create a callback for memtable becoming immutable Create a callback for memtable becoming immutable Create a callback for memtable becoming immutable moved notification outside the lock Move sealed notification to unlocked portion of SwitchMemtable * fix lite build/Small tweaks to logging to track the number of immutable memtables Summary: We see some write stalls because of number of unflushed memtables. With existing logging I couldnt figure out whats happening exactly. See internal task t11446054 for details if interested. This diff adds: logging of memtable creation at info level; I wanted it on multiple occasions for different reasons; also include number of immutable memtables, logging of number of remaining immutable memtables after a flush. Test Plan: ran tests Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
1299,1299,11.0,0.8133999705314636,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Only cache level 0 indexes and filter when opening table reader Summary: In T8216281 we decided to disable prefetching the index and filter during opening table handlers during startup (max_open_files Test Plan: Rely on `IndexAndFilterBlocksOfNewTableAddedToCache` to guarantee L0 indexes and filters are still cached and change `PinL0IndexAndFilterBlocksTest` to make sure other levels are not cached (maybe add one more test to test we dont cache other levels?) Reviewers: sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: a new feature to enforce a sync point only active on a thread Summary: Add markers to sync points. A marked sync point will only be active when it is on the same thread as the marker sync point. Test Plan: Write a unit test to validate. Reviewers: sdong, IslamAbdelRahman, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: mutex unlock issue between scheduled compaction and ReleaseCompactionFiles() Summary: NotifyOnCompactionCompleted can unlock the mutex. That mean that we can schedule a background compaction that will start before we ReleaseCompactionFiles(). Test Plan: added unittest existing unittest Reviewers: yhchiang, sdong Reviewed By: sdong Subscribers: yoshinorim, andrewkr, dhruba Differential Revision: PersistentCache abstraction Summary: Added a new abstraction to cache page to RocksDB designed for the read cache use. RocksDB current block cache is more of an object cache. For the persistent read cache project, what we need is a page cache equivalent. This changes adds a cache abstraction to RocksDB to cache pages called PersistentCache. PersistentCache can cache uncompressed pages or raw pages (content as in filesystem). The user can choose to operate PersistentCache either in COMPRESSED or UNCOMPRESSED mode. Blame Rev: Test Plan: Run unit tests Reviewers: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1300,1300,19.0,0.9869999885559082,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Introduce FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision:"
1301,1301,19.0,0.9897000193595886,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","UniversalCompaction should ignore sorted runs being compacted (when compacting for file num) Summary: If we have total number of sorted runs greater than level0_file_num_compaction_trigger, Universal compaction will always issue a compaction even if the number of sorted runs that are not being compacted is less than level0_file_num_compaction_trigger. This diff changes this behaviour to relay on the `number of sorted runs not being compacted` instead of `total number of sorted runs` Test Plan: New unit test Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: picker to expand output level files for keys cross files boundary too. Summary: We may wrongly drop delete operation if we pick a file with the entry to be delete, the put entry of the same user key is in the next file in the level, and the next file is not picked. We expand compaction inputs for output level too. Test Plan: Add unit tests that reproduct the bug of dropping delete entry. Change compaction_picker_test to assert the new behavior. Reviewers: IslamAbdelRahman, igor Reviewed By: igor Subscribers: leveldb, andrewkr, dhruba Differential Revision: make more options dynamic Summary: make more ColumnFamilyOptions dynamic: compression soft_pending_compaction_bytes_limit hard_pending_compaction_bytes_limit min_partial_merge_operands report_bg_io_stats paranoid_file_checks Test Plan: Add sanity check in `db_test.cc` for all above options except for soft_pending_compaction_bytes_limit and hard_pending_compaction_bytes_limit. All passed. Reviewers: andrewkr, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1302,1302,14.0,0.9635000228881836,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Add InternalStats and logging for AddFile() Summary: We dont report the bytes that we ingested from AddFile which make the write amplification numbers incorrect Update InternalStats and add logging for AddFile() Test Plan: Make sure the code compile and existing tests pass Reviewers: lightmark, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
1303,1303,13.0,0.9945999979972839,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Miscellaneous performance improvements Summary: I was investigating performance issues in the SstFileWriter and found all of the following: The SstFileWriter::Add() function created a local InternalKey every time it was called generating a allocation and free each time. Changed to have an InternalKey member variable that can be reset with the new InternalKey::Set() function. In SstFileWriter::Add() the smallest_key and largest_key values were assigned the result of a ToString() call, but it is simpler to just assign them directly from the users key. The Slice class had no move constructor so each time one was returned from a function a new one had to be allocated, the old data copied to the new, and the old one was freed. I added the move constructor which also required a copy constructor and assignment operator. The BlockBuilder::CurrentSizeEstimate() function calculates the current estimate size, but was being called 2 or 3 times for each key added. I changed the class to maintain a running estimate (equal to the original calculation) so that the function can return an already calculated value. The code in BlockBuilder::Add() that calculated the shared bytes between the last key and the new key duplicated what Slice::difference_offset does, so I replaced it with the standard function. BlockBuilder::Add() had code to copy just the changed portion into the last key value (and asserted that it now matched the new key). It is more efficient just to copy the whole new key over. Moved this same code up into the if (use_delta_encoding_) since the last key value is only needed when delta encoding is on. FlushBlockBySizePolicy::BlockAlmostFull calculated a standard deviation value each time it was called, but this information would only change if block_size of block_size_deviation changed, so I created a member variable to hold the value to avoid the calculation each time. Each PutVarint??() function has a buffer and calls std::string::append(). Two or three calls in a row could share a buffer and a single call to std::string::append(). Some of these will be helpful outside of the SstFileWriter. Im not 100% the addition of the move constructor is appropriate as I wonder why this wasnt done before maybe because of compiler compatibility? I tried it on gcc 4.8 and 4.9. Test Plan: The changes should not affect the results so the existing tests should all still work and no new tests were added. The value of the changes was seen by manually testing the SstFileWriter class through MyRocks and adding timing code to identify problem areas. Reviewers: sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba Differential Revision:"
1304,1304,1.0,0.9839000105857849,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Added ""number of merge operands"" to statistics in ssts. Summary: A couple of notes from the diff: The namespace block I added at the top of table_properties_collector.cc was in reaction to an issue i was having with PutVarint64 and reusing the ""val"" string. Im not sure this is the cleanest way of doing this, but abstracting this out at least results in the correct behavior. I chose ""rocksdb.merge.operands"" as the property name. I am open to suggestions for better names. The change to sst_dump_tool.cc seems a bit inelegant to me. Is there a better way to do the if-else block? Test Plan: I added a test case in table_properties_collector_test.cc. It adds two merge operands and checks to make sure that both of them are reflected by GetMergeOperands. It also checks to make sure the wasPropertyPresent bool is properly set in the method. Running both of these tests should pass: ./table_properties_collector_test ./sst_dump_test Reviewers: IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba Differential Revision:"
1305,1305,13.0,0.9945999979972839,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Miscellaneous performance improvements Summary: I was investigating performance issues in the SstFileWriter and found all of the following: The SstFileWriter::Add() function created a local InternalKey every time it was called generating a allocation and free each time. Changed to have an InternalKey member variable that can be reset with the new InternalKey::Set() function. In SstFileWriter::Add() the smallest_key and largest_key values were assigned the result of a ToString() call, but it is simpler to just assign them directly from the users key. The Slice class had no move constructor so each time one was returned from a function a new one had to be allocated, the old data copied to the new, and the old one was freed. I added the move constructor which also required a copy constructor and assignment operator. The BlockBuilder::CurrentSizeEstimate() function calculates the current estimate size, but was being called 2 or 3 times for each key added. I changed the class to maintain a running estimate (equal to the original calculation) so that the function can return an already calculated value. The code in BlockBuilder::Add() that calculated the shared bytes between the last key and the new key duplicated what Slice::difference_offset does, so I replaced it with the standard function. BlockBuilder::Add() had code to copy just the changed portion into the last key value (and asserted that it now matched the new key). It is more efficient just to copy the whole new key over. Moved this same code up into the if (use_delta_encoding_) since the last key value is only needed when delta encoding is on. FlushBlockBySizePolicy::BlockAlmostFull calculated a standard deviation value each time it was called, but this information would only change if block_size of block_size_deviation changed, so I created a member variable to hold the value to avoid the calculation each time. Each PutVarint??() function has a buffer and calls std::string::append(). Two or three calls in a row could share a buffer and a single call to std::string::append(). Some of these will be helpful outside of the SstFileWriter. Im not 100% the addition of the move constructor is appropriate as I wonder why this wasnt done before maybe because of compiler compatibility? I tried it on gcc 4.8 and 4.9. Test Plan: The changes should not affect the results so the existing tests should all still work and no new tests were added. The value of the changes was seen by manually testing the SstFileWriter class through MyRocks and adding timing code to identify problem areas. Reviewers: sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba Differential Revision:"
1306,1306,19.0,0.9254999756813049,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Introduce FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision: Statistics to track Compression/Decompression (#1197) * Added new statistics and refactored to allow ioptions to be passed around as required to access environment and statistics pointers (and, as a convenient side effect, info_log pointer). * Prevent incrementing compression counter when compression is turned off in options. * Prevent incrementing compression counter when compression is turned off in options. * Added two more supported compression types to test code in db_test.cc * Prevent incrementing compression counter when compression is turned off in options. * Added new StatsLevel that excludes compression timing. * Fixed casting error in coding.h * Fixed CompressionStatsTest for new StatsLevel. * Removed unused variable that was breaking the Linux build/[rocksdb] make more options dynamic Summary: make more ColumnFamilyOptions dynamic: compression soft_pending_compaction_bytes_limit hard_pending_compaction_bytes_limit min_partial_merge_operands report_bg_io_stats paranoid_file_checks Test Plan: Add sanity check in `db_test.cc` for all above options except for soft_pending_compaction_bytes_limit and hard_pending_compaction_bytes_limit. All passed. Reviewers: andrewkr, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1307,1307,12.0,0.6920999884605408,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","Only cache level 0 indexes and filter when opening table reader Summary: In T8216281 we decided to disable prefetching the index and filter during opening table handlers during startup (max_open_files Test Plan: Rely on `IndexAndFilterBlocksOfNewTableAddedToCache` to guarantee L0 indexes and filters are still cached and change `PinL0IndexAndFilterBlocksTest` to make sure other levels are not cached (maybe add one more test to test we dont cache other levels?) Reviewers: sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: Statistics to track Compression/Decompression (#1197) * Added new statistics and refactored to allow ioptions to be passed around as required to access environment and statistics pointers (and, as a convenient side effect, info_log pointer). * Prevent incrementing compression counter when compression is turned off in options. * Prevent incrementing compression counter when compression is turned off in options. * Added two more supported compression types to test code in db_test.cc * Prevent incrementing compression counter when compression is turned off in options. * Added new StatsLevel that excludes compression timing. * Fixed casting error in coding.h * Fixed CompressionStatsTest for new StatsLevel. * Removed unused variable that was breaking the Linux build/Fix clang analyzer errors Summary: Fixing erros reported by clang static analyzer. * Removing some unused variables. * Adding assertions to fix false positives reported by clang analyzer. * Adding `__clang_analyzer__` macro to suppress false positive warnings. Test Plan: USE_CLANG=1 OPT=-g make analyze Reviewers: andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: options.write_buffer_manager: control total memtable size across DB instances Summary: Add option write_buffer_manager to help users control total memory spent on memtables across multiple DB instances. Test Plan: Add a new unit test. Reviewers: yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: adela, benj, sumeet, muthu, leveldb, andrewkr, dhruba Differential Revision:"
1308,1308,17.0,0.9587000012397766,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",Split WinEnv into separate classes. (#1128) For ease of reuse and customization as a library without wrapping. WinEnvThreads is a class for replacement. WintEnvIO is a class for reuse and behavior override. Added private virtual functions for custom override of fallocate pread for io classes./
1309,1309,10.0,0.8490999937057495,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Allowed delayed_write_rate option to be dynamically set. Summary: Closes Differential Revision: D4157784 Pulled By: siying fbshipit-source-id: f150081/[db_bench] add filldeterministic (Universal+level compaction) Summary: in db_bench, we can dynamically create a rocksdb database that guarantees the shape of its LSM. universal + level compaction no fifo compaction no multi db support Test Plan: ./db_bench ``` LSM Level[0]: /000480.sst(size: 35060275 bytes) Level[0]: /000479.sst(size: 70443197 bytes) Level[0]: /000478.sst(size: 141600383 bytes) Level[1]: /000341.sst /000475.sst(total size: 284726629 bytes) Level[2]: /000071.sst /000340.sst(total size: 568649806 bytes) fillseqdeterministic : 60.447 micros/op 16543 ops/sec; 16.0 MB/s ``` Reviewers: sdong, andrewkr, IslamAbdelRahman, yhchiang Reviewed By: yhchiang Subscribers: andrewkr, dhruba, leveldb Differential Revision: Support single benchmark arguments (Repeat for X times, Warm up for X times), Support CombinedStats (AVG / MEDIAN) Summary: This diff allow us to run a single benchmark X times and warm it up for Y times. and see the AVG & MEDIAN throughput of these X runs for example ``` $ ./db_bench Initializing RocksDB Options from the specified file Initializing RocksDB Options from command-line flags RocksDB: version 4.12 Date: Wed Aug 24 10:45:26 2016 CPU: 32 * Intel(R) Xeon(R) CPU E5-2660 0 2.20GHz CPUCache: 20480 KB Keys: 16 bytes each Values: 100 bytes each (50 bytes after compression) Entries: 1000000 Prefix: 0 bytes Keys per prefix: 0 RawSize: 110.6 MB (estimated) FileSize: 62.9 MB (estimated) Write rate: 0 bytes/second Compression: Snappy Memtablerep: skip_list Perf Level: 1 WARNING: Assertions are enabled; benchmarks unnecessarily slow Initializing RocksDB Options from the specified file Initializing RocksDB Options from command-line flags DB path: [/tmp/rocksdbtest-8616/dbbench] fillseq : 4.695 micros/op 212971 ops/sec; 23.6 MB/s DB path: [/tmp/rocksdbtest-8616/dbbench] Warming up benchmark by running 2 times readseq : 0.214 micros/op 4677005 ops/sec; 517.4 MB/s readseq : 0.212 micros/op 4706834 ops/sec; 520.7 MB/s Running benchmark for 5 times readseq : 0.218 micros/op 4588187 ops/sec; 507.6 MB/s readseq : 0.208 micros/op 4816538 ops/sec; 532.8 MB/s readseq : 0.213 micros/op 4685376 ops/sec; 518.3 MB/s readseq : 0.214 micros/op 4676787 ops/sec; 517.4 MB/s readseq : 0.217 micros/op 4618532 ops/sec; 510.9 MB/s readseq [AVG 5 runs] : 4677084 ops/sec; 517.4 MB/sec readseq [MEDIAN 5 runs] : 4676787 ops/sec; 517.4 MB/sec ``` Test Plan: run db_bench Reviewers: sdong, andrewkr, yhchiang Reviewed By: yhchiang Subscribers: andrewkr, dhruba Differential Revision: ClockCache Summary: Clock-based cache implemenetation aim to have better concurreny than default LRU cache. See inline comments for implementation details. Test Plan: Update cache_test to run on both LRUCache and ClockCache. Adding some new tests to catch some of the bugs that I fixed while implementing the cache. Reviewers: kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1310,1310,8.0,0.983299970626831,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","DeleteRange user iterator support Summary: Note: reviewed in DBIter maintains a range tombstone accumulator. We dont cleanup obsolete tombstones yet, so if the user seeks back and forth, the same tombstones would be added to the accumulator multiple times. DBImpl::NewInternalIterator() (used to make DBIters underlying iterator) adds memtable/L0 range tombstones, L1+ range tombstones are added on-demand during NewSecondaryIterator() (see D62205) DBIter uses ShouldDelete() when advancing to check whether keys are covered by range tombstones Closes Differential Revision: D4131753 Pulled By: ajkr fbshipit-source-id: be86559/Support ZSTD with finalized format Summary: ZSTD 1.0.0 is coming. We can finally add a support of ZSTD without worrying about compatibility. Still keep ZSTDNotFinal for compatibility reason. Test Plan: Run all tests. Run db_bench with ZSTD version with RocksDB built with ZSTD 1.0 and older. Reviewers: andrewkr, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: cyan, igor, IslamAbdelRahman, leveldb, andrewkr, dhruba Differential Revision:"
1311,1311,7.0,0.9366000294685364,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Merge options source_compaction_factor, max_grandparent_overlap_bytes and expanded_compaction_factor into max_compaction_bytes Summary: To reduce number of options, merge source_compaction_factor, max_grandparent_overlap_bytes and expanded_compaction_factor into max_compaction_bytes. Test Plan: Add two new unit tests. Run all existing tests, including jtest. Reviewers: yhchiang, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision:"
1312,1312,10.0,0.9472000002861023,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1313,1313,10.0,0.9472000002861023,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1314,1314,10.0,0.9472000002861023,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1315,1315,11.0,0.8812000155448914,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/
1316,1316,10.0,0.9320999979972839,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Java API Implement GetFromBatch and GetFromBatchAndDB in WBWI Summary: Needed for working with `get` after `merge` on a WBWI. Closes Differential Revision: D4137978 Pulled By: yhchiang fbshipit-source-id: e18d50d/Add Status to RocksDBException so that meaningful function result Status from the C++ API isnt lost (#1273)/
1317,1317,9.0,0.6395000219345093,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Support IngestExternalFile (remove AddFile restrictions) Summary: Changes in the diff API changes: Introduce IngestExternalFile to replace AddFile (I think this make the API more clear) Introduce IngestExternalFileOptions (This struct will encapsulate the options for ingesting the external file) Deprecate AddFile() API Logic changes: If our file overlap with the memtable we will flush the memtable We will find the first level in the LSM tree that our file key range overlap with the keys in it We will find the lowest level in the LSM tree above the the level we found in step 2 that our file can fit in and ingest our file in it We will assign a global sequence number to our new file Remove AddFile restrictions by using global sequence numbers Other changes: Refactor all AddFile logic to be encapsulated in ExternalSstFileIngestionJob Test Plan: unit tests (still need to add more) addfile_stress ( Reviewers: yiwu, andrewkr, lightmark, yhchiang, sdong Reviewed By: sdong Subscribers: jkedgar, hcz, andrewkr, dhruba Differential Revision: Java API for SstFileWriter Add Java API for SstFileWriter. Closes jvalue to jval in rocksjni Summary: jvalue shadows a global name in Rename it to jval to fix java build. Test Plan: JAVA_HOME=/usr/local/jdk-7u10-64 make rocksdbjava Reviewers: adamretter, yhchiang, IslamAbdelRahman Reviewed By: yhchiang, IslamAbdelRahman Subscribers: andrewkr, dhruba, leveldb Differential Revision: an offset as well as a length to be specified for byte[] operations in RocksJava JNI (#1264) Test Plan: Execute the Java test suite Reviewers: yhchiang Subscribers: andrewkr, dhruba Differential Revision: singleDelete to RocksJava (#1275) * Rename RocksDB#remove RocksDB#delete to match C++ API; Added deprecated versions of RocksDB#remove for backwards compatibility. * Add missing experimental feature RocksDB#singleDelete/Fix java build Summary: Fix the java build Test Plan: make rocksdbjava Reviewers: yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
1318,1318,7.0,0.954800009727478,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[RocksJava] Adjusted RateLimiter to 3.10.0 (#1368) Summary: Deprecated RateLimiterConfig and GenericRateLimiterConfig Introduced RateLimiter It is now possible to use all C++ related methods also in RocksJava. A noteable method is setBytesPerSecond which can change the allowed number of bytes per second at runtime. Test Plan: make rocksdbjava make jtest Reviewers: adamretter, yhchiang, ankgup87 Subscribers: dhruba Differential Revision: bug in UnScSigned-off-by: xh931076284 (#1336) Fix HdfsEnv::UnSchedule() API error/"
1319,1319,7.0,0.949999988079071,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[RocksJava] Adjusted RateLimiter to 3.10.0 (#1368) Summary: Deprecated RateLimiterConfig and GenericRateLimiterConfig Introduced RateLimiter It is now possible to use all C++ related methods also in RocksJava. A noteable method is setBytesPerSecond which can change the allowed number of bytes per second at runtime. Test Plan: make rocksdbjava make jtest Reviewers: adamretter, yhchiang, ankgup87 Subscribers: dhruba Differential Revision:"
1320,1320,16.0,0.9136000275611877,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Add Java API for SstFileWriter Add Java API for SstFileWriter. Closes an offset as well as a length to be specified for byte[] operations in RocksJava JNI (#1264) Test Plan: Execute the Java test suite Reviewers: yhchiang Subscribers: andrewkr, dhruba Differential Revision:"
1321,1321,7.0,0.949999988079071,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[RocksJava] Adjusted RateLimiter to 3.10.0 (#1368) Summary: Deprecated RateLimiterConfig and GenericRateLimiterConfig Introduced RateLimiter It is now possible to use all C++ related methods also in RocksJava. A noteable method is setBytesPerSecond which can change the allowed number of bytes per second at runtime. Test Plan: make rocksdbjava make jtest Reviewers: adamretter, yhchiang, ankgup87 Subscribers: dhruba Differential Revision:"
1322,1322,7.0,0.949999988079071,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","[RocksJava] Adjusted RateLimiter to 3.10.0 (#1368) Summary: Deprecated RateLimiterConfig and GenericRateLimiterConfig Introduced RateLimiter It is now possible to use all C++ related methods also in RocksJava. A noteable method is setBytesPerSecond which can change the allowed number of bytes per second at runtime. Test Plan: make rocksdbjava make jtest Reviewers: adamretter, yhchiang, ankgup87 Subscribers: dhruba Differential Revision:"
1323,1323,0.0,0.9524999856948853,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","DB::GetOptions() reflect dynamic changed options Summary: DB::GetOptions() reflect dynamic changed options. Test Plan: See the new unit test. Reviewers: yhchiang, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1324,1324,5.0,0.9824000000953674,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Support SST files with Global sequence numbers [reland] Summary: reland Update SstFileWriter to include a property for a global sequence number in the SST file `rocksdb.external_sst_file.global_seqno` Update TableProperties to be aware of the offset of each property in the file Update BlockBasedTableReader and Block to be able to honor the sequence number in `rocksdb.external_sst_file.global_seqno` property and use it to overwrite all sequence number in the file Something worth mentioning is that we dont update the seqno in the index block since and when doing a binary search, the reason for that is that its guaranteed that SST files with global seqno will have only one user_key and each key will have seqno=0 encoded in it, This mean that this key is greater than any other key with seqno> 0. That mean that we can actually keep the current logic for these blocks Test Plan: unit tests Reviewers: sdong, yhchiang Subscribers: andrewkr, dhruba Differential Revision:"
1325,1325,10.0,0.9620000123977661,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","DeleteRange Get support Summary: During Get()/MultiGet(), build up a RangeDelAggregator with range tombstones as we search through live memtable, immutable memtables, and SST files. This aggregator is then used by memtable.ccs SaveValue() and GetContext::SaveValue() to check whether keys are covered. added tests for Get on memtables/files; end-to-end tests mainly in Closes Differential Revision: D4111271 Pulled By: ajkr fbshipit-source-id: 6e388d4/"
1326,1326,11.0,0.891700029373169,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Change ioptions to store user_comparator, fix bug Summary: change ioptions.comparator to user_comparator instread of internal_comparator. Also change Comparator* to InternalKeyComparator* to make its type explicitly. Test Plan: make all check Reviewers: andrewkr, sdong, yiwu Reviewed By: yiwu Subscribers: andrewkr, dhruba, leveldb Differential Revision: range tombstones in memtable Summary: Store range tombstones in a separate MemTableRep instantiated with ColumnFamilyOptions::memtable_factory MemTable::NewRangeTombstoneIterator() returns a MemTableIterator over the separate MemTableRep Part of the read path is not implemented yet (i.e., MemTable::Get()) Test Plan: see unit tests Reviewers: wanning Subscribers: andrewkr, dhruba, leveldb Differential Revision: prefix_extractor_name in table Summary: Make sure prefix extractor name is stored in SST files and if DB is opened with a prefix extractor of a different name, prefix bloom is skipped when read the file. Also add unit tests for that. Test Plan: before change: ``` Note: Google Test filter BlockBasedTableTest.SkipPrefixBloomFilter [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from BlockBasedTableTest [ RUN ] BlockBasedTableTest.SkipPrefixBloomFilter table/table_test.cc:1421: Failure Value of: db_iter->Valid() Actual: false Expected: true [ FAILED ] BlockBasedTableTest.SkipPrefixBloomFilter (1 ms) [----------] 1 test from BlockBasedTableTest (1 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (1 ms total) [ PASSED ] 0 tests. [ FAILED ] 1 test, listed below: [ FAILED ] BlockBasedTableTest.SkipPrefixBloomFilter 1 FAILED TEST ``` after: ``` Note: Google Test filter BlockBasedTableTest.SkipPrefixBloomFilter [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from BlockBasedTableTest [ RUN ] BlockBasedTableTest.SkipPrefixBloomFilter [ OK ] BlockBasedTableTest.SkipPrefixBloomFilter (0 ms) [----------] 1 test from BlockBasedTableTest (0 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (0 ms total) [ PASSED ] 1 test. ``` Reviewers: sdong, andrewkr, yiwu, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba, leveldb Differential Revision: / TableReader support for range deletion Summary: 1. Range Deletion Tombstone structure 2. Modify Add() in table_builder to make it usable for adding range del tombstones 3. Expose NewTombstoneIterator() API in table_reader Test Plan: table_test.cc (now BlockBasedTableBuilder::Add() only accepts InternalKey. I make table_test only pass InternalKey to BlockBasedTableBuidler. Also test writing/reading range deletion tombstones in table_test ) Reviewers: sdong, IslamAbdelRahman, lightmark, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1327,1327,7.0,0.49950000643730164,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Fix SstFileWriter destructor Summary: If user did not call SstFileWriter::Finish() or called Finish() but it failed. We need to abandon the builder, to avoid destructing it while its open Closes Differential Revision: D4171660 Pulled By: IslamAbdelRahman fbshipit-source-id: ab6f434/"
1328,1328,5.0,0.724399983882904,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","sst_dump support for range deletion Summary: Change DumpTable() so we can see the range deletion meta-block. Closes Differential Revision: D4172227 Pulled By: ajkr fbshipit-source-id: ae35665/Insert range deletion meta-block into block cache Summary: This handles two issues: (1) range deletion iterator sometimes outlives the table reader that created it, in which case the block must not be destroyed during table reader destruction; and (2) we prefer to read these range tombstone meta-blocks from file fewer times. Extracted cache-populating logic from NewDataBlockIterator() into a separate function: MaybeLoadDataBlockToCache() Use MaybeLoadDataBlockToCache() to load range deletion meta-block and pin it through the readers lifetime. This code reuse works since range deletion meta-block has same format as data blocks. Use NewDataBlockIterator() to create range deletion iterators, which uses block cache if enabled, otherwise reads the block from file. Either way, the underlying block wont disappear until after the iterator is destroyed. Closes Differential Revision: D4123175 Pulled By: ajkr fbshipit-source-id: 8f64281/Support SST files with Global sequence numbers [reland] Summary: reland Update SstFileWriter to include a property for a global sequence number in the SST file `rocksdb.external_sst_file.global_seqno` Update TableProperties to be aware of the offset of each property in the file Update BlockBasedTableReader and Block to be able to honor the sequence number in `rocksdb.external_sst_file.global_seqno` property and use it to overwrite all sequence number in the file Something worth mentioning is that we dont update the seqno in the index block since and when doing a binary search, the reason for that is that its guaranteed that SST files with global seqno will have only one user_key and each key will have seqno=0 encoded in it, This mean that this key is greater than any other key with seqno> 0. That mean that we can actually keep the current logic for these blocks Test Plan: unit tests Reviewers: sdong, yhchiang Subscribers: andrewkr, dhruba Differential Revision: block cache tickers Summary: Adding several missing block cache tickers. Test Plan: make all check Reviewers: IslamAbdelRahman, yhchiang, lightmark Reviewed By: lightmark Subscribers: andrewkr, dhruba, leveldb Differential Revision: nullptr check to internal_prefix_transform Summary: patch for D62361 Test Plan: make all check Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: Read amplification bitmap (read amp statistics) Summary: Add ReadOptions::read_amp_bytes_per_bit option which allow us to create a bitmap for every data block we read the bitmap will contain (block_size / read_amp_bytes_per_bit) bits. We will use this bitmap to mark which bytes have been used of the block so we can calculate the read amplification Test Plan: added new tests Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: yiwu, leveldb, march, andrewkr, dhruba Differential Revision: data race in NewIndexIterator() in block_based_table_reader.cc Summary: fixed data race described in and add regression test Test Plan: ./table_test make all check core dump before fix. ok after fix. Reviewers: andrewkr, sdong Reviewed By: sdong Subscribers: igor, andrewkr, dhruba, leveldb Differential Revision: to cache index/filter blocks with priority Summary: Add option to block based table to insert index/filter blocks to block cache with priority. Combined with LRUCache with high_pri_pool_ratio, we can reserved space for index/filter blocks, make them less likely to be evicted. Depends on D61977. Test Plan: See unit test. Reviewers: lightmark, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, march, leveldb Differential Revision: / TableReader support for range deletion Summary: 1. Range Deletion Tombstone structure 2. Modify Add() in table_builder to make it usable for adding range del tombstones 3. Expose NewTombstoneIterator() API in table_reader Test Plan: table_test.cc (now BlockBasedTableBuilder::Add() only accepts InternalKey. I make table_test only pass InternalKey to BlockBasedTableBuidler. Also test writing/reading range deletion tombstones in table_test ) Reviewers: sdong, IslamAbdelRahman, lightmark, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1329,1329,10.0,0.9472000002861023,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1330,1330,10.0,0.9472000002861023,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1331,1331,7.0,0.9824000000953674,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Avoid hard-coded sleep in EnvPosixTestWithParam.TwoPools Summary: EnvPosixTestWithParam.TwoPools relies on explicit sleeping, so it sometimes fail. Fix it. Test Plan: Run tests with high parallelism many times and make sure the test passes. Reviewers: yiwu, andrewkr Reviewed By: andrewkr Subscribers: leveldb, andrewkr, dhruba Differential Revision: build error on Windows (AppVeyor) (#1315) Add cf_options to source list and db_imple.cc fix casting/Introduce Read amplification bitmap (read amp statistics) Summary: Add ReadOptions::read_amp_bytes_per_bit option which allow us to create a bitmap for every data block we read the bitmap will contain (block_size / read_amp_bytes_per_bit) bits. We will use this bitmap to mark which bytes have been used of the block so we can calculate the read amplification Test Plan: added new tests Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: yiwu, leveldb, march, andrewkr, dhruba Differential Revision:"
1332,1332,10.0,0.5787000060081482,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","DeleteRange Get support Summary: During Get()/MultiGet(), build up a RangeDelAggregator with range tombstones as we search through live memtable, immutable memtables, and SST files. This aggregator is then used by memtable.ccs SaveValue() and GetContext::SaveValue() to check whether keys are covered. added tests for Get on memtables/files; end-to-end tests mainly in Closes Differential Revision: D4111271 Pulled By: ajkr fbshipit-source-id: 6e388d4/Fix uninitialized variable gcc error for MyRocks Summary: make sure seq_ is properly initialized even if ParseInternalKey() fails. Test Plan: run myrocks release tests Reviewers: lightmark, mung, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: SST files with Global sequence numbers [reland] Summary: reland Update SstFileWriter to include a property for a global sequence number in the SST file `rocksdb.external_sst_file.global_seqno` Update TableProperties to be aware of the offset of each property in the file Update BlockBasedTableReader and Block to be able to honor the sequence number in `rocksdb.external_sst_file.global_seqno` property and use it to overwrite all sequence number in the file Something worth mentioning is that we dont update the seqno in the index block since and when doing a binary search, the reason for that is that its guaranteed that SST files with global seqno will have only one user_key and each key will have seqno=0 encoded in it, This mean that this key is greater than any other key with seqno> 0. That mean that we can actually keep the current logic for these blocks Test Plan: unit tests Reviewers: sdong, yhchiang Subscribers: andrewkr, dhruba Differential Revision: range tombstones in memtable Summary: Store range tombstones in a separate MemTableRep instantiated with ColumnFamilyOptions::memtable_factory MemTable::NewRangeTombstoneIterator() returns a MemTableIterator over the separate MemTableRep Part of the read path is not implemented yet (i.e., MemTable::Get()) Test Plan: see unit tests Reviewers: wanning Subscribers: andrewkr, dhruba, leveldb Differential Revision: TableBuilderOptions::level and relevant changes (#1335)/fix data race in NewIndexIterator() in block_based_table_reader.cc Summary: fixed data race described in and add regression test Test Plan: ./table_test make all check core dump before fix. ok after fix. Reviewers: andrewkr, sdong Reviewed By: sdong Subscribers: igor, andrewkr, dhruba, leveldb Differential Revision: / TableReader support for range deletion Summary: 1. Range Deletion Tombstone structure 2. Modify Add() in table_builder to make it usable for adding range del tombstones 3. Expose NewTombstoneIterator() API in table_reader Test Plan: table_test.cc (now BlockBasedTableBuilder::Add() only accepts InternalKey. I make table_test only pass InternalKey to BlockBasedTableBuidler. Also test writing/reading range deletion tombstones in table_test ) Reviewers: sdong, IslamAbdelRahman, lightmark, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1333,1333,10.0,0.5343999862670898,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Fix deadlock between (WriterThread/Compaction/IngestExternalFile) Summary: A deadlock is possible if this happen (1) Writer thread is stopped because its waiting for compaction to finish (2) Compaction is waiting for current IngestExternalFile() calls to finish (3) IngestExternalFile() is waiting to be able to acquire the writer thread (4) WriterThread is held by stopped writes that are waiting for compactions to finish This patch fix the issue by not incrementing num_running_ingest_file_ except when we acquire the writer thread. This patch include a unittest to reproduce the described scenario Closes Differential Revision: D4151646 Pulled By: IslamAbdelRahman fbshipit-source-id: 09b39db/DeleteRange user iterator support Summary: Note: reviewed in DBIter maintains a range tombstone accumulator. We dont cleanup obsolete tombstones yet, so if the user seeks back and forth, the same tombstones would be added to the accumulator multiple times. DBImpl::NewInternalIterator() (used to make DBIters underlying iterator) adds memtable/L0 range tombstones, L1+ range tombstones are added on-demand during NewSecondaryIterator() (see D62205) DBIter uses ShouldDelete() when advancing to check whether keys are covered by range tombstones Closes Differential Revision: D4131753 Pulled By: ajkr fbshipit-source-id: be86559/DeleteRange Get support Summary: During Get()/MultiGet(), build up a RangeDelAggregator with range tombstones as we search through live memtable, immutable memtables, and SST files. This aggregator is then used by memtable.ccs SaveValue() and GetContext::SaveValue() to check whether keys are covered. added tests for Get on memtables/files; end-to-end tests mainly in Closes Differential Revision: D4111271 Pulled By: ajkr fbshipit-source-id: 6e388d4/Add avoid_flush_during_shutdown DB option Summary: Add avoid_flush_during_shutdown DB option. Closes Differential Revision: D4108643 Pulled By: yiwu-arbug fbshipit-source-id: abdaf4d/DeleteRange flush support Summary: Changed BuildTable() (used for flush) to (1) add range tombstones to the aggregator, which is used by CompactionIterator to determine which keys can be removed; and (2) add aggregators range tombstones to the table that is output for the flush. Closes Differential Revision: D4100025 Pulled By: ajkr fbshipit-source-id: cb01a70/Print compression and Fast CRC support info as Header level Summary: Currently the compression suppport and fast CRC support information is printed as info level. They should be in the same level as options, which is header level. Also add ZSTD to this printing. Closes Differential Revision: D4106608 Pulled By: yiwu-arbug fbshipit-source-id: cb9a076/Show More DB Stats in info logs Summary: DB Stats now are truncated if there are too many CFs. Extend the buffer size to allow more to be printed out. Also, separate out malloc to another log line. Closes Differential Revision: D4100943 Pulled By: yiwu-arbug fbshipit-source-id: 79f7218/Support IngestExternalFile (remove AddFile restrictions) Summary: Changes in the diff API changes: Introduce IngestExternalFile to replace AddFile (I think this make the API more clear) Introduce IngestExternalFileOptions (This struct will encapsulate the options for ingesting the external file) Deprecate AddFile() API Logic changes: If our file overlap with the memtable we will flush the memtable We will find the first level in the LSM tree that our file key range overlap with the keys in it We will find the lowest level in the LSM tree above the the level we found in step 2 that our file can fit in and ingest our file in it We will assign a global sequence number to our new file Remove AddFile restrictions by using global sequence numbers Other changes: Refactor all AddFile logic to be encapsulated in ExternalSstFileIngestionJob Test Plan: unit tests (still need to add more) addfile_stress ( Reviewers: yiwu, andrewkr, lightmark, yhchiang, sdong Reviewed By: sdong Subscribers: jkedgar, hcz, andrewkr, dhruba Differential Revision: WAL deletion when using avoid_flush_during_recovery Summary: Previously the WAL files that were avoided during recovery would never be considered for deletion. That was because alive_log_files_ was only populated when log files are created. This diff further populates alive_log_files_ with existing log files that arent flushed during recovery, such that FindObsoleteFiles() can find them later. Depends on D64053. Test Plan: new unit test, verifies it fails before this change and passes after Reviewers: sdong, IslamAbdelRahman, yiwu Reviewed By: yiwu Subscribers: leveldb, dhruba, andrewkr Differential Revision: max_background_compactions and base_background_compactions dynamic changeable Summary: Add DB::SetDBOptions to dynamic change max_background_compactions and base_background_compactions. Ill add more dynamic changeable options soon. Test Plan: unit test. Reviewers: yhchiang, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: Prev() prefix support using SeekForPrev() Summary: 1) The previous solution for Prev() prefix support is not clean. Since I add api SeekForPrev(), now the Prev() can be symmetric to Next(). and we do not need SeekToLast() to be called in Prev() any more. Also, Next() will Seek(prefix_seek_key_) to solve the problem of possible inconsistency between db_iter and merge_iter when there is merge_operator. And prefix_seek_key is only refreshed when change direction to forward. 2) This diff also solves the bug of Iterator::SeekToLast() with iterate_upper_bound_ with prefix extractor. add test cases for the above two cases. There are some tests for the SeekToLast() in Prev(), I will clean them later. Test Plan: make all check Reviewers: IslamAbdelRahman, andrewkr, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: facility to write only a portion of WriteBatch to WAL Summary: When constructing a write batch a client may now call MarkWalTerminationPoint() on that batch. No batch operations after this call will be added written to the WAL but will still be inserted into the Memtable. This facility is used to remove one of the three WriteImpl calls in 2PC transactions. This produces a ~1% perf improvement. ``` RocksDB unoptimized 2pc, sync_binlog=1, disable_2pc=off INFO 2016-08-31 14:30:38,814 [main]: REQUEST PHASE COMPLETED. 75000000 requests done in 2619 seconds. Requests/second 28628 RocksDB optimized 2pc , sync_binlog=1, disable_2pc=off INFO 2016-08-31 16:26:59,442 [main]: REQUEST PHASE COMPLETED. 75000000 requests done in 2581 seconds. Requests/second 29054 ``` Test Plan: Two unit tests added. Reviewers: sdong, yiwu, IslamAbdelRahman Reviewed By: yiwu Subscribers: hermanlee4, dhruba, andrewkr Differential Revision: conflict between AddFile() and CompactRange() Summary: Fix the conflict bug between AddFile() and CompactRange() by Make sure that no AddFile calls are running when asking CompactionPicker to pick compaction for manual compaction If AddFile() run after we pick the compaction for the manual compaction it will be aware of it since we will add the manual compaction to running_compactions_ after picking it This will solve these 2 scenarios If AddFile() is running, we will wait for it to finish before we pick a compaction for the manual compaction If we already picked a manual compaction and then AddFile() started ... we ensure that it never ingest a file in a level that will overlap with the manual compaction Test Plan: unit tests Reviewers: sdong Reviewed By: sdong Subscribers: andrewkr, yoshinorim, jkedgar, dhruba Differential Revision: AddFile() conflict with compaction output [WaitForAddFile()] Summary: Since AddFile unlock/lock the mutex inside LogAndApply() we need to ensure that during this period other compactions cannot run since such compactions are not aware of the file we are ingesting and could create a compaction that overlap wit this file this diff add WaitForAddFile() call that will ensure that no AddFile() calls are being processed right now Call `WaitForAddFile()` in 3 locations When doing manual Compaction When starting automatic Compaction When doing CompactFiles() Test Plan: unit test Reviewers: lightmark, yiwu, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, yoshinorim, jkedgar, dhruba Differential Revision: DBOptions into ImmutableDBOptions and MutableDBOptions Summary: Use ImmutableDBOptions/MutableDBOptions internally and DBOptions only for user-facing APIs. MutableDBOptions is barely a placeholder for now. Ill start to move options to MutableDBOptions in following diffs. Test Plan: make all check Reviewers: yhchiang, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: same sequence id from WAL (#1350) Summary: Revert the behavior where we dont read sequence id from WAL, but increase it as we replay the log. We still keep the behave for 2PC for now but will fix later. This change fixes github issue 1339, where some writes come with WAL disabled and we may recover records with wrong sequence id. Test Plan: Added unit test. Subscribers: andrewkr, dhruba Differential Revision: more factors when determining preallocation size of WAL files Summary: Currently the WAL file preallocation size is 1.1 * write_buffer_size. This, however, will be over-estimated if options.db_write_buffer_size or options.max_total_wal_size is set and is much smaller. Test Plan: Add a unit test. Reviewers: andrewkr, yiwu Reviewed By: yiwu Subscribers: leveldb, andrewkr, dhruba Differential Revision: (#1313) If log recycling is enabled with the rocksdb (recycle_log_file_num=16) db->Writebatch is erroring out with keynotfound after ~5-6 hours of run (1M seq but can happen to any workload I guess).See my detailed bug report here ( This commit is the fix for this, a check is been added not to delete the log file if it is already there in the recycle list. Test Plan: Unit tested it and ran the similar profile. Not reproducing anymore./support Prev() in prefix seek mode Summary: As title, make sure Prev() works as expected with Next() when the current iter->key() in the range of the same prefix in prefix seek mode Test Plan: make all check (add prefix_test with PrefixSeekModePrev test case) Reviewers: andrewkr, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, andrewkr, dhruba, leveldb Differential Revision: regression bug of options.max_successive_merges hit during DB Recovery Summary: After 1b8a2e8fdd1db0dac3cb50228065f8e7e43095f0, DB Pointer is passed to WriteBatchInternal::InsertInto() while DB recovery. This can cause deadlock if options.max_successive_merges hits. In that case DB::Get() will be called. Get() will try to acquire the DB mutex, which is already held by the DB::Open(), causing a deadlock condition. This commit mitigates the problem by not passing the DB pointer unless 2PC is allowed. Test Plan: Add a new test and run it. Reviewers: IslamAbdelRahman, andrewkr, kradhakrishnan, horuff Reviewed By: kradhakrishnan Subscribers: leveldb, andrewkr, dhruba Differential Revision: data during user initiated shutdown Summary: Move the manual memtable flush for databases containing data that has bypassed the WAL from DBImpls destructor to CancleAllBackgroundWork(). CancelAllBackgroundWork() is a publicly exposed API which allows async operations performed by background threads to be disabled on a database. In effect, this places the database into a ""shutdown"" state in advance of calling the database objects destructor. No compactions or flushing of SST files can occur once a call to this API completes. When writes are issued to a database with WriteOptions::disableWAL set to true, DBImpl::has_unpersisted_data_ is set so that memtables can be flushed when the database object is destroyed. If CancelAllBackgroundWork() has been called prior to DBImpls destructor, this flush operation is not possible and is skipped, causing unnecessary loss of data. Since CancelAllBackgroundWork() is already invoked by DBImpls destructor in order to perform the thread join portion of its cleanup processing, moving the manual memtable flush to CancelAllBackgroundWork() ensures data is persisted regardless of client behavior. Test Plan: Write an amount of data that will not cause a memtable flush to a rocksdb database with all writes marked with WriteOptions::disableWAL. Properly ""close"" the database. Reopen database and verify that the data was persisted. Reviewers: IslamAbdelRahman, yiwu, yoshinorim, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
1334,1334,7.0,0.9927999973297119,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Fix compaction conflict with running compaction Summary: Issue scenario: (1) We have 3 files in L1 and we issue a compaction that will compact them into 1 file in L2 (2) While compaction (1) is running, we flush a file into L0 and trigger another compaction that decide to move this file to L1 and then move it again to L2 (this file dont overlap with any other files) (3) compaction (1) finishes and install the file it generated in L2, but this file overlap with the file we generated in (2) so we break the LSM consistency Looks like this issue can be triggered by using non-exclusive manual compaction or AddFile() Test Plan: unit tests Reviewers: sdong Reviewed By: sdong Subscribers: hermanlee4, jkedgar, andrewkr, dhruba, yoshinorim Differential Revision: merge during recovery Summary: Mitigate regression bug of options.max_successive_merges hit during DB Recovery For Test Plan: make all check Reviewers: horuff, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: options source_compaction_factor, max_grandparent_overlap_bytes and expanded_compaction_factor into max_compaction_bytes Summary: To reduce number of options, merge source_compaction_factor, max_grandparent_overlap_bytes and expanded_compaction_factor into max_compaction_bytes. Test Plan: Add two new unit tests. Run all existing tests, including jtest. Reviewers: yhchiang, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision: Read amplification bitmap (read amp statistics) Summary: Add ReadOptions::read_amp_bytes_per_bit option which allow us to create a bitmap for every data block we read the bitmap will contain (block_size / read_amp_bytes_per_bit) bits. We will use this bitmap to mark which bytes have been used of the block so we can calculate the read amplification Test Plan: added new tests Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: yiwu, leveldb, march, andrewkr, dhruba Differential Revision: regression bug of options.max_successive_merges hit during DB Recovery Summary: After 1b8a2e8fdd1db0dac3cb50228065f8e7e43095f0, DB Pointer is passed to WriteBatchInternal::InsertInto() while DB recovery. This can cause deadlock if options.max_successive_merges hits. In that case DB::Get() will be called. Get() will try to acquire the DB mutex, which is already held by the DB::Open(), causing a deadlock condition. This commit mitigates the problem by not passing the DB pointer unless 2PC is allowed. Test Plan: Add a new test and run it. Reviewers: IslamAbdelRahman, andrewkr, kradhakrishnan, horuff Reviewed By: kradhakrishnan Subscribers: leveldb, andrewkr, dhruba Differential Revision:"
1335,1335,8.0,0.8226000070571899,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","DeleteRange user iterator support Summary: Note: reviewed in DBIter maintains a range tombstone accumulator. We dont cleanup obsolete tombstones yet, so if the user seeks back and forth, the same tombstones would be added to the accumulator multiple times. DBImpl::NewInternalIterator() (used to make DBIters underlying iterator) adds memtable/L0 range tombstones, L1+ range tombstones are added on-demand during NewSecondaryIterator() (see D62205) DBIter uses ShouldDelete() when advancing to check whether keys are covered by range tombstones Closes Differential Revision: D4131753 Pulled By: ajkr fbshipit-source-id: be86559/Support IngestExternalFile (remove AddFile restrictions) Summary: Changes in the diff API changes: Introduce IngestExternalFile to replace AddFile (I think this make the API more clear) Introduce IngestExternalFileOptions (This struct will encapsulate the options for ingesting the external file) Deprecate AddFile() API Logic changes: If our file overlap with the memtable we will flush the memtable We will find the first level in the LSM tree that our file key range overlap with the keys in it We will find the lowest level in the LSM tree above the the level we found in step 2 that our file can fit in and ingest our file in it We will assign a global sequence number to our new file Remove AddFile restrictions by using global sequence numbers Other changes: Refactor all AddFile logic to be encapsulated in ExternalSstFileIngestionJob Test Plan: unit tests (still need to add more) addfile_stress ( Reviewers: yiwu, andrewkr, lightmark, yhchiang, sdong Reviewed By: sdong Subscribers: jkedgar, hcz, andrewkr, dhruba Differential Revision:"
1336,1336,11.0,0.5138999819755554,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Add C api for RateLimiter Summary: Add C api for RateLimiter. Closes Differential Revision: D4116362 Pulled By: yiwu-arbug fbshipit-source-id: cb05a8d/expose IngestExternalFile to c abi Summary: IngestExternalFile is very useful when doing bulk load. This pr expose this API to c so many bindings can benefit from it too. Closes Differential Revision: D4113420 Pulled By: yiwu-arbug fbshipit-source-id: 307c6ae/Fix C api memtable rep bugs. (#1328)/add C api for set wal_recovery_mode (#1327) * add C api for set wal recovery mode * add test/c abi: allow compaction filter ignore snapshot (#1268) close
1337,1337,19.0,0.5680000185966492,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Change max_bytes_for_level_multiplier to double Summary: Closes Differential Revision: D4094732 Pulled By: yiwu-arbug fbshipit-source-id: b9b79e9/Fix a bug that may cause a deleted row to appear again Summary: The previous fix of reappearing of a deleted row 0ce258f9b37c8661ea326039372bef8f185615ef missed a corner case, which can be reproduced using test CompactionPickerTest.OverlappingUserKeys7. Consider such an example: input level file: 1[B E] 2[F H] output level file: 3[A C] 4[D I] 5[I K] First file 2 is picked, which overlaps to file 4. 4 expands to 5. Now the all range is [D K] with 2 output level files. When we try to expand that, [D K] overlaps with file 1 and 2 in the input level, and 1 and 2 overlaps with 3 and 4 in the output level. So we end up with picking 3 and 4 in the output level. Without expanding, it also has 2 files, so we determine the output level doesnt change, although they are the different two files. The fix is to expand the output level files after we picked 3 and 4. In that case, there will be three output level files so we will abort the expanding. I also added two unit tests related to marked_for_compaction and being_compacted. They have been passing though. Test Plan: Run the new unit test, as well as all other tests. Reviewers: andrewkr, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, leveldb, andrewkr, dhruba Differential Revision: compaction conflict with running compaction Summary: Issue scenario: (1) We have 3 files in L1 and we issue a compaction that will compact them into 1 file in L2 (2) While compaction (1) is running, we flush a file into L0 and trigger another compaction that decide to move this file to L1 and then move it again to L2 (this file dont overlap with any other files) (3) compaction (1) finishes and install the file it generated in L2, but this file overlap with the file we generated in (2) so we break the LSM consistency Looks like this issue can be triggered by using non-exclusive manual compaction or AddFile() Test Plan: unit tests Reviewers: sdong Reviewed By: sdong Subscribers: hermanlee4, jkedgar, andrewkr, dhruba, yoshinorim Differential Revision: MutableCFOptions Summary: * Change constructor of MutableCFOptions to depends only on ColumnFamilyOptions. * Move `max_subcompactions`, `compaction_options_fifo` and `compaction_pri` to ImmutableCFOptions to make it clear that they are immutable. Test Plan: existing unit tests. Reviewers: yhchiang, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: options source_compaction_factor, max_grandparent_overlap_bytes and expanded_compaction_factor into max_compaction_bytes Summary: To reduce number of options, merge source_compaction_factor, max_grandparent_overlap_bytes and expanded_compaction_factor into max_compaction_bytes. Test Plan: Add two new unit tests. Run all existing tests, including jtest. Reviewers: yhchiang, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, andrewkr, dhruba Differential Revision:"
1338,1338,4.0,0.9842000007629395,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Exporting compaction stats in the form of a map Summary: Currently the compaction stats are printed to stdout. We want to export the compaction stats in a map format so that the upper layer apps (e.g., MySQL) could present the stats in any format required by the them. Closes Differential Revision: D4149836 Pulled By: maysamyabandeh fbshipit-source-id: b3df19f/Add AddFile() InternalStats for Total files/L0 files/total keys ingested Summary: Report more information about the ingested files in CF InternalStats Total files Total L0 files Total keys There was also noticed that we were reporting files that failed to ingest, fix this bug Test Plan: print stats in tests Reviewers: sdong, andrewkr, lightmark Reviewed By: lightmark Subscribers: jkedgar, andrewkr, dhruba, yoshinorim Differential Revision:"
1339,1339,8.0,0.9728999733924866,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","DeleteRange user iterator support Summary: Note: reviewed in DBIter maintains a range tombstone accumulator. We dont cleanup obsolete tombstones yet, so if the user seeks back and forth, the same tombstones would be added to the accumulator multiple times. DBImpl::NewInternalIterator() (used to make DBIters underlying iterator) adds memtable/L0 range tombstones, L1+ range tombstones are added on-demand during NewSecondaryIterator() (see D62205) DBIter uses ShouldDelete() when advancing to check whether keys are covered by range tombstones Closes Differential Revision: D4131753 Pulled By: ajkr fbshipit-source-id: be86559/"
1340,1340,14.0,0.949999988079071,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Split DBOptions into ImmutableDBOptions and MutableDBOptions Summary: Use ImmutableDBOptions/MutableDBOptions internally and DBOptions only for user-facing APIs. MutableDBOptions is barely a placeholder for now. Ill start to move options to MutableDBOptions in following diffs. Test Plan: make all check Reviewers: yhchiang, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1341,1341,9.0,0.7520999908447266,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Store range tombstones in memtable Summary: Store range tombstones in a separate MemTableRep instantiated with ColumnFamilyOptions::memtable_factory MemTable::NewRangeTombstoneIterator() returns a MemTableIterator over the separate MemTableRep Part of the read path is not implemented yet (i.e., MemTable::Get()) Test Plan: see unit tests Reviewers: wanning Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1342,1342,14.0,0.8894000053405762,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Exporting compaction stats in the form of a map Summary: Currently the compaction stats are printed to stdout. We want to export the compaction stats in a map format so that the upper layer apps (e.g., MySQL) could present the stats in any format required by the them. Closes Differential Revision: D4149836 Pulled By: maysamyabandeh fbshipit-source-id: b3df19f/DBTest.GetThreadStatus: Wait for test results for longer Summary: The current 10 millisecond waiting for test results may not be sufficient in some test environments. Increase it to 60 seconds and check the results for every 1 milliseond. Already reviewed: Closes Differential Revision: D4099443 Pulled By: siying fbshipit-source-id: cf1f205/Support IngestExternalFile (remove AddFile restrictions) Summary: Changes in the diff API changes: Introduce IngestExternalFile to replace AddFile (I think this make the API more clear) Introduce IngestExternalFileOptions (This struct will encapsulate the options for ingesting the external file) Deprecate AddFile() API Logic changes: If our file overlap with the memtable we will flush the memtable We will find the first level in the LSM tree that our file key range overlap with the keys in it We will find the lowest level in the LSM tree above the the level we found in step 2 that our file can fit in and ingest our file in it We will assign a global sequence number to our new file Remove AddFile restrictions by using global sequence numbers Other changes: Refactor all AddFile logic to be encapsulated in ExternalSstFileIngestionJob Test Plan: unit tests (still need to add more) addfile_stress ( Reviewers: yiwu, andrewkr, lightmark, yhchiang, sdong Reviewed By: sdong Subscribers: jkedgar, hcz, andrewkr, dhruba Differential Revision: max_background_compactions and base_background_compactions dynamic changeable Summary: Add DB::SetDBOptions to dynamic change max_background_compactions and base_background_compactions. Ill add more dynamic changeable options soon. Test Plan: unit test. Reviewers: yhchiang, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: reflect dynamic changed options Summary: DB::GetOptions() reflect dynamic changed options. Test Plan: See the new unit test. Reviewers: yhchiang, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1343,1343,13.0,0.9049999713897705,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","add cfh deletion started listener Summary: add ColumnFamilyHandleDeletionStarted listener which can be called when user deletes handler. Test Plan: ./listener_test Reviewers: yiwu, IslamAbdelRahman, sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1344,1344,10.0,0.7615000009536743,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","DeleteRange user iterator support Summary: Note: reviewed in DBIter maintains a range tombstone accumulator. We dont cleanup obsolete tombstones yet, so if the user seeks back and forth, the same tombstones would be added to the accumulator multiple times. DBImpl::NewInternalIterator() (used to make DBIters underlying iterator) adds memtable/L0 range tombstones, L1+ range tombstones are added on-demand during NewSecondaryIterator() (see D62205) DBIter uses ShouldDelete() when advancing to check whether keys are covered by range tombstones Closes Differential Revision: D4131753 Pulled By: ajkr fbshipit-source-id: be86559/DeleteRange Get support Summary: During Get()/MultiGet(), build up a RangeDelAggregator with range tombstones as we search through live memtable, immutable memtables, and SST files. This aggregator is then used by memtable.ccs SaveValue() and GetContext::SaveValue() to check whether keys are covered. added tests for Get on memtables/files; end-to-end tests mainly in Closes Differential Revision: D4111271 Pulled By: ajkr fbshipit-source-id: 6e388d4/Support IngestExternalFile (remove AddFile restrictions) Summary: Changes in the diff API changes: Introduce IngestExternalFile to replace AddFile (I think this make the API more clear) Introduce IngestExternalFileOptions (This struct will encapsulate the options for ingesting the external file) Deprecate AddFile() API Logic changes: If our file overlap with the memtable we will flush the memtable We will find the first level in the LSM tree that our file key range overlap with the keys in it We will find the lowest level in the LSM tree above the the level we found in step 2 that our file can fit in and ingest our file in it We will assign a global sequence number to our new file Remove AddFile restrictions by using global sequence numbers Other changes: Refactor all AddFile logic to be encapsulated in ExternalSstFileIngestionJob Test Plan: unit tests (still need to add more) addfile_stress ( Reviewers: yiwu, andrewkr, lightmark, yhchiang, sdong Reviewed By: sdong Subscribers: jkedgar, hcz, andrewkr, dhruba Differential Revision: Support for Range Deletion Summary: This diff introduces RangeDelAggregator, which takes ownership of iterators provided to it via AddTombstones(). The tombstones are organized in a two-level map (snapshot stripe begin key tombstone). Tombstone creation avoids data copy by holding Slices returned by the iterator, which remain valid thanks to pinning. For compaction, we create a hierarchical range tombstone iterator with structure matching the iterator over compaction input data. An aggregator based on that iterator is used by CompactionIterator to determine which keys are covered by range tombstones. In case of merge operand, the same aggregator is used by MergeHelper. Upon finishing each file in the compaction, relevant range tombstones are added to the output files range tombstone metablock and file boundaries are updated accordingly. To check whether a key is covered by range tombstone, RangeDelAggregator::ShouldDelete() considers tombstones in the keys snapshot stripe. When this function is used outside of compaction, it also checks newer stripes, which can contain covering tombstones. Currently the intra-stripe check involves a linear scan; however, in the future we plan to collapse ranges within a stripe such that binary search can be used. RangeDelAggregator::AddToBuilder() adds all range tombstones in the tables key-range to a new tables range tombstone meta-block. Since range tombstones may fall in the gap between files, we may need to extend some files key-ranges. The strategy is (1) first file extends as far left as possible and other files do not extend left, (2) all files extend right until either the start of the next file or the end of the last range tombstone in the gap, whichever comes first. One other notable change is adding release/move semantics to ScopedArenaIterator such that it can be used to transfer ownership of an arena-allocated iterator, similar to how unique_ptr is used for mallocd data. Depends on D61473 Test Plan: compaction_iterator_test, mock_table, end-to-end tests in D63927 Reviewers: sdong, IslamAbdelRahman, wanning, yhchiang, lightmark Reviewed By: lightmark Subscribers: andrewkr, dhruba, leveldb Differential Revision: SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: DBOptions into ImmutableDBOptions and MutableDBOptions Summary: Use ImmutableDBOptions/MutableDBOptions internally and DBOptions only for user-facing APIs. MutableDBOptions is barely a placeholder for now. Ill start to move options to MutableDBOptions in following diffs. Test Plan: make all check Reviewers: yhchiang, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1345,1345,9.0,0.9567999839782715,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Store range tombstones in memtable Summary: Store range tombstones in a separate MemTableRep instantiated with ColumnFamilyOptions::memtable_factory MemTable::NewRangeTombstoneIterator() returns a MemTableIterator over the separate MemTableRep Part of the read path is not implemented yet (i.e., MemTable::Get()) Test Plan: see unit tests Reviewers: wanning Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
1346,1346,3.0,0.9366999864578247,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",expose IngestExternalFile to c abi Summary: IngestExternalFile is very useful when doing bulk load. This pr expose this API to c so many bindings can benefit from it too. Closes Differential Revision: D4113420 Pulled By: yiwu-arbug fbshipit-source-id: 307c6ae/Fix C api memtable rep bugs. (#1328)/
1347,1347,8.0,0.8100000023841858,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead",Implement WinRandomRW file and improve code reuse (#1388)/
1348,1348,11.0,0.9591000080108643,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Update db_bench and sst_dump to test with block cache mid-point inser? Summary: ?tion Add flags in db_bench to test with block cache mid-point insertion. Also update sst_dump to dump total block sizes of each type. I find it useful to look at these test db stats and I dont know if we have them elsewhere. Closes Differential Revision: D4355812 Pulled By: yiwu-arbug fbshipit-source-id: 3e4a348/db_bench: introduce Summary: Add the parameter in db_bench to help users to measure latency histogram with constant read rate. Closes Differential Revision: D4341387 Pulled By: siying fbshipit-source-id: 1b4b276/Option to expand range tombstones in db_bench Summary: When enabled, this option replaces range tombstones with a sequence of point tombstones covering the same range. This can be used to A/B test perf of range tombstones vs sequential point tombstones, and help us find the cross-over point, i.e., the size of the range above which range tombstones outperform point tombstones. Closes Differential Revision: D4246312 Pulled By: ajkr fbshipit-source-id: 3b00b23/Cache heap::downheap() root comparison (optimize heap cmp call) Summary: Reduce number of comparisons in heap by caching which child node in the first level is smallest (left_child or right_child) So next time we can compare directly against the smallest child I see that the total number of calls to comparator drops significantly when using this optimization Before caching (~2mil key comparison for iterating the DB) ``` $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 0.338 micros/op 2959201 ops/sec; 327.4 MB/s user_key_comparison_count 2000008 ``` After caching (~1mil key comparison for iterating the DB) ``` $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 0.309 micros/op 3236801 ops/sec; 358.1 MB/s user_key_comparison_count 1000011 ``` It also improves Closes Differential Revision: D4256027 Pulled By: IslamAbdelRahman fbshipit-source-id: 76fcc66/Kill flashcache code in RocksDB Summary: Now that we have userspace persisted cache, we dont need flashcache anymore. Closes Differential Revision: D4245114 Pulled By: igorcanadi fbshipit-source-id: e2c1c72/DeleteRange support for db_bench Summary: Added a few options to configure when to add range tombstones during any benchmark involving writes. Closes Differential Revision: D4187388 Pulled By: ajkr fbshipit-source-id: 2c8a473/"
1349,1349,4.0,0.6919999718666077,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Range deletion microoptimizations Summary: Made RangeDelAggregators InternalKeyComparator member a reference-to-const so we dont need to copy-construct it. Also added InternalKeyComparator to ImmutableCFOptions so we dont need to construct one for each DBIter. Made MemTable::NewRangeTombstoneIterator and the table readers NewRangeTombstoneIterator() functions return nullptr instead of NewEmptyInternalIterator to avoid the allocation. Updated callers accordingly. Closes Differential Revision: D4208169 Pulled By: ajkr fbshipit-source-id: 2fd65cf/
1350,1350,17.0,0.9648000001907349,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create",db_stress support for range deletions Summary: made db_stress capable of adding range deletions to its db and verifying their correctness. ill make db_crashtest.py use this option later once the collapsing optimization ( is committed because currently it slows down the test too much. Closes Differential Revision: D4293939 Pulled By: ajkr fbshipit-source-id: d3beb3a/
1351,1351,11.0,0.9768000245094299,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Update db_bench and sst_dump to test with block cache mid-point inser? Summary: ?tion Add flags in db_bench to test with block cache mid-point insertion. Also update sst_dump to dump total block sizes of each type. I find it useful to look at these test db stats and I dont know if we have them elsewhere. Closes Differential Revision: D4355812 Pulled By: yiwu-arbug fbshipit-source-id: 3e4a348/Print user collected properties in sst_dump Summary: Include a dump of user_collected_properties in sst_dump Closes Differential Revision: D4325078 Pulled By: IslamAbdelRahman fbshipit-source-id: 226b6d6/
1352,1352,11.0,0.967199981212616,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",change UseDirectIO() to use_direct_io() Summary: also change variable name `direct_io_` to `use_direct_io_` in WritableFile to make it consistent with read path. Closes Differential Revision: D4416435 Pulled By: lightmark fbshipit-source-id: 4143c53/Implement PositionedAppend for PosixWritableFile Summary: This patch clarifies the contract of PositionedAppend with some unit tests and also implements it for PosixWritableFile. (Tasks: 14524071) Closes Differential Revision: D4204907 Pulled By: maysamyabandeh fbshipit-source-id: 06eabd2/
1353,1353,11.0,0.48339998722076416,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Move ThreadLocal implementation into .cc Summary: Closes Differential Revision: D4502314 Pulled By: siying fbshipit-source-id: f46fac1/
1354,1354,15.0,0.8456000089645386,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Optimize sequential insert into memtable Part 1: Interface Summary: Currently our skip-list have an optimization to speedup sequential inserts from a single stream, by remembering the last insert position. We extend the idea to support sequential inserts from multiple streams, and even tolerate small reordering wihtin each stream. This PR is the interface part adding the following: Add `memtable_insert_prefix_extractor` to allow specifying prefix for each key. Add `InsertWithHint()` interface to memtable, to allow underlying implementation to return a hint of insert position, which can be later pass back to optimize inserts. Memtable will maintain a map from prefix to hints and pass the hint via `InsertWithHint()` if `memtable_insert_prefix_extractor` is non-null. Closes Differential Revision: D4079367 Pulled By: yiwu-arbug fbshipit-source-id: 3555326/"
1355,1355,11.0,0.84170001745224,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Fixes for MSVC compilation Summary: Closes Differential Revision: D4327421 Pulled By: yiwu-arbug fbshipit-source-id: 661ee0b/
1356,1356,3.0,0.9634000062942505,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Remove LATEST_BACKUP file Summary: This has been unused since D42069 but kept around for backward compatibility. I think it is unlikely anyone will use a much older version of RocksDB for restore than they use for backup, so I propose removing it. It is also causing recurring confusion, e.g., Ported from Closes Differential Revision: D4194199 Pulled By: ajkr fbshipit-source-id: 82f9bf4/"
1357,1357,4.0,0.9472000002861023,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Two-level Indexes Summary: Partition Index blocks and use a Partition-index as a 2nd level index. The two-level index can be used by setting BlockBasedTableOptions::kTwoLevelIndexSearch as the index type and configuring BlockBasedTableOptions::index_per_partition t15539501 Closes Differential Revision: D4473535 Pulled By: maysamyabandeh fbshipit-source-id: bffb87e/
1358,1358,1.0,0.4368000030517578,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Two-level Indexes Summary: Partition Index blocks and use a Partition-index as a 2nd level index. The two-level index can be used by setting BlockBasedTableOptions::kTwoLevelIndexSearch as the index type and configuring BlockBasedTableOptions::index_per_partition t15539501 Closes Differential Revision: D4473535 Pulled By: maysamyabandeh fbshipit-source-id: bffb87e/Avoid cache lookups for range deletion meta-block Summary: I added the Cache::Ref() function a couple weeks ago (#1761) to make this feature possible. Like other meta-blocks, rep_->range_del_entry holds a cache handle to pin the range deletion block in uncompressed block cache for the duration of the table readers lifetime. We can reuse this cache handle to create an iterator over this meta-block without any cache lookup. Ref() is used to increment the cache handles refcount in case the returned iterator outlives the table reader. Closes Differential Revision: D4458782 Pulled By: ajkr fbshipit-source-id: 2883f10/Range deletion microoptimizations Summary: Made RangeDelAggregators InternalKeyComparator member a reference-to-const so we dont need to copy-construct it. Also added InternalKeyComparator to ImmutableCFOptions so we dont need to construct one for each DBIter. Made MemTable::NewRangeTombstoneIterator and the table readers NewRangeTombstoneIterator() functions return nullptr instead of NewEmptyInternalIterator to avoid the allocation. Updated callers accordingly. Closes Differential Revision: D4208169 Pulled By: ajkr fbshipit-source-id: 2fd65cf/fix valgrind Summary: Closes Differential Revision: D4191257 Pulled By: ajkr fbshipit-source-id: d09dc76/"
1359,1359,5.0,0.6783999800682068,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check",Allow plain table to store index on file with bloom filter disabled Summary: Currently plain table bloom filter is required if storing metadata on file. Remove the constraint. Closes Differential Revision: D4190977 Pulled By: siying fbshipit-source-id: be60442/
1360,1360,5.0,0.5090000033378601,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Two-level Indexes Summary: Partition Index blocks and use a Partition-index as a 2nd level index. The two-level index can be used by setting BlockBasedTableOptions::kTwoLevelIndexSearch as the index type and configuring BlockBasedTableOptions::index_per_partition t15539501 Closes Differential Revision: D4473535 Pulled By: maysamyabandeh fbshipit-source-id: bffb87e/Windows thread Summary: introduce new methods into a public threadpool interface, allow submission of std::functions as they allow greater flexibility. add Joining methods to the implementation to join scheduled and submitted jobs with an option to cancel jobs that did not start executing. Remove ugly `#ifdefs` between pthread and std implementation, make it uniform. introduce pimpl for a drop in replacement of the implementation Introduce rocksdb::port::Thread typedef which is a replacement for std::thread. On Posix Thread defaults as before std::thread. Implement WindowsThread that allocates memory in a more controllable manner than windows std::thread with a replaceable implementation. should be no functionality changes. Closes Differential Revision: D4492902 Pulled By: siying fbshipit-source-id: c74cb11/"
1361,1361,11.0,0.5153999924659729,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Add test DBTest2.GetRaceFlush which can expose a data race bug Summary: A current data race issue in Get() and Flush() can cause a Get() to return wrong results when a flush happened in the middle. Disable the test for now. Closes Differential Revision: D4472310 Pulled By: siying fbshipit-source-id: 5755ebd/Avoid logs_ operation out of DB mutex Summary: logs_.back() is called out of DB mutex, which can cause data race. We move the access into the DB mutex protection area. Closes Reviewed By: AsyncDBConnMarkedDownDBException Differential Revision: D4417472 Pulled By: AsyncDBConnMarkedDownDBException fbshipit-source-id: 2da1f1e/Fix for 2PC causing WAL to grow too large Summary: Consider the following single column family scenario: prepare in log A commit in log B *WAL is too large, flush all CFs to releast log A* *CFA is on log B so we do not see CFA is depending on log A so no flush is requested* To fix this we must also consider the log containing the prepare section when determining what log a CF is dependent on. Closes Differential Revision: D4403265 Pulled By: reidHoruff fbshipit-source-id: ce800ff/Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/Revert ""PinnableSlice"" Summary: This reverts commit 54d94e9c2cc0bf6eeb2a165ada33fa9c174f0b16. The pull request was landed by mistake. Closes Differential Revision: D4391678 Pulled By: maysamyabandeh fbshipit-source-id: 36d5149/Disallow ingesting files into dropped CFs Summary: This PR update IngestExternalFile to return an error if we try to ingest a file into a dropped CF. Right now if IngestExternalFile want to flush a memtable, and its ingesting a file into a dropped CF, it will wait forever since flushing is not possible for the dropped CF Closes Differential Revision: D4318657 Pulled By: IslamAbdelRahman fbshipit-source-id: ed6ea2b/Add EventListener::OnExternalFileIngested() event Summary: Add EventListener::OnExternalFileIngested() to allow user to subscribe to external file ingestion events Closes Differential Revision: D4285844 Pulled By: IslamAbdelRahman fbshipit-source-id: 0b95a88/Add WriteOptions.no_slowdown Summary: If the WriteOptions.no_slowdown flag is set AND we need to wait or sleep for the write request, then fail immediately with Status::Incomplete(). Closes Differential Revision: D4191405 Pulled By: maysamyabandeh fbshipit-source-id: 7f3ce3f/Lazily initialize RangeDelAggregators map and pinning manager Summary: Since a RangeDelAggregator is created for each read request, these heap-allocating member variables were consuming significant CPU (~3% total) which slowed down request throughput. The map and pinning manager are only necessary when range deletions exist, so we can defer their initialization until the first range deletion is encountered. Currently lazy initialization is done for reads only since reads pass us a single snapshot, which is easier to store on the stack for later insertion into the map than the vector passed to us by flush or compaction. Note the Arena member variable is still expensive, I will figure out what to do with it in a subsequent diff. It cannot be lazily initialized because we currently use this arena even to allocate empty iterators, which is necessary even when no range deletions exist. Closes Differential Revision: D4203488 Pulled By: ajkr fbshipit-source-id: 3b36279/Dynamic max_total_wal_size option Summary: Closes Differential Revision: D4176426 Pulled By: yiwu-arbug fbshipit-source-id: b57689d/"
1362,1362,17.0,0.5964999794960022,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Windows thread Summary: introduce new methods into a public threadpool interface, allow submission of std::functions as they allow greater flexibility. add Joining methods to the implementation to join scheduled and submitted jobs with an option to cancel jobs that did not start executing. Remove ugly `#ifdefs` between pthread and std implementation, make it uniform. introduce pimpl for a drop in replacement of the implementation Introduce rocksdb::port::Thread typedef which is a replacement for std::thread. On Posix Thread defaults as before std::thread. Implement WindowsThread that allocates memory in a more controllable manner than windows std::thread with a replaceable implementation. should be no functionality changes. Closes Differential Revision: D4492902 Pulled By: siying fbshipit-source-id: c74cb11/Fix wrong result in data race case related to Get() Summary: In theory, Get() can get a wrong result, if it races in a special with with flush. The bug can be reproduced in DBTest2.GetRaceFlush. Fix this bug by getting snapshot after referencing the super version. Closes Differential Revision: D4475958 Pulled By: siying fbshipit-source-id: bd9e67a/Add test DBTest2.GetRaceFlush which can expose a data race bug Summary: A current data race issue in Get() and Flush() can cause a Get() to return wrong results when a flush happened in the middle. Disable the test for now. Closes Differential Revision: D4472310 Pulled By: siying fbshipit-source-id: 5755ebd/"
1363,1363,17.0,0.6963000297546387,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Windows thread Summary: introduce new methods into a public threadpool interface, allow submission of std::functions as they allow greater flexibility. add Joining methods to the implementation to join scheduled and submitted jobs with an option to cancel jobs that did not start executing. Remove ugly `#ifdefs` between pthread and std implementation, make it uniform. introduce pimpl for a drop in replacement of the implementation Introduce rocksdb::port::Thread typedef which is a replacement for std::thread. On Posix Thread defaults as before std::thread. Implement WindowsThread that allocates memory in a more controllable manner than windows std::thread with a replaceable implementation. should be no functionality changes. Closes Differential Revision: D4492902 Pulled By: siying fbshipit-source-id: c74cb11/Add EventListener::OnExternalFileIngested() event Summary: Add EventListener::OnExternalFileIngested() to allow user to subscribe to external file ingestion events Closes Differential Revision: D4285844 Pulled By: IslamAbdelRahman fbshipit-source-id: 0b95a88/"
1364,1364,14.0,0.9567999839782715,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Fix bug of Checkpoint loses recent transactions with 2PC Summary: If 2PC is enabled, checkpoint may not copy previous log files that contain uncommitted prepare records. In this diff we keep those files. Closes Differential Revision: D4368319 Pulled By: siying fbshipit-source-id: cc2c746/"
1365,1365,5.0,0.7904000282287598,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Support for range skips in compaction filter Summary: This adds the ability for compaction filter to say ""drop this key-value, and also drop everything up to key x"". This will cause the compaction to seek input iterator to x, without reading the data. This can make compaction much faster when large consecutive chunks of data are filtered out. See the changes in include/rocksdb/compaction_filter.h for the new API. Along the way this diff also adds ability for compaction filter changing merge operands, similar to how it can change values; were not going to use this feature, it just seemed easier and cleaner to implement it than to document that its not implemented :) The diff is not as big as it may seem, about half of the lines are a test. Closes Differential Revision: D4252092 Pulled By: al13n321 fbshipit-source-id: 41e1e48/"
1366,1366,6.0,0.9855999946594238,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Lazily initialize RangeDelAggregators map and pinning manager Summary: Since a RangeDelAggregator is created for each read request, these heap-allocating member variables were consuming significant CPU (~3% total) which slowed down request throughput. The map and pinning manager are only necessary when range deletions exist, so we can defer their initialization until the first range deletion is encountered. Currently lazy initialization is done for reads only since reads pass us a single snapshot, which is easier to store on the stack for later insertion into the map than the vector passed to us by flush or compaction. Note the Arena member variable is still expensive, I will figure out what to do with it in a subsequent diff. It cannot be lazily initialized because we currently use this arena even to allocate empty iterators, which is necessary even when no range deletions exist. Closes Differential Revision: D4203488 Pulled By: ajkr fbshipit-source-id: 3b36279/"
1367,1367,15.0,0.9567999839782715,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Report memory usage by memtable insert hints map. Summary: It is hard to measure acutal memory usage by std containers. Even providing a custom allocator will miss count some of the usage. Here we only do a wild guess on its memory usage. Closes Differential Revision: D4179945 Pulled By: yiwu-arbug fbshipit-source-id: 32ab929/
1368,1368,10.0,0.9855999946594238,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Adding GetApproximateMemTableStats method Summary: Added method that returns approx num of entries as well as size for memtables. Closes Differential Revision: D4511990 Pulled By: VitaliyLi fbshipit-source-id: 9a4576e/Change DB::GetApproximateSizes for more flexibility needed for MyRocks Summary: Added an option to GetApproximateSizes to exclude file stats, as MyRocks has those counted exactly and we need only stats from memtables. Closes Differential Revision: D4441111 Pulled By: IslamAbdelRahman fbshipit-source-id: c11f4c3/Fix rocksdb::Status::getState Summary: This fixes the Java API for Status#getState use in Native code and also simplifies the implementation of rocksdb::Status::getState. Closes Closes Differential Revision: D4364181 Pulled By: yiwu-arbug fbshipit-source-id: 8e073b4/Add WriteOptions.no_slowdown Summary: If the WriteOptions.no_slowdown flag is set AND we need to wait or sleep for the write request, then fail immediately with Status::Incomplete(). Closes Differential Revision: D4191405 Pulled By: maysamyabandeh fbshipit-source-id: 7f3ce3f/"
1369,1369,6.0,0.9854000210762024,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Revert ""PinnableSlice"" Summary: This reverts commit 54d94e9c2cc0bf6eeb2a165ada33fa9c174f0b16. The pull request was landed by mistake. Closes Differential Revision: D4391678 Pulled By: maysamyabandeh fbshipit-source-id: 36d5149/Decouple data iterator and range deletion iterator in TableCache Summary: Previously we used TableCache::NewIterator() for multiple purposes (data block iterator and range deletion iterator), and returned non-ok status in the data block iterator. In one case where the caller only used the range deletion block iterator ( we didnt check/free the data block iterator containing non-ok status, which caused a valgrind error. So, this diff decouples creation of data block and range deletion block iterators, and updates the callers accordingly. Both functions can return non-ok status in an InternalIterator. Since the non-ok status is returned in an iterator that the callers will definitely use, it should be more usable/less error-prone. Closes Differential Revision: D4181423 Pulled By: ajkr fbshipit-source-id: 835b8f5/"
1370,1370,1.0,0.949999988079071,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Fix c_test Summary: addfile phase in c_test could fail because in previous steps we did a DeleteRange. Fix the test by simply moving the addfile phase before DeleteRange Closes Differential Revision: D4328896 Pulled By: IslamAbdelRahman fbshipit-source-id: 1d946df/
1371,1371,19.0,0.5967000126838684,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Follow up for DirectIO refactor Summary: Windows follow up for Differential Revision: D4420337 Pulled By: IslamAbdelRahman fbshipit-source-id: fedc5b5/change UseDirectIO() to use_direct_io() Summary: also change variable name `direct_io_` to `use_direct_io_` in WritableFile to make it consistent with read path. Closes Differential Revision: D4416435 Pulled By: lightmark fbshipit-source-id: 4143c53/Fix Windows environment issues Summary: Enable directIO on WritableFileImpl::Append with offset being current length of the file. Enable UniqueID tests on Windows, disable others but leeting them to compile. Unique tests are valuable to detect failures on different filesystems and upcoming ReFS. Clear output in WinEnv Getchildren.This is different from previous strategy, do not touch output on failure. Make sure DBTest.OpenWhenOpen works with windows error message Closes Differential Revision: D4385681 Pulled By: IslamAbdelRahman fbshipit-source-id: c07b702/direct io write support Summary: rocksdb direct io support ``` ~/rocksdb] ./db_bench Initializing RocksDB Options from the specified file Initializing RocksDB Options from command-line flags RocksDB: version 5.0 Date: Wed Nov 23 13:17:43 2016 CPU: 40 * Intel(R) Xeon(R) CPU E5-2660 v2 2.20GHz CPUCache: 25600 KB Keys: 16 bytes each Values: 100 bytes each (50 bytes after compression) Entries: 1000000 Prefix: 0 bytes Keys per prefix: 0 RawSize: 110.6 MB (estimated) FileSize: 62.9 MB (estimated) Write rate: 0 bytes/second Compression: Snappy Memtablerep: skip_list Perf Level: 1 WARNING: Assertions are enabled; benchmarks unnecessarily slow Initializing RocksDB Options from the specified file Initializing RocksDB Options from command-line flags DB path: [/tmp/rocksdbtest-112628/dbbench] fillseq : 4.393 micros/op 227639 ops/sec; 25.2 MB/s ~/roc Closes Differential Revision: D4241093 Pulled By: lightmark fbshipit-source-id: 98c29e3/"
1372,1372,17.0,0.550000011920929,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Blob storage pr Summary: The final pull request for Blob Storage. Closes Differential Revision: D5033189 Pulled By: yiwu-arbug fbshipit-source-id: 6356b683ccd58cbf38a1dc55e2ea400feecd5d06/Allow IntraL0 compaction in FIFO Compaction Summary: Allow an option for users to do some compaction in FIFO compaction, to pay some write amplification for fewer number of files. Closes Differential Revision: D4895953 Pulled By: siying fbshipit-source-id: a1ab608dd0627211f3e1f588a2e97159646e1231/Add a verify phase to benchmarks Summary: Check the result of the benchmark againt a specified truth_db, which is expected to be produced using the same benchmark but perhaps on a different commit or with different configs. The verification is simple and assumes that key/values are generated deterministically. This assumption would break if db_bench using rand variable differently from the benchmark that produced truth_db. Currently it is checked to work on fillrandom and readwhilewriting. A param finish_after_writes is added to ensure that the background writing thread will write the same number of entries between two benchmarks. Example: $ TEST_TMPDIR=/dev/shm/truth_db ./db_bench $ TEST_TMPDIR=/dev/shm/tmpdb ./db_bench /dev/shm/truth_db/dbbench Verifying db truth_db... Verifying db >= truth_db... ...Verified Closes Differential Revision: D4839233 Pulled By: maysamyabandeh fbshipit-source-id: 2f4ed31/"
1373,1373,2.0,0.6940000057220459,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","tools/check_format_compatible.sh to cover option file loading too Summary: tools/check_format_compatible.sh will check a newer version of RocksDB can open option files generated by older version releases. In order to achieve that, a new parameter ""--try_load_options"" is added to ldb. With this parameter set, if option file exists, we load the option file and use it to open the DB. With this opiton set, we can validate option loading logic. Closes Differential Revision: D4914989 Pulled By: siying fbshipit-source-id: db114f7724fcb41e5e9483116d84d7c4b8389ca4/add checkpoint to ldb Summary: Closes Differential Revision: D4747656 Pulled By: lightmark fbshipit-source-id: c52f160/Add stderr log level for ldb backup commands Summary: Also extracted the common logic into a base class, BackupableCommand. Closes Differential Revision: D4630121 Pulled By: ajkr fbshipit-source-id: 04bb067/"
1374,1374,11.0,0.9735999703407288,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","change use_direct_writes to use_direct_io_for_flush_and_compaction Summary: Replace Options::use_direct_writes with Options::use_direct_io_for_flush_and_compaction Now if Options::use_direct_io_for_flush_and_compaction true, we will enable direct io for both reads and writes for flush and compaction job. Whereas Options::use_direct_reads controls user reads like iterator and Get(). Closes Differential Revision: D4860912 Pulled By: lightmark fbshipit-source-id: d93575a8a5e780cf7e40797287edc425ee648c19/add direct_io and compaction_readahead_size in db_stress Summary: add direct_io and compaction_readahead_size in db_stress test direct_io under db_stress with compaction_readahead_size enabled to capture bugs found in production. `./db_stress Closes Differential Revision: D4604514 Pulled By: IslamAbdelRahman fbshipit-source-id: ebbf0ee/"
1375,1375,11.0,0.7107999920845032,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Blob storage helper methods Summary: Split out interfaces needed for blob storage from including * CompactionEventListener and OnFlushBegin listener interfaces. * Blob filename support. Closes Differential Revision: D4905463 Pulled By: yiwu-arbug fbshipit-source-id: 564e73448f1b7a367e5e46216a521e57ea9011b5/readahead backwards from sst end Summary: prefetch some data from the end of the file for each compaction to reduce IO. Closes Differential Revision: D4880576 Pulled By: lightmark fbshipit-source-id: aa767cd1afc84c541837fbf1ad6c0d45b34d3932/change use_direct_writes to use_direct_io_for_flush_and_compaction Summary: Replace Options::use_direct_writes with Options::use_direct_io_for_flush_and_compaction Now if Options::use_direct_io_for_flush_and_compaction true, we will enable direct io for both reads and writes for flush and compaction job. Whereas Options::use_direct_reads controls user reads like iterator and Get(). Closes Differential Revision: D4860912 Pulled By: lightmark fbshipit-source-id: d93575a8a5e780cf7e40797287edc425ee648c19/"
1376,1376,15.0,0.9842000007629395,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Fix Compilation errors when using IBM Java Summary: PR to fix this issue Closes Differential Revision: D4682411 Pulled By: siying fbshipit-source-id: a519be1/Fixed various memory leaks and Java 8 JNI Compatibility Summary: I have manually audited the entire RocksJava code base. Sorry for the large pull-request, I have broken it down into many small atomic commits though. My initial intention was to fix the warnings that appear when running RocksJava on Java 8 with `-Xcheck:jni`, for example when running `make jtest` you would see many errors similar to: ``` WARNING in native method: JNI call made without checking exceptions when required to from CallObjectMethod WARNING in native method: JNI call made without checking exceptions when required to from CallVoidMethod WARNING in native method: JNI call made without checking exceptions when required to from CallStaticVoidMethod ... ``` A few of those warnings still remain, however they seem to come directly from the JVM and are not directly related to RocksJava; I am in contact with the OpenJDK hostpot-dev mailing list about these As a result of fixing these, I realised we were not r Closes Differential Revision: D4591758 Pulled By: siying fbshipit-source-id: 7f7fdf4/"
1377,1377,15.0,0.9824000000953674,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Fixed various memory leaks and Java 8 JNI Compatibility Summary: I have manually audited the entire RocksJava code base. Sorry for the large pull-request, I have broken it down into many small atomic commits though. My initial intention was to fix the warnings that appear when running RocksJava on Java 8 with `-Xcheck:jni`, for example when running `make jtest` you would see many errors similar to: ``` WARNING in native method: JNI call made without checking exceptions when required to from CallObjectMethod WARNING in native method: JNI call made without checking exceptions when required to from CallVoidMethod WARNING in native method: JNI call made without checking exceptions when required to from CallStaticVoidMethod ... ``` A few of those warnings still remain, however they seem to come directly from the JVM and are not directly related to RocksJava; I am in contact with the OpenJDK hostpot-dev mailing list about these As a result of fixing these, I realised we were not r Closes Differential Revision: D4591758 Pulled By: siying fbshipit-source-id: 7f7fdf4/"
1378,1378,18.0,0.9136000275611877,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Added missing options to RocksJava Summary: This adds almost all missing options to RocksJava Closes Differential Revision: D4779991 Pulled By: siying fbshipit-source-id: 4a1bf28/
1379,1379,4.0,0.864300012588501,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Expose DB::DeleteRange and WriteBath::DeleteRange in Java Summary: Added JNI wrapper from `DeleteRange` methods Closes Differential Revision: D4657746 Pulled By: yiwu-arbug fbshipit-source-id: 3fc7ab8/
1380,1380,18.0,0.9136000275611877,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Added missing options to RocksJava Summary: This adds almost all missing options to RocksJava Closes Differential Revision: D4779991 Pulled By: siying fbshipit-source-id: 4a1bf28/
1381,1381,15.0,0.9824000000953674,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Fixed various memory leaks and Java 8 JNI Compatibility Summary: I have manually audited the entire RocksJava code base. Sorry for the large pull-request, I have broken it down into many small atomic commits though. My initial intention was to fix the warnings that appear when running RocksJava on Java 8 with `-Xcheck:jni`, for example when running `make jtest` you would see many errors similar to: ``` WARNING in native method: JNI call made without checking exceptions when required to from CallObjectMethod WARNING in native method: JNI call made without checking exceptions when required to from CallVoidMethod WARNING in native method: JNI call made without checking exceptions when required to from CallStaticVoidMethod ... ``` A few of those warnings still remain, however they seem to come directly from the JVM and are not directly related to RocksJava; I am in contact with the OpenJDK hostpot-dev mailing list about these As a result of fixing these, I realised we were not r Closes Differential Revision: D4591758 Pulled By: siying fbshipit-source-id: 7f7fdf4/"
1382,1382,18.0,0.9136000275611877,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Added missing options to RocksJava Summary: This adds almost all missing options to RocksJava Closes Differential Revision: D4779991 Pulled By: siying fbshipit-source-id: 4a1bf28/
1383,1383,1.0,0.9847000241279602,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","backup garbage collect shared_checksum tmp files Summary: previously we only cleaned up .tmp files under ""shared/"" and ""private/"" directories in case the previous backup failed. we need to do the same for ""shared_checksum/""; otherwise, the subsequent backup will fail if it tries to backup at least one of the same files. Closes Differential Revision: D4805599 Pulled By: ajkr fbshipit-source-id: eaa6088/Gracefully handle previous backup interrupted Summary: As the last step in backup creation, the .tmp directory is renamed omitting the .tmp suffix. In case the process terminates before this, the .tmp directory will be left behind. Even if this happens, we want future backups to succeed, so I added some checks/cleanup for this case. Closes Differential Revision: D4597323 Pulled By: ajkr fbshipit-source-id: 48900d8/"
1384,1384,19.0,0.9847000241279602,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Pinnableslice (2nd attempt) Summary: PinnableSlice Summary: Currently the point lookup values are copied to a string provided by the user. This incures an extra memcpy cost. This patch allows doing point lookup via a PinnableSlice which pins the source memory location (instead of copying their content) and releases them after the content is consumed by the user. The old API of Get(string) is translated to the new API underneath. Here is the summary for improvements: value 100 byte: 1.8% regular, 1.2% merge values value 1k byte: 11.5% regular, 7.5% merge values value 10k byte: 26% regular, 29.9% merge values The improvement for merge could be more if we extend this approach to pin the merge output and delay the full merge operation until the user actually needs it. We have put that for future work. PS: Sometimes we observe a small decrease in performance when switching from t5452014 to this patch but with the old Get(string) API. The d Closes Differential Revision: D4391738 Pulled By: maysamyabandeh fbshipit-source-id: 6f3edd3/"
1385,1385,4.0,0.5044999718666077,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Blob storage pr Summary: The final pull request for Blob Storage. Closes Differential Revision: D5033189 Pulled By: yiwu-arbug fbshipit-source-id: 6356b683ccd58cbf38a1dc55e2ea400feecd5d06/
1386,1386,15.0,0.7264000177383423,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Builders for partition filter Summary: This is the second split of this pull request: which includes only the builder part. The testing will be included in the third split, where the reader is also included. Closes Differential Revision: D4660272 Pulled By: maysamyabandeh fbshipit-source-id: 36b3cf0/"
1387,1387,4.0,0.9801999926567078,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","do not read next datablock if upperbound is reached Summary: Now if we have iterate_upper_bound set, we continue read until get a key >= upper_bound. For a lot of cases that neighboring data blocks have a user key gap between them, our index key will be a user key in the middle to get a shorter size. For example, if we have blocks: [a b c d][f g h] Then the index key for the first block will be e. then if upper bound is any key between d and e, for example, d1, d2, ..., d99999999999, we dont have to read the second block and also know that we have done our iteration by reaching the last key that smaller the upper bound already. This diff can reduce RA in most cases. Closes Differential Revision: D4990693 Pulled By: lightmark fbshipit-source-id: ab30ea2e3c6edf3fddd5efed3c34fcf7739827ff/"
1388,1388,4.0,0.9934999942779541,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","do not read next datablock if upperbound is reached Summary: Now if we have iterate_upper_bound set, we continue read until get a key >= upper_bound. For a lot of cases that neighboring data blocks have a user key gap between them, our index key will be a user key in the middle to get a shorter size. For example, if we have blocks: [a b c d][f g h] Then the index key for the first block will be e. then if upper bound is any key between d and e, for example, d1, d2, ..., d99999999999, we dont have to read the second block and also know that we have done our iteration by reaching the last key that smaller the upper bound already. This diff can reduce RA in most cases. Closes Differential Revision: D4990693 Pulled By: lightmark fbshipit-source-id: ab30ea2e3c6edf3fddd5efed3c34fcf7739827ff/Revert ""Delete filter before closing the table"" Summary: This reverts commit 89833577a80ad7a2cbf6b99c5957f572b3548152. Closes Differential Revision: D4986982 Pulled By: maysamyabandeh fbshipit-source-id: 56c4c07b7b5b7c6fe122d5c2f2199d221c8510c0/readahead backwards from sst end Summary: prefetch some data from the end of the file for each compaction to reduce IO. Closes Differential Revision: D4880576 Pulled By: lightmark fbshipit-source-id: aa767cd1afc84c541837fbf1ad6c0d45b34d3932/Readers for partition filter Summary: This is the last split of this pull request: which includes the reader part as well as the tests. Closes Differential Revision: D4672216 Pulled By: maysamyabandeh fbshipit-source-id: 6a2b829/Add macros to include file name and line number during Logging Summary: current logging ``` 2017/03/14-14:20:30.393432 7fedde9f5700 (Original Log Time 2017/03/14-14:20:30.393414) [default] Level summary: base level 1 max bytes base 268435456 files[1 0 0 0 0 0 0] max score 0.25 2017/03/14-14:20:30.393438 7fedde9f5700 [JOB 2] Try to delete WAL files size 61417909, prev total WAL file size 73820858, number of live WAL files 2. 2017/03/14-14:20:30.393464 7fedde9f5700 [DEBUG] [JOB 2] Delete /dev/shm/old_logging//MANIFEST-000001 type=3 OK 2017/03/14-14:20:30.393472 7fedde9f5700 [DEBUG] [JOB 2] Delete /dev/shm/old_logging//000003.log type=0 OK 2017/03/14-14:20:31.427103 7fedd49f1700 [default] New memtable created with log file: Immutable memtables: 0. 2017/03/14-14:20:31.427179 7fedde9f5700 [JOB 3] Syncing log 2017/03/14-14:20:31.427190 7fedde9f5700 (Original Log Time 2017/03/14-14:20:31.427170) Calling FlushMemTableToOutputFile with column family [default], flush slots available 1, compaction slots allowed 1, compaction slots scheduled 1 2017/03/14-14:20:31. Closes Differential Revision: D4708695 Pulled By: IslamAbdelRahman fbshipit-source-id: cb8968f/"
1389,1389,4.0,0.9801999926567078,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","do not read next datablock if upperbound is reached Summary: Now if we have iterate_upper_bound set, we continue read until get a key >= upper_bound. For a lot of cases that neighboring data blocks have a user key gap between them, our index key will be a user key in the middle to get a shorter size. For example, if we have blocks: [a b c d][f g h] Then the index key for the first block will be e. then if upper bound is any key between d and e, for example, d1, d2, ..., d99999999999, we dont have to read the second block and also know that we have done our iteration by reaching the last key that smaller the upper bound already. This diff can reduce RA in most cases. Closes Differential Revision: D4990693 Pulled By: lightmark fbshipit-source-id: ab30ea2e3c6edf3fddd5efed3c34fcf7739827ff/"
1390,1390,2.0,0.9869999885559082,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","unbiase readamp bitmap Summary: Consider BlockReadAmpBitmap with bytes_per_bit 32. Suppose bytes [a, b) were used, while bytes [a-32, a) and [b+1, b+33) werent used; more formally, the union of ranges passed to BlockReadAmpBitmap::Mark() contains [a, b) and doesnt intersect with [a-32, a) and [b+1, b+33). Then bits [floor(a/32), ceil(b/32)] will be set, and so the number of useful bytes will be estimated as (ceil(b/32) floor(a/32)) * 32, which is on average equal to b-a+31. An extreme example: if we use 1 byte from each block, itll be counted as 32 bytes from each block. Its easy to remove this bias by slightly changing the semantics of the bitmap. Currently each bit represents a byte range [i*32, (i+1)*32). This diff makes each bit represent a single byte: i*32 + X, where X is a random number in [0, 31] generated when bitmap is created. So, e.g., if you read a single byte at random, with probability 31/32 it wont be counted at all, and with probability 1/32 it will be counted as 32 bytes; so, on average its counted as 1 byte. *But there is one exception: the last bit will always set with the old way.* (*) assuming read_amp_bytes_per_bit 32. Closes Differential Revision: D5035652 Pulled By: lightmark fbshipit-source-id: bd98b1b9b49fbe61f9e3781d07f624e3cbd92356/"
1391,1391,16.0,0.944100022315979,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Object lifetime in cache Summary: Any non-raw-data dependent object must be destructed before the table closes. There was a bug of not doing that for filter object. This patch fixes the bug and adds a unit test to prevent such bugs in future. Closes Differential Revision: D5001318 Pulled By: maysamyabandeh fbshipit-source-id: 6d8772e58765485868094b92964da82ef9730b6d/
1392,1392,4.0,0.970300018787384,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Blob storage helper methods Summary: Split out interfaces needed for blob storage from including * CompactionEventListener and OnFlushBegin listener interfaces. * Blob filename support. Closes Differential Revision: D4905463 Pulled By: yiwu-arbug fbshipit-source-id: 564e73448f1b7a367e5e46216a521e57ea9011b5/[rocksdb][PR] Remove option min_partial_merge_operands and verify_checksums_in_comp Summary: action The two options, min_partial_merge_operands and verify_checksums_in_compaction, are not seldom used. Remove them to reduce the total number of options. Also remove them from Java and C interface. Closes Differential Revision: D4601219 Pulled By: siying fbshipit-source-id: aad4cb2/"
1393,1393,11.0,0.6424999833106995,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Blob storage pr Summary: The final pull request for Blob Storage. Closes Differential Revision: D5033189 Pulled By: yiwu-arbug fbshipit-source-id: 6356b683ccd58cbf38a1dc55e2ea400feecd5d06/Add bulk create/drop column family API Summary: Adding DB::CreateColumnFamilie() and DB::DropColumnFamilies() to bulk create/drop column families. This is to address the problem creating/dropping 1k column families takes minutes. The bottleneck is we persist options files for every single column family create/drop, and it parses the persisted options file for verification, which take a lot CPU time. The new APIs simply create/drop column families individually, and persist options file once at the end. This improves create 1k column families to within ~0.1s. Further improvement can be merge manifest write to one IO. Closes Differential Revision: D5001578 Pulled By: yiwu-arbug fbshipit-source-id: d4e00bda671451e0b314c13e12ad194b1704aa03/support bulk loading with universal compaction Summary: Support buck load with universal compaction. More test cases to be added. Closes Differential Revision: D4935360 Pulled By: lightmark fbshipit-source-id: cc3ca1b6f42faa503207dab1408d6bcf393ee5b5/Add DB:ResetStats() Summary: Add a function to allow users to reset internal stats without restarting the DB. Closes Differential Revision: D4907939 Pulled By: siying fbshipit-source-id: ab2dd85b88aabe9380da7485320a1d460d3e1f68/File level histogram should be printed per CF, not per DB Summary: Currently level histogram is only printed out for DB stats and for default CF. This is confusing. Change to print for every CF instead. Closes Differential Revision: D4865373 Pulled By: siying fbshipit-source-id: 1c853e0ac66e00120ee931cabc9daf69ccc2d577/CMake: more MinGW fixes Summary: siying this is a resubmission of with the 4th commit fixed. From that commit message: > Note that the previous use of quotes in PLATFORM_{CC,CXX}FLAGS was incorrect and caused GCC to produce the incorrect define: > > ROCKSDB_JEMALLOC 1 > > This was the cause of the Linux build failure on the previous version of this change. Ive tested this locally, and the Linux build succeeds now. Closes Differential Revision: D4839964 Pulled By: siying fbshipit-source-id: cc51322/Divide db/db_impl.cc Summary: db_impl.cc is too large to manage. Divide db_impl.cc into db/db_impl.cc, db/db_impl_compaction_flush.cc, db/db_impl_files.cc, db/db_impl_open.cc and db/db_impl_write.cc. Closes Differential Revision: D4838188 Pulled By: siying fbshipit-source-id: c5f3059/Revert ""[rocksdb][PR] CMake: more MinGW fixes"" fbshipit-source-id: 43b4529/CMake: more MinGW fixes Summary: See individual commits. yuslepukhin siying Closes Differential Revision: D4824639 Pulled By: IslamAbdelRahman fbshipit-source-id: 2fc2b00/replace sometimes-undefined uint type with unsigned int Summary: `uint` is nonstandard and not a built-in type on all compilers; replace it with the always-valid `unsigned int`. I assume this went unnoticed because its inside an `#ifdef ROCKDB_JEMALLOC`. Closes Differential Revision: D4820427 Pulled By: ajkr fbshipit-source-id: 0876561/Flush triggered by DB write buffer size picks the oldest unflushed CF Summary: Previously, when DB write buffer size triggers, we always pick the CF with most data in its memtable to flush. This approach can minimize total flush happens. Change the behavior to always pick the oldest unflushed CF, which makes it the same behavior when max_total_wal_size hits. This approach will minimize size used by max_total_wal_size. Closes Differential Revision: D4703214 Pulled By: siying fbshipit-source-id: 9ff8b09/Add macros to include file name and line number during Logging Summary: current logging ``` 2017/03/14-14:20:30.393432 7fedde9f5700 (Original Log Time 2017/03/14-14:20:30.393414) [default] Level summary: base level 1 max bytes base 268435456 files[1 0 0 0 0 0 0] max score 0.25 2017/03/14-14:20:30.393438 7fedde9f5700 [JOB 2] Try to delete WAL files size 61417909, prev total WAL file size 73820858, number of live WAL files 2. 2017/03/14-14:20:30.393464 7fedde9f5700 [DEBUG] [JOB 2] Delete /dev/shm/old_logging//MANIFEST-000001 type=3 OK 2017/03/14-14:20:30.393472 7fedde9f5700 [DEBUG] [JOB 2] Delete /dev/shm/old_logging//000003.log type=0 OK 2017/03/14-14:20:31.427103 7fedd49f1700 [default] New memtable created with log file: Immutable memtables: 0. 2017/03/14-14:20:31.427179 7fedde9f5700 [JOB 3] Syncing log 2017/03/14-14:20:31.427190 7fedde9f5700 (Original Log Time 2017/03/14-14:20:31.427170) Calling FlushMemTableToOutputFile with column family [default], flush slots available 1, compaction slots allowed 1, compaction slots scheduled 1 2017/03/14-14:20:31. Closes Differential Revision: D4708695 Pulled By: IslamAbdelRahman fbshipit-source-id: cb8968f/Set logs as getting flushed before releasing lock, race condition fix Summary: Relating to In MaybeFlushColumnFamilies() we want to modify the getting_flushed flag before releasing the db mutex when SwitchMemtable() is called. The following 2 actions need to be atomic in MaybeFlushColumnFamilies() getting_flushed is false on oldest log we determine that all CFs can be flushed to successfully release oldest log we set getting_flushed true on the oldest log. getting_flushed is false on oldest log we determine that all CFs can NOT be flushed to successfully release oldest log we set unable_to_flush_oldest_log_ true on the oldest log. In the 2pc case: T1 enters function but is unable to flush all CFs to release log T1 sets unable_to_flush_oldest_log_ true T1 begins flushing all CFs possible T2 enters function but is unable to flush all CFs to release log T2 sees unable_to_flush_oldes_log_ has been set so exits T3 enters function and will be able to flush all CFs to release oldest log T3 sets getting_flushed true on oldes Closes Differential Revision: D4646235 Pulled By: reidHoruff fbshipit-source-id: c8d0447/Get unique_ptr to use delete[] for char[] in DumpMallocStats Summary: Avoid mismatched free() / delete / delete [] in DumpMallocStats Closes Differential Revision: D4622045 Pulled By: siying fbshipit-source-id: 1131b30/Fix interference between max_total_wal_size and db_write_buffer_size checks Summary: This is a trivial fix for OOMs weve seen a few days ago in logdevice. RocksDB get into the following state: (1) Write throughput is too high for flushes to keep up. Compactions are out of the picture automatic compactions are disabled, and for manual compactions we dont care that much if they fall behind. We write to many CFs, with only a few L0 sst files in each, so compactions are not needed most of the time. (2) total_log_size_ is consistently greater than GetMaxTotalWalSize(). It doesnt get smaller since flushes are falling ever further behind. (3) Total size of memtables is way above db_write_buffer_size and keeps growing. But the write_buffer_manager_->ShouldFlush() is not checked because (2) prevents it (for no good reason, afaict; this is what this commit fixes). (4) Every call to WriteImpl() hits the MaybeFlushColumnFamilies() path. This keeps flushing the memtables one by one in order of increasing log file number. (5) No write stalling trigger is hit. We rely on max_write_buffer_number Closes Differential Revision: D4593590 Pulled By: yiwu-arbug fbshipit-source-id: af79c5f/Fail IngestExternalFile when bg_error_ exists Summary: Fail IngestExternalFile() when bg_error_ exists Closes Differential Revision: D4580621 Pulled By: IslamAbdelRahman fbshipit-source-id: 1194913/"
1394,1394,13.0,0.6198999881744385,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",support bulk loading with universal compaction Summary: Support buck load with universal compaction. More test cases to be added. Closes Differential Revision: D4935360 Pulled By: lightmark fbshipit-source-id: cc3ca1b6f42faa503207dab1408d6bcf393ee5b5/Remove bulk loading and auto_roll_logger in rocksdb_lite Summary: shrink lite size Closes Differential Revision: D4622059 Pulled By: siying fbshipit-source-id: 050b796/
1395,1395,18.0,0.9821000099182129,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Add C API functions (and tests) for WriteBatchWithIndex Summary: Ive added functions to the C API to support WriteBatchWithIndex as requested in Ive also added unit tests to c_test Ive implemented the WriteBatchWithIndex variation of every function available for regular WriteBatch. And added additional functions unique to WriteBatchWithIndex. For now, the following is omitted: 1. The ability to create WriteBatchWithIndexs custom batch-only iterator as Im not sure what its purpose is. It should be possible to add later if anyone wants it. 2. The ability to create the batch with a fallback comparator, since it appears to be unnecessary. I believe the column family comparator will be used for this, meaning those using a custom comparator can just use the column family variations. Closes Differential Revision: D4760039 Pulled By: siying fbshipit-source-id: 393227e/"
1396,1396,19.0,0.7989000082015991,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Allow IntraL0 compaction in FIFO Compaction Summary: Allow an option for users to do some compaction in FIFO compaction, to pay some write amplification for fewer number of files. Closes Differential Revision: D4895953 Pulled By: siying fbshipit-source-id: a1ab608dd0627211f3e1f588a2e97159646e1231/Refactor compaction picker code Summary: 1. Move universal compaction picker to separate files compaction_picker_universal.cc and compaction_picker_universal.h. 2. Rename some functions to make the code easier to understand. 3. Move leveled compaction picking code to a dedicated class, so that we we dont need to pass some common variable around when calling functions. It also allowed us to break down LevelCompactionPicker::PickCompaction() to smaller functions. Closes Differential Revision: D4845948 Pulled By: siying fbshipit-source-id: efa0ab4/Level-based L0->L0 compaction Summary: Level-based L0->L0 compaction operates on spans of files that arent currently being compacted. It reduces the number of L0 files, thus making write stall conditions harder to reach. L0->L0 is triggered when base level is unavailable due to pending compactions L0->L0 always outputs one file of at most `max_level0_burst_file_size` bytes. Subcompactions are disabled for L0->L0 since we want to output one file. Input files are chosen as the longest span of available files that will fit within the size limit. This minimizes number of files in L0. Closes Differential Revision: D4760318 Pulled By: ajkr fbshipit-source-id: 9d07183/Add macros to include file name and line number during Logging Summary: current logging ``` 2017/03/14-14:20:30.393432 7fedde9f5700 (Original Log Time 2017/03/14-14:20:30.393414) [default] Level summary: base level 1 max bytes base 268435456 files[1 0 0 0 0 0 0] max score 0.25 2017/03/14-14:20:30.393438 7fedde9f5700 [JOB 2] Try to delete WAL files size 61417909, prev total WAL file size 73820858, number of live WAL files 2. 2017/03/14-14:20:30.393464 7fedde9f5700 [DEBUG] [JOB 2] Delete /dev/shm/old_logging//MANIFEST-000001 type=3 OK 2017/03/14-14:20:30.393472 7fedde9f5700 [DEBUG] [JOB 2] Delete /dev/shm/old_logging//000003.log type=0 OK 2017/03/14-14:20:31.427103 7fedd49f1700 [default] New memtable created with log file: Immutable memtables: 0. 2017/03/14-14:20:31.427179 7fedde9f5700 [JOB 3] Syncing log 2017/03/14-14:20:31.427190 7fedde9f5700 (Original Log Time 2017/03/14-14:20:31.427170) Calling FlushMemTableToOutputFile with column family [default], flush slots available 1, compaction slots allowed 1, compaction slots scheduled 1 2017/03/14-14:20:31. Closes Differential Revision: D4708695 Pulled By: IslamAbdelRahman fbshipit-source-id: cb8968f/level compaction expansion Summary: reimplement the compaction expansion on lower level. Considering such a case: input level file: 1[B E] 2[F G] 3[H I] 4 [J M] output level file: 5[A C] 6[D K] 7[L O] If we initially pick file 2, now we will compact file 2 and 6. But we can safely compact 2, 3 and 6 without expanding the output level. The previous code is messy and wrong. In this diff, I first determine the input range [a, b], and output range [c, d], then we get the range [e,f] [min(a, c), max(b, d] and put all eligible clean-cut files within [e, f] into this compaction. **Note: clean-cut means the files dont have the same user key on the boundaries of some files that are not chosen in this compaction**. Closes Differential Revision: D4395564 Pulled By: lightmark fbshipit-source-id: 2dc2c5c/"
1397,1397,11.0,0.9735999703407288,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","File level histogram should be printed per CF, not per DB Summary: Currently level histogram is only printed out for DB stats and for default CF. This is confusing. Change to print for every CF instead. Closes Differential Revision: D4865373 Pulled By: siying fbshipit-source-id: 1c853e0ac66e00120ee931cabc9daf69ccc2d577/Expose the stalling information through DB::GetProperty() Summary: Add two DB properties: rocksdb.actual_delayed_write_rate and rocksdb.is_write_stooped, for people to know whether current writes are being throttled. Closes Differential Revision: D4782975 Pulled By: siying fbshipit-source-id: 6b2f5cf/"
1398,1398,19.0,0.9904999732971191,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Reduce the number of params needed to construct DBIter Summary: DBIter, and in-turn NewDBIterator and NewArenaWrappedDBIterator, take a bunch of params. They can be reduced by passing in ReadOptions directly instead of passing in every new param separately. It also seems much cleaner as a bunch of the params towards the end seem to be optional. (Recently I introduced max_skippable_internal_keys, which added one more to the already huge count). Idea courtesy IslamAbdelRahman Closes Differential Revision: D4857128 Pulled By: sagar0 fbshipit-source-id: 7d239df094b94bd9ea79d145cdf825478ac037a8/Pinnableslice (2nd attempt) Summary: PinnableSlice Summary: Currently the point lookup values are copied to a string provided by the user. This incures an extra memcpy cost. This patch allows doing point lookup via a PinnableSlice which pins the source memory location (instead of copying their content) and releases them after the content is consumed by the user. The old API of Get(string) is translated to the new API underneath. Here is the summary for improvements: value 100 byte: 1.8% regular, 1.2% merge values value 1k byte: 11.5% regular, 7.5% merge values value 10k byte: 26% regular, 29.9% merge values The improvement for merge could be more if we extend this approach to pin the merge output and delay the full merge operation until the user actually needs it. We have put that for future work. PS: Sometimes we observe a small decrease in performance when switching from t5452014 to this patch but with the old Get(string) API. The d Closes Differential Revision: D4391738 Pulled By: maysamyabandeh fbshipit-source-id: 6f3edd3/"
1399,1399,19.0,0.9900000095367432,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Pinnableslice (2nd attempt) Summary: PinnableSlice Summary: Currently the point lookup values are copied to a string provided by the user. This incures an extra memcpy cost. This patch allows doing point lookup via a PinnableSlice which pins the source memory location (instead of copying their content) and releases them after the content is consumed by the user. The old API of Get(string) is translated to the new API underneath. Here is the summary for improvements: value 100 byte: 1.8% regular, 1.2% merge values value 1k byte: 11.5% regular, 7.5% merge values value 10k byte: 26% regular, 29.9% merge values The improvement for merge could be more if we extend this approach to pin the merge output and delay the full merge operation until the user actually needs it. We have put that for future work. PS: Sometimes we observe a small decrease in performance when switching from t5452014 to this patch but with the old Get(string) API. The d Closes Differential Revision: D4391738 Pulled By: maysamyabandeh fbshipit-source-id: 6f3edd3/Statistic for how often rate limiter is drained Summary: This is the metric I plan to use for adaptive rate limiting. The statistics are updated only if the rate limiter is drained by flush or compaction. I believe (but am not certain) that this is the normal case. The Statistics object is passed in RateLimiter::Request() to avoid requiring changes to client code, which wouldve been necessary if we passed it in the RateLimiter constructor. Closes Differential Revision: D4646489 Pulled By: ajkr fbshipit-source-id: d8e0161/"
1400,1400,19.0,0.7613999843597412,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","do not read next datablock if upperbound is reached Summary: Now if we have iterate_upper_bound set, we continue read until get a key >= upper_bound. For a lot of cases that neighboring data blocks have a user key gap between them, our index key will be a user key in the middle to get a shorter size. For example, if we have blocks: [a b c d][f g h] Then the index key for the first block will be e. then if upper bound is any key between d and e, for example, d1, d2, ..., d99999999999, we dont have to read the second block and also know that we have done our iteration by reaching the last key that smaller the upper bound already. This diff can reduce RA in most cases. Closes Differential Revision: D4990693 Pulled By: lightmark fbshipit-source-id: ab30ea2e3c6edf3fddd5efed3c34fcf7739827ff/max_open_files dynamic set, follow up Summary: Followup to make 0x40000 a TableCache constant that indicates infinite capacity Closes Differential Revision: D5001349 Pulled By: lgalanis fbshipit-source-id: ce7bd2e54b0975bb9f8680fdaa0f8bb0e7ae81a2/Level-based L0->L0 compaction Summary: Level-based L0->L0 compaction operates on spans of files that arent currently being compacted. It reduces the number of L0 files, thus making write stall conditions harder to reach. L0->L0 is triggered when base level is unavailable due to pending compactions L0->L0 always outputs one file of at most `max_level0_burst_file_size` bytes. Subcompactions are disabled for L0->L0 since we want to output one file. Input files are chosen as the longest span of available files that will fit within the size limit. This minimizes number of files in L0. Closes Differential Revision: D4760318 Pulled By: ajkr fbshipit-source-id: 9d07183/Pinnableslice (2nd attempt) Summary: PinnableSlice Summary: Currently the point lookup values are copied to a string provided by the user. This incures an extra memcpy cost. This patch allows doing point lookup via a PinnableSlice which pins the source memory location (instead of copying their content) and releases them after the content is consumed by the user. The old API of Get(string) is translated to the new API underneath. Here is the summary for improvements: value 100 byte: 1.8% regular, 1.2% merge values value 1k byte: 11.5% regular, 7.5% merge values value 10k byte: 26% regular, 29.9% merge values The improvement for merge could be more if we extend this approach to pin the merge output and delay the full merge operation until the user actually needs it. We have put that for future work. PS: Sometimes we observe a small decrease in performance when switching from t5452014 to this patch but with the old Get(string) API. The d Closes Differential Revision: D4391738 Pulled By: maysamyabandeh fbshipit-source-id: 6f3edd3/level compaction expansion Summary: reimplement the compaction expansion on lower level. Considering such a case: input level file: 1[B E] 2[F G] 3[H I] 4 [J M] output level file: 5[A C] 6[D K] 7[L O] If we initially pick file 2, now we will compact file 2 and 6. But we can safely compact 2, 3 and 6 without expanding the output level. The previous code is messy and wrong. In this diff, I first determine the input range [a, b], and output range [c, d], then we get the range [e,f] [min(a, c), max(b, d] and put all eligible clean-cut files within [e, f] into this compaction. **Note: clean-cut means the files dont have the same user key on the boundaries of some files that are not chosen in this compaction**. Closes Differential Revision: D4395564 Pulled By: lightmark fbshipit-source-id: 2dc2c5c/Remove disableDataSync option Summary: Remove disableDataSync, and another similarly named disable_data_sync options. This is being done to simplify options, and also because the performance gains of this feature can be achieved by other methods. Closes Differential Revision: D4541292 Pulled By: sagar0 fbshipit-source-id: 5b3a6ca/"
1401,1401,18.0,0.9821000099182129,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Add C API functions (and tests) for WriteBatchWithIndex Summary: Ive added functions to the C API to support WriteBatchWithIndex as requested in Ive also added unit tests to c_test Ive implemented the WriteBatchWithIndex variation of every function available for regular WriteBatch. And added additional functions unique to WriteBatchWithIndex. For now, the following is omitted: 1. The ability to create WriteBatchWithIndexs custom batch-only iterator as Im not sure what its purpose is. It should be possible to add later if anyone wants it. 2. The ability to create the batch with a fallback comparator, since it appears to be unnecessary. I believe the column family comparator will be used for this, meaning those using a custom comparator can just use the column family variations. Closes Differential Revision: D4760039 Pulled By: siying fbshipit-source-id: 393227e/"
1402,1402,4.0,0.9843999743461609,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Remove double buffering on RandomRead on Windows. Summary: Remove double buffering on RandomRead on Windows. With more logic appear in file reader/write Read no longer obeys forwarding calls to Windows implementation. Previously direct_io (unbuffered) was only available on Windows but now is supported as generic. We remove intermediate buffering on Windows. Remove random_access_max_buffer_size option which was windows specific. Non-zero values for that opton introduced unnecessary lock contention. Remove Env::EnableReadAhead(), Env::ShouldForwardRawRequest() that are no longer necessary. Add aligned buffer reads for cases when requested reads exceed read ahead size. Closes Differential Revision: D4847770 Pulled By: siying fbshipit-source-id: 8ab48f8e854ab498a4fd398a6934859792a2788f/"
1403,1403,18.0,0.9876999855041504,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Allow ignoring unknown options when loading options from a file Summary: Added a flag, `ignore_unknown_options`, to skip unknown options when loading an options file (using `LoadLatestOptions`/`LoadOptionsFromFile`) or while verifying options (using `CheckOptionsCompatibility`). This will help in downgrading the db to an older version. Also added `--ignore_unknown_options` flag to ldb **Example Use case:** In MyRocks, if copying from newer version to older version, it is often impossible to start because of new RocksDB options that dont exist in older version, even though data format is compatible. MyRocks uses these load and verify functions in [ha_rocksdb.cc::check_rocksdb_options_compatibility]( **Test Plan:** Updated the unit tests. `make check` ldb: $ ./ldb put a1 b1 OK Now edit /tmp/test_db/<OPTIONS-file> and add an unknown option. Try loading the options now, and it fails: $ ./ldb get a1 Failed: Invalid argument: Unrecognized option DBOptions:: abcd Passes with the new flag $ ./ldb get a1 b1 Closes Differential Revision: D5212091 Pulled By: sagar0 fbshipit-source-id: 2ec17636feb47dc0351b53a77e5f15ef7cbf2ca7/"
1404,1404,13.0,0.970300018787384,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",db_bench to by default verify checksum Summary: Closes Differential Revision: D5417350 Pulled By: siying fbshipit-source-id: 4bc11e35a7256167a5a7d2f586f2ac74c0deddb0/Fixes db_bench with blob db Summary: * Create info log before db open to make blob db able to log to LOG file. * Properly destroy blob db. Closes Differential Revision: D5400034 Pulled By: yiwu-arbug fbshipit-source-id: a49cfaf4b5c67d42d4cbb872bd5a9441828c17ce/Fix db_bench build break with blob db Summary: Lite build does not recognize FLAGS_use_blob_db. Fixing it. Closes Reviewed By: anirbanr-fb Differential Revision: D5130773 Pulled By: yiwu-arbug fbshipit-source-id: 43131d9d0be5811f2129af562be72cca26369cb3/
1405,1405,18.0,0.9876999855041504,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Allow ignoring unknown options when loading options from a file Summary: Added a flag, `ignore_unknown_options`, to skip unknown options when loading an options file (using `LoadLatestOptions`/`LoadOptionsFromFile`) or while verifying options (using `CheckOptionsCompatibility`). This will help in downgrading the db to an older version. Also added `--ignore_unknown_options` flag to ldb **Example Use case:** In MyRocks, if copying from newer version to older version, it is often impossible to start because of new RocksDB options that dont exist in older version, even though data format is compatible. MyRocks uses these load and verify functions in [ha_rocksdb.cc::check_rocksdb_options_compatibility]( **Test Plan:** Updated the unit tests. `make check` ldb: $ ./ldb put a1 b1 OK Now edit /tmp/test_db/<OPTIONS-file> and add an unknown option. Try loading the options now, and it fails: $ ./ldb get a1 Failed: Invalid argument: Unrecognized option DBOptions:: abcd Passes with the new flag $ ./ldb get a1 b1 Closes Differential Revision: D5212091 Pulled By: sagar0 fbshipit-source-id: 2ec17636feb47dc0351b53a77e5f15ef7cbf2ca7/"
1406,1406,10.0,0.9814000129699707,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","fix tsan crash data race Summary: rand_ has data race risk TEST_TMPDIR=\/dev\/shm\/rocksdb OPT=-g COMPILE_WITH_TSAN=1 CRASH_TEST_KILL_ODD=1887 make J=1 crash_test Closes Differential Revision: D5127424 Pulled By: lightmark fbshipit-source-id: b7f4d1430a5769b57da9f99037106749264b2ced/Fix release build on Linux Summary: Release builds are failing on Linux with the error: ``` tools/db_stress.cc: In function int main(int, char**): tools/db_stress.cc:2365:12: error: rocksdb::SyncPoint has not been declared rocksdb::SyncPoint::GetInstance()->SetCallBack( ^ tools/db_stress.cc:2370:12: error: rocksdb::SyncPoint has not been declared rocksdb::SyncPoint::GetInstance()->SetCallBack( ^ tools/db_stress.cc:2375:12: error: rocksdb::SyncPoint has not been declared rocksdb::SyncPoint::GetInstance()->EnableProcessing(); ^ make[1]: *** [tools/db_stress.o] Error 1 make[1]: Leaving directory `/data/sandcastle/boxes/trunk-git-rocksdb-public make: *** [release] Error 2 ``` Closes Differential Revision: D5113552 Pulled By: sagar0 fbshipit-source-id: 351df707277787da5633ba4a40e52edc7c895dc4/disable direct reads for log and manifest and add direct io to tests Summary: Disable direct reads for log and manifest. Direct reads should not affect sequential_file Also add kDirectIO for option_config_ in db_test_util Closes Differential Revision: D5100261 Pulled By: lightmark fbshipit-source-id: 0ebfd13b93fa1b8f9acae514ac44f8125a05868b/"
1407,1407,5.0,0.9861999750137329,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Improve Status message for block checksum mismatches Summary: Weve got some DBs where iterators return Status with message ""Corruption: block checksum mismatch"" all the time. Thats not very informative. It would be much easier to investigate if the error message contained the file name then we would know e.g. how old the corrupted file is, which would be very useful for finding the root cause. This PR adds file name, offset and other stuff to some block corruption-related status messages. It doesnt improve all the error messages, just a few that were easy to improve. Im mostly interested in ""block checksum mismatch"" and ""Bad table magic number"" since theyre the only corruption errors that Ive ever seen in the wild. Closes Differential Revision: D5345702 Pulled By: al13n321 fbshipit-source-id: fc8023d43f1935ad927cef1b9c55481ab3cb1339/"
1408,1408,3.0,0.9635000228881836,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Java APIs for put, merge and delete in file ingestion Summary: Adding SSTFileWriters newly introduced put, merge and delete apis to the Java api. The C++ APIs were first introduced in Add is deprecated in favor of Put. Merge is especially needed to support streaming for Cassandra-on-RocksDB work in Closes Differential Revision: D5165091 Pulled By: sagar0 fbshipit-source-id: 6f0ad396a7cbd2e27ca63e702584784dd72acaab/"
1409,1409,14.0,0.609499990940094,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",Improve the error message for I/O related errors. Summary: Force people to write something other than file name while returning status for IOError. Closes Differential Revision: D5321309 Pulled By: siying fbshipit-source-id: 38bcf6c19e80831cd3e300a047e975cbb131d822/
1410,1410,13.0,0.40720000863075256,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Fix mock_env.cc uninitialized variable Summary: Mingw is complaining about uninitialized variable in mock_env.cc. e.g. The fix is to initialize the variable. Closes Differential Revision: D5211306 Pulled By: yiwu-arbug fbshipit-source-id: ee02bf0327dcea8590a2aa087f0176fecaf8621c/fix travis error with init time in mockenv Summary: /home/travis/build/facebook/rocksdb/env/mock_env.cc: In member function virtual void rocksdb::{anonymous}::TestMemLogger::Logv(const char*, va_list): /home/travis/build/facebook/rocksdb/env/mock_env.cc:391:53: error: t.tm::tm_year may be used uninitialized in this function [-Werror=maybe-uninitialized] static_cast<int>(now_tv.tv_usec)); Closes Differential Revision: D5193597 Pulled By: maysamyabandeh fbshipit-source-id: 8801a3ef27f33eb419d534f7de747702cdf504a0/"
1411,1411,19.0,0.864300012588501,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Revert ""comment out unused parameters"" Summary: This reverts the previous commit 1d7048c5985e60be8e356663ec3cb6d020adb44d, which broke the build. Did a `git revert 1d7048c`. Closes Differential Revision: D5476473 Pulled By: sagar0 fbshipit-source-id: 4756ff5c0dfc88c17eceb00e02c36176de728d06/"
1412,1412,9.0,0.41620001196861267,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Optimize for serial commits in 2PC Summary: Throughput: 46k tps in our sysbench settings (filling the details later) The idea is to have the simplest change that gives us a reasonable boost in 2PC throughput. Major design changes: 1. The WAL file internal buffer is not flushed after each write. Instead it is flushed before critical operations (WAL copy via fs) or when FlushWAL is called by MySQL. Flushing the WAL buffer is also protected via mutex_. 2. Use two sequence numbers: last seq, and last seq for write. Last seq is the last visible sequence number for reads. Last seq for write is the next sequence number that should be used to write to WAL/memtable. This allows to have a memtable write be in parallel to WAL writes. 3. BatchGroup is not used for writes. This means that we can have parallel writers which changes a major assumption in the code base. To accommodate for that i) allow only 1 WriteImpl that intends to write to memtable via mem_mutex_--which is fine since in 2PC almost all of the memtable writes come via group commit phase which is serial anyway, ii) make all the parts in the code base that assumed to be the only writer (via EnterUnbatched) to also acquire mem_mutex_, iii) stat updates are protected via a stat_mutex_. Note: the first commit has the approach figured out but is not clean. Submitting the PR anyway to get the early feedback on the approach. If we are ok with the approach I will go ahead with this updates: 0) Rebase with Yis pipelining changes 1) Currently batching is disabled by default to make sure that it will be consistent with all unit tests. Will make this optional via a config. 2) A couple of unit tests are disabled. They need to be updated with the serial commit of 2PC taken into account. 3) Replacing BatchGroup with mem_mutex_ got a bit ugly as it requires releasing mutex_ beforehand (the same way EnterUnbatched does). This needs to be cleaned up. Closes Differential Revision: D5210732 Pulled By: maysamyabandeh fbshipit-source-id: 78653bd95a35cd1e831e555e0e57bdfd695355a4/"
1413,1413,11.0,0.996999979019165,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Fix blob DB transaction usage while GC Summary: While GC, blob DB use optimistic transaction to delete or replace the index entry in LSM, to guarantee correctness if theres a normal write writing to the same key. However, the previous implementation doesnt call SetSnapshot() nor use GetForUpdate() of transaction API, instead it do its own sequence number checking before beginning the transaction. A normal write can sneak in after the sequence number check and overwrite the key, and the GC will delete or relocate the old version of the key by mistake. Update the code to property use GetForUpdate() to check the existing index entry. After the patch the sequence number store with each blob record is useless, So Im considering remove the sequence number from blob record, in another patch. Closes Differential Revision: D5589178 Pulled By: yiwu-arbug fbshipit-source-id: 8dc960cd5f4e61b36024ba7c32d05584ce149c24/Allow concurrent writes to blob db Summary: Im going with brute-force solution, just letting Put() and Write() holding a mutex before writing. May improve concurrent writing with finer granularity locking later. Closes Differential Revision: D5552690 Pulled By: yiwu-arbug fbshipit-source-id: 039abd675b5d274a7af6428198d1733cafecef4c/Blob DB garbage collection should keep keys with newer version Summary: Fix the bug where if blob db garbage collection revmoe keys with newer version. It shouldnt delete the key from base db when sequence number in base db is not equal to the one in blob log. Closes Differential Revision: D5549752 Pulled By: yiwu-arbug fbshipit-source-id: abb8649260963b5c389748023970fd746279d227/Dump Blob DB options to info log Summary: * Dump blob db options to info log * Remove BlobDBOptionsImpl to disallow dynamic cast *BlobDBOptions into *BlobDBOptionsImpl. Move options there to be constants or into BlobDBOptions. The dynamic cast is broken after * Change some of the default options * Remove blob_db_options.min_blob_size, which is unimplemented. Will implement it soon. Closes Differential Revision: D5529912 Pulled By: yiwu-arbug fbshipit-source-id: dcd58ca981db5bcc7f123b65a0d6f6ae0dc703c7/Blob DB TTL extractor Summary: Introducing blob_db::TTLExtractor to replace extract_ttl_fn. The TTL extractor can be use to extract TTL from keys insert with Put or WriteBatch. Change over existing extract_ttl_fn are: * If value is changed, it will be return via std::string* (rather than Slice*). With Slice* the new value has to be part of the existing value. With std::string* the limitation is removed. * It can optionally return TTL or expiration. Other changes in this PR: * replace `std::chrono::system_clock` with `Env::NowMicros` so that I can mock time in tests. * add several TTL tests. * other minor naming change. Closes Differential Revision: D5512627 Pulled By: yiwu-arbug fbshipit-source-id: 0dfcb00d74d060b8534c6130c808e4d5d0a54440/Fix BlobDB::Get which only get out the value offset Summary: Blob db use StackableDB::get which only get out the value offset, but not the value. Fix by making BlobDB::Get override the designated getter. Closes Differential Revision: D5396823 Pulled By: yiwu-arbug fbshipit-source-id: 5a7d1cf77ee44490f836a6537225955382296878/Implement ReopenWritibaleFile on Windows and other fixes Summary: Make default impl return NoSupported so the db_blob tests exist in a meaningful manner. Replace std::thread to port::Thread Closes Differential Revision: D5275563 Pulled By: yiwu-arbug fbshipit-source-id: cedf1a18a2c05e20d768c1308b3f3224dbd70ab6/Update blob_db_test Summary: Im trying to improve unit test of blob db. Im rewriting blob db test. In this patch: * Rewrite tests of basic put/write/delete operations. * Add disable_background_tasks to BlobDBOptionsImpl to allow me not running any background job for basic unit tests. * Move DestroyBlobDB out from BlobDBImpl to be a standalone function. * Remove all garbage collection related tests. Will rewrite them in following patch. * Disabled compression test since it is failing. Will fix in a followup patch. Closes Differential Revision: D5243306 Pulled By: yiwu-arbug fbshipit-source-id: 157c71ad3b699307cb88baa3830e9b6e74f8e939/write exact sequence number for each put in write batch Summary: At the beginning of write batch write, grab the latest sequence from base db and assume sequence number will increment by 1 for each put and delete, and write the exact sequence number with each put. This is assuming we are the only writer to increment sequence number (no external file ingestion, etc) and there should be no holes in the sequence number. Also having some minor naming changes. Closes Differential Revision: D5176134 Pulled By: yiwu-arbug fbshipit-source-id: cb4712ee44478d5a2e5951213a10b72f08fe8c88/Fixing blob db sequence number handling Summary: Blob db rely on base db returning sequence number through write batch after DB::Write(). However after recent changes to the write path, DB::Writ()e no longer return sequence number in some cases. Fixing it by have WriteBatchInternal::InsertInto() always encode sequence number into write batch. Stacking on Closes Differential Revision: D5148358 Pulled By: yiwu-arbug fbshipit-source-id: 8bda0aa07b9334ed03ed381548b39d167dc20c33/update blob_db_test Summary: Re-enable blob_db_test with some update: * Commented out delay at the end of GC tests. Will update the logic later with sync point to properly trigger GC. * Added some helper functions. Also update make files to include blob_dump tool. Closes Differential Revision: D5133793 Pulled By: yiwu-arbug fbshipit-source-id: 95470b26d0c1f9592ba4b7637e027fdd263f425c/"
1414,1414,19.0,0.8118000030517578,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Revert ""comment out unused parameters"" Summary: This reverts the previous commit 1d7048c5985e60be8e356663ec3cb6d020adb44d, which broke the build. Did a `git revert 1d7048c`. Closes Differential Revision: D5476473 Pulled By: sagar0 fbshipit-source-id: 4756ff5c0dfc88c17eceb00e02c36176de728d06/Simple blob file dumper Summary: A simple blob file dumper. Closes Differential Revision: D5097553 Pulled By: yiwu-arbug fbshipit-source-id: c6e00d949fcd3658f9f68da9352f06339fac418d/"
1415,1415,11.0,0.9979000091552734,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Fix blob DB transaction usage while GC Summary: While GC, blob DB use optimistic transaction to delete or replace the index entry in LSM, to guarantee correctness if theres a normal write writing to the same key. However, the previous implementation doesnt call SetSnapshot() nor use GetForUpdate() of transaction API, instead it do its own sequence number checking before beginning the transaction. A normal write can sneak in after the sequence number check and overwrite the key, and the GC will delete or relocate the old version of the key by mistake. Update the code to property use GetForUpdate() to check the existing index entry. After the patch the sequence number store with each blob record is useless, So Im considering remove the sequence number from blob record, in another patch. Closes Differential Revision: D5589178 Pulled By: yiwu-arbug fbshipit-source-id: 8dc960cd5f4e61b36024ba7c32d05584ce149c24/Avoid blob db call Sync() while writing Summary: The FsyncFiles background job call Fsync() periodically for blob files. However it can access WritableFileWriter concurrently with a Put() or Write(). And WritableFileWriter does not support concurrent access. It will lead to WritableFileWriter buffer being flush with same content twice, and blob file end up corrupted. Fixing by simply let FsyncFiles hold write_mutex_. Closes Differential Revision: D5561908 Pulled By: yiwu-arbug fbshipit-source-id: f0bb5bcab0e05694e053b8c49eab43640721e872/Update all blob db TTL and timestamps to uint64_t Summary: The current blob db implementation use mix of int32_t, uint32_t and uint64_t for TTL and expiration. Update all timestamps to uint64_t for consistency. Closes Differential Revision: D5557103 Pulled By: yiwu-arbug fbshipit-source-id: e4eab2691629a755e614e8cf1eed9c3a681d0c42/Allow concurrent writes to blob db Summary: Im going with brute-force solution, just letting Put() and Write() holding a mutex before writing. May improve concurrent writing with finer granularity locking later. Closes Differential Revision: D5552690 Pulled By: yiwu-arbug fbshipit-source-id: 039abd675b5d274a7af6428198d1733cafecef4c/Blob DB garbage collection should keep keys with newer version Summary: Fix the bug where if blob db garbage collection revmoe keys with newer version. It shouldnt delete the key from base db when sequence number in base db is not equal to the one in blob log. Closes Differential Revision: D5549752 Pulled By: yiwu-arbug fbshipit-source-id: abb8649260963b5c389748023970fd746279d227/Dump Blob DB options to info log Summary: * Dump blob db options to info log * Remove BlobDBOptionsImpl to disallow dynamic cast *BlobDBOptions into *BlobDBOptionsImpl. Move options there to be constants or into BlobDBOptions. The dynamic cast is broken after * Change some of the default options * Remove blob_db_options.min_blob_size, which is unimplemented. Will implement it soon. Closes Differential Revision: D5529912 Pulled By: yiwu-arbug fbshipit-source-id: dcd58ca981db5bcc7f123b65a0d6f6ae0dc703c7/Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/Blob DB TTL extractor Summary: Introducing blob_db::TTLExtractor to replace extract_ttl_fn. The TTL extractor can be use to extract TTL from keys insert with Put or WriteBatch. Change over existing extract_ttl_fn are: * If value is changed, it will be return via std::string* (rather than Slice*). With Slice* the new value has to be part of the existing value. With std::string* the limitation is removed. * It can optionally return TTL or expiration. Other changes in this PR: * replace `std::chrono::system_clock` with `Env::NowMicros` so that I can mock time in tests. * add several TTL tests. * other minor naming change. Closes Differential Revision: D5512627 Pulled By: yiwu-arbug fbshipit-source-id: 0dfcb00d74d060b8534c6130c808e4d5d0a54440/Revert ""comment out unused parameters"" Summary: This reverts the previous commit 1d7048c5985e60be8e356663ec3cb6d020adb44d, which broke the build. Did a `git revert 1d7048c`. Closes Differential Revision: D5476473 Pulled By: sagar0 fbshipit-source-id: 4756ff5c0dfc88c17eceb00e02c36176de728d06/Reduce blob db noisy logging Summary: Remove some of the per-key logging by blob db to reduce noise. Closes Differential Revision: D5429115 Pulled By: yiwu-arbug fbshipit-source-id: b89328282fb8b3c64923ce48738c16017ce7feaf/Update blob db to use ROCKS_LOG_* macro Summary: Update blob db to use the newer ROCKS_LOG_* macro. Closes Differential Revision: D5414526 Pulled By: yiwu-arbug fbshipit-source-id: e428753aa5917e8b435cead2db26df586e5d1def/Fix BlobDB::Get which only get out the value offset Summary: Blob db use StackableDB::get which only get out the value offset, but not the value. Fix by making BlobDB::Get override the designated getter. Closes Differential Revision: D5396823 Pulled By: yiwu-arbug fbshipit-source-id: 5a7d1cf77ee44490f836a6537225955382296878/Make ""make analyze"" happy Summary: ""make analyze"" is reporting some errors. Its complicated to look but it seems to me that they are all false positive. Anyway, I think cleaning them up is a good idea. Some of the changes are hacky but I dont know a better way. Closes Differential Revision: D5341710 Pulled By: siying fbshipit-source-id: 6070e430e0e41a080ef441e05e8ec827d45efab6/Fix blob db compression bug Summary: `CompressBlock()` will return the uncompressed slice (i.e. `Slice(value_unc)`) if compression ratio is not good enough. This is undesired. We need to always assign the compressed slice to `value`. Closes Differential Revision: D5244682 Pulled By: yiwu-arbug fbshipit-source-id: 6989dd8852c9622822ba9acec9beea02007dff09/Update blob_db_test Summary: Im trying to improve unit test of blob db. Im rewriting blob db test. In this patch: * Rewrite tests of basic put/write/delete operations. * Add disable_background_tasks to BlobDBOptionsImpl to allow me not running any background job for basic unit tests. * Move DestroyBlobDB out from BlobDBImpl to be a standalone function. * Remove all garbage collection related tests. Will rewrite them in following patch. * Disabled compression test since it is failing. Will fix in a followup patch. Closes Differential Revision: D5243306 Pulled By: yiwu-arbug fbshipit-source-id: 157c71ad3b699307cb88baa3830e9b6e74f8e939/write exact sequence number for each put in write batch Summary: At the beginning of write batch write, grab the latest sequence from base db and assume sequence number will increment by 1 for each put and delete, and write the exact sequence number with each put. This is assuming we are the only writer to increment sequence number (no external file ingestion, etc) and there should be no holes in the sequence number. Also having some minor naming changes. Closes Differential Revision: D5176134 Pulled By: yiwu-arbug fbshipit-source-id: cb4712ee44478d5a2e5951213a10b72f08fe8c88/Fix clang errors by asserting the precondition Summary: USE_CLANG=1 make analyze The two errors would disappear after the assertion. Closes Differential Revision: D5193526 Pulled By: maysamyabandeh fbshipit-source-id: 16a21f18f68023f862764dd3ab9e00ca60b0eefa/Fixing blob db sequence number handling Summary: Blob db rely on base db returning sequence number through write batch after DB::Write(). However after recent changes to the write path, DB::Writ()e no longer return sequence number in some cases. Fixing it by have WriteBatchInternal::InsertInto() always encode sequence number into write batch. Stacking on Closes Differential Revision: D5148358 Pulled By: yiwu-arbug fbshipit-source-id: 8bda0aa07b9334ed03ed381548b39d167dc20c33/"
1416,1416,5.0,0.9861999750137329,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Improve Status message for block checksum mismatches Summary: Weve got some DBs where iterators return Status with message ""Corruption: block checksum mismatch"" all the time. Thats not very informative. It would be much easier to investigate if the error message contained the file name then we would know e.g. how old the corrupted file is, which would be very useful for finding the root cause. This PR adds file name, offset and other stuff to some block corruption-related status messages. It doesnt improve all the error messages, just a few that were easy to improve. Im mostly interested in ""block checksum mismatch"" and ""Bad table magic number"" since theyre the only corruption errors that Ive ever seen in the wild. Closes Differential Revision: D5345702 Pulled By: al13n321 fbshipit-source-id: fc8023d43f1935ad927cef1b9c55481ab3cb1339/"
1417,1417,11.0,0.4357999861240387,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Gcc 7 ignored quantifiers Summary: The casting seemed to cause a problem. I think this might increase it to unsigned long. Closes Differential Revision: D5406842 Pulled By: siying fbshipit-source-id: 736adef31448229a58a1a48bdbe77792f36736e8/Fix clang error in PartitionedFilterBlockBuilder Summary: Closes Differential Revision: D5371271 Pulled By: maysamyabandeh fbshipit-source-id: f1355ac658a79c9982a24986f0925c9e24fc39d5/Cut filter partition based on metadata_block_size Summary: Currently metadata_block_size controls only index partition size. With this patch a partition is cut after any of index or filter partitions reaches metadata_block_size. Closes Differential Revision: D5275651 Pulled By: maysamyabandeh fbshipit-source-id: 5057e4424b4c8902043782e6bf8c38f0c4f25160/FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/record index partition properties Summary: When Partitioning index/filter is enabled the user might need to check the index block size as well as the top-level index size via sst_dump. This patch records i) number of partitions, ii) top-level index size and make it accessible through sst_dump. The number of partitions for filters is the same as that of indexes. The top-level index for filters has a similar size to top-level index for indexes, so it is not repeated. Closes Differential Revision: D5224225 Pulled By: maysamyabandeh fbshipit-source-id: 5324598c75793523aef1bb7ee225a5475e95a9cb/"
1418,1418,4.0,0.9855999946594238,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Support prefetch last 512KB with direct I/O in block based file reader Summary: Right now, if direct I/O is enabled, prefetching the last 512KB cannot be applied, except compaction inputs or readahead is enabled for iterators. This can create a lot of I/O for HDD cases. To solve the problem, the 512KB is prefetched in block based table if direct I/O is enabled. The prefetched buffer is passed in totegher with random access file reader, so that we try to read from the buffer before reading from the file. This can be extended in the future to support flexible user iterator readahead too. Closes Differential Revision: D5593091 Pulled By: siying fbshipit-source-id: ee36ff6d8af11c312a2622272b21957a7b5c81e7/add VerifyChecksum() to db.h Summary: We need a tool to check any sst file corruption in the db. It will check all the sst files in current version and read all the blocks (data, meta, index) with checksum verification. If any verification fails, the function will return non-OK status. Closes Differential Revision: D5324269 Pulled By: lightmark fbshipit-source-id: 6f8a272008b722402a772acfc804524c9d1a483b/remove unnecessary internal_comparator param in newIterator Summary: solved Closes Differential Revision: D5504875 Pulled By: lightmark fbshipit-source-id: c14bb62ccbdc9e7bda9cd914cae4ea0765d882ee/"
1419,1419,4.0,0.9789000153541565,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Support prefetch last 512KB with direct I/O in block based file reader Summary: Right now, if direct I/O is enabled, prefetching the last 512KB cannot be applied, except compaction inputs or readahead is enabled for iterators. This can create a lot of I/O for HDD cases. To solve the problem, the 512KB is prefetched in block based table if direct I/O is enabled. The prefetched buffer is passed in totegher with random access file reader, so that we try to read from the buffer before reading from the file. This can be extended in the future to support flexible user iterator readahead too. Closes Differential Revision: D5593091 Pulled By: siying fbshipit-source-id: ee36ff6d8af11c312a2622272b21957a7b5c81e7/"
1420,1420,3.0,0.5286999940872192,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size",fix asan and valgrind leak report in test Summary: Closes Differential Revision: D5371433 Pulled By: maysamyabandeh fbshipit-source-id: 90d3e8bb1a8576f48b1ddf1bdbba5512b5986ba0/Fix clang error in PartitionedFilterBlockBuilder Summary: Closes Differential Revision: D5371271 Pulled By: maysamyabandeh fbshipit-source-id: f1355ac658a79c9982a24986f0925c9e24fc39d5/Cut filter partition based on metadata_block_size Summary: Currently metadata_block_size controls only index partition size. With this patch a partition is cut after any of index or filter partitions reaches metadata_block_size. Closes Differential Revision: D5275651 Pulled By: maysamyabandeh fbshipit-source-id: 5057e4424b4c8902043782e6bf8c38f0c4f25160/
1421,1421,14.0,0.977400004863739,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Replace dynamic_cast<> Summary: Replace dynamic_cast<> so that users can choose to build with RTTI off, so that they can save several bytes per object, and get tiny more memory available. Some nontrivial changes: 1. Add Comparator::GetRootComparator() to get around the internal comparator hack 2. Add the two experiemental functions to DB 3. Add TableFactory::GetOptionString() to avoid unnecessary casting to get the option string 4. Since 3 is done, move the parsing option functions for table factory to table factory files too, to be symmetric. Closes Differential Revision: D5502723 Pulled By: siying fbshipit-source-id: fd13cec5601cf68a554d87bfcf056f2ffa5fbf7c/"
1422,1422,6.0,0.5699999928474426,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Revert ""comment out unused parameters"" Summary: This reverts the previous commit 1d7048c5985e60be8e356663ec3cb6d020adb44d, which broke the build. Did a `git revert 1d7048c`. Closes Differential Revision: D5476473 Pulled By: sagar0 fbshipit-source-id: 4756ff5c0dfc88c17eceb00e02c36176de728d06/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
1423,1423,11.0,0.7731999754905701,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","add VerifyChecksum() to db.h Summary: We need a tool to check any sst file corruption in the db. It will check all the sst files in current version and read all the blocks (data, meta, index) with checksum verification. If any verification fails, the function will return non-OK status. Closes Differential Revision: D5324269 Pulled By: lightmark fbshipit-source-id: 6f8a272008b722402a772acfc804524c9d1a483b/Introduce bottom-pri thread pool for large universal compactions Summary: When we had a single thread pool for compactions, a thread could be busy for a long time (minutes) executing a compaction involving the bottom level. In multi-instance setups, the entire thread pool could be consumed by such bottom-level compactions. Then, top-level compactions (e.g., a few L0 files) would be blocked for a long time (""head-of-line blocking""). Such top-level compactions are critical to prevent compaction stalls as they can quickly reduce number of L0 files / sorted runs. This diff introduces a bottom-priority queue for universal compactions including the bottom level. This alleviates the head-of-line blocking situation for fast, top-level compactions. Added `Env::Priority::BOTTOM` thread pool. This feature is only enabled if user explicitly configures it to have a positive number of threads. Changed `ThreadPoolImpl`s default thread limit from one to zero. This change is invisible to users as we call `IncBackgroundThreadsIfNeeded` on the low-pri/high-pri pools during `DB::Open` with values of at least one. It is necessary, though, for bottom-pri to start with zero threads so the feature is disabled by default. Separated `ManualCompaction` into two parts in `PrepickedCompaction`. `PrepickedCompaction` is used for any compaction thats picked outside of its execution thread, either manual or automatic. Forward universal compactions involving last level to the bottom pool (worker threads entry point is `BGWorkBottomCompaction`). Track `bg_bottom_compaction_scheduled_` so we can wait for bottom-level compactions to finish. We dont count them against the background jobs limits. So users of this feature will get an extra compaction for free. Closes Differential Revision: D5422916 Pulled By: ajkr fbshipit-source-id: a74bd11f1ea4933df3739b16808bb21fcd512333/Add Iterator::Refresh() Summary: Add and implement Iterator::Refresh(). When this function is called, if the super version doesnt change, update the sequence number of the iterator to the latest one and invalidate the iterator. If the super version changed, recreated the whole iterator. This can help users reuse the iterator more easily. Closes Differential Revision: D5464500 Pulled By: siying fbshipit-source-id: f548bd35e85c1efca2ea69273802f6704eba6ba9/Optimize for serial commits in 2PC Summary: Throughput: 46k tps in our sysbench settings (filling the details later) The idea is to have the simplest change that gives us a reasonable boost in 2PC throughput. Major design changes: 1. The WAL file internal buffer is not flushed after each write. Instead it is flushed before critical operations (WAL copy via fs) or when FlushWAL is called by MySQL. Flushing the WAL buffer is also protected via mutex_. 2. Use two sequence numbers: last seq, and last seq for write. Last seq is the last visible sequence number for reads. Last seq for write is the next sequence number that should be used to write to WAL/memtable. This allows to have a memtable write be in parallel to WAL writes. 3. BatchGroup is not used for writes. This means that we can have parallel writers which changes a major assumption in the code base. To accommodate for that i) allow only 1 WriteImpl that intends to write to memtable via mem_mutex_--which is fine since in 2PC almost all of the memtable writes come via group commit phase which is serial anyway, ii) make all the parts in the code base that assumed to be the only writer (via EnterUnbatched) to also acquire mem_mutex_, iii) stat updates are protected via a stat_mutex_. Note: the first commit has the approach figured out but is not clean. Submitting the PR anyway to get the early feedback on the approach. If we are ok with the approach I will go ahead with this updates: 0) Rebase with Yis pipelining changes 1) Currently batching is disabled by default to make sure that it will be consistent with all unit tests. Will make this optional via a config. 2) A couple of unit tests are disabled. They need to be updated with the serial commit of 2PC taken into account. 3) Replacing BatchGroup with mem_mutex_ got a bit ugly as it requires releasing mutex_ beforehand (the same way EnterUnbatched does). This needs to be cleaned up. Closes Differential Revision: D5210732 Pulled By: maysamyabandeh fbshipit-source-id: 78653bd95a35cd1e831e555e0e57bdfd695355a4/Support ingest_behind for IngestExternalFile Summary: First cut for early review; there are few conceptual points to answer and some code structure issues. For conceptual points restriction-wise, were going to disallow ingest_behind if (use_seqno_zero_out=true || disable_auto_compaction=false), the user is responsible to properly open and close DB with required params we wanted to ingest into reserved bottom most level. Should we fail fast if bottom level isnt empty, or should we attempt to ingest if file fits there key-ranges-wise? Modifying AssignLevelForIngestedFile seems the place we wed handle that. On code structure going to refactor GenerateAndAddExternalFile call in the test class to allow passing instance of IngestionOptions, thats just going to incur lots of changes at callsites. Closes Differential Revision: D4873732 Pulled By: lightmark fbshipit-source-id: 81cb698106b68ef8797f564453651d50900e153a/"
1424,1424,10.0,0.550599992275238,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Encryption at rest support Summary: This PR adds support for encrypting data stored by RocksDB when written to disk. It adds an `EncryptedEnv` override of the `Env` class with matching overrides for sequential&random access files. The encryption itself is done through a configurable `EncryptionProvider`. This class creates is asked to create `BlockAccessCipherStream` for a file. This is where the actual encryption/decryption is being done. Currently there is a Counter mode implementation of `BlockAccessCipherStream` with a `ROT13` block cipher (NOTE the `ROT13` is for demo purposes only). The Counter operation mode uses an initial counter & random initialization vector (IV). Both are created randomly for each file and stored in a 4K (default size) block that is prefixed to that file. The `EncryptedEnv` implementation is such that clients of the `Env` class do not see this prefix (nor data, nor in filesize). The largest part of the prefix block is also encrypted, and there is room left for implementation specific settings/values/keys in there. To test the encryption, the `DBTestBase` class has been extended to consider a new environment variable called `ENCRYPTED_ENV`. If set, the test will setup a encrypted instance of the `Env` class to use for all tests. Typically you would run it like this: ``` ENCRYPTED_ENV=1 make check_some ``` There is also an added test that checks that some data inserted into the database is or is not ""visible"" on disk. With `ENCRYPTED_ENV` active it must not find plain text strings, with `ENCRYPTED_ENV` unset, it must find the plain text strings. Closes Differential Revision: D5322178 Pulled By: sdwilsh fbshipit-source-id: 253b0a9c2c498cc98f580df7f2623cbf7678a27f/Call RateLimiter for compaction reads Summary: Allow users to rate limit background work based on read bytes, written bytes, or sum of read and written bytes. Support these by changing the RateLimiter API, so no additional options were needed. Closes Differential Revision: D5216946 Pulled By: ajkr fbshipit-source-id: aec57a8357dbb4bfde2003261094d786d94f724e/WriteOptions.low_pri which can throttle low pri writes if needed Summary: If ReadOptions.low_pri=true and compaction is behind, the write will either return immediate or be slowed down based on ReadOptions.no_slowdown. Closes Differential Revision: D5127619 Pulled By: siying fbshipit-source-id: d30e1cff515890af0eff32dfb869d2e4c9545eb0/Improve write buffer manager (and allow the size to be tracked in block cache) Summary: Improve write buffer manager in several ways: 1. Size is tracked when arena block is allocated, rather than every allocation, so that it can better track actual memory usage and the tracking overhead is slightly lower. 2. We start to trigger memtable flush when 7/8 of the memory cap hits, instead of 100%, and make 100% much harder to hit. 3. Allow a cache object to be passed into buffer manager and the size allocated by memtable can be costed there. This can help users have one single memory cap across block cache and memtable. Closes Differential Revision: D5110648 Pulled By: siying fbshipit-source-id: b4238113094bf22574001e446b5d88523ba00017/"
1425,1425,11.0,0.7853999733924866,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Encryption at rest support Summary: This PR adds support for encrypting data stored by RocksDB when written to disk. It adds an `EncryptedEnv` override of the `Env` class with matching overrides for sequential&random access files. The encryption itself is done through a configurable `EncryptionProvider`. This class creates is asked to create `BlockAccessCipherStream` for a file. This is where the actual encryption/decryption is being done. Currently there is a Counter mode implementation of `BlockAccessCipherStream` with a `ROT13` block cipher (NOTE the `ROT13` is for demo purposes only). The Counter operation mode uses an initial counter & random initialization vector (IV). Both are created randomly for each file and stored in a 4K (default size) block that is prefixed to that file. The `EncryptedEnv` implementation is such that clients of the `Env` class do not see this prefix (nor data, nor in filesize). The largest part of the prefix block is also encrypted, and there is room left for implementation specific settings/values/keys in there. To test the encryption, the `DBTestBase` class has been extended to consider a new environment variable called `ENCRYPTED_ENV`. If set, the test will setup a encrypted instance of the `Env` class to use for all tests. Typically you would run it like this: ``` ENCRYPTED_ENV=1 make check_some ``` There is also an added test that checks that some data inserted into the database is or is not ""visible"" on disk. With `ENCRYPTED_ENV` active it must not find plain text strings, with `ENCRYPTED_ENV` unset, it must find the plain text strings. Closes Differential Revision: D5322178 Pulled By: sdwilsh fbshipit-source-id: 253b0a9c2c498cc98f580df7f2623cbf7678a27f/Fixing blob db sequence number handling Summary: Blob db rely on base db returning sequence number through write batch after DB::Write(). However after recent changes to the write path, DB::Writ()e no longer return sequence number in some cases. Fixing it by have WriteBatchInternal::InsertInto() always encode sequence number into write batch. Stacking on Closes Differential Revision: D5148358 Pulled By: yiwu-arbug fbshipit-source-id: 8bda0aa07b9334ed03ed381548b39d167dc20c33/disable direct reads for log and manifest and add direct io to tests Summary: Disable direct reads for log and manifest. Direct reads should not affect sequential_file Also add kDirectIO for option_config_ in db_test_util Closes Differential Revision: D5100261 Pulled By: lightmark fbshipit-source-id: 0ebfd13b93fa1b8f9acae514ac44f8125a05868b/"
1426,1426,10.0,0.9825999736785889,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","expose set_skip_stats_update_on_db_open to C bindings Summary: It would be super helpful to not have to recompile rocksdb to get this performance tweak for mechanical disks. I have signed the CLA. Closes Differential Revision: D5606994 Pulled By: yiwu-arbug fbshipit-source-id: c05e92bad0d03bd38211af1e1ced0d0d1e02f634/Added db paths to c Summary: Closes Differential Revision: D5476064 Pulled By: sagar0 fbshipit-source-id: 6b30a9eacb93a945bbe499eafb90565fa9f1798b/add Transactions and Checkpoint to C API Summary: Ive added functions to the C API to support Transactions as requested in and to support Checkpoint. I have also added the corresponding tests to c_test.c For now, the following is omitted: 1. Optimistic Transactions 2. The column family variation of functions Closes Differential Revision: D4989510 Pulled By: yiwu-arbug fbshipit-source-id: 518cb39f76d5e9ec9690d633fcdc014b98958071/C API: support pinnable get Summary: Closes Differential Revision: D5053590 Pulled By: yiwu-arbug fbshipit-source-id: 2f365a031b3a2947b4fba21d26d4f8f52af9b9f0/"
1427,1427,10.0,0.8039000034332275,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/"
1428,1428,10.0,0.8192999958992004,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/"
1429,1429,10.0,0.6877999901771545,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","enable PinnableSlice for RowCache Summary: This patch enables using PinnableSlice for RowCache, changes include not releasing the cache handle immediately after lookup in TableCache::Get, instead pass a Cleanble function which does Cache::RleaseHandle. Closes Differential Revision: D5316216 Pulled By: maysamyabandeh fbshipit-source-id: d2a684bd7e4ba73772f762e58a82b5f4fbd5d362/Temporarily disable FIFOCompactionWithTTLTest Summary: FIFOCompactionWithTTLTests are flaky when run in parallel, as there is a time element involved to it. Temporarily disabling them while I investigate a more robust testing solution like, say, mocking time. Closes Differential Revision: D5386084 Pulled By: sagar0 fbshipit-source-id: 262886b25bdf091021d8553e780443a985e9bac4/FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/"
1430,1430,13.0,0.9801999926567078,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Introduce OnBackgroundError callback Summary: Some users want to prevent rocksdb from entering read-only mode in certain error cases. This diff gives them a callback, `OnBackgroundError`, that they can use to achieve it. call `OnBackgroundError` every time we consider setting `bg_error_`. Use its result to assign `bg_error_` but not to change the functions return status. classified calls using `BackgroundErrorReason` to give the callback some info about where the error happened renamed `ParanoidCheck` to something more specific so we can provide a clear `BackgroundErrorReason` unit tests for the most common cases: flush or compaction errors Closes Differential Revision: D5300190 Pulled By: ajkr fbshipit-source-id: a0ea4564249719b83428e3f4c6ca2c49e366e9b3/"
1431,1431,10.0,0.6646000146865845,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","fix asan/valgrind for TableCache cleanup Summary: Breaking commit: d12691b86fb788f0ee7180db626c4ea2445fa976 In the above commit, I moved the `TableCache` cleanup logic from `Version` destructor into `PurgeObsoleteFiles`. I missed cleaning up `TableCache` entries for the current `Version` during DB destruction. This PR adds that logic to `VersionSet` destructor. One unfortunate side effect is now were potentially deleting `TableReader`s after `column_family_set_.reset()`, which means we cant call `BlockBasedTableReader::Close` a second time as the block cache might already be destroyed. Closes Differential Revision: D5515108 Pulled By: ajkr fbshipit-source-id: 2cb820e19aa813e0d258d17f76b2d7b6b7ee0b18/Revert ""comment out unused parameters"" Summary: This reverts the previous commit 1d7048c5985e60be8e356663ec3cb6d020adb44d, which broke the build. Did a `git revert 1d7048c`. Closes Differential Revision: D5476473 Pulled By: sagar0 fbshipit-source-id: 4756ff5c0dfc88c17eceb00e02c36176de728d06/FIFO Compaction with TTL Summary: Introducing FIFO compactions with TTL. FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size. To address that request: Added a new TTL option to FIFO compaction options. Updated FIFO compaction score to take TTL into consideration. Added a new table property, creation_time, to keep track of when the SST file is created. Creation_time is set as below: On Flush: Set to the time of flush. On Compaction: Set to the max creation_time of all the files involved in the compaction. On Repair and Recovery: Set to the time of repair/recovery. Old files created prior to this code change will have a creation_time of 0. FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time (current_time ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day. FIFO compaction will fall back to the prior way of deleting files based on size if: the creation_time of all files involved in compaction is 0. the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted. This feature is not supported if max_open_files or with table formats other than Block-based. **Test Plan:** Added tests. **Benchmark results:** Base: FIFO with max size: 100MB :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.924 micros/op 519858 ops/sec; 13.6 MB/s (1176277 of 5000000 found) ``` With TTL (a low one for testing) :: ``` ~/rocksdb (fifo-compaction) $ TEST_TMPDIR=/dev/shm ./db_bench readwhilewriting : 1.902 micros/op 525817 ops/sec; 13.7 MB/s (1185057 of 5000000 found) ``` Example Log lines: ``` 2017/06/26-15:17:24.609249 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609177) [db/compaction_picker.cc:1471] [default] FIFO compaction: picking file 40 with creation time 1498515423 for deletion 2017/06/26-15:17:24.609255 7fd5a45ff700 (Original Log Time 2017/06/26-15:17:24.609234) [db/db_impl_compaction_flush.cc:1541] [default] Deleted 1 files ... 2017/06/26-15:17:25.553185 7fd5a61a5800 [DEBUG] [db/db_impl_files.cc:309] [JOB 0] Delete /dev/shm/dbbench/000040.sst type=2 OK 2017/06/26-15:17:25.553205 7fd5a61a5800 EVENT_LOG_v1 {""time_micros"": 1498515445553199, ""job"": 0, ""event"": ""table_file_deletion"", ""file_number"": 40} ``` SST Files remaining in the dbbench dir, after db_bench execution completed: ``` ~/rocksdb (fifo-compaction) $ ls /dev/shm//dbbench/*.sst 1 svemuri users 30749887 Jun 26 15:17 /dev/shm//dbbench/000042.sst 1 svemuri users 30768779 Jun 26 15:17 /dev/shm//dbbench/000044.sst 1 svemuri users 30757481 Jun 26 15:17 /dev/shm//dbbench/000046.sst ``` Closes Differential Revision: D5305116 Pulled By: sagar0 fbshipit-source-id: 3e5cfcf5dd07ed2211b5b37492eb235b45139174/Sample number of reads per SST file Summary: We estimate number of reads per SST files, by updating the counter per file in sampled read requests. This information can later be used to trigger compactions to improve read performacne. Closes Differential Revision: D5193528 Pulled By: siying fbshipit-source-id: b4241c5ad0eaf444b61afb53f8e6290d9f5da2df/Support ingest file when range deletions exist Summary: Previously we returned NotSupported when ingesting files into a database containing any range deletions. This diff adds the support. Flush if any memtable contains range deletions overlapping the to-be-ingested file Place to-be-ingested file before any level that contains range deletions overlapping it. Added support for `Version` to return iterators over range deletions in a given level. Previously, we piggybacked getting range deletions onto `Version`s `Get()` / `AddIterator()` functions by passing them a `RangeDelAggregator*`. But file ingestion needs to get iterators over range deletions, not populate an aggregator (since the aggregator does collapsing and doesnt expose the actual ranges). Closes Differential Revision: D5127648 Pulled By: ajkr fbshipit-source-id: 816faeb9708adfa5287962bafdde717db56e3f1a/disable direct reads for log and manifest and add direct io to tests Summary: Disable direct reads for log and manifest. Direct reads should not affect sequential_file Also add kDirectIO for option_config_ in db_test_util Closes Differential Revision: D5100261 Pulled By: lightmark fbshipit-source-id: 0ebfd13b93fa1b8f9acae514ac44f8125a05868b/"
1432,1432,16.0,0.9603999853134155,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","CodeMod: Prefer ADD_FAILURE() over EXPECT_TRUE(false), et cetera Summary: CodeMod: Prefer `ADD_FAILURE()` over `EXPECT_TRUE(false)`, et cetera. The tautologically-conditioned and tautologically-contradicted boolean expectations/assertions have better alternatives: unconditional passes and failures. Reviewed By: Orvid Differential Revision: D5432398 Tags: codemod, codemod-opensource fbshipit-source-id: d16b447e8696a6feaa94b41199f5052226ef6914/"
1433,1433,10.0,0.9546999931335449,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Implement ReopenWritibaleFile on Windows and other fixes Summary: Make default impl return NoSupported so the db_blob tests exist in a meaningful manner. Replace std::thread to port::Thread Closes Differential Revision: D5275563 Pulled By: yiwu-arbug fbshipit-source-id: cedf1a18a2c05e20d768c1308b3f3224dbd70ab6/Dedup release Summary: cc tamird sagar0 Closes Differential Revision: D5098302 Pulled By: sagar0 fbshipit-source-id: 297c5506b5d9b2ed1d7719c8caf0b96cffe503b8/
1434,1434,18.0,0.9821000099182129,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","db_bench compression options Summary: moved existing compression options to `InitializeOptionsGeneral` since they cannot be set through options file added flag for `zstd_max_train_bytes` which was recently introduced by Closes Differential Revision: D6240460 Pulled By: ajkr fbshipit-source-id: 27dbebd86a55de237ba6a45cc79cff9214e82ebc/support db_bench compact benchmark on bottommost files Summary: Without this option, running the compact benchmark on a DB containing only bottommost files simply returned immediately. Closes Differential Revision: D6256660 Pulled By: ajkr fbshipit-source-id: e3b64543acd503d821066f4200daa201d4fb3a9d/make rate limiter a general option Summary: its unsupported in options file, so the flag should be respected by db_bench even when an options file is provided. Closes Differential Revision: D5869836 Pulled By: ajkr fbshipit-source-id: f67f591ae083e95e989f86b6fad50765d2e3d855/"
1435,1435,1.0,0.9891999959945679,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","db_stress support long-held snapshots Summary: Add options to `db_stress` (correctness testing tool) to randomly acquire snapshot and release it after some period of time. Its useful for correctness testing of as well as other parts of compaction that behave differently depending on which snapshots are held. Closes Differential Revision: D6086501 Pulled By: ajkr fbshipit-source-id: 3ec0d8666c78ac507f1f808887c4ff759ba9b865/support disabling checksum in block-based table Summary: store a zero as the checksum when disabled since its easier to keep block trailer a fixed length. Closes Differential Revision: D5694702 Pulled By: ajkr fbshipit-source-id: 69cea9da415778ba2b600dfd9d0dfc8cb5188ecd/minor improvements to db_stress Summary: fix some things that made this command hard to use from CLI: use default values for `target_file_size_base` and `max_bytes_for_level_base`. previously we were using small values for these but default value of `write_buffer_size`, which led to enormous number of L1 files. failure message for `value_size_mult` too big. previously there was just an assert, so in non-debug mode itd overrun the value buffer and crash mysteriously. only print verification success if theres no failure. before itd print both in the failure case. support `memtable_prefix_bloom_size_ratio` support `num_bottom_pri_threads` (universal compaction) Closes Differential Revision: D5629495 Pulled By: ajkr fbshipit-source-id: ddad97d6d4ba0884e7c0f933b0a359712514fc1d/"
1436,1436,4.0,0.864300012588501,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",EnvWrapper: Forward more functions Summary: Closes Differential Revision: D5738335 Pulled By: ajkr fbshipit-source-id: f371303c42b144d0a0424e9304b0df545f073ad1/
1437,1437,14.0,0.8436999917030334,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Added support for differential snapshots Summary: The motivation for this PR is to add to RocksDB support for differential (incremental) snapshots, as snapshot of the DB changes between two points in time (one can think of it as diff between to sequence numbers, or the diff D which can be thought of as an SST file or just set of KVs that can be applied to sequence number S1 to get the database to the state at sequence number S2). This feature would be useful for various distributed storages layers built on top of RocksDB, as it should help reduce resources (time and network bandwidth) needed to recover and rebuilt DB instances as replicas in the context of distributed storages. From the API standpoint that would like client app requesting iterator between (start seqnum) and current DB state, and reading the ""diff"". This is a very draft PR for initial review in the discussion on the approach, im going to rework some parts and keep updating the PR. For now, whats done here according to initial discussions: Preserving deletes: We want to be able to optionally preserve recent deletes for some defined period of time, so that if a delete came in recently and might need to be included in the next incremental snapshot it wouldt get dropped by a compaction. This is done by adding new param to Options (preserve deletes flag) and new variable to DB Impl where we keep track of the sequence number after which we dont want to drop tombstones, even if they are otherwise eligible for deletion. I also added a new API call for clients to be able to advance this cutoff seqnum after which we drop deletes; i assume its more flexible to let clients control this, since otherwise wed need to keep some kind of timestamp > seqnum mapping inside the DB, which sounds messy and painful to support. Clients could make use of it by periodically calling GetLatestSequenceNumber(), noting the timestamp, doing some calculation and figuring out by how much we need to advance the cutoff seqnum. Compaction codepath in compaction_iterator.cc has been modified to avoid dropping tombstones with seqnum > cutoff seqnum. Iterator changes: couple params added to ReadOptions, to optionally allow client to request internal keys instead of user keys (so that client can get the latest value of a key, be it delete marker or a put), as well as min timestamp and min seqnum. TableCache changes: I modified table_cache code to be able to quickly exclude SST files from iterators heep if creation_time on the file is less then iter_start_ts as passed in ReadOptions. That would help a lot in some DB settings (like reading very recent data only or using FIFO compactions), but not so much for universal compaction with more or less long iterator time span. Whats left: Still looking at how to best plug that inside DBIter codepath. So far it seems that FindNextUserKeyInternal only parses values as UserKeys, and iter->key() call generally returns user key. Can we add new API to DBIter as internal_key(), and modify this internal method to optionally set saved_key_ to point to the full internal key? I dont need to store actual seqnum there, but I do need to store type. Closes Differential Revision: D6175602 Pulled By: mikhail-antonov fbshipit-source-id: c779a6696ee2d574d86c69cec866a3ae095aa900/"
1438,1438,17.0,0.9801999926567078,"android, summary, fix, add, also, test_plan, instead, call, issue, commit, test, refactor, file, make, cleanup, run, change, thread, option, create","Provide byte[] version of SstFileWriter.merge to reduce GC Stall Summary: In Java API, `SstFileWriter.put/merge/delete` takes `Slice` type of key and value, which is a Java wrapper object around C++ Slice object. The Slice object inherited [ `finalize`]( method, which [added huge overhead]( to JVM while creating new SstFile. To address this issue, this PR overload the merge method to take Java byte array instead of the Slice object, and added unit test for it. We also benchmark these two different merge function, where we could see GC Stall reduced from 50% to 1%, and the throughput increased from 50MB to 200MB. Closes Reviewed By: sagar0 Differential Revision: D5653145 Pulled By: scv119 fbshipit-source-id: b55ea58554b573d0b1c6f6170f8d9223811bc4f5/"
1439,1439,1.0,0.9366000294685364,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
1440,1440,1.0,0.9366000294685364,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
1441,1441,1.0,0.9366000294685364,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Added CompactionFilterFactory support to RocksJava Summary: This PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support. Closes Differential Revision: D6012778 Pulled By: sagar0 fbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c/"
1442,1442,11.0,0.6308000087738037,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Fix crashes, address test issues and adjust windows test script Summary: Add per-exe execution capability Add fix parsing of groups/tests Add timer test exclusion Fix unit tests Ifdef threadpool specific tests that do not pass on Vista threadpool. Remove spurious outout from prefix_test so test case listing works properly. Fix not using standard test directories results in file creation errors in sst_dump_test. BlobDb fixes: In C++ end() iterators can not be dereferenced. They are not valid. When deleting blob_db_ set it to nullptr before any other code executes. Not fixed:. On Windows you can not delete a file while it is open. [ RUN ] BlobDBTest.ReadWhileGC d:\dev\rocksdb\rocksdb\utilities\blob_db\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options) IO error: Failed to delete: d:/mnt/db\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied d:\dev\rocksdb\rocksdb\utilities\blob_db\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options) IO error: Failed to delete: d:/mnt/db\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied write_batch Should not call front() if there is a chance the container is empty Closes Differential Revision: D6293274 Pulled By: sagar0 fbshipit-source-id: 318c3717c22087fae13b18715dffb24565dbd956/Blob DB: Fix race condition between flush and write Summary: A race condition will happen when: * a user thread writes a value, but it hits the write stop condition because there are too many un-flushed memtables, while holding blob_db_impl.write_mutex_. * Flush is triggered and call flush begin listener and try to acquire blob_db_impl.write_mutex_. Fixing it. Closes Differential Revision: D6279805 Pulled By: yiwu-arbug fbshipit-source-id: 0e3c58afb78795ebe3360a2c69e05651e3908c40/Blob DB: use compression in file header instead of global options Summary: To fix the issue of failing to decompress existing value after reopen DB with a different compression settings. Closes Differential Revision: D6267260 Pulled By: yiwu-arbug fbshipit-source-id: c7cf7f3e33b0cd25520abf4771cdf9180cc02a5f/Fix PinnableSlice move assignment Summary: After move assignment, we need to re-initialized the moved PinnableSlice. Also update blob_db_impl.cc to not reuse the moved PinnableSlice since it is supposed to be in an undefined state after move. Closes Differential Revision: D6238585 Pulled By: yiwu-arbug fbshipit-source-id: bd99f2e37406c4f7de160c7dee6a2e8126bc224e/Blob DB: fix snapshot handling Summary: Blob db will keep blob file if data in the file is visible to an active snapshot. Before this patch it checks whether there is an active snapshot has sequence number greater than the earliest sequence in the file. This is problematic since we take snapshot on every read, if it keep having reads, old blob files will not be cleanup. Change to check if there is an active snapshot falls in the range of [earliest_sequence, obsolete_sequence) where obsolete sequence is 1. if data is relocated to another file by garbage collection, it is the latest sequence at the time garbage collection finish 2. otherwise, it is the latest sequence of the file Closes Differential Revision: D6182519 Pulled By: yiwu-arbug fbshipit-source-id: cdf4c35281f782eb2a9ad6a87b6727bbdff27a45/Blob DB: option to enable garbage collection Summary: Add an option to enable/disable auto garbage collection, where we keep counting how many keys have been evicted by either deletion or compaction and decide whether to garbage collect a blob file. Default disable auto garbage collection for now since the whole logic is not fully tested and we plan to make major change to it. Closes Differential Revision: D6224756 Pulled By: yiwu-arbug fbshipit-source-id: cdf53bdccec96a4580a2b3a342110ad9e8864dfe/Blob DB: Evict oldest blob file when close to blob db size limit Summary: Evict oldest blob file and put it in obsolete_files list when close to blob db size limit. The file will be delete when the `DeleteObsoleteFiles` background job runs next time. For now I set `kEvictOldestFileAtSize` constant, which controls when to evict the oldest file, at 90%. It could be tweaked or made into an option if really needed; I didnt want to expose it as an option pre-maturely as there are already too many :) . Closes Differential Revision: D6187340 Pulled By: sagar0 fbshipit-source-id: 687f8262101b9301bf964b94025a2fe9d8573421/Blob DB: cleanup unused options Summary: * cleanup num_concurrent_simple_blobs. We dont do concurrent writes (by taking write_mutex_) so it doesnt make sense to have multiple non TTL files open. We can revisit later when we want to improve writes. * cleanup eviction callback. we dont have plan to use it now. * rename s/open_simple_blob_files_/open_non_ttl_file_/ and s/open_blob_files_/open_ttl_files_/ to avoid confusion. Closes Differential Revision: D6182598 Pulled By: yiwu-arbug fbshipit-source-id: 99e6f5e01fa66d31309cdb06ce48502464bac6ad/Blob DB: Initialize all fields in Blob Header, Footer and Record structs Summary: Fixing un-itializations caught by valgrind. Closes Differential Revision: D6200195 Pulled By: sagar0 fbshipit-source-id: bf35a3fb03eb1d308e4c5ce30dee1e345d7b03b3/Blob DB: update blob file format Summary: Changing blob file format and some code cleanup around the change. The change with blob log format are: * Remove timestamp field in blob file header, blob file footer and blob records. The field is not being use and often confuse with expiration field. * Blob file header now come with column family id, which always equal to default column family id. It leaves room for future support of column family. * Compression field in blob file header now is a standalone byte (instead of compact encode with flags field) * Blob file footer now come with its own crc. * Key length now being uint64_t instead of uint32_t * Blob CRC now checksum both key and value (instead of value only). * Some reordering of the fields. The list of cleanups: * Better inline comments in blob_log_format.h * rename ttlrange_t and snrange_t to ExpirationRange and SequenceRange respectively. * simplify blob_db::Reader * Move crc checking logic to inside blob_log_format.cc Closes Differential Revision: D6171304 Pulled By: yiwu-arbug fbshipit-source-id: e4373e0d39264441b7e2fbd0caba93ddd99ea2af/Blob DB: Inline small values in base DB Summary: Adding the `min_blob_size` option to allow storing small values in base db (in LSM tree) together with the key. The goal is to improve performance for small values, while taking advantage of blob dbs low write amplification for large values. Also adding expiration timestamp to blob index. It will be useful to evict stale blob indexes in base db by adding a compaction filter. Ill work on the compaction filter in future patches. See blob_index.h for the new blob index format. There are 4 cases when writing a new key: * small value w/o TTL: put in base db as normal value (i.e. ValueType::kTypeValue) * small value w/ TTL: put (type, expiration, value) to base db. * large value w/o TTL: write value to blob log and put (type, file, offset, size, compression) to base db. * large value w/TTL: write value to blob log and put (type, expiration, file, offset, size, compression) to base db. Closes Differential Revision: D6142115 Pulled By: yiwu-arbug fbshipit-source-id: 9526e76e19f0839310a3f5f2a43772a4ad182cd0/Return write error on reaching blob dir size limit Summary: I found that we continue accepting writes even when the blob db goes beyond the configured blob directory size limit. Now, we return an error for writes on reaching `blob_dir_size` limit and if `is_fifo` is set to false. (We cannot just drop any file when `is_fifo` is true.) Deleting the oldest file when `is_fifo` is true will be handled in a later PR. Closes Differential Revision: D6136156 Pulled By: sagar0 fbshipit-source-id: 2f11cb3f2eedfa94524fbfa2613dd64bfad7a23c/Fix unused var warnings in Release mode Summary: MSVC does not support unused attribute at this time. A separate assignment line fixes the issue probably by being counted as usage for MSVC and it no longer complains about unused var. Closes Differential Revision: D6126272 Pulled By: maysamyabandeh fbshipit-source-id: 4907865db45fd75a39a15725c0695aaa17509c1f/Blob DB: Store blob index as kTypeBlobIndex in base db Summary: Blob db insert blob index to base db as kTypeBlobIndex type, to tell apart values written by plain rocksdb or blob db. This is to make it possible to migrate from existing rocksdb to blob db. Also with the patch blob db garbage collection get away from OptimisticTransaction. Instead it use a custom write callback to achieve similar behavior as OptimisticTransaction. This is because we need to pass the is_blob_index flag to DBImpl::Get but OptimisticTransaction dont support it. Closes Differential Revision: D6050044 Pulled By: yiwu-arbug fbshipit-source-id: 61dc72ab9977625e75f78cd968e7d8a3976e3632/Blob DB: not writing sequence number as blob record footer Summary: Previously each time we write a blob we write blog_record_header + key + value + blob_record_footer to blob log. The footer only contains a sequence and a crc for the sequence number. The sequence number was used in garbage collection to verify the value is recent. After we moved to use optimistic transaction and no longer use sequence number from the footer. Remove the footer altogether. Theres another usage of sequence number and we are keeping it: Each blob log file keep track of sequence number range of keys in it, and use it to check if it is reference by a snapshot, before being deleted. Closes Differential Revision: D6057585 Pulled By: yiwu-arbug fbshipit-source-id: d6da53c457a316e9723f359a1b47facfc3ffe090/add GetLiveFiles and GetLiveFilesMetaData for BlobDB Summary: Closes Differential Revision: D5994759 Pulled By: miasantreble fbshipit-source-id: 985c31dccb957cb970c302f813cd07a1e8cb6438/Make it explicit blob db doesnt support CF Summary: Blob db doesnt currently support column families. Return NotSupported status explicitly. Closes Differential Revision: D5757438 Pulled By: yiwu-arbug fbshipit-source-id: 44de9408fd032c98e8ae337d4db4ed37169bd9fa/make blob file close synchronous Summary: Fixing flaky blob_db_test. To close a blob file, blob db used to add a CloseSeqWrite job to the background thread to close it. Changing file close to be synchronous in order to simplify logic, and fix flaky blob_db_test. Closes Differential Revision: D5699387 Pulled By: yiwu-arbug fbshipit-source-id: dd07a945cd435cd3808fce7ee4ea57817409474a/Blob db create a snapshot before every read Summary: If GC kicks in between * A Get() reads index entry from base db. * The Get() read from a blob file The GC can delete the corresponding blob file, making the key not found. Fortunately we have existing logic to avoid deleting a blob file if it is referenced by a snapshot. So the fix is to explicitly create a snapshot before reading index entry from base db. Closes Differential Revision: D5655956 Pulled By: yiwu-arbug fbshipit-source-id: e4ccbc51331362542e7343175bbcbdea5830f544/GC the oldest file when out of space Summary: When out of space, blob db should GC the oldest file. The current implementation GC the newest one instead. Fixing it. Closes Differential Revision: D5657611 Pulled By: yiwu-arbug fbshipit-source-id: 56c30a4c52e6ab04551dda8c5c46006d4070b28d/Fix blob db crash during calculating write amp Summary: On initial call to BlobDBImpl::WaStats() `all_periods_write_` would be empty, so it will crash when we call pop_front() at line 1627. Apparently it is mean to pop only when `all_periods_write_.size() > kWriteAmplificationStatsPeriods`. The whole write amp calculation doesnt seems to be correct and it is not being exposed. Will work on it later. Test Plan Change kWriteAmplificationStatsPeriodMillisecs to 1000 (1 second) and run db_bench for 5 minutes. Closes Differential Revision: D5648269 Pulled By: yiwu-arbug fbshipit-source-id: b843d9a09bb5f9e1b713d101ec7b87e54b5115a4/"
1443,1443,6.0,0.9839000105857849,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Add DB::Properties::kEstimateOldestKeyTime Summary: With FIFO compaction we would like to get the oldest data time for monitoring. The problem is we dont have timestamp for each key in the DB. As an approximation, we expose the earliest of sst file ""creation_time"" property. My plan is to override the property with a more accurate value with blob db, where we actually have timestamp. Closes Differential Revision: D5770600 Pulled By: yiwu-arbug fbshipit-source-id: 03833c8f10bbfbee62f8ea5c0d03c0cafb5d853a/No need for Restart Interval for meta blocks Summary: In SST files, restart interval helps us search in data blocks. However, some meta blocks will be read sequentially, so theres no need for restart points. Restart interval will introduce extra space in the block ( We will see if we can remove this redundant space. (Maybe set restart interval to infinite.) Closes Differential Revision: D5930139 Pulled By: miasantreble fbshipit-source-id: 92b1b23c15cffa90378343ac846b713623b19c21/"
1444,1444,10.0,0.6732000112533569,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","prevent nullptr dereference in table reader error case Summary: A user encountered segfault on the call to `CacheDependencies()`, probably because `NewIndexIterator()` failed before populating `*index_entry`. Lets avoid the call in that case. Closes Differential Revision: D5928611 Pulled By: ajkr fbshipit-source-id: 484be453dbb00e5e160e9c6a1bc933df7d80f574/Extend pin_l0 to filter partitions Summary: This is the continuation of for filter partitions. When pin_l0 is set (along with cache_xxx), then open table open the filter partitions are loaded into the cache and pinned there. Closes Differential Revision: D5671098 Pulled By: maysamyabandeh fbshipit-source-id: 174f24018f1d7f1129621e7380287b65b67d2115/Preload l0 index partitions Summary: This fixes the existing logic for pinning l0 index partitions. The patch preloads the partitions into block cache and pin them if they belong to level 0 and pin_l0 is set. The drawback is that it does many small IOs when preloading all the partitions into the cache is direct io is enabled. Working for a solution for that. Closes Differential Revision: D5554010 Pulled By: maysamyabandeh fbshipit-source-id: 1e6f32a3524d71355c77d4138516dcfb601ca7b2/"
1445,1445,14.0,0.760200023651123,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Added support for differential snapshots Summary: The motivation for this PR is to add to RocksDB support for differential (incremental) snapshots, as snapshot of the DB changes between two points in time (one can think of it as diff between to sequence numbers, or the diff D which can be thought of as an SST file or just set of KVs that can be applied to sequence number S1 to get the database to the state at sequence number S2). This feature would be useful for various distributed storages layers built on top of RocksDB, as it should help reduce resources (time and network bandwidth) needed to recover and rebuilt DB instances as replicas in the context of distributed storages. From the API standpoint that would like client app requesting iterator between (start seqnum) and current DB state, and reading the ""diff"". This is a very draft PR for initial review in the discussion on the approach, im going to rework some parts and keep updating the PR. For now, whats done here according to initial discussions: Preserving deletes: We want to be able to optionally preserve recent deletes for some defined period of time, so that if a delete came in recently and might need to be included in the next incremental snapshot it wouldt get dropped by a compaction. This is done by adding new param to Options (preserve deletes flag) and new variable to DB Impl where we keep track of the sequence number after which we dont want to drop tombstones, even if they are otherwise eligible for deletion. I also added a new API call for clients to be able to advance this cutoff seqnum after which we drop deletes; i assume its more flexible to let clients control this, since otherwise wed need to keep some kind of timestamp > seqnum mapping inside the DB, which sounds messy and painful to support. Clients could make use of it by periodically calling GetLatestSequenceNumber(), noting the timestamp, doing some calculation and figuring out by how much we need to advance the cutoff seqnum. Compaction codepath in compaction_iterator.cc has been modified to avoid dropping tombstones with seqnum > cutoff seqnum. Iterator changes: couple params added to ReadOptions, to optionally allow client to request internal keys instead of user keys (so that client can get the latest value of a key, be it delete marker or a put), as well as min timestamp and min seqnum. TableCache changes: I modified table_cache code to be able to quickly exclude SST files from iterators heep if creation_time on the file is less then iter_start_ts as passed in ReadOptions. That would help a lot in some DB settings (like reading very recent data only or using FIFO compactions), but not so much for universal compaction with more or less long iterator time span. Whats left: Still looking at how to best plug that inside DBIter codepath. So far it seems that FindNextUserKeyInternal only parses values as UserKeys, and iter->key() call generally returns user key. Can we add new API to DBIter as internal_key(), and modify this internal method to optionally set saved_key_ to point to the full internal key? I dont need to store actual seqnum there, but I do need to store type. Closes Differential Revision: D6175602 Pulled By: mikhail-antonov fbshipit-source-id: c779a6696ee2d574d86c69cec866a3ae095aa900/WritePrepared Txn: Compaction/Flush Summary: Update Compaction/Flush to support WritePreparedTxnDB: Add SnapshotChecker which is a proxy to query WritePreparedTxnDB::IsInSnapshot. Pass SnapshotChecker to DBImpl on WritePreparedTxnDB open. CompactionIterator use it to check if a key has been committed and if it is visible to a snapshot. In CompactionIterator: * check if key has been committed. If not, output uncommitted keys AS-IS. * use SnapshotChecker to check if key is visible to a snapshot when in need. * do not output key with seq 0 if the key is not committed. Closes Differential Revision: D5902907 Pulled By: yiwu-arbug fbshipit-source-id: 945e037fdf0aa652dc5ba0ad879461040baa0320/Allow merge operator to be called even with a single operand Summary: Added a function `MergeOperator::DoesAllowSingleMergeOperand()` to allow invoking a merge operator even with a single merge operand, if overriden. This is needed for Cassandra-on-RocksDB work. All Cassandra writes are through merges and this will allow a single merge-value to be updated in the merge-operator invoked via a compaction, if needed, due to an expired TTL. Closes Differential Revision: D5608706 Pulled By: sagar0 fbshipit-source-id: f299f9f91c4d1ac26e48bd5906e122c1c5e5f3fc/"
1446,1446,19.0,0.4259999990463257,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Blob DB: fix snapshot handling Summary: Blob db will keep blob file if data in the file is visible to an active snapshot. Before this patch it checks whether there is an active snapshot has sequence number greater than the earliest sequence in the file. This is problematic since we take snapshot on every read, if it keep having reads, old blob files will not be cleanup. Change to check if there is an active snapshot falls in the range of [earliest_sequence, obsolete_sequence) where obsolete sequence is 1. if data is relocated to another file by garbage collection, it is the latest sequence at the time garbage collection finish 2. otherwise, it is the latest sequence of the file Closes Differential Revision: D6182519 Pulled By: yiwu-arbug fbshipit-source-id: cdf4c35281f782eb2a9ad6a87b6727bbdff27a45/Added support for differential snapshots Summary: The motivation for this PR is to add to RocksDB support for differential (incremental) snapshots, as snapshot of the DB changes between two points in time (one can think of it as diff between to sequence numbers, or the diff D which can be thought of as an SST file or just set of KVs that can be applied to sequence number S1 to get the database to the state at sequence number S2). This feature would be useful for various distributed storages layers built on top of RocksDB, as it should help reduce resources (time and network bandwidth) needed to recover and rebuilt DB instances as replicas in the context of distributed storages. From the API standpoint that would like client app requesting iterator between (start seqnum) and current DB state, and reading the ""diff"". This is a very draft PR for initial review in the discussion on the approach, im going to rework some parts and keep updating the PR. For now, whats done here according to initial discussions: Preserving deletes: We want to be able to optionally preserve recent deletes for some defined period of time, so that if a delete came in recently and might need to be included in the next incremental snapshot it wouldt get dropped by a compaction. This is done by adding new param to Options (preserve deletes flag) and new variable to DB Impl where we keep track of the sequence number after which we dont want to drop tombstones, even if they are otherwise eligible for deletion. I also added a new API call for clients to be able to advance this cutoff seqnum after which we drop deletes; i assume its more flexible to let clients control this, since otherwise wed need to keep some kind of timestamp > seqnum mapping inside the DB, which sounds messy and painful to support. Clients could make use of it by periodically calling GetLatestSequenceNumber(), noting the timestamp, doing some calculation and figuring out by how much we need to advance the cutoff seqnum. Compaction codepath in compaction_iterator.cc has been modified to avoid dropping tombstones with seqnum > cutoff seqnum. Iterator changes: couple params added to ReadOptions, to optionally allow client to request internal keys instead of user keys (so that client can get the latest value of a key, be it delete marker or a put), as well as min timestamp and min seqnum. TableCache changes: I modified table_cache code to be able to quickly exclude SST files from iterators heep if creation_time on the file is less then iter_start_ts as passed in ReadOptions. That would help a lot in some DB settings (like reading very recent data only or using FIFO compactions), but not so much for universal compaction with more or less long iterator time span. Whats left: Still looking at how to best plug that inside DBIter codepath. So far it seems that FindNextUserKeyInternal only parses values as UserKeys, and iter->key() call generally returns user key. Can we add new API to DBIter as internal_key(), and modify this internal method to optionally set saved_key_ to point to the full internal key? I dont need to store actual seqnum there, but I do need to store type. Closes Differential Revision: D6175602 Pulled By: mikhail-antonov fbshipit-source-id: c779a6696ee2d574d86c69cec866a3ae095aa900/Blob DB: Inline small values in base DB Summary: Adding the `min_blob_size` option to allow storing small values in base db (in LSM tree) together with the key. The goal is to improve performance for small values, while taking advantage of blob dbs low write amplification for large values. Also adding expiration timestamp to blob index. It will be useful to evict stale blob indexes in base db by adding a compaction filter. Ill work on the compaction filter in future patches. See blob_index.h for the new blob index format. There are 4 cases when writing a new key: * small value w/o TTL: put in base db as normal value (i.e. ValueType::kTypeValue) * small value w/ TTL: put (type, expiration, value) to base db. * large value w/o TTL: write value to blob log and put (type, file, offset, size, compression) to base db. * large value w/TTL: write value to blob log and put (type, expiration, file, offset, size, compression) to base db. Closes Differential Revision: D6142115 Pulled By: yiwu-arbug fbshipit-source-id: 9526e76e19f0839310a3f5f2a43772a4ad182cd0/single-file bottom-level compaction when snapshot released Summary: When snapshots are held for a long time, files may reach the bottom level containing overwritten/deleted keys. We previously had no mechanism to trigger compaction on such files. This particularly impacted DBs that write to different parts of the keyspace over time, as such files would never be naturally compacted due to second-last level files moving down. This PR introduces a mechanism for bottommost files to be recompacted upon releasing all snapshots that prevent them from dropping their deleted/overwritten keys. Changed `CompactionPicker` to compact files in `BottommostFilesMarkedForCompaction()`. These are the last choice when picking. Each file will be compacted alone and output to the same level in which it originated. The goal of this type of compaction is to rewrite the data excluding deleted/overwritten keys. Changed `ReleaseSnapshot()` to recompute the bottom files marked for compaction when the oldest existing snapshot changes, and schedule a compaction if needed. We cache the value that oldest existing snapshot needs to exceed in order for another file to be marked in `bottommost_files_mark_threshold_`, which allows us to avoid recomputing marked files for most snapshot releases. Changed `VersionStorageInfo` to track the list of bottommost files, which is recomputed every time the version changes by `UpdateBottommostFiles()`. The list of marked bottommost files is first computed in `ComputeBottommostFilesMarkedForCompaction()` when the version changes, but may also be recomputed when `ReleaseSnapshot()` is called. Extracted core logic of `Compaction::IsBottommostLevel()` into `VersionStorageInfo::RangeMightExistAfterSortedRun()` since logic to check whether a file is bottommost is now necessary outside of compaction. Closes Differential Revision: D6062044 Pulled By: ajkr fbshipit-source-id: 123d201cf140715a7d5928e8b3cb4f9cd9f7ad21/Enable two write queues for transactions Summary: Enable concurrent_prepare flag for WritePrepared transactions and extend the existing transaction tests with this config. Closes Differential Revision: D6106534 Pulled By: maysamyabandeh fbshipit-source-id: 88c8d21d45bc492beb0a131caea84a2ac5e7d38c/WritePrepared Txn: Disable GC during recovery Summary: Disables GC during recovery of a WritePrepared txn db to avoid GCing uncommitted key values. Closes Differential Revision: D6000191 Pulled By: maysamyabandeh fbshipit-source-id: fc4d522c643d24ebf043f811fe4ecd0dd0294675/Blob DB: Store blob index as kTypeBlobIndex in base db Summary: Blob db insert blob index to base db as kTypeBlobIndex type, to tell apart values written by plain rocksdb or blob db. This is to make it possible to migrate from existing rocksdb to blob db. Also with the patch blob db garbage collection get away from OptimisticTransaction. Instead it use a custom write callback to achieve similar behavior as OptimisticTransaction. This is because we need to pass the is_blob_index flag to DBImpl::Get but OptimisticTransaction dont support it. Closes Differential Revision: D6050044 Pulled By: yiwu-arbug fbshipit-source-id: 61dc72ab9977625e75f78cd968e7d8a3976e3632/WritePrepared Txn: Iterator Summary: On iterator create, take a snapshot, create a ReadCallback and pass the ReadCallback to the underlying DBIter to check if key is committed. Closes Differential Revision: D6001471 Pulled By: yiwu-arbug fbshipit-source-id: 3565c4cdaf25370ba47008b0e0cb65b31dfe79fe/Inform caller when rocksdb is stalling writes Summary: Add a new function in Listener to let the caller know when rocksdb is stalling writes. Closes Differential Revision: D5860124 Pulled By: schischi fbshipit-source-id: ee791606169aa64f772c86f817cebf02624e05e1/Add ValueType::kTypeBlobIndex Summary: Add kTypeBlobIndex value type, which will be used by blob db only, to insert a (key, blob_offset) KV pair. The purpose is to 1. Make it possible to open existing rocksdb instance as blob db. Existing value will be of kTypeIndex type, while value inserted by blob db will be of kTypeBlobIndex. 2. Make rocksdb able to detect if the db contains value written by blob db, if so return error. 3. Make it possible to have blob db optionally store value in SST file (with kTypeValue type) or as a blob value (with kTypeBlobIndex type). The root db (DBImpl) basically pretended kTypeBlobIndex are normal value on write. On Get if is_blob is provided, return whether the value read is of kTypeBlobIndex type, or return Status::NotSupported() status if is_blob is not provided. On scan allow_blob flag is pass and if the flag is true, return wether the value is of kTypeBlobIndex type via iter->IsBlob(). Changes on blob db side will be in a separate patch. Closes Differential Revision: D5838431 Pulled By: yiwu-arbug fbshipit-source-id: 3c5306c62bc13bb11abc03422ec5cbcea1203cca/Make bytes_per_sync and wal_bytes_per_sync mutable Summary: SUMMARY Moves the bytes_per_sync and wal_bytes_per_sync options from immutableoptions to mutable options. Also if wal_bytes_per_sync is changed, the wal file and memtables are flushed. TEST PLAN ran make check all passed Two new tests SetBytesPerSync, SetWalBytesPerSync check that after issuing setoptions with a new value for the var, the db options have the new value. Closes Reviewed By: yiwu-arbug Differential Revision: D5845814 Pulled By: TheRushingWookie fbshipit-source-id: 93b52d779ce623691b546679dcd984a06d2ad1bd/WritePrepared Txn: Advance seq one per batch Summary: By default the seq number in DB is increased once per written key. WritePrepared txns requires the seq to be increased once per the entire batch so that the seq would be used as the prepare timestamp by which the transaction is identified. Also we need to increase seq for the commit marker since it would give a unique id to the commit timestamp of transactions. Two unit tests are added to verify our understanding of how the seq should be increased. The recovery path requires much more work and is left to another patch. Closes Differential Revision: D5837843 Pulled By: maysamyabandeh fbshipit-source-id: a08960b93d727e1cf438c254d0c2636fb133cc1c/write-prepared txn: call IsInSnapshot Summary: This patch instruments the read path to verify each read value against an optional ReadCallback class. If the value is rejected, the reader moves on to the next value. The WritePreparedTxn makes use of this feature to skip sequence numbers that are not in the read snapshot. Closes Differential Revision: D5787375 Pulled By: maysamyabandeh fbshipit-source-id: 49d808b3062ab35e7ae98ad388f659757794184c/Dump non-final ZSTD compression type support Summary: Closes Differential Revision: D5739947 Pulled By: ajkr fbshipit-source-id: 09f99718b6b083c2711dcf17f7b68c305f3fd261/perf_context measure user bytes read Summary: With this PR, we can measure read-amp for queries where perf_context is enabled as follows: ``` SetPerfLevel(kEnableCount); Get(1, ""foo""); double read_amp static_cast<double>(get_perf_context()->block_read_byte / get_perf_context()->get_read_bytes); SetPerfLevel(kDisable); ``` Our internal infra enables perf_context for a sampling of queries. So well be able to compute the read-amp for the sample set, which can give us a good estimate of read-amp. Closes Differential Revision: D5647240 Pulled By: ajkr fbshipit-source-id: ad73550b06990cf040cc4528fa885360f308ec12/"
1447,1447,19.0,0.9847000241279602,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","pass key/value samples through zstd compression dictionary generator Summary: Instead of using samples directly, we now support passing the samples through zstds dictionary generator when `CompressionOptions::zstd_max_train_bytes` is set to nonzero. If set to zero, we will use the samples directly as the dictionary same as before. Note this is the first step of extracted into a separate PR per reviewer request. Closes Differential Revision: D6116891 Pulled By: ajkr fbshipit-source-id: 70ab13cc4c734fa02e554180eed0618b75255497/write-prepared txn: call IsInSnapshot Summary: This patch instruments the read path to verify each read value against an optional ReadCallback class. If the value is rejected, the reader moves on to the next value. The WritePreparedTxn makes use of this feature to skip sequence numbers that are not in the read snapshot. Closes Differential Revision: D5787375 Pulled By: maysamyabandeh fbshipit-source-id: 49d808b3062ab35e7ae98ad388f659757794184c/"
1448,1448,19.0,0.9648000001907349,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write",Fix build on OpenBSD Summary: A few simple changes to allow RocksDB to be built on OpenBSD. Let me know if any further changes are needed. Closes Differential Revision: D6138800 Pulled By: ajkr fbshipit-source-id: a13a17b5dc051e6518bd56a8c5efd1d24dd81b0c/Updated CRC32 Power Optimization Changes Summary: Support for PowerPC Architecture Detecting AltiVec Support Closes Differential Revision: D5606836 Pulled By: siying fbshipit-source-id: 720262453b1546e5fdbbc668eff56848164113f3/
1449,1449,1.0,0.970300018787384,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Make writable_file_max_buffer_size dynamic Summary: The DBOptions::writable_file_max_buffer_size can be changed dynamically. Closes Differential Revision: D6152720 Pulled By: shligit fbshipit-source-id: aa0c0cfcfae6a54eb17faadb148d904797c68681/Improved transactions support in C API Summary: Solves Added OptimisticTransactionDB to the C API. Added missing merge operations to Transaction. Added missing get_for_update operation to transaction If required I will create tests for this another day. Closes Differential Revision: D5600906 Pulled By: yiwu-arbug fbshipit-source-id: da23e4484433d8f59d471f778ff2ae210e3fe4eb/
1450,1450,6.0,0.967199981212616,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Add DB::Properties::kEstimateOldestKeyTime Summary: With FIFO compaction we would like to get the oldest data time for monitoring. The problem is we dont have timestamp for each key in the DB. As an approximation, we expose the earliest of sst file ""creation_time"" property. My plan is to override the property with a more accurate value with blob db, where we actually have timestamp. Closes Differential Revision: D5770600 Pulled By: yiwu-arbug fbshipit-source-id: 03833c8f10bbfbee62f8ea5c0d03c0cafb5d853a/"
1451,1451,19.0,0.5658000111579895,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","single-file bottom-level compaction when snapshot released Summary: When snapshots are held for a long time, files may reach the bottom level containing overwritten/deleted keys. We previously had no mechanism to trigger compaction on such files. This particularly impacted DBs that write to different parts of the keyspace over time, as such files would never be naturally compacted due to second-last level files moving down. This PR introduces a mechanism for bottommost files to be recompacted upon releasing all snapshots that prevent them from dropping their deleted/overwritten keys. Changed `CompactionPicker` to compact files in `BottommostFilesMarkedForCompaction()`. These are the last choice when picking. Each file will be compacted alone and output to the same level in which it originated. The goal of this type of compaction is to rewrite the data excluding deleted/overwritten keys. Changed `ReleaseSnapshot()` to recompute the bottom files marked for compaction when the oldest existing snapshot changes, and schedule a compaction if needed. We cache the value that oldest existing snapshot needs to exceed in order for another file to be marked in `bottommost_files_mark_threshold_`, which allows us to avoid recomputing marked files for most snapshot releases. Changed `VersionStorageInfo` to track the list of bottommost files, which is recomputed every time the version changes by `UpdateBottommostFiles()`. The list of marked bottommost files is first computed in `ComputeBottommostFilesMarkedForCompaction()` when the version changes, but may also be recomputed when `ReleaseSnapshot()` is called. Extracted core logic of `Compaction::IsBottommostLevel()` into `VersionStorageInfo::RangeMightExistAfterSortedRun()` since logic to check whether a file is bottommost is now necessary outside of compaction. Closes Differential Revision: D6062044 Pulled By: ajkr fbshipit-source-id: 123d201cf140715a7d5928e8b3cb4f9cd9f7ad21/Make FIFO compaction options dynamically configurable Summary: ColumnFamilyOptions::compaction_options_fifo and all its sub-fields can be set dynamically now. Some of the ways in which the fifo compaction options can be set are: `SetOptions({{""compaction_options_fifo"", ""{max_table_files_size=1024}""}})` `SetOptions({{""compaction_options_fifo"", ""{ttl=600;}""}})` `SetOptions({{""compaction_options_fifo"", ""{max_table_files_size=1024;ttl=600;}""}})` `SetOptions({{""compaction_options_fifo"", ""{max_table_files_size=51;ttl=49;allow_compaction=true;}""}})` Most of the code has been made generic enough so that it could be reused later to make universal options (and other such nested defined-types) dynamic with very few lines of parsing/serializing code changes. Introduced a few new functions like `ParseStruct`, `SerializeStruct` and `GetStringFromStruct`. The duplicate code in `GetStringFromDBOptions` and `GetStringFromColumnFamilyOptions` has been moved into `GetStringFromStruct`. So they become just simple wrappers now. Closes Differential Revision: D6058619 Pulled By: sagar0 fbshipit-source-id: 1e8f78b3374ca5249bb4f3be8a6d3bb4cbc52f92/Use L1 size as estimate for L0 size in LevelCompactionBuilder::GetPathID Summary: Fix for [2461]( Problem: When using multiple db_paths setting with RocksDB, RocksDB incorrectly calculates the size of L1 in LevelCompactionBuilder::GetPathId. max_bytes_for_level_base is used as L0 size and L1 size is calculated as (L0 size * max_bytes_for_level_multiplier). However, L1 size should be max_bytes_for_level_base. Solution: Use max_bytes_for_level_base as L1 size. Also, use L1 size as the estimated size of L0. Closes Differential Revision: D5885442 Pulled By: maysamyabandeh fbshipit-source-id: 036da1c9298d173b9b80479cc6661ee4b7a951f6/"
1452,1452,6.0,0.967199981212616,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Add DB::Properties::kEstimateOldestKeyTime Summary: With FIFO compaction we would like to get the oldest data time for monitoring. The problem is we dont have timestamp for each key in the DB. As an approximation, we expose the earliest of sst file ""creation_time"" property. My plan is to override the property with a more accurate value with blob db, where we actually have timestamp. Closes Differential Revision: D5770600 Pulled By: yiwu-arbug fbshipit-source-id: 03833c8f10bbfbee62f8ea5c0d03c0cafb5d853a/"
1453,1453,10.0,0.9320999979972839,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Split CompactionFilterWithValueChange Summary: The test currently times out when it is run under tsan. This patch split it into 4 tests. Closes Differential Revision: D6106515 Pulled By: maysamyabandeh fbshipit-source-id: 03a28cdf8b1c097be2361b1b0cc3dc1acf2b5d63/
1454,1454,6.0,0.9320999979972839,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Inform caller when rocksdb is stalling writes Summary: Add a new function in Listener to let the caller know when rocksdb is stalling writes. Closes Differential Revision: D5860124 Pulled By: schischi fbshipit-source-id: ee791606169aa64f772c86f817cebf02624e05e1/
1455,1455,19.0,0.9968000054359436,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","WritePrepared Txn: Refactor conf params Summary: Summary of changes: Move seq_per_batch out of Options Rename concurrent_prepare to two_write_queues Add allocate_seq_only_for_data_ Closes Differential Revision: D6304458 Pulled By: maysamyabandeh fbshipit-source-id: 08e685bfa82bbc41b5b1c5eb7040a8ca6e05e58c/Make writable_file_max_buffer_size dynamic Summary: The DBOptions::writable_file_max_buffer_size can be changed dynamically. Closes Differential Revision: D6152720 Pulled By: shligit fbshipit-source-id: aa0c0cfcfae6a54eb17faadb148d904797c68681/single-file bottom-level compaction when snapshot released Summary: When snapshots are held for a long time, files may reach the bottom level containing overwritten/deleted keys. We previously had no mechanism to trigger compaction on such files. This particularly impacted DBs that write to different parts of the keyspace over time, as such files would never be naturally compacted due to second-last level files moving down. This PR introduces a mechanism for bottommost files to be recompacted upon releasing all snapshots that prevent them from dropping their deleted/overwritten keys. Changed `CompactionPicker` to compact files in `BottommostFilesMarkedForCompaction()`. These are the last choice when picking. Each file will be compacted alone and output to the same level in which it originated. The goal of this type of compaction is to rewrite the data excluding deleted/overwritten keys. Changed `ReleaseSnapshot()` to recompute the bottom files marked for compaction when the oldest existing snapshot changes, and schedule a compaction if needed. We cache the value that oldest existing snapshot needs to exceed in order for another file to be marked in `bottommost_files_mark_threshold_`, which allows us to avoid recomputing marked files for most snapshot releases. Changed `VersionStorageInfo` to track the list of bottommost files, which is recomputed every time the version changes by `UpdateBottommostFiles()`. The list of marked bottommost files is first computed in `ComputeBottommostFilesMarkedForCompaction()` when the version changes, but may also be recomputed when `ReleaseSnapshot()` is called. Extracted core logic of `Compaction::IsBottommostLevel()` into `VersionStorageInfo::RangeMightExistAfterSortedRun()` since logic to check whether a file is bottommost is now necessary outside of compaction. Closes Differential Revision: D6062044 Pulled By: ajkr fbshipit-source-id: 123d201cf140715a7d5928e8b3cb4f9cd9f7ad21/fix file numbers after repair Summary: The file numbers assigned post-repair were sometimes smaller than older files numbers due to `LogAndApply` saving the wrong next file number in the manifest. Mark the highest file seen during repair as used before `LogAndApply` so the correct next file number will be stored. Renamed `MarkFileNumberUsedDuringRecovery` to `MarkFileNumberUsed` since now its used during repair in addition to during recovery Added `TEST_Current_Next_FileNo` to expose the next file number for the unit test. Closes Differential Revision: D6018083 Pulled By: ajkr fbshipit-source-id: 3f25cbf74439cb8f16dd12af90b67f9f9f75e718/Add ValueType::kTypeBlobIndex Summary: Add kTypeBlobIndex value type, which will be used by blob db only, to insert a (key, blob_offset) KV pair. The purpose is to 1. Make it possible to open existing rocksdb instance as blob db. Existing value will be of kTypeIndex type, while value inserted by blob db will be of kTypeBlobIndex. 2. Make rocksdb able to detect if the db contains value written by blob db, if so return error. 3. Make it possible to have blob db optionally store value in SST file (with kTypeValue type) or as a blob value (with kTypeBlobIndex type). The root db (DBImpl) basically pretended kTypeBlobIndex are normal value on write. On Get if is_blob is provided, return whether the value read is of kTypeBlobIndex type, or return Status::NotSupported() status if is_blob is not provided. On scan allow_blob flag is pass and if the flag is true, return wether the value is of kTypeBlobIndex type via iter->IsBlob(). Changes on blob db side will be in a separate patch. Closes Differential Revision: D5838431 Pulled By: yiwu-arbug fbshipit-source-id: 3c5306c62bc13bb11abc03422ec5cbcea1203cca/write-prepared txn: call IsInSnapshot Summary: This patch instruments the read path to verify each read value against an optional ReadCallback class. If the value is rejected, the reader moves on to the next value. The WritePreparedTxn makes use of this feature to skip sequence numbers that are not in the read snapshot. Closes Differential Revision: D5787375 Pulled By: maysamyabandeh fbshipit-source-id: 49d808b3062ab35e7ae98ad388f659757794184c/"
1456,1456,16.0,0.9049999713897705,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Additions for `OptimisticTransactionDB` in C API Summary: Added some bindings for `OptimisticTransactionDB` in C API Closes Differential Revision: D5820672 Pulled By: yiwu-arbug fbshipit-source-id: 7efd17f619cc0741feddd2050b8fc856f9288350/
1457,1457,2.0,0.9943000078201294,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table","Consider an increase to buffer size when reading option file, from 4K to 8K. Summary: Hello and thank you for RocksDB, While looking into the buffered io used when an `OPTIONS` file is read I noticed the `OPTIONS` files produced by RocksDB 5.8.8 (and head of master) were just over 4096 bytes in size, resulting in the version of glibc I am using (glibc-2.17-196.el7) (on the filesystem used) being passed a 4K buffer for the `fread_unlocked` call and 2 system call reads using a 4096 buffer being used to read the contents of the `OPTIONS` file. If the buffer size is increased to 8192 then 1 system call read is used to read the contents. As I think the buffer size is just used for reading `OPTIONS` files, and I thought it likely that `OPTIONS` files have increased in size (as more options are added), I thought I would suggest an increase. [ If the comments from the top of the `OPTIONS` file are removed, and white space from the start of lines is removed then the size can be reduced to be under 4K, but as more options are added the size seems likely to grow again. ] Create a new database: ``` > ./ldb put 1 1 OK ``` The OPTIONS file is 4252 bytes: ``` > stat /tmp/rdb_tmp/OPTIONS* | head 2 File: /tmp/rdb_tmp/OPTIONS-000005 Size: 4252 Blocks: 16 IO Block: 4096 regular file ``` Before, the 4096 byte buffer is used from 2 system read calls: ``` > strace ./ldb get DOES_NOT_EXIST 2>&1 | grep 1 RocksDB option file read(3, ""# This is a RocksDB option file.""..., 4096) 4096 read(3, ""e\n metadata_block_size=4096\n c""..., 4096) 156 ``` ltrace shows 4096 passed to fread_unlocked ``` > ltrace ./ldb get DOES_NOT_EXIST 2>&1 | grep 3 RocksDB option file [pid 51013] fread_unlocked(0x7ffd5fbf2d50, 1, 4096, 0x7fd2e084e780 ...> [pid 51013] 0x7ffd5fbf28f0) 0 [pid 51013] 4096, 3, 34, 0) 0x7fd2e318c000 [pid 51013] ""# This is a RocksDB option file.""..., 4096) 4096 [pid 51013] fread_unlocked resumed> ) 4096 ... ``` After, the 8192 byte buffer is used from 1 system read call: ``` > strace ./ldb get DOES_NOT_EXIST 2>&1 | grep 1 RocksDB option file read(3, ""# This is a RocksDB option file.""..., 8192) 4252 read(3, """", 4096) 0 ``` ltrace shows 8192 passed to fread_unlocked ``` > ltrace ./ldb get DOES_NOT_EXIST 2>&1 | grep 3 RocksDB option file [pid 146611] fread_unlocked(0x7ffcfba382f0, 1, 8192, 0x7fc4e844e780 ...> [pid 146611] 0x7ffcfba380f0) 0 [pid 146611] 4096, 3, 34, 0) 0x7fc4eaee0000 [pid 146611] ""# This is a RocksDB option file.""..., 8192) 4252 [pid 146611] """", 4096) 0 [pid 146611] fread_unlocked resumed> ) 4252 [pid 146611] feof(0x7fc4e844e780) 1 ``` Closes Differential Revision: D6653684 Pulled By: ajkr fbshipit-source-id: 222f25f5442fefe1dcec18c700bd9e235bb63491/"
1458,1458,14.0,0.9648000001907349,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make",Suppress unused warnings Summary: Use `__unused__` everywhere Suppress unused warnings in Release mode + This currently affects non-MSVC builds (e.g. mingw64). Closes Differential Revision: D6885496 Pulled By: miasantreble fbshipit-source-id: f2f6adacec940cc3851a9eee328fafbf61aad211/Blob DB: Add db_bench options Summary: Adding more BlobDB db_bench options which are needed for benchmarking. Closes Differential Revision: D6500711 Pulled By: sagar0 fbshipit-source-id: 91d63122905854ef7c9148a0235568719146e6c5/
1459,1459,14.0,0.743399977684021,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","WritePrepared Txn: PreReleaseCallback Summary: Add PreReleaseCallback to be called at the end of WriteImpl but before publishing the sequence number. The callback is used in WritePrepareTxn to i) update the commit map, ii) update the last published sequence number in the 2nd write queue. It also ensures that all the commits will go to the 2nd queue. These changes will ensure that the commit map is updated before the sequence number is published and used by reading snapshots. If we use two write queues, the snapshots will use the seq number published by the 2nd queue. If we use one write queue (the default, the snapshots will use the last seq number in the memtable, which also indicates the last published seq number. Closes Differential Revision: D6438959 Pulled By: maysamyabandeh fbshipit-source-id: f8b6c434e94bc5f5ab9cb696879d4c23e2577ab9/improve ldb CLI option support Summary: Made CLI arguments take precedence over options file when both are provided. Note some of the CLI args are not settable via options file, like `--compression_max_dict_bytes`, so its necessary to allow both ways of providing options simultaneously. Changed `PrepareOptionsForOpenDB` to update the proper `ColumnFamilyOptions` if one exists for the users `--column_family_name` argument. I supported this only in the base class, `LDBCommand`, so it works for the general arguments. Will defer adding support for subcommand-specific arguments. Made the command fail if `--try_load_options` is provided and loading options file returns NotFound. I found the previous behavior of silently continuing confusing. Closes Differential Revision: D6270544 Pulled By: ajkr fbshipit-source-id: 7c2eac9f9b38720523d74466fb9e78db53561367/"
1460,1460,10.0,0.9793000221252441,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","db_stress: skip snapshot check if cf is dropped Summary: We added a new verification that ensures a value that snapshot reads when is released is the same as when it was created. This test however fails when the cf is dropped in between. The patch skips the tests if that was the case. Closes Differential Revision: D6581584 Pulled By: maysamyabandeh fbshipit-source-id: afe37d371c0f91818d2e279b3949b810e112e8eb/WritePrepared Txn: make db_stress transactional Summary: Add ""--use_txn"" option to use transactional API in db_stress, default being WRITE_PREPARED policy, which is the main intention of modifying db_stress. It also extend the existing snapshots to verify that before releasing a snapshot a read from it returns the same value as before. Closes Differential Revision: D6556912 Pulled By: maysamyabandeh fbshipit-source-id: 1ae31465be362d44bd06e635e2e9e49a1da11268/"
1461,1461,1.0,0.9603999853134155,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","WritePrepared Txn: Duplicate Keys, Memtable part Summary: Currently DB does not accept duplicate keys (keys with the same user key and the same sequence number). If Memtable returns false when receiving such keys, we can benefit from this signal to properly increase the sequence number in the rare cases when we have a duplicate key in the write batch written to DB under WritePrepared transactions. Closes Differential Revision: D6822412 Pulled By: maysamyabandeh fbshipit-source-id: adea3ce5073131cd38ed52b16bea0673b1a19e77/"
1462,1462,1.0,0.9603999853134155,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","WritePrepared Txn: Duplicate Keys, Memtable part Summary: Currently DB does not accept duplicate keys (keys with the same user key and the same sequence number). If Memtable returns false when receiving such keys, we can benefit from this signal to properly increase the sequence number in the rare cases when we have a duplicate key in the write batch written to DB under WritePrepared transactions. Closes Differential Revision: D6822412 Pulled By: maysamyabandeh fbshipit-source-id: adea3ce5073131cd38ed52b16bea0673b1a19e77/"
1463,1463,1.0,0.9603999853134155,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","WritePrepared Txn: Duplicate Keys, Memtable part Summary: Currently DB does not accept duplicate keys (keys with the same user key and the same sequence number). If Memtable returns false when receiving such keys, we can benefit from this signal to properly increase the sequence number in the rare cases when we have a duplicate key in the write batch written to DB under WritePrepared transactions. Closes Differential Revision: D6822412 Pulled By: maysamyabandeh fbshipit-source-id: adea3ce5073131cd38ed52b16bea0673b1a19e77/"
1464,1464,1.0,0.9603999853134155,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","WritePrepared Txn: Duplicate Keys, Memtable part Summary: Currently DB does not accept duplicate keys (keys with the same user key and the same sequence number). If Memtable returns false when receiving such keys, we can benefit from this signal to properly increase the sequence number in the rare cases when we have a duplicate key in the write batch written to DB under WritePrepared transactions. Closes Differential Revision: D6822412 Pulled By: maysamyabandeh fbshipit-source-id: adea3ce5073131cd38ed52b16bea0673b1a19e77/"
1465,1465,16.0,0.9817000031471252,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Add a Close() method to DB to return status when closing a db Summary: Currently, the only way to close an open DB is to destroy the DB object. There is no way for the caller to know the status. In one instance, the destructor encountered an error due to failure to close a log file on HDFS. In order to prevent silent failures, we add DB::Close() that calls CloseImpl() which must be implemented by its descendants. The main failure point in the destructor is closing the log file. This patch also adds a Close() entry point to Logger in order to get status. When DBOptions::info_log is allocated and owned by the DBImpl, it is explicitly closed by DBImpl::CloseImpl(). Closes Differential Revision: D6698158 Pulled By: anand1976 fbshipit-source-id: 9468e2892553eb09c4c41b8723f590c0dbd8ab7d/"
1466,1466,16.0,0.9817000031471252,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Add a Close() method to DB to return status when closing a db Summary: Currently, the only way to close an open DB is to destroy the DB object. There is no way for the caller to know the status. In one instance, the destructor encountered an error due to failure to close a log file on HDFS. In order to prevent silent failures, we add DB::Close() that calls CloseImpl() which must be implemented by its descendants. The main failure point in the destructor is closing the log file. This patch also adds a Close() entry point to Logger in order to get status. When DBOptions::info_log is allocated and owned by the DBImpl, it is explicitly closed by DBImpl::CloseImpl(). Closes Differential Revision: D6698158 Pulled By: anand1976 fbshipit-source-id: 9468e2892553eb09c4c41b8723f590c0dbd8ab7d/"
1467,1467,16.0,0.9817000031471252,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Add a Close() method to DB to return status when closing a db Summary: Currently, the only way to close an open DB is to destroy the DB object. There is no way for the caller to know the status. In one instance, the destructor encountered an error due to failure to close a log file on HDFS. In order to prevent silent failures, we add DB::Close() that calls CloseImpl() which must be implemented by its descendants. The main failure point in the destructor is closing the log file. This patch also adds a Close() entry point to Logger in order to get status. When DBOptions::info_log is allocated and owned by the DBImpl, it is explicitly closed by DBImpl::CloseImpl(). Closes Differential Revision: D6698158 Pulled By: anand1976 fbshipit-source-id: 9468e2892553eb09c4c41b8723f590c0dbd8ab7d/"
1468,1468,6.0,0.6931999921798706,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","WritePrepared Txn: Duplicate Keys, Txn Part Summary: This patch takes advantage of memtable being able to detect duplicate and returning TryAgain to handle duplicate keys in WritePrepared Txns. Through WriteBatchWithIndexs index it detects existence of at least a duplicate key in the write batch. If duplicate key was reported, it then pays the cost of counting the number of sub-patches by iterating over the write batch and pass it to DBImpl::Write. DB will make use of the provided batch_count to assign proper sequence numbers before sending them to the WAL. When later inserting the batch to the memtable, it increases the seq each time memtbale reports a duplicate (a sub-patch in our counting) and tries again. Closes Differential Revision: D6873699 Pulled By: maysamyabandeh fbshipit-source-id: db8487526c3a5dc1ddda0ea49f0f979b26ae648d/WritePrepared Txn: address some pending TODOs Summary: This patch addresses a couple of minor TODOs for WritePrepared Txn such as double checking some assert statements at runtime as well, skip extra AddPrepared in non-2pc transactions, and safety check for infinite loops. Closes Differential Revision: D6617002 Pulled By: maysamyabandeh fbshipit-source-id: ef6673c139cb49f64c0879508d2f573b78609aca/WritePrepared Txn: non-2pc write in one round Summary: Currently non-2pc writes do the 2nd dummy write to actually commit the transaction. This was necessary to ensure that publishing the commit sequence number will be done only from one queue (the queue that does not write to memtable). This is however not necessary when we have only one write queue, which is actually the setup that would be used by non-2pc writes. This patch eliminates the 2nd write when two_write_queues are disabled by updating the commit map in the 1st write. Closes Differential Revision: D6575392 Pulled By: maysamyabandeh fbshipit-source-id: 8ab458f7ca506905962f9166026b2ec81e749c46/WritePrepared Txn: PreReleaseCallback Summary: Add PreReleaseCallback to be called at the end of WriteImpl but before publishing the sequence number. The callback is used in WritePrepareTxn to i) update the commit map, ii) update the last published sequence number in the 2nd write queue. It also ensures that all the commits will go to the 2nd queue. These changes will ensure that the commit map is updated before the sequence number is published and used by reading snapshots. If we use two write queues, the snapshots will use the seq number published by the 2nd queue. If we use one write queue (the default, the snapshots will use the last seq number in the memtable, which also indicates the last published seq number. Closes Differential Revision: D6438959 Pulled By: maysamyabandeh fbshipit-source-id: f8b6c434e94bc5f5ab9cb696879d4c23e2577ab9/"
1469,1469,6.0,0.9003000259399414,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Blob DB: miscellaneous changes Summary: * Expose garbage collection related options * Minor logging and counter name update * Remove unused constants. Closes Differential Revision: D6867077 Pulled By: yiwu-arbug fbshipit-source-id: 6c3272a9c9d78b125a0bd6b2e56d00d087cdd6c8/Blob DB: fix crash when DB full but no candidate file to evict Summary: When blob_files is empty, std::min_element will return blobfiles.end(), which cannot be dereference. Fixing it. Closes Differential Revision: D6764927 Pulled By: yiwu-arbug fbshipit-source-id: 86f78700132be95760d35ac63480dfd3a8bbe17a/Blob DB: avoid having a separate read of checksum Summary: Previously on a blob db read, we are making a read of the blob value, and then make another read to get CRC checksum. Im combining the two read into one. readrandom db_bench with 1G database with base db size of 13M, value size 1k: `./db_bench master: throughput 234MB/s, get micros p50 5.984 p95 9.998 p99 20.817 p100 787 this PR: throughput 261MB/s, get micros p50 5.157 p95 9.928 p99 20.724 p100 190 Closes Differential Revision: D6615950 Pulled By: yiwu-arbug fbshipit-source-id: 052410c6d8539ec0cc305d53793bbc8f3616baa3/BlobDB: Remove the need to get sequence number per write Summary: Previously we store sequence number range of each blob files, and use the sequence number range to check if the file can be possibly visible by a snapshot. But it adds complexity to the code, since the sequence number is only available after a write. (The current implementation get sequence number by calling GetLatestSequenceNumber(), which is wrong.) With the patch, we are not storing sequence number range, and check if snapshot_sequence obsolete_sequence to decide if the file is visible by a snapshot (previously we check if first_sequence snapshot_sequence obsolete_sequence). Closes Differential Revision: D6571497 Pulled By: yiwu-arbug fbshipit-source-id: ca06479dc1fcd8782f6525b62b7762cd47d61909/BlobDB: refactor DB open logic Summary: Refactor BlobDB open logic. List of changes: Major: * On reopen, mark blob files found as immutable, do not use them for writing new keys. * Not to scan the whole file to find file footer. Instead just seek to the end of the file and try to read footer. Minor: * Move most of the real logic from blob_db.cc to blob_db_impl.cc. * Not to hold shared_ptr of event listeners in global maps in blob_db.cc * Some changes to BlobFile interface. * Improve logging and error handling. Closes Differential Revision: D6526147 Pulled By: yiwu-arbug fbshipit-source-id: 9dc4cdd63359a2f9b696af817086949da8d06952/utilities: Fix coverity issues in blob_db and col_buf_decoder Summary: utilities/blob_db/blob_db_impl.cc 265 : bdb_options_.blob_dir; 3. uninit_member: Non-static class member env_ is not initialized in this constructor nor in any functions that it calls. 5. uninit_member: Non-static class member ttl_extractor_ is not initialized in this constructor nor in any functions that it calls. 7. uninit_member: Non-static class member open_p1_done_ is not initialized in this constructor nor in any functions that it calls. CID 1418245 (#1 of 1): Uninitialized pointer field (UNINIT_CTOR) 9. uninit_member: Non-static class member debug_level_ is not initialized in this constructor nor in any functions that it calls. 266} 4. past_the_end: Function end creates an iterator. CID 1418258 (#1 of 1): Using invalid iterator (INVALIDATE_ITERATOR) 5. deref_iterator: Dereferencing iterator file_nums.end() though it is already past the end of its container. utilities/col_buf_decoder.h: nullable_(nullable), 2. uninit_member: Non-static class member remain_runs_ is not initialized in this constructor nor in any functions that it calls. 4. uninit_member: Non-static class member run_val_ is not initialized in this constructor nor in any functions that it calls. CID 1396134 (#1 of 1): Uninitialized scalar field (UNINIT_CTOR) 6. uninit_member: Non-static class member last_val_ is not initialized in this constructor nor in any functions that it calls. 46 big_endian_(big_endian) {} Closes Differential Revision: D6340607 Pulled By: sagar0 fbshipit-source-id: 25c52566e2ff979fe6c7abb0f40c27fc16597054/Blob DB: Add statistics Summary: Adding a list of blob db counters. Also remove WaStats() which doesnt expose the stats and can be substitute by (BLOB_DB_BYTES_WRITTEN / BLOB_DB_BLOB_FILE_BYTES_WRITTEN). Closes Differential Revision: D6394216 Pulled By: yiwu-arbug fbshipit-source-id: 017508c8ff3fcd7ea7403c64d0f9834b24816803/Blob DB: not using PinnableSlice move assignment Summary: The current implementation of PinnableSlice move assignment have an issue We are moving away from it instead of try to get the move assignment right, since it is too tricky. Closes Differential Revision: D6319201 Pulled By: yiwu-arbug fbshipit-source-id: 8f3279021f3710da4a4caa14fd238ed2df902c48/"
1470,1470,1.0,0.9735999703407288,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Add a BlockBasedTableOption to turn off index block compression. Summary: Add a new bool option index_uncompressed in BlockBasedTableOptions. Closes Differential Revision: D6686161 Pulled By: anand1976 fbshipit-source-id: 748b46993d48a01e5f89b6bd3e41f06a59ec6054/Revert ""No need for Restart Interval for meta blocks"" Summary: See [issue 3169]( for more information This reverts commit 593d3de37171d99a761ce2ab34ffa12654acd055. Closes Differential Revision: D6379271 Pulled By: miasantreble fbshipit-source-id: 88f9ed67ba52237ad9b6f7251db83672b62d7537/Fix calculating filter partition target size Summary: block_size_deviation is in percentage while the partition size is in bytes. The current code fails to take that into account resulting into very large target size for filter partitions. Closes Differential Revision: D6376069 Pulled By: maysamyabandeh fbshipit-source-id: 276546fc68f50e0da32c462abb46f6cf676db9b2/"
1471,1471,4.0,0.9052000045776367,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Eliminate a memcpy for uncompressed blocks Summary: `ReadBlockFromFile` uses a stack buffer to hold small data blocks before passing them to the compression library, which outputs uncompressed data in a heap buffer. In the case of `kNoCompression` there is a `memcpy` to copy from stack buffer to heap buffer. This PR optimizes `ReadBlockFromFile` to skip the stack buffer for files whose blocks are known to be uncompressed. We determine this using the SST file property, ""compression_name"", if its available. Closes Differential Revision: D6920848 Pulled By: ajkr fbshipit-source-id: 5c753e804efc178b9229ae5dbe6a4adc32031f07/Update rocksdb.read.block.get.micros when block cache disabled Summary: Previously `ReadBlockFromFile` for data blocks was only measured when reading a block to populate block cache. This PR adds the corresponding measurements for users who disabled block cache. Closes Differential Revision: D6848671 Pulled By: ajkr fbshipit-source-id: bb4bbe1797fa2cc1d9a5bad44891af2b55384b41/Use block cache to track memory usage when ReadOptions.fill_cache=false Summary: ReadOptions.fill_cache is set in compaction inputs and can be set by users in their queries too. It tells RocksDB not to put a data block used to block cache. The memory used by the data block is, however, not trackable by users. To make the system more manageable, we can cost the block to block cache while using it, and then release it after using. Closes Differential Revision: D6670230 Pulled By: miasantreble fbshipit-source-id: ab848d3ed286bd081a13ee1903de357b56cbc308/Improve performance of long range scans with readahead Summary: This change improves the performance of iterators doing long range scans (e.g. big/full table scans in MyRocks) by using readahead and prefetching additional data on each disk IO. This prefetching is automatically enabled on noticing more than 2 IOs for the same table file during iteration. The readahead size starts with 8KB and is exponentially increased on each additional sequential IO, up to a max of 256 KB. This helps in cutting down the number of IOs needed to complete the range scan. Constraints: The prefetched data is stored by the OS in page cache. So this currently works only for non direct-reads use-cases i.e applications which use page cache. (Direct-I/O support will be enabled in a later PR). This gets currently enabled only when ReadOptions.readahead_size 0 (which is the default value). Thanks to siying for the original idea and implementation. **Benchmarks:** Data fill: ``` TEST_TMPDIR=/data/users/$USER/benchmarks/iter ./db_bench ``` Do a long range scan: Seekrandom with large number of nexts ``` TEST_TMPDIR=/data/users/$USER/benchmarks/iter ./db_bench ``` Page cache was cleared before each experiment with the command: ``` sudo sh ""echo 3 > /proc/sys/vm/drop_caches"" ``` ``` Before: seekrandom : 34020.945 micros/op 29 ops/sec; 32.5 MB/s (1636 of 1999 found) With this change: seekrandom : 8726.912 micros/op 114 ops/sec; 126.8 MB/s (5702 of 6999 found) ``` ~3.9X performance improvement. Also verified with strace and gdb that the readahead size is increasing as expected. ``` strace readahead process pid> ``` Closes Differential Revision: D6586477 Pulled By: sagar0 fbshipit-source-id: 8a118a0ed4594fbb7f5b1cafb242d7a4033cb58c/BlockBasedTable::NewDataBlockIterator to always return BlockIter Summary: This is a pre-cleaning up before a major block based table iterator refactoring. BlockBasedTable::NewDataBlockIterator() will always return BlockIter. This simplifies the logic and code and enable further refactoring and optimization. Closes Differential Revision: D6780165 Pulled By: siying fbshipit-source-id: 273f7dc896724f682c0118fb69a359d9cc4418b4/Eliminate some redundant block reads. Summary: Re-use metadata for reading Compression Dictionary on BlockBased table open, this saves two reads from disk. This helps to our 999 percentile in 5.6.1 where prefetch buffer is not present. Closes Differential Revision: D6695753 Pulled By: ajkr fbshipit-source-id: bb8acd9e9e66e65b89c548ab8940570ae360333c/Reduce heavy hitter for Get operation Summary: This PR addresses the following heavy hitters in `Get` operation by moving calls to `StatisticsImpl::recordTick` from `BlockBasedTable` to `Version::Get` rocksdb.block.cache.bytes.write rocksdb.block.cache.add rocksdb.block.cache.data.miss rocksdb.block.cache.data.bytes.insert rocksdb.block.cache.data.add rocksdb.block.cache.hit rocksdb.block.cache.data.hit rocksdb.block.cache.bytes.read The db_bench statistics before and after the change are: |1GB block read|Children |Self |Command |Shared Object |Symbol| |---|---|---|---|---|---| |master: |4.22% |1.31% |db_bench |db_bench |[.] rocksdb::StatisticsImpl::recordTick| |updated: |0.51% |0.21% |db_bench |db_bench |[.] rocksdb::StatisticsImpl::recordTick| | |0.14% |0.14% |db_bench |db_bench |[.] rocksdb::GetContext::record_counters| |1MB block read|Children |Self |Command |Shared Object |Symbol| |---|---|---|---|---|---| |master: |3.48% |1.08% |db_bench |db_bench |[.] rocksdb::StatisticsImpl::recordTick| |updated: |0.80% |0.31% |db_bench |db_bench |[.] rocksdb::StatisticsImpl::recordTick| | |0.35% |0.35% |db_bench |db_bench |[.] rocksdb::GetContext::record_counters| Closes Differential Revision: D6330532 Pulled By: miasantreble fbshipit-source-id: 2b492959e00a3db29e9437ecdcc5e48ca4ec5741/Fix memory issue introduced by 2f1a3a4d748ea92c282a1302b1523adc6d67ce81 Summary: Closes Differential Revision: D6541714 Pulled By: siying fbshipit-source-id: 40efd89b68587a9d58cfe6f4eebd771c2d9f1542/Refactor ReadBlockContents() Summary: Divide ReadBlockContents() to multiple sub-functions. Maintaining the input and intermediate data in a new class BlockFetcher. I hope in general it makes the code easier to maintain. Another motivation to do it is to clearly divide the logic before file reading and after file reading. The refactor will help us evaluate how can we make I/O async in the future. Closes Differential Revision: D6520983 Pulled By: siying fbshipit-source-id: 338d90bc0338472d46be7a7682028dc9114b12e9/"
1472,1472,11.0,0.8812000155448914,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",Split HarnessTest_Randomized to avoid timeout Summary: Split HarnessTest_Randomized to two tests Closes Differential Revision: D6826006 Pulled By: maysamyabandeh fbshipit-source-id: 59c9a11c7da092206effce6e4fa3792f9c66bef2/
1473,1473,6.0,0.9789000153541565,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","WritePrepared Txn: update compaction_iterator_test and db_iterator_test Summary: Update compaction_iterator_test with write-prepared transaction DB related tests. Transaction related tests are group in CompactionIteratorWithSnapshotCheckerTest. The existing test are duplicated to make them also test with dummy SnapshotChecker that will say every key is visible to every snapshot (this is okay, we still compare sequence number to verify visibility). Merge related tests are disabled and will be revisit in another PR. Existing db_iterator_tests are also duplicated to test with dummy read_callback that will say every key is committed. Closes Differential Revision: D6909253 Pulled By: yiwu-arbug fbshipit-source-id: 2ae4656b843a55e2e9ff8beecf21f2832f96cd25/"
1474,1474,11.0,0.48350000381469727,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","log flush reason for better debugging experience Summary: Its always a mystery from the logs why flush was triggered user triggered it manually, WriteBufferManager triggered it, logs were full, write buffer was full, etc. This PR logs Flush reason whenever a flush is scheduled. Closes Differential Revision: D6788142 Pulled By: miasantreble fbshipit-source-id: a867e54d493c06adf5172bd36a180fb3faae3511/fix live WALs purged while file deletions disabled Summary: When calling `DisableFileDeletions` followed by `GetSortedWalFiles`, we guarantee the files returned by the latter call wont be deleted until after file deletions are re-enabled. However, `GetSortedWalFiles` didnt omit files already planned for deletion via `PurgeObsoleteFiles`, so the guarantee could be broken. We fix it by making `GetSortedWalFiles` wait for the number of pending purges to hit zero if file deletions are disabled. This condition is eventually met since `PurgeObsoleteFiles` is guaranteed to be called for the existing pending purges, and new purges cannot be scheduled while file deletions are disabled. Once the condition is met, `GetSortedWalFiles` simply returns the content of DB and archive directories, which nobody can delete (except for deletion scheduler, for which I plan to fix this bug later) until deletions are re-enabled. Closes Differential Revision: D6681131 Pulled By: ajkr fbshipit-source-id: 90b1e2f2362ea9ef715623841c0826611a817634/Add a Close() method to DB to return status when closing a db Summary: Currently, the only way to close an open DB is to destroy the DB object. There is no way for the caller to know the status. In one instance, the destructor encountered an error due to failure to close a log file on HDFS. In order to prevent silent failures, we add DB::Close() that calls CloseImpl() which must be implemented by its descendants. The main failure point in the destructor is closing the log file. This patch also adds a Close() entry point to Logger in order to get status. When DBOptions::info_log is allocated and owned by the DBImpl, it is explicitly closed by DBImpl::CloseImpl(). Closes Differential Revision: D6698158 Pulled By: anand1976 fbshipit-source-id: 9468e2892553eb09c4c41b8723f590c0dbd8ab7d/WritePrepared Txn: PreReleaseCallback Summary: Add PreReleaseCallback to be called at the end of WriteImpl but before publishing the sequence number. The callback is used in WritePrepareTxn to i) update the commit map, ii) update the last published sequence number in the 2nd write queue. It also ensures that all the commits will go to the 2nd queue. These changes will ensure that the commit map is updated before the sequence number is published and used by reading snapshots. If we use two write queues, the snapshots will use the seq number published by the 2nd queue. If we use one write queue (the default, the snapshots will use the last seq number in the memtable, which also indicates the last published seq number. Closes Differential Revision: D6438959 Pulled By: maysamyabandeh fbshipit-source-id: f8b6c434e94bc5f5ab9cb696879d4c23e2577ab9/"
1475,1475,11.0,0.5103999972343445,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","fix DBTest2.ReadAmpBitmapLiveInCacheAfterDBClose file ID support check Summary: Updated the test case to handle tmpfs mounted at directories different from ""/dev/shm/"". Closes Differential Revision: D6848213 Pulled By: ajkr fbshipit-source-id: 465e9dbf0921d0930161f732db6b3766bb030589/fix live WALs purged while file deletions disabled Summary: When calling `DisableFileDeletions` followed by `GetSortedWalFiles`, we guarantee the files returned by the latter call wont be deleted until after file deletions are re-enabled. However, `GetSortedWalFiles` didnt omit files already planned for deletion via `PurgeObsoleteFiles`, so the guarantee could be broken. We fix it by making `GetSortedWalFiles` wait for the number of pending purges to hit zero if file deletions are disabled. This condition is eventually met since `PurgeObsoleteFiles` is guaranteed to be called for the existing pending purges, and new purges cannot be scheduled while file deletions are disabled. Once the condition is met, `GetSortedWalFiles` simply returns the content of DB and archive directories, which nobody can delete (except for deletion scheduler, for which I plan to fix this bug later) until deletions are re-enabled. Closes Differential Revision: D6681131 Pulled By: ajkr fbshipit-source-id: 90b1e2f2362ea9ef715623841c0826611a817634/"
1476,1476,18.0,0.9366999864578247,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet",Support skipping bloom filters for SstFileWriter Summary: Add an option for SstFileWriter to skip building bloom filters Closes Differential Revision: D6709120 Pulled By: IslamAbdelRahman fbshipit-source-id: 964d4bce38822a048691792f447bcfbb4b6bd809/
1477,1477,16.0,0.9922999739646912,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Fix DBTest::SoftLimit TSAN failure Summary: Fix data race found by TSAN around WriteStallListener: Closes Differential Revision: D6762167 Pulled By: yiwu-arbug fbshipit-source-id: cd3a5c9f806de390bd1af6077ea6dbbc8bcaec09/fix DBTest.AutomaticConflictsWithManualCompaction Summary: After af92d4ad112f192693f6017f24f9ae1b00e1f053, only exclusive manual compaction can have conflict. dc360df81ec48e56a5d9cee4adb7f11ef0ca82ac updated the conflict-checking test case accordingly. But we missed the point that exclusive manual compaction can only conflict with automatic compactions scheduled after it, since it waits on pending automatic compactions before it begins running. This PR updates the test case to ensure the automatic compactions are scheduled after the manual compaction starts but before it finishes, thus ensuring a conflict. I also cleaned up the test case to use less space as I saw it cause out-of-space error on travis. Closes Differential Revision: D6735162 Pulled By: ajkr fbshipit-source-id: 020530a4e150a4786792dce7cec5d66b420cb884/Fix multiple build failures Summary: * Fix DBTest.CompactRangeWithEmptyBottomLevel lite build failure * Fix DBTest.AutomaticConflictsWithManualCompaction failure introduce by * Fix BlockBasedTableTest::IndexUncompressed should be disabled if snappy is disabled * Fix ASAN failure with DBBasicTest::DBClose test Closes Differential Revision: D6732313 Pulled By: yiwu-arbug fbshipit-source-id: 1eb9b9d9a8d795f56188fa9770db9353f6fdedc5/Add a Close() method to DB to return status when closing a db Summary: Currently, the only way to close an open DB is to destroy the DB object. There is no way for the caller to know the status. In one instance, the destructor encountered an error due to failure to close a log file on HDFS. In order to prevent silent failures, we add DB::Close() that calls CloseImpl() which must be implemented by its descendants. The main failure point in the destructor is closing the log file. This patch also adds a Close() entry point to Logger in order to get status. When DBOptions::info_log is allocated and owned by the DBImpl, it is explicitly closed by DBImpl::CloseImpl(). Closes Differential Revision: D6698158 Pulled By: anand1976 fbshipit-source-id: 9468e2892553eb09c4c41b8723f590c0dbd8ab7d/"
1478,1478,11.0,0.9921000003814697,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Reduce heavy hitter for Get operation Summary: This PR addresses the following heavy hitters in `Get` operation by moving calls to `StatisticsImpl::recordTick` from `BlockBasedTable` to `Version::Get` rocksdb.block.cache.bytes.write rocksdb.block.cache.add rocksdb.block.cache.data.miss rocksdb.block.cache.data.bytes.insert rocksdb.block.cache.data.add rocksdb.block.cache.hit rocksdb.block.cache.data.hit rocksdb.block.cache.bytes.read The db_bench statistics before and after the change are: |1GB block read|Children |Self |Command |Shared Object |Symbol| |---|---|---|---|---|---| |master: |4.22% |1.31% |db_bench |db_bench |[.] rocksdb::StatisticsImpl::recordTick| |updated: |0.51% |0.21% |db_bench |db_bench |[.] rocksdb::StatisticsImpl::recordTick| | |0.14% |0.14% |db_bench |db_bench |[.] rocksdb::GetContext::record_counters| |1MB block read|Children |Self |Command |Shared Object |Symbol| |---|---|---|---|---|---| |master: |3.48% |1.08% |db_bench |db_bench |[.] rocksdb::StatisticsImpl::recordTick| |updated: |0.80% |0.31% |db_bench |db_bench |[.] rocksdb::StatisticsImpl::recordTick| | |0.35% |0.35% |db_bench |db_bench |[.] rocksdb::GetContext::record_counters| Closes Differential Revision: D6330532 Pulled By: miasantreble fbshipit-source-id: 2b492959e00a3db29e9437ecdcc5e48ca4ec5741/Preserve overlapping file endpoint invariant Summary: Fix for In `DeleteFilesInRange`, use `GetCleanInputsWithinInterval` instead of `GetOverlappingInputs` to make sure we get a clean cut set of files to delete. In `GetCleanInputsWithinInterval`, support nullptr as `begin_key` or `end_key`. In `GetOverlappingInputsRangeBinarySearch`, move the assertion for non-empty range away from `ExtendFileRangeWithinInterval`, which should be allowed to return an empty range (via `end_index begin_index`). Closes Differential Revision: D5772387 Pulled By: ajkr fbshipit-source-id: e554e8461823c6be82b21a9262a2da02b3957881/WritePrepared Txn: PreReleaseCallback Summary: Add PreReleaseCallback to be called at the end of WriteImpl but before publishing the sequence number. The callback is used in WritePrepareTxn to i) update the commit map, ii) update the last published sequence number in the 2nd write queue. It also ensures that all the commits will go to the 2nd queue. These changes will ensure that the commit map is updated before the sequence number is published and used by reading snapshots. If we use two write queues, the snapshots will use the seq number published by the 2nd queue. If we use one write queue (the default, the snapshots will use the last seq number in the memtable, which also indicates the last published seq number. Closes Differential Revision: D6438959 Pulled By: maysamyabandeh fbshipit-source-id: f8b6c434e94bc5f5ab9cb696879d4c23e2577ab9/Make DBOption compaction_readahead_size dynamic Summary: Closes Differential Revision: D6056141 Pulled By: miasantreble fbshipit-source-id: 56df1630f464fd56b07d25d38161f699e0528b7f/"
1479,1479,7.0,0.9883999824523926,"test_plan, test, summary, file, revision, compaction, use, new, review, change, make, reviewer, differential_revision, level, option, write, also, case, add, unit_test","Disable onboard cache for compaction output Summary: FILE_FLAG_WRITE_THROUGH is for disabling device on-board cache in windows API, which should be disabled if user doesnt need system cache. There was a perf issue related with this, we found during memtable flush, the high percentile latency jumps significantly. During profiling, we found those high latency (P99.9) read requests got queue-jumped by write requests from memtable flush and takes 80ms or even more time to wait, even when SSD overall IO throughput is relatively low. After enabling FILE_FLAG_WRITE_THROUGH, we rerun the test found high percentile latency drops a lot without observable impact on writes. Scenario 1: 40MB/s + 40MB/s R/W compaction throughput Original | FILE_FLAG_WRITE_THROUGH | Percentage reduction P99.9 | 56.897 ms | 35.593 ms | P99 | 3.905 ms | 3.896 ms | Scenario 2: 14MB/s + 14MB/s R/W compaction throughput, cohosted with 100+ other rocksdb instances have manually triggered memtable flush operations (memtable is tiny), creating a lot of randomized the small file writes operations during test. Original | FILE_FLAG_WRITE_THROUGH | Percentage reduction P99.9 | 86.227 ms | 50.436 ms | P99 | 8.415 ms | 3.356 ms | Closes Differential Revision: D6624174 Pulled By: miasantreble fbshipit-source-id: 321b86aee9d74470840c70e5d0d4fa9880660a91/"
1480,1480,13.0,0.4099999964237213,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Adjust pread/pwrite to return Status Summary: Returning bytes_read causes the caller to call GetLastError() to report failure but the lasterror may be overwritten by then so we lose the error code. Fix up CMake file to include xpress source code only when needed. Fix warning for the uninitialized var. Closes Differential Revision: D7832935 Pulled By: anand1976 fbshipit-source-id: 4be21affb9b85d361b96244f4ef459f492b7cb2b/Second attempt at db_stress crash-recovery verification Summary: Original commit: a4fb1f8c049ee9d61a9da8cf23b64d2c7d36a33f Revert commit (we reverted as a quick fix to get crash tests passing): 6afe22db2e667799d8c903db61750d676bffe152 This PR includes the contents of the original commit plus two bug fixes, which are: In whitebox crash test, only set `--expected_values_path` for `db_stress` runs in the first half of the crash tests duration. In the second half, a fresh DB is created for each `db_stress` run, so we cannot maintain expected state across `db_stress` runs. Made `Exists()` return true for `UNKNOWN_SENTINEL` values. I previously had an assert in `Exists()` that value was not `UNKNOWN_SENTINEL`. But it is possible for post-crash-recovery expected values to be `UNKNOWN_SENTINEL` (i.e., if the crash happens in the middle of an update), in which case this assertion would be tripped. The effect of returning true in this case is there may be cases where a `SingleDelete` deletes no data. But if we had returned false, the effect would be calling `SingleDelete` on a key with multiple older versions, which is not supported. Closes Differential Revision: D7811671 Pulled By: ajkr fbshipit-source-id: 67e0295bfb1695ff9674837f2e05bb29c50efc30/revert db_stress crash-recovery verification Summary: crash-recovery verification is failing in the whitebox testing, which may or may not be a valid correctness issue need more time to investigate. In the meantime, reverting so we dont mask other failures. Closes Differential Revision: D7794516 Pulled By: ajkr fbshipit-source-id: 28ccdfdb9ec9b3b0fb08c15cbf9d2e282201ff33/Allow options file in db_stress and db_crashtest Summary: When options file is provided to db_stress, take supported options from the file instead of from flags Call `BuildOptionsTable` after `Open` so it can use `options_` once it has been populated either from flags or from file Allow options filename to be passed via `db_crashtest.py` Closes Differential Revision: D7755331 Pulled By: ajkr fbshipit-source-id: 5205cc5deb0d74d677b9832174153812bab9a60a/Add crash-recovery correctness check to db_stress Summary: Previously, our `db_stress` tool held the expected state of the DB in-memory, so after crash-recovery, there was no way to verify data correctness. This PR adds an option, `--expected_values_file`, which specifies a file holding the expected values. In black-box testing, the `db_stress` process can be killed arbitrarily, so updates to the `--expected_values_file` must be atomic. We achieve this by `mmap`ing the file and relying on `std::atomic<uint32_t>` for atomicity. Actually this doesnt provide a total guarantee on what we want as `std::atomic<uint32_t>` could, in theory, be translated into multiple stores surrounded by a mutex. We can verify our assumption by looking at `std::atomic::is_always_lock_free`. For the `mmap`d file, we didnt have an existing way to expose its contents as a raw memory buffer. This PR adds it in the `Env::NewMemoryMappedFileBuffer` function, and `MemoryMappedFileBuffer` class. `db_crashtest.py` is updated to use an expected values file for black-box testing. On the first iteration (when the DB is created), an empty file is provided as `db_stress` will populate it when it runs. On subsequent iterations, that same filename is provided so `db_stress` can check the data is as expected on startup. Closes Differential Revision: D7463144 Pulled By: ajkr fbshipit-source-id: c8f3e82c93e045a90055e2468316be155633bd8b/Improve db_stress with transactions Summary: db_stress was already capable running transactions by setting use_txn. Running it under stress showed a couple of problems fixed in this patch. The uncommitted transaction must be either rolled back or commit after recovery. Current implementation of WritePrepared transaction cannot handle cf drop before crash. Clarified that in the comments and added safety checks. When running with use_txn, clear_column_family_one_in must be set to 0. Closes Differential Revision: D7654419 Pulled By: maysamyabandeh fbshipit-source-id: a024bad80a9dc99677398c00d29ff17d4436b7f3/Implemented Knuth shuffle to construct permutation for selecting no_o Summary: verwrite_keys. Also changed each no_overwrite_key set to an unordered set, otherwise Knuth shuffle only gets you 2x time improvement, because insertion (and subsequent internal sorting) into an ordered set is the bottleneck. With this change, each iteration of permutation construction and prefix selection takes around 40 secs, as opposed to 360 secs previously. However, this still means that with the default 10 CF per blackbox test case, the test is going to time out given the default interval of 200 secs. Also, there is currently an assertion error affecting all blackbox tests in db_crashtest.py; this assertion error will be fixed in a future PR. Closes Differential Revision: D7624616 Pulled By: amytai fbshipit-source-id: ea64fbe83407ff96c1c0ecabbc6c830576939393/Support for Column family specific paths. Summary: In this change, an option to set different paths for different column families is added. This option is set via cf_paths setting of ColumnFamilyOptions. This option will work in a similar fashion to db_paths setting. Cf_paths is a vector of Dbpath values which contains a pair of the absolute path and target size. Multiple levels in a Column family can go to different paths if cf_paths has more than one path. To maintain backward compatibility, if cf_paths is not specified for a column family, db_paths setting will be used. Note that, if db_paths setting is also not specified, RocksDB already has code to use db_name as the only path. Changes : 1) A new member ""cf_paths"" is added to ImmutableCfOptions. This is set, based on cf_paths setting of ColumnFamilyOptions and db_paths setting of ImmutableDbOptions. This member is used to identify the path information whenever files are accessed. 2) Validation checks are added for cf_paths setting based on existing checks for db_paths setting. 3) DestroyDB, PurgeObsoleteFiles etc. are edited to support multiple cf_paths. 4) Unit tests are added appropriately. Closes Differential Revision: D6951697 Pulled By: ajkr fbshipit-source-id: 60d2262862b0a8fd6605b09ccb0da32bb331787d/"
1481,1481,10.0,0.9869999885559082,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Enable cancelling manual compactions if they hit the sfm size limit Summary: Manual compactions should be cancelled, just like scheduled compactions are cancelled, if sfm->EnoughRoomForCompaction is not true. Closes Differential Revision: D7457683 Pulled By: amytai fbshipit-source-id: 669b02fdb707f75db576d03d2c818fb98d1876f5/SstFileManager: add bytes_max_delete_chunk Summary: Add `bytes_max_delete_chunk` in SstFileManager so that we can drop a large file in multiple batches. Closes Differential Revision: D7358679 Pulled By: siying fbshipit-source-id: ef17f0da2f5723dbece2669485a9b91b3edc0bb7/Disallow compactions if there isnt enough free space Summary: This diff handles cases where compaction causes an ENOSPC error. This does not handle corner cases where another background job is started while compaction is running, and the other background job triggers ENOSPC, although we do allow the user to provision for these background jobs with SstFileManager::SetCompactionBufferSize. It also does not handle the case where compaction has finished and some other background job independently triggers ENOSPC. Usage: Functionality is inside SstFileManager. In particular, users should set SstFileManager::SetMaxAllowedSpaceUsage, which is the reference highwatermark for determining whether to cancel compactions. Closes Differential Revision: D7016941 Pulled By: amytai fbshipit-source-id: 8965ab8dd8b00972e771637a41b4e6c645450445/"
1482,1482,13.0,0.9940999746322632,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Second attempt at db_stress crash-recovery verification Summary: Original commit: a4fb1f8c049ee9d61a9da8cf23b64d2c7d36a33f Revert commit (we reverted as a quick fix to get crash tests passing): 6afe22db2e667799d8c903db61750d676bffe152 This PR includes the contents of the original commit plus two bug fixes, which are: In whitebox crash test, only set `--expected_values_path` for `db_stress` runs in the first half of the crash tests duration. In the second half, a fresh DB is created for each `db_stress` run, so we cannot maintain expected state across `db_stress` runs. Made `Exists()` return true for `UNKNOWN_SENTINEL` values. I previously had an assert in `Exists()` that value was not `UNKNOWN_SENTINEL`. But it is possible for post-crash-recovery expected values to be `UNKNOWN_SENTINEL` (i.e., if the crash happens in the middle of an update), in which case this assertion would be tripped. The effect of returning true in this case is there may be cases where a `SingleDelete` deletes no data. But if we had returned false, the effect would be calling `SingleDelete` on a key with multiple older versions, which is not supported. Closes Differential Revision: D7811671 Pulled By: ajkr fbshipit-source-id: 67e0295bfb1695ff9674837f2e05bb29c50efc30/Add crash-recovery correctness check to db_stress Summary: Previously, our `db_stress` tool held the expected state of the DB in-memory, so after crash-recovery, there was no way to verify data correctness. This PR adds an option, `--expected_values_file`, which specifies a file holding the expected values. In black-box testing, the `db_stress` process can be killed arbitrarily, so updates to the `--expected_values_file` must be atomic. We achieve this by `mmap`ing the file and relying on `std::atomic<uint32_t>` for atomicity. Actually this doesnt provide a total guarantee on what we want as `std::atomic<uint32_t>` could, in theory, be translated into multiple stores surrounded by a mutex. We can verify our assumption by looking at `std::atomic::is_always_lock_free`. For the `mmap`d file, we didnt have an existing way to expose its contents as a raw memory buffer. This PR adds it in the `Env::NewMemoryMappedFileBuffer` function, and `MemoryMappedFileBuffer` class. `db_crashtest.py` is updated to use an expected values file for black-box testing. On the first iteration (when the DB is created), an empty file is provided as `db_stress` will populate it when it runs. On subsequent iterations, that same filename is provided so `db_stress` can check the data is as expected on startup. Closes Differential Revision: D7463144 Pulled By: ajkr fbshipit-source-id: c8f3e82c93e045a90055e2468316be155633bd8b/"
1483,1483,1.0,0.8944000005722046,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Comment out unused variables Summary: Submitting on behalf of another employee. Closes Differential Revision: D7146025 Pulled By: ajkr fbshipit-source-id: 495ca5db5beec3789e671e26f78170957704e77e/
1484,1484,8.0,0.7590000033378601,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","Add a stat for MultiGet keys found, update memtable hit/miss stats Summary: 1. Add a new ticker stat rocksdb.number.multiget.keys.found to track the number of keys successfully read 2. Update rocksdb.memtable.hit/miss in DBImpl::MultiGet(). It was being done in DBImpl::GetImpl(), but not MultiGet Closes Differential Revision: D7677364 Pulled By: anand1976 fbshipit-source-id: af22bd0ef8ddc5cf2b4244b0a024e539fe48bca5/Brings the Java API for WriteBatch inline with the C++ API Summary: * Exposes status * Corrects some method naming * Adds missing functionality Closes Differential Revision: D7140790 Pulled By: sagar0 fbshipit-source-id: cbdab6c5a7ae4f3030fb46739e9060e381b26fa6/"
1485,1485,10.0,0.920799970626831,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Brings the Java API for WriteBatch inline with the C++ API Summary: * Exposes status * Corrects some method naming * Adds missing functionality Closes Differential Revision: D7140790 Pulled By: sagar0 fbshipit-source-id: cbdab6c5a7ae4f3030fb46739e9060e381b26fa6/
1486,1486,5.0,0.967199981212616,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Java wrapper for Native Comparators Summary: This is an abstraction for working with custom Comparators implemented in native C++ code from Java. Native code must directly extend `rocksdb::Comparator`. When the native code comparator is compiled into the RocksDB codebase, you can then create a Java Class, and JNI stub to wrap it. Useful if the C++/JNI barrier overhead is too much for your applications comparator performance. An example is provided in `java/rocksjni/native_comparator_wrapper_test.cc` and `java/src/main/java/org/rocksdb/NativeComparatorWrapperTest.java`. Closes Differential Revision: D7172605 Pulled By: miasantreble fbshipit-source-id: e24b7eb267a3bcb6afa214e0379a1d5e8a2ceabe/"
1487,1487,10.0,0.8242999911308289,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Brings the Java API for WriteBatch inline with the C++ API Summary: * Exposes status * Corrects some method naming * Adds missing functionality Closes Differential Revision: D7140790 Pulled By: sagar0 fbshipit-source-id: cbdab6c5a7ae4f3030fb46739e9060e381b26fa6/
1488,1488,16.0,0.8812000155448914,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need",Add TransactionDB and OptimisticTransactionDB to the Java API Summary: Closes Closes Closes Differential Revision: D7131402 Pulled By: sagar0 fbshipit-source-id: bcd34ce95ed88cc641786089ff4232df7b2f089f/
1489,1489,5.0,0.967199981212616,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Java wrapper for Native Comparators Summary: This is an abstraction for working with custom Comparators implemented in native C++ code from Java. Native code must directly extend `rocksdb::Comparator`. When the native code comparator is compiled into the RocksDB codebase, you can then create a Java Class, and JNI stub to wrap it. Useful if the C++/JNI barrier overhead is too much for your applications comparator performance. An example is provided in `java/rocksjni/native_comparator_wrapper_test.cc` and `java/src/main/java/org/rocksdb/NativeComparatorWrapperTest.java`. Closes Differential Revision: D7172605 Pulled By: miasantreble fbshipit-source-id: e24b7eb267a3bcb6afa214e0379a1d5e8a2ceabe/"
1490,1490,5.0,0.967199981212616,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Java wrapper for Native Comparators Summary: This is an abstraction for working with custom Comparators implemented in native C++ code from Java. Native code must directly extend `rocksdb::Comparator`. When the native code comparator is compiled into the RocksDB codebase, you can then create a Java Class, and JNI stub to wrap it. Useful if the C++/JNI barrier overhead is too much for your applications comparator performance. An example is provided in `java/rocksjni/native_comparator_wrapper_test.cc` and `java/src/main/java/org/rocksdb/NativeComparatorWrapperTest.java`. Closes Differential Revision: D7172605 Pulled By: miasantreble fbshipit-source-id: e24b7eb267a3bcb6afa214e0379a1d5e8a2ceabe/"
1491,1491,13.0,0.9174000024795532,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review","Second attempt at db_stress crash-recovery verification Summary: Original commit: a4fb1f8c049ee9d61a9da8cf23b64d2c7d36a33f Revert commit (we reverted as a quick fix to get crash tests passing): 6afe22db2e667799d8c903db61750d676bffe152 This PR includes the contents of the original commit plus two bug fixes, which are: In whitebox crash test, only set `--expected_values_path` for `db_stress` runs in the first half of the crash tests duration. In the second half, a fresh DB is created for each `db_stress` run, so we cannot maintain expected state across `db_stress` runs. Made `Exists()` return true for `UNKNOWN_SENTINEL` values. I previously had an assert in `Exists()` that value was not `UNKNOWN_SENTINEL`. But it is possible for post-crash-recovery expected values to be `UNKNOWN_SENTINEL` (i.e., if the crash happens in the middle of an update), in which case this assertion would be tripped. The effect of returning true in this case is there may be cases where a `SingleDelete` deletes no data. But if we had returned false, the effect would be calling `SingleDelete` on a key with multiple older versions, which is not supported. Closes Differential Revision: D7811671 Pulled By: ajkr fbshipit-source-id: 67e0295bfb1695ff9674837f2e05bb29c50efc30/Add crash-recovery correctness check to db_stress Summary: Previously, our `db_stress` tool held the expected state of the DB in-memory, so after crash-recovery, there was no way to verify data correctness. This PR adds an option, `--expected_values_file`, which specifies a file holding the expected values. In black-box testing, the `db_stress` process can be killed arbitrarily, so updates to the `--expected_values_file` must be atomic. We achieve this by `mmap`ing the file and relying on `std::atomic<uint32_t>` for atomicity. Actually this doesnt provide a total guarantee on what we want as `std::atomic<uint32_t>` could, in theory, be translated into multiple stores surrounded by a mutex. We can verify our assumption by looking at `std::atomic::is_always_lock_free`. For the `mmap`d file, we didnt have an existing way to expose its contents as a raw memory buffer. This PR adds it in the `Env::NewMemoryMappedFileBuffer` function, and `MemoryMappedFileBuffer` class. `db_crashtest.py` is updated to use an expected values file for black-box testing. On the first iteration (when the DB is created), an empty file is provided as `db_stress` will populate it when it runs. On subsequent iterations, that same filename is provided so `db_stress` can check the data is as expected on startup. Closes Differential Revision: D7463144 Pulled By: ajkr fbshipit-source-id: c8f3e82c93e045a90055e2468316be155633bd8b/Comment out unused variables Summary: Submitting on behalf of another employee. Closes Differential Revision: D7146025 Pulled By: ajkr fbshipit-source-id: 495ca5db5beec3789e671e26f78170957704e77e/"
1492,1492,16.0,0.9728999733924866,"close, summary, test, add, would, change, use, fix, call, table, support, also, file, revision_pulle, test_plan, failure, closes_differential, make, option, need","Fix the Logger::Close() and DBImpl::Close() design pattern Summary: The recent Logger::Close() and DBImpl::Close() implementation rely on calling the CloseImpl() virtual function from the destructor, which will not work. Refactor the implementation to have a private close helper function in derived classes that can be called by both CloseImpl() and the destructor. Closes Reviewed By: gfosco Differential Revision: D7049303 Pulled By: anand1976 fbshipit-source-id: 76a64cbf403209216dfe4864ecf96b5d7f3db9f4/"
1493,1493,4.0,0.9902999997138977,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Avoid directory renames in BackupEngine Summary: We used to name private directories like ""1.tmp"" while BackupEngine populated them, and then rename without the "".tmp"" suffix (i.e., rename ""1.tmp"" to ""1"") after all files were copied. On glusterfs, directory renames like this require operations across many hosts, and partial failures have caused operational problems. Fortunately we dont need to rename private directories. We already have a meta-file that uses the tempfile-rename pattern to commit a backup atomically after all its files have been successfully copied. So we can copy private files directly to their final location, so now theres no directory rename. Closes Differential Revision: D7705610 Pulled By: ajkr fbshipit-source-id: fd724a28dd2bf993ce323a5f2cb7e7d6980cc346/Comment out unused variables Summary: Submitting on behalf of another employee. Closes Differential Revision: D7146025 Pulled By: ajkr fbshipit-source-id: 495ca5db5beec3789e671e26f78170957704e77e/BackupEngine gluster-friendly file naming convention Summary: Use the rsync tempfile naming convention in our `BackupEngine`. The temp file follows the format, `.<filename>.<suffix>`, which is later renamed to `<filename>`. We fix `tmp` as the `<suffix>` as we dont need to use random bytes for now. The benefit is gluster treats this tempfile naming convention specially and applies hashing only to `<filename>`, so the file wont need to be linked or moved when its renamed. Our gluster team suggested this will make things operationally easier. Closes Differential Revision: D6893333 Pulled By: ajkr fbshipit-source-id: fd7622978f4b2487fce33cde40dd3124f16bcaa8/"
1494,1494,1.0,0.8446000218391418,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","WritePrepared Txn: rollback via commit Summary: Currently WritePrepared rolls back a transaction with prepare sequence number prepare_seq by i) write a single rollback batch with rollback_seq, ii) add rollback_seq> to commit cache, iii) remove prepare_seq from PrepareHeap. This is correct assuming that there is no snapshot taken when a transaction is rolled back. This is the case the way MySQL does rollback which is after recovery. Otherwise if max_evicted_seq advances the prepare_seq, the live snapshot might assume data as committed since it does not find them in CommitCache. The change is to simply add rollback_seq> to commit cache before removing prepare_seq from PrepareHeap. In this way if max_evicted_seq advances prpeare_seq, the existing mechanism that we have to check evicted entries against live snapshots will make sure that the live snapshot will not see the data of rolled back transaction. Closes Differential Revision: D7696193 Pulled By: maysamyabandeh fbshipit-source-id: c9a2d46341ddc03554dded1303520a1cab74ef9c/comment unused parameters to turn on flag Summary: This PR comments out the rest of the unused arguments which allow us to turn on the flag. This is the second part of a codemod relating to Closes Differential Revision: D7426121 Pulled By: Dayvedde fbshipit-source-id: 223994923b42bd4953eb016a0129e47560f7e352/WritePrepared Txn: rollback_merge_operands hack Summary: This is a hack as temporary fix of MyRocks with rollbacking the merge operands. The way MyRocks uses merge operands is without protection of locks, which violates the assumption behind the rollback algorithm. They are ok with not being rolled back as it would just create a gap in the autoincrement column. The hack add an option to disable the rollback of merge operands by default and only enables it to let the unit test pass. Closes Differential Revision: D7597177 Pulled By: maysamyabandeh fbshipit-source-id: 544be0f666c7e7abb7f651ec8b23124e05056728/WritePrepared Txn: fix smallest_prep atomicity issue Summary: We introduced smallest_prep optimization in this commit b225de7e10f02be6d00e96b9fb86dfef880babdf, which enables storing the smallest uncommitted sequence number along with the snapshot. This enables the readers that read from the snapshot to skip further checks and safely assumed the data is committed if its sequence number is less than smallest uncommitted when the snapshot was taken. The problem was that smallest uncommitted and the snapshot must be taken atomically, and the lack of atomicity had led to readers using a smallest uncommitted after the snapshot was taken and hence mistakenly skipping some data. This patch fixes the problem by i) separating the process of removing of prepare entries from the AddCommitted function, ii) removing the prepare entires AFTER the committed sequence number is published, iii) getting smallest uncommitted (from the prepare list) BEFORE taking a snapshot. This guarantees that the smallest uncommitted that is accompanied with a snapshot is less than or equal of such number if it was obtained atomically. Tested by running MySQLStyleTransactionTest/MySQLStyleTransactionTest.TransactionStressTest that was failing sporadically. Closes Differential Revision: D7581934 Pulled By: maysamyabandeh fbshipit-source-id: dc9d6f4fb477eba75d4d5927326905b548a96a32/WritePrepared Txn: smallest_prepare optimization Summary: The is an optimization to reduce lookup in the CommitCache when querying IsInSnapshot. The optimization takes the smallest uncommitted data at the time that the snapshot was taken and if the sequence number of the read data is lower than that number it assumes the data as committed. To implement this optimization two changes are required: i) The AddPrepared function must be called sequentially to avoid out of order insertion in the PrepareHeap (otherwise the top of the heap does not indicate the smallest prepare in future too), ii) non-2PC transactions also call AddPrepared if they do not commit in one step. Closes Differential Revision: D7388630 Pulled By: maysamyabandeh fbshipit-source-id: b79506238c17467d590763582960d4d90181c600/WritePrepared Txn: AddPrepared for all sub-batches Summary: Currently AddPrepared is performed only on the first sub-batch if there are duplicate keys in the write batch. This could cause a problem if the transaction takes too long to commit and the seq number of the first sub-patch moved to old_prepared_ but not the seq of the later ones. The patch fixes this by calling AddPrepared for all sub-patches. Closes Differential Revision: D7388635 Pulled By: maysamyabandeh fbshipit-source-id: 0ccd80c150d9bc42fe955e49ddb9d7ca353067b4/WritePrepared Txn: fix race condition on publishing seq Summary: This commit fixes a race condition on calling SetLastPublishedSequence. The function must be called only from the 2nd write queue when two_write_queues is enabled. However there was a bug that would also call it from the main write queue if CommitTimeWriteBatch is provided to the commit request and yet use_only_the_last_commit_time_batch_for_recovery optimization is not enabled. To fix that we penalize the commit request in such cases by doing an additional write solely to publish the seq number from the 2nd queue. Closes Differential Revision: D7361508 Pulled By: maysamyabandeh fbshipit-source-id: bf8f7a27e5cccf5425dccbce25eb0032e8e5a4d7/WritePrepared Txn: fix non-emptied PreparedHeap bug Summary: Under a certain sequence of accessing PreparedHeap, there was a bug that would not successfully empty the heap. This would result in performance issues when the heap content is moved to old_prepared_ after max_evicted_seq_ advances the orphan prepared sequence numbers. The patch fixed the bug and add more unit tests. It also does more logging when the unlikely scenarios are faced Closes Differential Revision: D7038486 Pulled By: maysamyabandeh fbshipit-source-id: f1e40bea558f67b03d2a29131fcb8734c65fce97/"
1495,1495,9.0,0.6355999708175659,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","BlobDB: Fix BlobDBImpl::GCFileAndUpdateLSM issues Summary: * Fix BlobDBImpl::GCFileAndUpdateLSM doesnt close the new file, and the new file will not be able to be garbage collected later. * Fix BlobDBImpl::GCFileAndUpdateLSM doesnt copy over metadata from old file to new file. Closes Differential Revision: D7355092 Pulled By: yiwu-arbug fbshipit-source-id: 4fa3594ac5ce376bed1af04a545c532cfc0088c4/Blob DB: Improve FIFO eviction Summary: Improving blob db FIFO eviction with the following changes, * Change blob_dir_size to max_db_size. Take into account SST file size when computing DB size. * FIFO now only take into account live sst files and live blob files. It is normal for disk usage to go over max_db_size because there are obsolete sst files and blob files pending deletion. * FIFO eviction now also evict TTL blob files thats still open. It doesnt evict non-TTL blob files. * If FIFO is triggered, it will pass an expiration and the current sequence number to compaction filter. Compaction filter will then filter inlined keys to evict those with an earlier expiration and smaller sequence number. So call LSM FIFO. * Compaction filter also filter those blob indexes where corresponding blob file is gone. * Add an event listener to listen compaction/flush event and update sst file size. * Implement DB::Close() to make sure base db, as well as event listener and compaction filter, destruct before blob db. * More blob db statistics around FIFO. * Fix some locking issue when accessing a blob file. Closes Differential Revision: D7139328 Pulled By: yiwu-arbug fbshipit-source-id: ea5edb07b33dfceacb2682f4789bea61de28bbfa/Comment out unused variables Summary: Submitting on behalf of another employee. Closes Differential Revision: D7146025 Pulled By: ajkr fbshipit-source-id: 495ca5db5beec3789e671e26f78170957704e77e/Blob DB: remove existing garbage collection implementation Summary: Red diff to remove existing implementation of garbage collection. The current approach is reference counting kind of approach and require a lot of effort to get the size counter right on compaction and deletion. Im going to go with a simple mark-sweep kind of approach and will send another PR for that. CompactionEventListener was added solely for blob db and it adds complexity and overhead to compaction iterator. Removing it as well. Closes Differential Revision: D7130190 Pulled By: yiwu-arbug fbshipit-source-id: c3a375ad2639a3f6ed179df6eda602372cc5b8df/Back out ""[codemod] comment out unused parameters"" Reviewed By: igorsugak fbshipit-source-id: 4a93675cc1931089ddd574cacdb15d228b1e5f37/"
1496,1496,6.0,0.9049999713897705,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Ignore empty filter block when data block is empty Summary: Close Closes Differential Revision: D7291706 Pulled By: ajkr fbshipit-source-id: 9dd8f40bd7716588e1e3fd6be0c2bc2766861f8c/
1497,1497,15.0,0.9761999845504761,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle",Skip duplicate bloom keys when whole_key and prefix are mixed Summary: Currently we rely on FilterBitsBuilder to skip the duplicate keys. It does that by comparing that hash of the key to the hash of the last added entry. This logic breaks however when we have whole_key_filtering mixed with prefix blooms as their addition to FilterBitsBuilder will be interleaved. The patch fixes that by comparing the last whole key and last prefix with the whole key and prefix of the new key respectively and skip the call to FilterBitsBuilder if it is a duplicate. Closes Differential Revision: D7744413 Pulled By: maysamyabandeh fbshipit-source-id: 15df73bbbafdfd754d4e1f42ea07f47b03bc5eb8/
1498,1498,0.0,0.9962000250816345,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Propagate fill_cache config to partitioned index iterator Summary: Currently the partitioned index iterator creates a new ReadOptions which ignores the fill_cache config set to ReadOptions passed by the user. The patch propagates fill_cache from the users ReadOptions to that of partition index iterator. Also it clarifies the contract of fill_cache that i) it does not apply to filters, ii) it still charges block cache for the size of the data block, it still pin the block if it is already in the block cache. Closes Differential Revision: D7678308 Pulled By: maysamyabandeh fbshipit-source-id: 53ed96424ae922e499e2d4e3580ddc3f0db893da/Stats for false positive rate of full filtesr Summary: Adds two stats to allow us measuring the false positive rate of full filters: The total count of positives: rocksdb.bloom.filter.full.positive The total count of true positives: rocksdb.bloom.filter.full.true.positive Not the term ""full"" in the stat name to indicate that they are meaningful in full filters. block-based filters are to be deprecated soon and supporting it is not worth the the additional cost of if-then-else branches. Closes Tested by: $ ./db_bench /dev/shm/rocksdb-tmpdb $ ./db_bench /dev/shm/rocksdb-tmpdb 2>&1 > /tmp/full.log $ grep filter.full /tmp/full.log rocksdb.bloom.filter.full.positive COUNT : 3628593 rocksdb.bloom.filter.full.true.positive COUNT : 3536026 which gives the false positive rate of 2.5% Closes Differential Revision: D7517570 Pulled By: maysamyabandeh fbshipit-source-id: 630ab1a473afdce404916d297035b6318de4c052/uint64_t and size_t changes to compile for iOS Summary: In attempting to build a static lib for use in iOS, I ran in to lots of type errors between uint64_t and size_t. This PR contains the changes I made to get `TARGET_OS=IOS make static_lib` to succeed while also getting Xcode to build successfully with the resulting `librocksdb.a` library imported. This also compiles for me on macOS and tests fine, but Im really not sure if I made the correct decisions about where to `static_cast` and where to change types. Also up for discussion: is iOS worth supporting? Getting the static lib is just part one, we arent providing any bridging headers or wrappers like the ObjectiveRocks project, it wont be a great experience. Closes Differential Revision: D7106457 Pulled By: gfosco fbshipit-source-id: 82ac2073de7e1f09b91f6b4faea91d18bd311f8e/Comment out unused variables Summary: Submitting on behalf of another employee. Closes Differential Revision: D7146025 Pulled By: ajkr fbshipit-source-id: 495ca5db5beec3789e671e26f78170957704e77e/Customized BlockBasedTableIterator and LevelIterator Summary: Use a customzied BlockBasedTableIterator and LevelIterator to replace current implementations leveraging two-level-iterator. Hope the customized logic will make code easier to understand. As a side effect, BlockBasedTableIterator reduces the allocation for the data block iterator object, and avoid the virtual function call to it, because we can directly reference BlockIter, a final class. Similarly, LevelIterator reduces virtual function call to the dummy iterator iterating the file metadata. It also enabled further optimization. The upper bound check is also moved from index block to data block. This implementation fits this iterator better. After the change, forwared iterator is slightly optimized to ensure we trim those iterators. The two-level-iterator now is only used by partitioned index, so it is simplified. Closes Differential Revision: D6809041 Pulled By: siying fbshipit-source-id: 7da3b9b1d3c8e9d9405302c15920af1fcaf50ffa/"
1499,1499,0.0,0.9872000217437744,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","Stats for false positive rate of full filtesr Summary: Adds two stats to allow us measuring the false positive rate of full filters: The total count of positives: rocksdb.bloom.filter.full.positive The total count of true positives: rocksdb.bloom.filter.full.true.positive Not the term ""full"" in the stat name to indicate that they are meaningful in full filters. block-based filters are to be deprecated soon and supporting it is not worth the the additional cost of if-then-else branches. Closes Tested by: $ ./db_bench /dev/shm/rocksdb-tmpdb $ ./db_bench /dev/shm/rocksdb-tmpdb 2>&1 > /tmp/full.log $ grep filter.full /tmp/full.log rocksdb.bloom.filter.full.positive COUNT : 3628593 rocksdb.bloom.filter.full.true.positive COUNT : 3536026 which gives the false positive rate of 2.5% Closes Differential Revision: D7517570 Pulled By: maysamyabandeh fbshipit-source-id: 630ab1a473afdce404916d297035b6318de4c052/"
1500,1500,3.0,0.9921000003814697,"use, summary, support, file, test, change, harmony_xnet, revision_pulle, main_java, add, new, closes_differential, remove, check, key, revision, case, also, would, size","Evenly split HarnessTest.Randomized Summary: Currently HarnessTest.Randomized is already split but some of the splits are faster than the others. The reason is that each split takes a continuous range of the generated args and the test with later args takes longer to finish. The patch evenly split the args among splits in a round robin fashion. Before: ``` [ OK ] HarnessTest.Randomized1n2 (2278 ms) [ OK ] HarnessTest.Randomized3n4 (1095 ms) [ OK ] HarnessTest.Randomized5 (658 ms) [ OK ] HarnessTest.Randomized6 (1258 ms) [ OK ] HarnessTest.Randomized7 (6476 ms) [ OK ] HarnessTest.Randomized8 (8182 ms) ``` After ``` [ OK ] HarnessTest.Randomized1 (2649 ms) [ OK ] HarnessTest.Randomized2 (2645 ms) [ OK ] HarnessTest.Randomized3 (2577 ms) [ OK ] HarnessTest.Randomized4 (2490 ms) [ OK ] HarnessTest.Randomized5 (2553 ms) [ OK ] HarnessTest.Randomized6 (2560 ms) [ OK ] HarnessTest.Randomized7 (2501 ms) [ OK ] HarnessTest.Randomized8 (2574 ms) ``` Closes Differential Revision: D7882663 Pulled By: maysamyabandeh fbshipit-source-id: 09b749a9684b6d7d65466aa4b00c5334a49e833e/Fix the memory leak with pinned partitioned filters Summary: The existing unit test did not set the level so the check for pinned partitioned filter/index being properly released from the block cache was not properly exercised as they only take effect in level 0. As a result a memory leak in pinned partitioned filters was hidden. The patch fix the test as well as the bug. Closes Differential Revision: D7559763 Pulled By: maysamyabandeh fbshipit-source-id: 55eff274945838af983c764a7d71e8daff092e4a/Align SST file data blocks to avoid spanning multiple pages Summary: Provide a block_align option in BlockBasedTableOptions to allow alignment of SST file data blocks. This will avoid higher IOPS/throughput load due to 4KB data blocks spanning 2 4KB pages. When this option is set to true, the block alignment is set to lower of block size and 4KB. Closes Differential Revision: D7400897 Pulled By: anand1976 fbshipit-source-id: 04cc3bd144e88e3431a4f97604e63ad7a0f06d44/Comment out unused variables Summary: Submitting on behalf of another employee. Closes Differential Revision: D7146025 Pulled By: ajkr fbshipit-source-id: 495ca5db5beec3789e671e26f78170957704e77e/"
1501,1501,6.0,0.9049999713897705,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix",Ignore empty filter block when data block is empty Summary: Close Closes Differential Revision: D7291706 Pulled By: ajkr fbshipit-source-id: 9dd8f40bd7716588e1e3fd6be0c2bc2766861f8c/
1502,1502,4.0,0.9805999994277954,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",Blob DB: remove existing garbage collection implementation Summary: Red diff to remove existing implementation of garbage collection. The current approach is reference counting kind of approach and require a lot of effort to get the size counter right on compaction and deletion. Im going to go with a simple mark-sweep kind of approach and will send another PR for that. CompactionEventListener was added solely for blob db and it adds complexity and overhead to compaction iterator. Removing it as well. Closes Differential Revision: D7130190 Pulled By: yiwu-arbug fbshipit-source-id: c3a375ad2639a3f6ed179df6eda602372cc5b8df/
1503,1503,9.0,0.9284999966621399,"file, change, new, use, summary, compaction, make, android, add, test_plan, call, fix, memtable, revision, support, write, level, test, get, flush","Better destroydb Summary: Delete archive directory before WAL folder since archive may be contained as a subfolder. Also improve loop readability. Closes Differential Revision: D7866378 Pulled By: riversand963 fbshipit-source-id: 0c45d97677ce6fbefa3f8d602ef5e2a2a925e6f5/Sync parent directory after deleting a file in delete scheduler Summary: sync parent directory after deleting a file in delete scheduler. Otherwise, trim speed may not be as smooth as what we want. Closes Differential Revision: D7760136 Pulled By: siying fbshipit-source-id: ec131d53b61953f09c60d67e901e5eeb2716b05f/comment unused parameters to turn on flag Summary: This PR comments out the rest of the unused arguments which allow us to turn on the flag. This is the second part of a codemod relating to Closes Differential Revision: D7426121 Pulled By: Dayvedde fbshipit-source-id: 223994923b42bd4953eb016a0129e47560f7e352/WritePrepared Txn: smallest_prepare optimization Summary: The is an optimization to reduce lookup in the CommitCache when querying IsInSnapshot. The optimization takes the smallest uncommitted data at the time that the snapshot was taken and if the sequence number of the read data is lower than that number it assumes the data as committed. To implement this optimization two changes are required: i) The AddPrepared function must be called sequentially to avoid out of order insertion in the PrepareHeap (otherwise the top of the heap does not indicate the smallest prepare in future too), ii) non-2PC transactions also call AddPrepared if they do not commit in one step. Closes Differential Revision: D7388630 Pulled By: maysamyabandeh fbshipit-source-id: b79506238c17467d590763582960d4d90181c600/FlushReason improvement Summary: Right now flush reason ""SuperVersion Change"" covers a few different scenarios which is a bit vague. For example, the following db_bench job should trigger ""Write Buffer Full"" > $ TEST_TMPDIR=/dev/shm ./db_bench $ grep flush_reason /dev/shm/dbbench/LOG ... 2018/03/06-17:30:42.543638 7f2773b99700 EVENT_LOG_v1 {""time_micros"": 1520386242543634, ""job"": 192, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7006, ""num_deletes"": 0, ""memory_usage"": 1018024, ""flush_reason"": ""SuperVersion Change""} 2018/03/06-17:30:42.569541 7f2773b99700 EVENT_LOG_v1 {""time_micros"": 1520386242569536, ""job"": 193, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7006, ""num_deletes"": 0, ""memory_usage"": 1018104, ""flush_reason"": ""SuperVersion Change""} 2018/03/06-17:30:42.596396 7f2773b99700 EVENT_LOG_v1 {""time_micros"": 1520386242596392, ""job"": 194, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7008, ""num_deletes"": 0, ""memory_usage"": 1018048, ""flush_reason"": ""SuperVersion Change""} 2018/03/06-17:30:42.622444 7f2773b99700 EVENT_LOG_v1 {""time_micros"": 1520386242622440, ""job"": 195, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7006, ""num_deletes"": 0, ""memory_usage"": 1018104, ""flush_reason"": ""SuperVersion Change""} With the fix: > 2018/03/19-14:40:02.341451 7f11dc257700 EVENT_LOG_v1 {""time_micros"": 1521495602341444, ""job"": 98, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7009, ""num_deletes"": 0, ""memory_usage"": 1018008, ""flush_reason"": ""Write Buffer Full""} 2018/03/19-14:40:02.379655 7f11dc257700 EVENT_LOG_v1 {""time_micros"": 1521495602379642, ""job"": 100, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7006, ""num_deletes"": 0, ""memory_usage"": 1018016, ""flush_reason"": ""Write Buffer Full""} 2018/03/19-14:40:02.418479 7f11dc257700 EVENT_LOG_v1 {""time_micros"": 1521495602418474, ""job"": 101, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7009, ""num_deletes"": 0, ""memory_usage"": 1018104, ""flush_reason"": ""Write Buffer Full""} 2018/03/19-14:40:02.455084 7f11dc257700 EVENT_LOG_v1 {""time_micros"": 1521495602455079, ""job"": 102, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7009, ""num_deletes"": 0, ""memory_usage"": 1018048, ""flush_reason"": ""Write Buffer Full""} 2018/03/19-14:40:02.492293 7f11dc257700 EVENT_LOG_v1 {""time_micros"": 1521495602492288, ""job"": 104, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7007, ""num_deletes"": 0, ""memory_usage"": 1018056, ""flush_reason"": ""Write Buffer Full""} 2018/03/19-14:40:02.528720 7f11dc257700 EVENT_LOG_v1 {""time_micros"": 1521495602528715, ""job"": 105, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7006, ""num_deletes"": 0, ""memory_usage"": 1018104, ""flush_reason"": ""Write Buffer Full""} 2018/03/19-14:40:02.566255 7f11dc257700 EVENT_LOG_v1 {""time_micros"": 1521495602566238, ""job"": 107, ""event"": ""flush_started"", ""num_memtables"": 1, ""num_entries"": 7009, ""num_deletes"": 0, ""memory_usage"": 1018112, ""flush_reason"": ""Write Buffer Full""} Closes Differential Revision: D7328772 Pulled By: miasantreble fbshipit-source-id: 67c94065fbdd36930f09930aad0aaa6d2c152bb8/skip CompactRange flush based on memtable contents Summary: CompactRange has a call to Flush because we guarantee that, at the time its called, all existing keys in the range will be pushed through the users compaction filter. However, previously the flush was done blindly, so itd happen even if the memtable does not contain keys in the range specified by the user. This caused unnecessarily many L0 files to be created, leading to write stalls in some cases. This PR checks the memtables contents, and decides to flush only if it overlaps with `CompactRange`s range. Move the memtable overlap check logic from `ExternalSstFileIngestionJob` to `ColumnFamilyData::RangesOverlapWithMemtables` Reuse the above logic in `CompactRange` and skip flushing if no overlap Closes Differential Revision: D7018897 Pulled By: ajkr fbshipit-source-id: a3c6b1cfae56687b49dd89ccac7c948e53545934/Fix the Logger::Close() and DBImpl::Close() design pattern Summary: The recent Logger::Close() and DBImpl::Close() implementation rely on calling the CloseImpl() virtual function from the destructor, which will not work. Refactor the implementation to have a private close helper function in derived classes that can be called by both CloseImpl() and the destructor. Closes Reviewed By: gfosco Differential Revision: D7049303 Pulled By: anand1976 fbshipit-source-id: 76a64cbf403209216dfe4864ecf96b5d7f3db9f4/"
1504,1504,14.0,0.9883999824523926,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Support for Column family specific paths. Summary: In this change, an option to set different paths for different column families is added. This option is set via cf_paths setting of ColumnFamilyOptions. This option will work in a similar fashion to db_paths setting. Cf_paths is a vector of Dbpath values which contains a pair of the absolute path and target size. Multiple levels in a Column family can go to different paths if cf_paths has more than one path. To maintain backward compatibility, if cf_paths is not specified for a column family, db_paths setting will be used. Note that, if db_paths setting is also not specified, RocksDB already has code to use db_name as the only path. Changes : 1) A new member ""cf_paths"" is added to ImmutableCfOptions. This is set, based on cf_paths setting of ColumnFamilyOptions and db_paths setting of ImmutableDbOptions. This member is used to identify the path information whenever files are accessed. 2) Validation checks are added for cf_paths setting based on existing checks for db_paths setting. 3) DestroyDB, PurgeObsoleteFiles etc. are edited to support multiple cf_paths. 4) Unit tests are added appropriately. Closes Differential Revision: D6951697 Pulled By: ajkr fbshipit-source-id: 60d2262862b0a8fd6605b09ccb0da32bb331787d/"
1505,1505,18.0,0.9887999892234802,"summary, option, test_plan, change, add, file, make, support, use, test, commit, main_java, allow, set, also, git_svn, new, closes_differential, write, harmony_xnet","Improve accuracy of I/O stats collection of external SST ingestion. Summary: RocksDB supports ingestion of external ssts. If ingestion_options.move_files is true, when performing ingestion, RocksDB first tries to link external ssts. If external SST file resides on a different FS, or the underlying FS does not support hard link, then RocksDB performs actual file copy. However, no matter which choice is made, current code increase bytes-written when updating compaction stats, which is inaccurate when RocksDB does NOT copy file. Rename a sync point. Closes Differential Revision: D7604151 Pulled By: riversand963 fbshipit-source-id: dd0c0d9b9a69c7d9ffceafc3d9c23371aa413586/Skip deleted WALs during recovery Summary: This patch record the deleted WAL numbers in the manifest to ignore them and any WAL older than them during recovery. This is to avoid scenarios when we have a gap between the WAL files are fed to the recovery procedure. The gap could happen by for example out-of-order WAL deletion. Such gap could cause problems in 2PC recovery where the prepared and commit entry are placed into two separate WAL and gap in the WALs could result into not processing the WAL with the commit entry and hence breaking the 2PC recovery logic. Closes Differential Revision: D6967893 Pulled By: maysamyabandeh fbshipit-source-id: 13119feb155a08ab6d4909f437c7a750480dc8a1/"
1506,1506,1.0,0.993399977684021,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","WritePrepared Txn: enable TryAgain for duplicates at the end of the batch Summary: The WriteBatch::Iterate will try with a larger sequence number if the memtable reports a duplicate. This status is specified with TryAgain status. So far the assumption was that the last entry in the batch will never return TryAgain, which is correct when WAL is created via WritePrepared since it always appends a batch separator if a natural one does not exist. However when reading a WAL generated by WriteCommitted this batch separator might not exist. Although WritePrepared is not supposed to be able to read the WAL generated by WriteCommitted we should avoid confusing scenarios in which the behavior becomes unpredictable. The path fixes that by allowing TryAgain even for the last entry of the write batch. Closes Differential Revision: D7708391 Pulled By: maysamyabandeh fbshipit-source-id: bfaddaa9b14a4cdaff6977f6f63c789a6ab1ee0d/WritePrepared Txn: Fix bug with duplicate keys during recovery Summary: Fix the following bugs: During recovery a duplicate key was inserted twice into the write batch of the recovery transaction, once when the memtable returns false (because it was duplicates) and once for the 2nd attempt. This would result into different SubBatch count measured when the recovered transactions is committing. If a cf is flushed during recovery the memtable is not available to assist in detecting the duplicate key. This could result into not advancing the sequence number when iterating over duplicate keys of a flushed cf and hence inserting the next key with the wrong sequence number. SubBacthCounter would reset the comparator to default comparator after the first duplicate key. The 2nd duplicate key hence would have gone through a wrong comparator and not being detected. Closes Differential Revision: D7149440 Pulled By: maysamyabandeh fbshipit-source-id: 91ec317b165f363f5d11ff8b8c47c81cebb8ed77/WritePrepared Txn: optimizations for sysbench update_noindex Summary: These are optimization that we applied to improve sysbechs update_noindex performance. 1. Make use of LIKELY compiler hint 2. Move std::atomic so the subclass 3. Make use of skip_prepared in non-2pc transactions. Closes Differential Revision: D7000075 Pulled By: maysamyabandeh fbshipit-source-id: 1ab8292584df1f6305a4992973fb1b7933632181/"
1507,1507,4.0,0.9405999779701233,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove",expose WAL iterator in the C API Summary: A minor change: I wrapped TransactionLogIterator for the C API. I needed that for the golang binding. Closes Differential Revision: D6628736 Pulled By: miasantreble fbshipit-source-id: 3374f3c64b1d7b225696b8767090917761e2f30a/C API for PerfContext Summary: This pull request exposes the interface of PerfContext as C API Closes Differential Revision: D7294225 Pulled By: ajkr fbshipit-source-id: eddcfbc13538f379950b2c8b299486695ffb5e2c/Add rocksdb_open_with_ttl function in C API Summary: Change-Id: Ie6f9b10bce459f6bf0ade0e5877264b4e10da3f5 Signed-off-by: Stuart Closes Differential Revision: D7144833 Pulled By: sagar0 fbshipit-source-id: 815225fa6e560d8a5bc47ffd0a98118b107ce264/
1508,1508,14.0,0.9908000230789185,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Add max_subcompactions as a compaction option Summary: Sometimes we want to compact files as fast as possible, but dont want to set a large `max_subcompactions` in the `DBOptions` by default. I add a `max_subcompactions` options to `CompactionOptions` so that we can choose a proper concurrency dynamically. Closes Differential Revision: D7792357 Pulled By: ajkr fbshipit-source-id: 94f54c3784dce69e40a229721a79a97e80cd6a6c/Support for Column family specific paths. Summary: In this change, an option to set different paths for different column families is added. This option is set via cf_paths setting of ColumnFamilyOptions. This option will work in a similar fashion to db_paths setting. Cf_paths is a vector of Dbpath values which contains a pair of the absolute path and target size. Multiple levels in a Column family can go to different paths if cf_paths has more than one path. To maintain backward compatibility, if cf_paths is not specified for a column family, db_paths setting will be used. Note that, if db_paths setting is also not specified, RocksDB already has code to use db_name as the only path. Changes : 1) A new member ""cf_paths"" is added to ImmutableCfOptions. This is set, based on cf_paths setting of ColumnFamilyOptions and db_paths setting of ImmutableDbOptions. This member is used to identify the path information whenever files are accessed. 2) Validation checks are added for cf_paths setting based on existing checks for db_paths setting. 3) DestroyDB, PurgeObsoleteFiles etc. are edited to support multiple cf_paths. 4) Unit tests are added appropriately. Closes Differential Revision: D6951697 Pulled By: ajkr fbshipit-source-id: 60d2262862b0a8fd6605b09ccb0da32bb331787d/"
1509,1509,1.0,0.8111000061035156,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Comment out unused variables Summary: Submitting on behalf of another employee. Closes Differential Revision: D7146025 Pulled By: ajkr fbshipit-source-id: 495ca5db5beec3789e671e26f78170957704e77e/Back out ""[codemod] comment out unused parameters"" Reviewed By: igorsugak fbshipit-source-id: 4a93675cc1931089ddd574cacdb15d228b1e5f37/"
1510,1510,6.0,0.7608000040054321,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Skip deleted WALs during recovery Summary: This patch record min log number to keep to the manifest while flushing SST files to ignore them and any WAL older than them during recovery. This is to avoid scenarios when we have a gap between the WAL files are fed to the recovery procedure. The gap could happen by for example out-of-order WAL deletion. Such gap could cause problems in 2PC recovery where the prepared and commit entry are placed into two separate WAL and gap in the WALs could result into not processing the WAL with the commit entry and hence breaking the 2PC recovery logic. Before the commit, for 2PC case, we determined which log number to keep in FindObsoleteFiles(). We looked at the earliest logs with outstanding prepare entries, or prepare entries whose respective commit or abort are in memtable. With the commit, the same calculation is done while we apply the SST flush. Just before installing the flush file, we precompute the earliest log file to keep after the flush finishes using the same logic (but skipping the memtables just flushed), record this information to the manifest entry for this new flushed SST file. This pre-computed value is also remembered in memory, and will later be used to determine whether a log file can be deleted. This value is unlikely to change until next flush because the commit entry will stay in memtable. (In WritePrepared, we could have removed the older log files as soon as all prepared entries are committed. Its not yet done anyway. Even if we do it, the only thing we loss with this new approach is earlier log deletion between two flushes, which does not guarantee to happen anyway because the obsolete file clean-up function is only executed after flush or compaction) This min log number to keep is stored in the manifest using the safely-ignore customized field of AddFile entry, in order to guarantee that the DB generated using newer release can be opened by previous releases no older than 4.2. Closes Differential Revision: D7747618 Pulled By: siying fbshipit-source-id: d00c92105b4f83852e9754a1b70d6b64cb590729/Skip deleted WALs during recovery Summary: This patch record the deleted WAL numbers in the manifest to ignore them and any WAL older than them during recovery. This is to avoid scenarios when we have a gap between the WAL files are fed to the recovery procedure. The gap could happen by for example out-of-order WAL deletion. Such gap could cause problems in 2PC recovery where the prepared and commit entry are placed into two separate WAL and gap in the WALs could result into not processing the WAL with the commit entry and hence breaking the 2PC recovery logic. Closes Differential Revision: D6967893 Pulled By: maysamyabandeh fbshipit-source-id: 13119feb155a08ab6d4909f437c7a750480dc8a1/"
1511,1511,11.0,0.5009999871253967,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","comment unused parameters to turn on flag Summary: This PR comments out the rest of the unused arguments which allow us to turn on the flag. This is the second part of a codemod relating to Closes Differential Revision: D7426121 Pulled By: Dayvedde fbshipit-source-id: 223994923b42bd4953eb016a0129e47560f7e352/Windows cumulative patch Summary: This patch addressed several issues. Portability including db_test std::thread port::Thread Cc: and %z to ROCKSDB portable macro. Cc: maysamyabandeh Implement Env::AreFilesSame Make the implementation of file unique number more robust Get rid of C-runtime and go directly to Windows API when dealing with file primitives. Implement GetSectorSize() and aling unbuffered read on the value if available. Adjust Windows Logger for the new interface, implement CloseImpl() Cc: anand1976 Fix test running script issue where $status var was of incorrect scope so the failures were swallowed and not reported. DestroyDB() creates a logger and opens a LOG file in the directory being cleaned up. This holds a lock on the folder and the cleanup is prevented. This fails one of the checkpoin tests. We observe the same in production. We close the log file in this change. Fix DBTest2.ReadAmpBitmapLiveInCacheAfterDBClose failure where the test attempts to open a directory with NewRandomAccessFile which does not work on Windows. Fix DBTest.SoftLimit as it is dependent on thread timing. CC: yiwu-arbug Closes Differential Revision: D7156304 Pulled By: siying fbshipit-source-id: 43db0a757f1dfceffeb2b7988043156639173f5b/Fix deadlock in ColumnFamilyData::InstallSuperVersion() Summary: Deadlock: a memtable flush holds DB::mutex_ and calls ThreadLocalPtr::Scrape(), which locks ThreadLocalPtr mutex; meanwhile, a thread exit handler locks ThreadLocalPtr mutex and calls SuperVersionUnrefHandle, which tries to lock DB::mutex_. This deadlock is hit all the time on our workload. It blocks our release. In general, the problem is that ThreadLocalPtr takes an arbitrary callback and calls it while holding a lock on a global mutex. The same global mutex is (at least in some cases) locked by almost all ThreadLocalPtr methods, on any instance of ThreadLocalPtr. So, therell be a deadlock if the callback tries to do anything to any instance of ThreadLocalPtr, or waits for another thread to do so. So, probably the only safe way to use ThreadLocalPtr callbacks is to do only do simple and lock-free things in them. This PR fixes the deadlock by making sure that local_sv_ never holds the last reference to a SuperVersion, and therefore SuperVersionUnrefHandle never has to do any nontrivial cleanup. I also searched for other uses of ThreadLocalPtr to see if they may have similar bugs. Theres only one other use, in transaction_lock_mgr.cc, and it looks fine. Closes Reviewed By: sagar0 Differential Revision: D7005346 Pulled By: al13n321 fbshipit-source-id: 37575591b84f07a891d6659e87e784660fde815f/"
1512,1512,1.0,0.8944000005722046,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",Comment out unused variables Summary: Submitting on behalf of another employee. Closes Differential Revision: D7146025 Pulled By: ajkr fbshipit-source-id: 495ca5db5beec3789e671e26f78170957704e77e/
1513,1513,6.0,0.3422999978065491,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Skip deleted WALs during recovery Summary: This patch record min log number to keep to the manifest while flushing SST files to ignore them and any WAL older than them during recovery. This is to avoid scenarios when we have a gap between the WAL files are fed to the recovery procedure. The gap could happen by for example out-of-order WAL deletion. Such gap could cause problems in 2PC recovery where the prepared and commit entry are placed into two separate WAL and gap in the WALs could result into not processing the WAL with the commit entry and hence breaking the 2PC recovery logic. Before the commit, for 2PC case, we determined which log number to keep in FindObsoleteFiles(). We looked at the earliest logs with outstanding prepare entries, or prepare entries whose respective commit or abort are in memtable. With the commit, the same calculation is done while we apply the SST flush. Just before installing the flush file, we precompute the earliest log file to keep after the flush finishes using the same logic (but skipping the memtables just flushed), record this information to the manifest entry for this new flushed SST file. This pre-computed value is also remembered in memory, and will later be used to determine whether a log file can be deleted. This value is unlikely to change until next flush because the commit entry will stay in memtable. (In WritePrepared, we could have removed the older log files as soon as all prepared entries are committed. Its not yet done anyway. Even if we do it, the only thing we loss with this new approach is earlier log deletion between two flushes, which does not guarantee to happen anyway because the obsolete file clean-up function is only executed after flush or compaction) This min log number to keep is stored in the manifest using the safely-ignore customized field of AddFile entry, in order to guarantee that the DB generated using newer release can be opened by previous releases no older than 4.2. Closes Differential Revision: D7747618 Pulled By: siying fbshipit-source-id: d00c92105b4f83852e9754a1b70d6b64cb590729/Revert ""Skip deleted WALs during recovery"" Summary: This reverts commit 73f21a7b2177aeb82b9f518222e2b9ea8fbb7c4f. It breaks compatibility. When created a DB using a build with this new change, opening the DB and reading the data will fail with this error: ""Corruption: Cant access /000000.sst: IO error: while stat a file for size: /tmp/xxxx/000000.sst: No such file or directory"" This is because the dummy AddFile4 entry generated by the new code will be treated as a real entry by an older build. The older build will think there is a real file with number 0, but there isnt such a file. Closes Differential Revision: D7730035 Pulled By: siying fbshipit-source-id: f2051859eff20ef1837575ecb1e1bb96b3751e77/Level Compaction with TTL Summary: Level Compaction with TTL. As of today, a file could exist in the LSM tree without going through the compaction process for a really long time if there are no updates to the data in the files key range. For example, in certain use cases, the keys are not actually ""deleted""; instead they are just set to empty values. There might not be any more writes to this ""deleted"" key range, and if so, such data could remain in the LSM for a really long time resulting in wasted space. Introducing a TTL could solve this problem. Files (and, in turn, data) older than TTL will be scheduled for compaction when there is no other background work. This will make the data go through the regular compaction process and get rid of old unwanted data. This also has the (good) side-effect of all the data in the non-bottommost level being newer than ttl, and all data in the bottommost level older than ttl. It could lead to more writes while reducing space. This functionality can be controlled by the newly introduced column family option ttl. TODO for later: Make ttl mutable Extend TTL to Universal compaction as well? (TTL is already supported in FIFO) Maybe deprecate CompactionOptionsFIFO.ttl in favor of this new ttl option. Closes Differential Revision: D7275442 Pulled By: sagar0 fbshipit-source-id: dcba484717341200d419b0953dafcdf9eb2f0267/Skip deleted WALs during recovery Summary: This patch record the deleted WAL numbers in the manifest to ignore them and any WAL older than them during recovery. This is to avoid scenarios when we have a gap between the WAL files are fed to the recovery procedure. The gap could happen by for example out-of-order WAL deletion. Such gap could cause problems in 2PC recovery where the prepared and commit entry are placed into two separate WAL and gap in the WALs could result into not processing the WAL with the commit entry and hence breaking the 2PC recovery logic. Closes Differential Revision: D6967893 Pulled By: maysamyabandeh fbshipit-source-id: 13119feb155a08ab6d4909f437c7a750480dc8a1/Optimize overlap checking for external file ingestion Summary: If there are a lot of overlapped files in L0, creating a merging iterator for all files in L0 to check overlap can be very slow because we need to read and seek all files in L0. However, in that case, the ingested file is likely to overlap with some files in L0, so if we check those files one by one, we can stop once we encounter overlap. Ref: Closes Differential Revision: D7196784 Pulled By: anand1976 fbshipit-source-id: 8700c1e903bd515d0fa7005b6ce9b3a3d9db2d67/Comment out unused variables Summary: Submitting on behalf of another employee. Closes Differential Revision: D7146025 Pulled By: ajkr fbshipit-source-id: 495ca5db5beec3789e671e26f78170957704e77e/Customized BlockBasedTableIterator and LevelIterator Summary: Use a customzied BlockBasedTableIterator and LevelIterator to replace current implementations leveraging two-level-iterator. Hope the customized logic will make code easier to understand. As a side effect, BlockBasedTableIterator reduces the allocation for the data block iterator object, and avoid the virtual function call to it, because we can directly reference BlockIter, a final class. Similarly, LevelIterator reduces virtual function call to the dummy iterator iterating the file metadata. It also enabled further optimization. The upper bound check is also moved from index block to data block. This implementation fits this iterator better. After the change, forwared iterator is slightly optimized to ensure we trim those iterators. The two-level-iterator now is only used by partitioned index, so it is simplified. Closes Differential Revision: D6809041 Pulled By: siying fbshipit-source-id: 7da3b9b1d3c8e9d9405302c15920af1fcaf50ffa/"
1514,1514,11.0,0.9911999702453613,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Adjust pread/pwrite to return Status Summary: Returning bytes_read causes the caller to call GetLastError() to report failure but the lasterror may be overwritten by then so we lose the error code. Fix up CMake file to include xpress source code only when needed. Fix warning for the uninitialized var. Closes Differential Revision: D7832935 Pulled By: anand1976 fbshipit-source-id: 4be21affb9b85d361b96244f4ef459f492b7cb2b/Windows cumulative patch Summary: This patch addressed several issues. Portability including db_test std::thread port::Thread Cc: and %z to ROCKSDB portable macro. Cc: maysamyabandeh Implement Env::AreFilesSame Make the implementation of file unique number more robust Get rid of C-runtime and go directly to Windows API when dealing with file primitives. Implement GetSectorSize() and aling unbuffered read on the value if available. Adjust Windows Logger for the new interface, implement CloseImpl() Cc: anand1976 Fix test running script issue where $status var was of incorrect scope so the failures were swallowed and not reported. DestroyDB() creates a logger and opens a LOG file in the directory being cleaned up. This holds a lock on the folder and the cleanup is prevented. This fails one of the checkpoin tests. We observe the same in production. We close the log file in this change. Fix DBTest2.ReadAmpBitmapLiveInCacheAfterDBClose failure where the test attempts to open a directory with NewRandomAccessFile which does not work on Windows. Fix DBTest.SoftLimit as it is dependent on thread timing. CC: yiwu-arbug Closes Differential Revision: D7156304 Pulled By: siying fbshipit-source-id: 43db0a757f1dfceffeb2b7988043156639173f5b/"
1515,1515,15.0,0.9648000001907349,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Allow storing metadata with backups for Java API (#4111) Summary: Exposes BackupEngine::CreateNewBackupWithMetadata and BackupInfo metadata to the Java API. Full disclaimer, Im not familiar with JNI stuff, so I might have forgotten something (hopefully no memory leaks). I also tried to find contributing guidelines but didnt see any, but I hope the PR style is consistent with the rest of the code base. Pull Request resolved: Differential Revision: D8811180 Pulled By: ajkr fbshipit-source-id: e38b3e396c7574328c2a1a0e55acc8d092b6a569/"
1516,1516,11.0,0.9620000123977661,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update",option for timing measurement of non-blocking ops during compaction (#4029) Summary: For example calling CompactionFilter is always timed and gives the user no way to disable. This PR will disable the timer if `Statistics::stats_level_` (which is part of DBOptions) is `kExceptDetailedTimers` Closes Differential Revision: D8583670 Pulled By: miasantreble fbshipit-source-id: 913be9fe433ae0c06e88193b59d41920a532307f/
1517,1517,13.0,0.5142999887466431,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Support range deletion tombstones in IngestExternalFile SSTs (#3778) Summary: Fixes This change adds a `DeleteRange` method to `SstFileWriter` and adds support for ingesting SSTs with range deletion tombstones. This is important for applications that need to atomically ingest SSTs while clearing out any existing keys in a given key range. Pull Request resolved: Differential Revision: D8821836 Pulled By: anand1976 fbshipit-source-id: ca7786c1947ff129afa703dab011d524c7883844/Fix ExternalSSTFileTest::OverlappingRanges test on Solaris Sparc (#4012) Summary: Fix of Closes Differential Revision: D8499173 Pulled By: sagar0 fbshipit-source-id: cbb2b90c544ed364a3640ea65835d577b2dbc5df/
1518,1518,8.0,0.9711999893188477,"android, use, update, commit, key, add, test, support, summary, file, fix, change, type, write, would, new, call, test_plan, make, instead","WriteUnPrepared: Add new WAL marker kTypeBeginUnprepareXID (#4069) Summary: This adds a new WAL marker of type kTypeBeginUnprepareXID. Also, DBImpl now contains a field called batch_per_txn (meaning one WriteBatch per transaction, or possibly multiple WriteBatches). This would also indicate that this DB is using WriteUnprepared policy. Recovery code would be able to make use of this extra field on DBImpl in a separate diff. For now, it is just used to determine whether the WAL is compatible or not. Closes Differential Revision: D8675099 Pulled By: lth fbshipit-source-id: ca27cae1738e46d65f2bb92860fc759deb874749/"
1519,1519,1.0,0.5112000107765198,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup",c api set bottommost level compaction Summary: Closes Differential Revision: D8224962 Pulled By: ajkr fbshipit-source-id: 3caf463509a935bff46530f27232a85ae7e4e484/add flush_before_backup parameter to c api rocksdb_backup_engine_create_new_backup Summary: Add flush_before_backup to rocksdb_backup_engine_create_new_backup. make c api able to control the flush before backup behavior. Closes Differential Revision: D8157676 Pulled By: ajkr fbshipit-source-id: 88998c62f89f087bf8672398fd7ddafabbada505/
1520,1520,2.0,0.949999988079071,"file, test_plan, summary, option, make, use, test, add, read, call, change, size, revision, write, case, differential_revision, new, also, fix, table",Catchup with posix features Summary: Catch up with Posix features NewWritableRWFile must fail when file does not exists Implement Env::Truncate() Adjust Env options optimization functions Implement MemoryMappedBuffer on Windows. Closes Differential Revision: D8053610 Pulled By: ajkr fbshipit-source-id: ccd0d46c29648a9f6f496873bc1c9d6c5547487e/
1521,1521,5.0,0.9793000221252441,"test, summary, file, add, test_plan, change, make, fix, option, revision, key, native, type, value, block, use, need, write, also, check","Memory usage stats in C API (#4340) Summary: Please consider this small PR providing access to the `MemoryUsage::GetApproximateMemoryUsageByType` function in plain C API. Actually Im working on Go application and now trying to investigate the reasons of high memory consumption (#4313). Go [wrappers]( are built on the top of Rocksdb C API. According to the `MemoryUsage::GetApproximateMemoryUsageByType` is considered as the best option to get database internal memory usage stats, but it wasnt supported in C API yet. Pull Request resolved: Differential Revision: D9655135 Pulled By: ajkr fbshipit-source-id: a3d2f3f47c143ae75862fbcca2f571ea1b49e14a/c-api: add some missing options Summary: Pull Request resolved: Differential Revision: D9309505 Pulled By: anand1976 fbshipit-source-id: eb9fee8037f4ff24dc1cdd5cc5ef41c231a03e1f/"
1522,1522,1.0,0.8273000121116638,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Use only ""local"" range tombstones during Get (#4449) Summary: Previously, range tombstones were accumulated from every level, which was necessary if a range tombstone in a higher level covered a key in a lower level. However, RangeDelAggregator::AddTombstoness complexity is based on the number of tombstones that are currently stored in it, which is wasteful in the Get case, where we only need to know the highest sequence number of range tombstones that cover the key from higher levels, and compute the highest covering sequence number at the current level. This change introduces this optimization, and removes the use of RangeDelAggregator from the Get path. In the benchmark results, the following command was used to initialize the database: ``` ./db_bench ``` ...and the following command was used to measure read throughput: ``` ./db_bench ``` The filluniquerandom command was only run once, and the resulting database was used to measure read performance before and after the PR. Both binaries were compiled with `DEBUG_LEVEL=0`. Readrandom results before PR: ``` readrandom : 4.544 micros/op 220090 ops/sec; 16.9 MB/s (63103 of 100000 found) ``` Readrandom results after PR: ``` readrandom : 11.147 micros/op 89707 ops/sec; 6.9 MB/s (63103 of 100000 found) ``` So its actually slower right now, but this PR paves the way for future optimizations (see Pull Request resolved: Differential Revision: D10370575 Pulled By: abhimadan fbshipit-source-id: 9a2e152be1ef36969055c0e9eb4beb0d96c11f4d/Add support to flush multiple CFs atomically (#4262) Summary: Leverage existing `FlushJob` to implement atomic flush of multiple column families. This PR depends on other PRs and is a subset of . This PR itself is not sufficient in fulfilling atomic flush. Pull Request resolved: Differential Revision: D9283109 Pulled By: riversand963 fbshipit-source-id: 65401f913e4160b0a61c0be6cd02adc15dad28ed/Update recovery code for version edits group commit. (#3945) Summary: During recovery, RocksDB is able to handle version edits that belong to group commits. This PR is a subset of [PR 3752]( Pull Request resolved: Differential Revision: D8529122 Pulled By: riversand963 fbshipit-source-id: 57cb0f9cc55ecca684a837742d6626dc9c07f37e/"
1523,1523,1.0,0.42100000381469727,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Revert ""apply ReadOptions.iterate_upper_bound to transaction iterator (#4705) Summary:  (#4656)"" This reverts commit b76398a82bde58bfcfa3ed5ba3dbfb6168c241de. Will add test coverage for iterate_upper_bound before re-commit b76398 Pull Request resolved: Differential Revision: D13148592 Pulled By: miasantreble fbshipit-source-id: 4d1ce0bfd9f7a5359a7688bd780eb06a66f45b1f/apply ReadOptions.iterate_upper_bound to transaction iterator (#4656) Summary: Currently transaction iterator does not apply `ReadOptions.iterate_upper_bound` when iterating. This PR attempts to fix the problem by having `BaseDeltaIterator` enforcing the upper bound check when iterator state is changed. Pull Request resolved: Differential Revision: D13039257 Pulled By: miasantreble fbshipit-source-id: 909eb9f6b4597a4d80418fb139f32ec82c6ec1d1/"
1524,1524,1.0,0.9136000275611877,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Revert ""apply ReadOptions.iterate_upper_bound to transaction iterator (#4705) Summary:  (#4656)"" This reverts commit b76398a82bde58bfcfa3ed5ba3dbfb6168c241de. Will add test coverage for iterate_upper_bound before re-commit b76398 Pull Request resolved: Differential Revision: D13148592 Pulled By: miasantreble fbshipit-source-id: 4d1ce0bfd9f7a5359a7688bd780eb06a66f45b1f/"
1525,1525,10.0,0.9269000291824341,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle",Fix db_stress for custom env (#5122) Summary: Fix some hdfs-related code so that it can compile and run db_stress Pull Request resolved: Differential Revision: D14675495 Pulled By: riversand963 fbshipit-source-id: cac280479efcf5451982558947eac1732e8bc45a/
1526,1526,15.0,0.9589999914169312,"test, use, android, summary, also, change, support, new, compaction, key, test_plan, file, call, type, would, fix, check, add, option, revision_pulle","Consolidate hash function used for non-persistent data in a new function (#5155) Summary: Create new function NPHash64() and GetSliceNPHash64(), which are currently implemented using murmurhash. Replace the current direct call of murmurhash() to use the new functions if the hash results are not used in on-disk format. This will make it easier to try out or switch to alternative functions in the uses where data format compatibility doesnt need to be considered. This part shouldnt have any performance impact. Also, the sharded cache hash function is changed to the new format, because it falls into this categoery. It doesnt show visible performance impact in db_bench results. CPU showed by perf is increased from about 0.2% to 0.4% in an extreme benchmark setting (4KB blocks, no-compression, everything cached in block cache). Weve known that the current hash function used, our own Hash() has serious hash quality problem. It can generate a lots of conflicts with similar input. In this use case, it means extra lock contention for reads from the same file. This slight CPU regression is worthy to me to counter the potential bad performance with hot keys. And hopefully this will get further improved in the future with a better hash function. cache_tests condition is relaxed a little bit to. The new hash is slightly more skewed in this use case, but I manually checked the data and see the hash results are still in a reasonable range. Pull Request resolved: Differential Revision: D14834821 Pulled By: siying fbshipit-source-id: ec9a2c0a2f8ae4b54d08b13a5c2e9cc97aa80cb5/"
1527,1527,4.0,0.9883999824523926,"file, summary, revision_pulle, closes_differential, case, read, size, compaction, key, make, support, use, call, android, add, option, test_plan, set, also, remove","Dictionary compression for files written by SstFileWriter (#4978) Summary: If `CompressionOptions::max_dict_bytes` and/or `CompressionOptions::zstd_max_train_bytes` are set, `SstFileWriter` will now generate files respecting those options. I refactored the logic a bit for deciding when to use dictionary compression. Previously we plumbed `is_bottommost_level` down to the table builder and used that. However it was kind of confusing in `SstFileWriter`s context since we dont know what level the file will be ingested to. Instead, now the higher-level callers (e.g., flush, compaction, file writer) are responsible for building the right `CompressionOptions` to give the table builder. Pull Request resolved: Differential Revision: D14060763 Pulled By: ajkr fbshipit-source-id: dc802c327896df2b319dc162d6acc82b9cdb452a/Atomic ingest (#4895) Summary: Make file ingestion atomic. as title. Ingesting external SST files into multiple column families should be atomic. If a crash occurs and db reopens, either all column families have successfully ingested the files before the crash, or non of the ingestions have any effect on the state of the db. Also add unit tests for atomic ingestion. Note that the unit test here does not cover the case of incomplete atomic group in the MANIFEST, which is covered in VersionSetTest already. Pull Request resolved: Differential Revision: D13718245 Pulled By: riversand963 fbshipit-source-id: 7df97cc483af73ad44dd6993008f99b083852198/"
1528,1528,0.0,0.991599977016449,"change, summary, file, add, new, compaction, test, make, test_plan, also, differential_revision, implementation, call, use, option, create, delete, value, full, index","refactor SavePoints (#5192) Summary: Savepoints are assumed to be used in a stack-wise fashion (only the top element should be used), so they were stored by `WriteBatch` in a member variable `save_points` using an std::stack. Conceptually this is fine, but the implementation had a few issues: the `save_points_` instance variable was a plain pointer to a heap- allocated `SavePoints` struct. The destructor of `WriteBatch` simply deletes this pointer. However, the copy constructor of WriteBatch just copied that pointer, meaning that copying a WriteBatch with active savepoints will very likely have crashed before. Now a proper copy of the savepoints is made in the copy constructor, and not just a copy of the pointer `save_points_` was an std::stack, which defaults to `std::deque` for the underlying container. A deque is a bit over the top here, as we only need access to the most recent savepoint (i.e. stack.top()) but never any elements at the front. std::deque is rather expensive to initialize in common environments. For example, the STL implementation shipped with GNU g++ will perform a heap allocation of more than 500 bytes to create an empty deque object. Although the `save_points_` container is created lazily by RocksDB, moving from a deque to a plain `std::vector` is much more memory-efficient. So `save_points_` is now a vector. `save_points_` was changed from a plain pointer to an `std::unique_ptr`, making ownership more explicit. Pull Request resolved: Differential Revision: D15024074 Pulled By: maysamyabandeh fbshipit-source-id: 5b128786d3789cde94e46465c9e91badd07a25d7/"
1529,1529,19.0,0.9789000153541565,"file, readseq_micro, summary, compaction, change, make, value, level, revision_pulle, new, test, call, option, closes_differential, need, use, user, test_plan, key, write","Refresh snapshot list during long compactions (#5099) Summary: Part of compaction cpu goes to processing snapshot list, the larger the list the bigger the overhead. Although the lifetime of most of the snapshots is much shorter than the lifetime of compactions, the compaction conservatively operates on the list of snapshots that it initially obtained. This patch allows the snapshot list to be updated via a callback if the compaction is taking long. This should let the compaction to continue more efficiently with much smaller snapshot list. Pull Request resolved: Differential Revision: D15086710 Pulled By: maysamyabandeh fbshipit-source-id: 7649f56c3b6b2fb334962048150142a3bf9c1a12/"
1530,1530,1.0,0.7275999784469604,"summary, add, would, test, file, test_plan, support, change, key, case, use, new, revision_pulle, revision, closes_differential, run, fail, issue, differential_revision, backup","Provide an option so that SST ingestion wont fall back to copy after hard linking fails (#5333) Summary: RocksDB always tries to perform a hard link operation on the external SST file to ingest. This operation can fail if the external SST resides on a different device/FS, or the underlying FS does not support hard link. Currently RocksDB assumes that if the link fails, the user is willing to perform file copy, which is not true according to the post. This commit provides an option named failed_move_fall_back_to_copy for users to choose which behavior they want. Pull Request resolved: Differential Revision: D15457597 Pulled By: HaoyuHuang fbshipit-source-id: f3626e13f845db4f7ed970a53ec8a2b1f0d62214/"
1531,1531,11.0,0.9889000058174133,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/"
1532,1532,11.0,0.6089000105857849,"summary, test, file, change, write, closes_differential, revision_pulle, test_plan, use, call, make, also, compaction, would, option, add, case, differential_revision, run, update","Misc hashing updates / upgrades (#5909) Summary: Updated our included xxhash implementation to version 0.7.2 (== the latest dev version as of 2019-10-09). Using XXH_NAMESPACE (like other fb projects) to avoid potential name collisions. Added fastrange64, and unit tests for it and fastrange32. These are faster alternatives to hash % range. Use preview version of XXH3 instead of MurmurHash64A for NPHash64 Had to update cache_test to increase probability of passing for any given hash function. Use fastrange64 instead of % with uses of NPHash64 Had to fix WritePreparedTransactionTest.CommitOfDelayedPrepared to avoid deadlock apparently caused by new hash collision. Set default seed for NPHash64 because specifying a seed rarely makes sense for it. Removed unnecessary include xxhash.h in a popular .h file Rename preview version of XXH3 to XXH3p for clarity and to ease backward compatibility in case final version of XXH3 is integrated. Relying on existing unit tests for NPHash64-related changes. Each new implementation of fastrange64 passed unit tests when manipulating my local build to select it. I havent done any integration performance tests, but I consider the improved performance of the pieces being swapped in to be well established. Pull Request resolved: Differential Revision: D18125196 Pulled By: pdillinger fbshipit-source-id: f6bf83d49d20cbb2549926adf454fd035f0ecc0d/Faster new DynamicBloom implementation (for memtable) (#5762) Summary: Since DynamicBloom is now only used in-memory, were free to change it without schema compatibility issues. The new implementation is drawn from (with manifest permission) This has several speed advantages over the prior implementation: * Uses fastrange instead of % * Minimum logic to determine first (and all) probed memory addresses * (Major) Two probes per 64-bit memory fetch/write. * Very fast and effective (murmur-like) hash expansion/re-mixing. (At least on recent CPUs, integer multiplication is very cheap.) While a Bloom filter with 512-bit cache locality has about a 1.15x FP rate penalty (e.g. 0.84% to 0.97%), further restricting to two probes per 64 bits incurs an additional 1.12x FP rate penalty (e.g. 0.97% to 1.09%). Nevertheless, the unit tests show no ""mediocre"" FP rate samples, unlike the old implementation with more erratic FP rates. Especially for the memtable, we expect speed to outweigh somewhat higher FP rates. For example, a negative table query would have to be 1000x slower than a BF query to justify doubling BF query time to shave 10% off FP rate (working assumption around 1% FP rate). While that seems likely for SSTs, my data suggests a speed factor of roughly 50x for the memtable (vs. BF; ~1.5% lower write throughput when enabling memtable Bloom filter, after this change). Thus, its probably not worth even 5% more time in the Bloom filter to shave off 1/10th of the Bloom FP rate, or 0.1% in absolute terms, and its probably at least 20% slower to recoup that much FP rate from this new implementation. Because of this, we do not see a need for a locality option that affects the MemTable Bloom filter and have decoupled the MemTable Bloom filter from Options::bloom_locality. Note that just 3% more memory to the Bloom filter (10.3 bits per key vs. just 10) is able to make up for the ~12% FP rate drop in the new implementation: [] Nearly ""ideal"" FP-wise but reasonably fast cache-local implementation [~/wormhashing/bloom_simulation_tests] ./foo_gcc_IMPL_CACHE_WORM64_FROM32_any.out 10000000 6 10 $RANDOM 100000000 ./foo_gcc_IMPL_CACHE_WORM64_FROM32_any.out time: 3.29372 sampled_fp_rate: 0.00985956 ... [] Close match to this new implementation [~/wormhashing/bloom_simulation_tests] ./foo_gcc_IMPL_CACHE_MUL64_BLOCK_FROM32_any.out 10000000 6 10.3 $RANDOM 100000000 ./foo_gcc_IMPL_CACHE_MUL64_BLOCK_FROM32_any.out time: 2.10072 sampled_fp_rate: 0.00985655 ... [] Old locality=1 implementation [~/wormhashing/bloom_simulation_tests] ./foo_gcc_IMPL_CACHE_ROCKSDB_DYNAMIC_any.out 10000000 6 10 $RANDOM 100000000 ./foo_gcc_IMPL_CACHE_ROCKSDB_DYNAMIC_any.out time: 3.95472 sampled_fp_rate: 0.00988943 ... Also note the dramatic speed improvement vs. alternatives. Performance unit test: DynamicBloomTest.concurrent_with_perf is updated to report more precise timing data. (Measure running time of each thread, not just longest running thread, etc.) Results averaged over various sizes enabled with and 20 runs each; old dynamic bloom refers to locality=1, the faster of the old: old dynamic bloom, avg add latency 65.6468 new dynamic bloom, avg add latency 44.3809 old dynamic bloom, avg query latency 50.6485 new dynamic bloom, avg query latency 43.2186 old avg parallel add latency 41.678 new avg parallel add latency 24.5238 old avg parallel hit latency 14.6322 new avg parallel hit latency 12.3939 old avg parallel miss latency 16.7289 new avg parallel miss latency 12.2134 Tested on a dedicated 64-bit production machine at Facebook. Significant improvement all around. Despite now using std::atomic<uint64_t>, quick before-and-after test on a 32-bit machine (Intel Atom N270, released 2008) shows no regression in performance, in some cases modest improvement. Performance integration test (synthetic): with DEBUG_LEVEL=0, used TEST_TMPDIR=/dev/shm ./db_bench and optionally with 300 runs each configuration. Write throughput change by enabling memtable bloom: Old locality=0: Old locality=1: New: conclusion seems to substantially close the gap Readmissing throughput change by enabling memtable bloom: Old locality=0: +34.47% Old locality=1: +34.80% New: +33.25% conclusion maybe a small new penalty from FP rate Readrandom throughput change by enabling memtable bloom: Old locality=0: +31.54% Old locality=1: +31.13% New: +30.60% conclusion maybe also from FP rate (after memtable flush) Another conclusion we can draw from this new implementation is that the existing 32-bit hash function is not inherently crippling the Bloom filter speed or accuracy, below about 5 million keys. For speed, the implementation is essentially the same whether starting with 32-bits or 64-bits of hash; it just determines whether the first multiplication after fastrange is a pseudorandom expansion or needed re-mix. Note that this multiplication can occur while memory is fetching. For accuracy, in a standard configuration, you need about 5 million keys before you have about a 1.1x FP penalty due to using a 32-bit hash vs. 64-bit: [~/wormhashing/bloom_simulation_tests] ./foo_gcc_IMPL_CACHE_MUL64_BLOCK_FROM32_any.out $((5 * 1000 * 1000 * 10)) 6 10 $RANDOM 100000000 ./foo_gcc_IMPL_CACHE_MUL64_BLOCK_FROM32_any.out time: 2.52069 sampled_fp_rate: 0.0118267 ... [~/wormhashing/bloom_simulation_tests] ./foo_gcc_IMPL_CACHE_MUL64_BLOCK_any.out $((5 * 1000 * 1000 * 10)) 6 10 $RANDOM 100000000 ./foo_gcc_IMPL_CACHE_MUL64_BLOCK_any.out time: 2.43871 sampled_fp_rate: 0.0109059 Pull Request resolved: Differential Revision: D17214194 Pulled By: pdillinger fbshipit-source-id: ad9da031772e985fd6b62a0e1db8e81892520595/"
1533,1533,12.0,0.6567999720573425,"test, summary, file, test_plan, add, level, new, change, use, revision, fix, support, compaction, make, set, write, instead, also, unit_test, reviewer","test size was wrong in fillbatch benchmark (#5198) Summary: for fillbatch benchmar, the numEntries should be [num_] but not [num_ / 1000] because numEntries is just the total entries we want to test Pull Request resolved: Differential Revision: D17274664 Pulled By: anand1976 fbshipit-source-id: f96e952babdbac63fb99d14e1254d478a10437be/"
1534,1534,14.0,0.9926000237464905,"android, summary, file, option, test_plan, time_taken, batch_size, new, test, add, change, write, update, set, use, us_mqps, support, revision, need, make","Refactor trimming logic for immutable memtables (#5022) Summary: MyRocks currently sets `max_write_buffer_number_to_maintain` in order to maintain enough history for transaction conflict checking. The effectiveness of this approach depends on the size of memtables. When memtables are small, it may not keep enough history; when memtables are large, this may consume too much memory. We are proposing a new way to configure memtable list history: by limiting the memory usage of immutable memtables. The new option is `max_write_buffer_size_to_maintain` and it will take precedence over the old `max_write_buffer_number_to_maintain` if they are both set to non-zero values. The new option accounts for the total memory usage of flushed immutable memtables and mutable memtable. When the total usage exceeds the limit, RocksDB may start dropping immutable memtables (which is also called trimming history), starting from the oldest one. The semantics of the old option actually works both as an upper bound and lower bound. History trimming will start if number of immutable memtables exceeds the limit, but it will never go below (limit-1) due to history trimming. In order the mimic the behavior with the new option, history trimming will stop if dropping the next immutable memtable causes the total memory usage go below the size limit. For example, assuming the size limit is set to 64MB, and there are 3 immutable memtables with sizes of 20, 30, 30. Although the total memory usage is 80MB > 64MB, dropping the oldest memtable will reduce the memory usage to 60MB 64MB, so in this case no memtable will be dropped. Pull Request resolved: Differential Revision: D14394062 Pulled By: miasantreble fbshipit-source-id: 60457a509c6af89d0993f988c9b5c2aa9e45f5c5/"
1535,1535,13.0,0.9821000099182129,"test, use, summary, add, test_plan, call, change, new, make, file, revision, run, fix, check, option, case, support, android, set, review",Do not schedule memtable trimming if there is no history (#6177) Summary: We have observed an increase in CPU load caused by frequent calls to `ColumnFamilyData::InstallSuperVersion` from `DBImpl::TrimMemtableHistory` when using `max_write_buffer_size_to_maintain` to limit the amount of memtable history maintained for transaction conflict checking. Part of the issue is that trimming can potentially be scheduled even if there is no memtable history. The patch adds a check that fixes this. See also Pull Request resolved: Test Plan: Compared `perf` output for ``` ./db_bench ``` before and after the change. There is a significant reduction for the call chain `rocksdb::DBImpl::TrimMemtableHistory` `rocksdb::ColumnFamilyData::InstallSuperVersion` `rocksdb::ThreadLocalPtr::StaticMeta::Scrape` even without Differential Revision: D19057445 Pulled By: ltamasi fbshipit-source-id: dff81882d7b280e17eda7d9b072a2d4882c50f79/
1536,1536,10.0,0.9815000295639038,"summary, test, test_plan, file, add, support, make, new, run, revision, compaction, use, also, fix, call, option, differential_revision, case, change, revision_pulle","Expose atomic flush option in C API (#6307) Summary: This PR adds a `rocksdb_options_set_atomic_flush` function to the C API. Pull Request resolved: Differential Revision: D19451313 Pulled By: ltamasi fbshipit-source-id: 750495642ef55b1ea7e13477f85c38cd6574849c/More const pointers in C API (#6283) Summary: This makes it easier to call the functions from Rust as otherwise they require mutable types. Pull Request resolved: Differential Revision: D19349991 Pulled By: wqfish fbshipit-source-id: e8da7a75efe8cd97757baef8ca844a054f2519b4/Add range delete function to C-API (#6259) Summary: It seems that the C-API doesnt expose the range delete functionality at the moment, so add the API. Pull Request resolved: Differential Revision: D19290320 Pulled By: pdillinger fbshipit-source-id: 3f403a4c3446d2042d55f1ece7cdc9c040f40c27/Fix & test rocksdb_filterpolicy_create_bloom_full (#6132) Summary: Add overrides needed in FilterPolicy wrapper to fix rocksdb_filterpolicy_create_bloom_full (see issue Re-enabled assertion in BloomFilterPolicy::CreateFilter that was being violated. Expanded c_test to identify Bloom filter implementations by FP counts. (Without the fix, updated test will trigger assertion and fail otherwise without the assertion.) Fixes Pull Request resolved: Test Plan: updated c_test, also run under valgrind. Differential Revision: D18864911 Pulled By: pdillinger fbshipit-source-id: 08e81d7b5368b08e501cd402ef5583f2650c19fa/"
1537,1537,6.0,0.9711999893188477,"test, summary, file, write, read, key, update, revision_pulle, use, closes_differential, would, test_plan, iterator, change, android, call, case, add, also, fix","Replace namespace name ""rocksdb"" with ROCKSDB_NAMESPACE (#6433) Summary: When dynamically linking two binaries together, different builds of RocksDB from two sources might cause errors. To provide a tool for user to solve the problem, the RocksDB namespace is changed to a flag which can be overridden in build time. Pull Request resolved: Test Plan: Build release, all and jtest. Try to build with ROCKSDB_NAMESPACE with another flag. Differential Revision: D19977691 fbshipit-source-id: aa7f2d0972e1c31d75339ac48478f34f6cfcfb3e/"
