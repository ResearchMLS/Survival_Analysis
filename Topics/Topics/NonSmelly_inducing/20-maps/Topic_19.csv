Topic_no,Keywords,Contrib,System,Text
19,"iterator, adam_mitz, change, make, status, seek, support, key, readseq_micro, make_sure, case, error, mitz, dbiter, update, transportinst_object, datum, pass, diff, return",0.0968,conscrypt,"X509Certificate: SignatureException for verify Any verification error can throw random things like BadPaddingException. Swallow it and catch Exception for all these cases and rethrow as a SignatureException to avoid acting as any kind of oracle. Change-Id: I6b515148f86529fbe0895c9fdb0954306724ae54/OpenSSLX509Certificate: negative serial numbers The constructor BigInteger(byte[]) expects twos complement encoding, but thats not what OpenSSL bn2bin returns. Bug: 12761797 Change-Id: I6c71f6fb88c2b1df7c372bf697728dac26571634/"
,,0.0556,conscrypt,Use the new endpointVerificationAlgorithm API Use the new X509ExtendedTrustManager and use the new getEndpointVerificationAlgorithm to check the hostname during the handshake. Bug: 13103812 Change-Id: Id0a74d4ef21a7d7c90357a111f99b09971e535d0/
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.1761,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.1742,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.1761,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.1742,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.1761,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.1742,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.1742,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.17800000000000002,conscrypt,Introducing top-level Conscrypt class (#152) This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./
,,0.0639,frostwire,[common] TorrentHandle isValid protection before requesting status/
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1017,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1017,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1017,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1038,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.1017,javacpp,"* Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,0.066,jna,fix WinDef.POINT references/
,,0.0652,jna,Copy changes into Github checkout./
,,0.1514,jna,Improved COM support in com.sun.jna.platform.win32.COM.util. Use of interfaces and annotations to provide easier implementation of COM interfaces (uses InvocationHandler). Support for COM event callbacks. Support for COM interface discovery by iteration over the RunningObjectTable./
,,0.0639,jna,Added OpenGL32 support./
,,0.1604,jna,Improved COM support in com.sun.jna.platform.win32.COM.util. Use of interfaces and annotations to provide easier implementation of COM interfaces (uses InvocationHandler). Support for COM event callbacks. Support for COM interface discovery by iteration over the RunningObjectTable./
,,0.1676,jna,Improved COM support in com.sun.jna.platform.win32.COM.util. Use of interfaces and annotations to provide easier implementation of COM interfaces (uses InvocationHandler). Support for COM event callbacks. Support for COM interface discovery by iteration over the RunningObjectTable./
,,0.1658,jna,Improved COM support in com.sun.jna.platform.win32.COM.util. Use of interfaces and annotations to provide easier implementation of COM interfaces (uses InvocationHandler). Support for COM event callbacks. Support for COM interface discovery by iteration over the RunningObjectTable./
,,0.1604,jna,Improved COM support in com.sun.jna.platform.win32.COM.util. Use of interfaces and annotations to provide easier implementation of COM interfaces (uses InvocationHandler). Support for COM event callbacks. Support for COM interface discovery by iteration over the RunningObjectTable./
,,0.0867,jna,Improve binding of TypeLib bindings fix wrong function signature in ITypeLib/TypeLib for FindName/IsName and adjust callers TypeLib#ReleaseLibAttr did not pass parameter to native method LPOLESTR.ByReference is derived from wrong type (BSTR instead of LPOLESTR) Fix/Add unittests/
,,0.1478,jna,Improved COM support in com.sun.jna.platform.win32.COM.util. Use of interfaces and annotations to provide easier implementation of COM interfaces (uses InvocationHandler). Support for COM event callbacks. Support for COM interface discovery by iteration over the RunningObjectTable./
,,0.1694,jna,Improved COM support in com.sun.jna.platform.win32.COM.util. Use of interfaces and annotations to provide easier implementation of COM interfaces (uses InvocationHandler). Support for COM event callbacks. Support for COM interface discovery by iteration over the RunningObjectTable./
,,0.1568,jna,Improved COM support in com.sun.jna.platform.win32.COM.util. Use of interfaces and annotations to provide easier implementation of COM interfaces (uses InvocationHandler). Support for COM event callbacks. Support for COM interface discovery by iteration over the RunningObjectTable./
,,0.1676,jna,Improved COM support in com.sun.jna.platform.win32.COM.util. Use of interfaces and annotations to provide easier implementation of COM interfaces (uses InvocationHandler). Support for COM event callbacks. Support for COM interface discovery by iteration over the RunningObjectTable./
,,0.1676,jna,Improved COM support in com.sun.jna.platform.win32.COM.util. Use of interfaces and annotations to provide easier implementation of COM interfaces (uses InvocationHandler). Support for COM event callbacks. Support for COM interface discovery by iteration over the RunningObjectTable./
,,0.0686,OpenDDS,Fri Jan 27 13:54:43 2006 Paul Calabrese Jan 26 09:36:24 2006 Paul Calabrese
,,0.0673,OpenDDS,Mon Sep 11 07:13:48 UTC 2006 Yan Dai Sep 7 06:59:35 UTC 2006 Yan Dai Jul 3 8:42:38 UTC 2006 Yan Dai
,,0.0686,OpenDDS,ChangeLogTag: Mon Jan 29 18:36:36 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Feb 1 16:40:24 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Feb 1 16:40:24 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Feb 1 16:40:24 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Feb 1 16:40:24 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Feb 1 16:40:24 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Feb 1 16:40:24 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Feb 1 16:40:24 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Feb 1 16:40:24 UTC 2007 Adam Mitz
,,0.0673,OpenDDS,ChangeLogTag: Wed May 30 19:19:38 UTC 2007 Scott Harris
,,0.0652,OpenDDS,ChangeLogTag: Wed May 30 19:19:38 UTC 2007 Scott Harris
,,0.0673,OpenDDS,ChangeLogTag: Wed May 30 19:19:38 UTC 2007 Scott Harris
,,0.0865,OpenDDS,ChangeLogTag: Tue Jun 26 19:47:51 UTC 2007 Adam Mitz Thu Jun 14 19:55:48 UTC 2007 Adam Mitz
,,0.0828,OpenDDS,ChangeLogTag: Wed Jun 27 20:25:51 UTC 2007 Jonathan S. Pollack Fri Jun 22 14:55:35 UTC 2007 Jonathan S. Pollack
,,0.0621,OpenDDS,ChangeLogTag: Mon Jun 25 16:24:29 UTC 2007 Jonathan S. Pollack
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Thu Jul 5 22:21:50 UTC 2007 Yan Dai and Thu Jul 5 22:21:49 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 5 16:27:10 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Oct 10 14:27:11 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Oct 10 14:27:11 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Oct 10 14:27:11 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Oct 10 14:27:11 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Oct 10 14:27:11 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Oct 10 14:27:11 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Oct 10 14:27:11 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Mon Nov 12 22:58:14 UTC 2007 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue May 20 20:04:28 UTC 2008 Adam Mitz
,,0.1017,OpenDDS,ChangeLogTag:Wed Jun 18 08:00:17 UTC 2008 Ossama Othman May 19 20:04:23 UTC 2008 Ossama Othman
,,0.1446,OpenDDS,Sat Jul 12 20:52:56 UTC 2008 Yan Dai Jul 11 22:45:25 UTC 2008 Yan Dai Jun 10 12:23:45 UTC 2008 Ossama Othman Jun 6 08:27:23 UTC 2008 Ossama Othman May 23 06:02:44 UTC 2008 Ossama Othman May 21 06:23:12 UTC 2008 Ossama Othman May 19 20:04:23 UTC 2008 Ossama Othman May 13 00:13:18 UTC 2008 Yan Dai Fri May 9 15:15:29 UTC 2008 Adam Mitz
,,0.0801,OpenDDS,ChangeLogTag:Mon May 19 20:04:23 UTC 2008 Ossama Othman
,,0.1316,OpenDDS,Sat Jul 12 20:52:56 UTC 2008 Yan Dai Jul 11 22:45:25 UTC 2008 Yan Dai Jun 25 18:39:10 UTC 2008 Yan Dai Jun 17 07:41:11 UTC 2008 Ossama Othman Jun 17 04:49:17 UTC 2008 Ossama Othman Jun 16 08:47:45 UTC 2008 Ossama Othman Jun 6 08:27:23 UTC 2008 Ossama Othman May 19 20:04:23 UTC 2008 Ossama Othman May 5 06:04:05 UTC 2008 Yan Dai
,,0.0686,OpenDDS,ChangeLogTag: Thu Oct 23 21:43:57 UTC 2008 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Dec 5 23:23:24 UTC 2008 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Dec 5 23:23:24 UTC 2008 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Dec 5 23:23:24 UTC 2008 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Dec 5 23:23:24 UTC 2008 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Jun 17 20:44:46 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Jun 17 20:44:46 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Jun 17 20:44:46 UTC 2009 Adam Mitz
,,0.0865,OpenDDS,BranchChangeLogTag: Fri Jun 19 22:14:25 UTC 2009 Adam Mitz
,,0.0865,OpenDDS,BranchChangeLogTag: Fri Jun 19 22:14:25 UTC 2009 Adam Mitz
,,0.0865,OpenDDS,BranchChangeLogTag: Fri Jun 19 22:14:25 UTC 2009 Adam Mitz
,,0.0865,OpenDDS,BranchChangeLogTag: Fri Jun 19 22:14:25 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Jun 17 20:44:46 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Jun 17 20:44:46 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Jun 17 20:44:46 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Wed Jun 17 20:44:46 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0865,OpenDDS,BranchChangeLogTag: Fri Jun 12 18:55:57 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.1038,OpenDDS,Sun Aug 16 19:17:56 UTC 2009 Yan Dai Mon Jun 22 18:45:04 UTC 2009 Adam Mitz Fri Jun 12 18:55:57 UTC 2009 Adam Mitz Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0865,OpenDDS,BranchChangeLogTag: Fri Jun 19 22:14:25 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0865,OpenDDS,BranchChangeLogTag: Fri Jun 19 22:14:25 UTC 2009 Adam Mitz
,,0.0865,OpenDDS,BranchChangeLogTag: Mon Jun 15 21:11:35 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0833,OpenDDS,Fri Jul 17 21:46:45 UTC 2009 Steven Stallion Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0686,OpenDDS,BranchChangeLogTag: Tue Jun 9 20:48:07 UTC 2009 Adam Mitz
,,0.0813,OpenDDS,Tue Nov 24 21:22:37 UTC 2009 Paul Calabrese Nov 23 14:28:01 UTC 2009 Paul Calabrese Sep 10 01:16:24 UTC 2009 Steven Stallion
,,0.0833,OpenDDS,Tue Dec 1 20:19:28 UTC 2009 Paul Calabrese Nov 23 14:28:01 UTC 2009 Paul Calabrese Sep 10 01:16:24 UTC 2009 Steven Stallion
,,0.0865,OpenDDS,ChangeLogTag: Wed Apr 28 20:07:21 UTC 2010 Adam Mitz
,,0.1339,OpenDDS,Fri May 14 17:37:54 UTC 2010 Yan Dai May 13 05:51:44 UTC 2010 Yan Dai May 1 17:34:52 UTC 2010 Yan Dai Thu Apr 29 21:37:51 UTC 2010 Adam Mitz Tue Apr 27 16:22:44 UTC 2010 Adam Mitz Tue Apr 13 21:58:14 UTC 2010 Adam Mitz Apr 9 18:11:38 UTC 2010 Yan Dai Wed Apr 7 15:32:56 UTC 2010 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Wed Apr 28 20:07:21 UTC 2010 Adam Mitz Wed Apr 7 15:32:56 UTC 2010 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Apr 7 15:32:56 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Apr 27 16:22:44 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Apr 27 16:22:44 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,Thu May 13 05:51:44 UTC 2010 Yan Dai Tue Apr 27 21:59:49 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Apr 27 16:22:44 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jun 24 21:21:57 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jun 24 21:21:57 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jun 24 21:21:57 UTC 2010 Adam Mitz
,,0.0849,OpenDDS,Wed Jul 14 18:21:35 UTC 2010 Yan Dai Jun 18 22:42:30 UTC 2010 Yan Dai Fri Jun 18 20:23:40 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jun 24 21:21:57 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 22 21:24:00 UTC 2010 Adam Mitz
,,0.0982,OpenDDS,Thu Aug 5 23:24:58 UTC 2010 Yan Dai Aug 4 16:45:10 UTC 2010 Yan Dai Thu Jul 22 22:01:55 UTC 2010 Adam Mitz Thu Jul 22 21:24:00 UTC 2010 Adam Mitz Jul 14 20:56:38 UTC 2010 Brian Johnson Jul 14 18:21:35 UTC 2010 Yan Dai Jul 12 20:03:15 UTC 2010 Yan Dai Jul 6 07:25:37 UTC 2010 Yan Dai Fri Jul 2 20:06:45 UTC 2010 Adam Mitz Fri Jun 18 20:23:40 UTC 2010 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Jun 23 21:44:45 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Mon Nov 22 22:19:36 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Dec 2 21:24:59 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Oct 12 15:03:53 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Mon Dec 27 22:40:03 UTC 2010 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 4 20:35:40 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 4 20:35:40 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Mon Jan 10 23:44:28 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Mar 8 20:11:23 UTC 2011 Adam Mitz
,,0.0673,OpenDDS,ChangeLogTag: Fri Feb 11 20:22:38 UTC 2011 Adam Mitz Feb 11 13:16:02 UTC 2011 Don Hudson
,,0.0686,OpenDDS,ChangeLogTag: Tue Mar 8 20:11:23 UTC 2011 Adam Mitz Fri Mar 4 20:35:40 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Mar 8 20:11:23 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Fri Mar 4 20:35:40 UTC 2011 Adam Mitz Thu Feb 3 15:54:39 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 4 20:35:40 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 4 20:35:40 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 4 20:35:40 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Tue Mar 8 20:11:23 UTC 2011 Adam Mitz Mon Mar 7 22:45:36 UTC 2011 Adam Mitz Fri Mar 4 20:35:40 UTC 2011 Adam Mitz
,,0.1162,OpenDDS,ChangeLogTag: Wed Mar 9 20:54:03 UTC 2011 Adam Mitz Wed Mar 9 19:30:30 UTC 2011 Adam Mitz Wed Mar 9 19:30:30 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 4 20:35:40 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 4 20:35:40 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 4 20:35:40 UTC 2011 Adam Mitz
,,0.0823,OpenDDS,ChangeLogTag: Mon Mar 14 23:41:58 UTC 2011 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Fri Apr 29 13:54:43 UTC 2011 Adam Mitz Apr 15 15:23:13 UTC 2011 Paul Calabrese Wed Mar 23 15:42:44 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Apr 15 14:50:56 UTC 2011 Adam Mitz
,,0.0844,OpenDDS,ChangeLogTag: Tue Jun 7 22:37:49 UTC 2011 Adam Mitz May 6 14:50:23 UTC 2011 Paul Calabrese Apr 27 18:21:08 UTC 2011 Paul Calabrese
,,0.0823,OpenDDS,ChangeLogTag: Mon Mar 14 23:41:58 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Apr 12 18:07:41 UTC 2011 Adam Mitz
,,0.1343,OpenDDS,Tue May 31 20:04:00 UTC 2011 Paul Calabrese Apr 27 21:25:31 UTC 2011 Paul Calabrese Apr 27 18:21:08 UTC 2011 Paul Calabrese Apr 15 15:23:13 UTC 2011 Paul Calabrese Thu Mar 17 21:30:03 UTC 2011 Adam Mitz Mon Mar 14 23:41:58 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Apr 13 21:32:02 UTC 2011 Adam Mitz
,,0.0844,OpenDDS,ChangeLogTag: Wed Mar 23 15:42:44 UTC 2011 Adam Mitz
,,0.0828,OpenDDS,Thu May 19 14:23:50 UTC 2011 Mike Martinez Apr 15 16:46:30 UTC 2011 Paul Calabrese Apr 15 15:23:13 UTC 2011 Paul Calabrese
,,0.1667,OpenDDS,Tue May 31 20:04:00 UTC 2011 Paul Calabrese Apr 27 21:25:31 UTC 2011 Paul Calabrese Apr 15 15:23:13 UTC 2011 Paul Calabrese Wed Apr 13 21:32:02 UTC 2011 Adam Mitz Tue Mar 15 22:09:26 UTC 2011 Adam Mitz Tue Mar 15 15:36:21 UTC 2011 Adam Mitz Mon Mar 14 23:41:58 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jul 26 20:12:33 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jul 26 20:12:33 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Thu Aug 4 14:30:19 UTC 2011 Adam Mitz Mon Jul 18 22:16:34 UTC 2011 Adam Mitz Mon Jul 18 21:05:58 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Mon Jul 18 21:05:58 UTC 2011 Adam Mitz
,,0.1204,OpenDDS,ChangeLogTag: Thu Sep 1 17:37:46 UTC 2011 Adam Mitz Tue Aug 30 14:05:01 UTC 2011 Adam Mitz Thu Aug 25 19:21:03 UTC 2011 Adam Mitz Wed Aug 24 18:36:46 UTC 2011 Adam Mitz Jun 15 15:33:47 UTC 2011 Paul Calabrese
,,0.0865,OpenDDS,ChangeLogTag: Wed Jul 13 22:28:25 UTC 2011 Adam Mitz
,,0.16399999999999998,OpenDDS,ChangeLogTag: Wed Aug 10 21:37:10 UTC 2011 Adam Mitz Tue Aug 2 19:13:57 UTC 2011 Adam Mitz Thu Jul 14 20:20:05 UTC 2011 Adam Mitz on transport refactoring: adapted the rest of the DDS_no_tests workspace; fixed bugs in wait_for_acks and shutdown/
,,0.2009,OpenDDS,ChangeLogTag: Fri Aug 12 22:03:37 UTC 2011 Adam Mitz Wed Aug 10 21:37:10 UTC 2011 Adam Mitz Tue Aug 9 22:03:14 UTC 2011 Adam Mitz Tue Aug 2 19:13:57 UTC 2011 Adam Mitz Thu Jul 14 20:20:05 UTC 2011 Adam Mitz on transport refactoring: adapted the rest of the DDS_no_tests workspace; fixed bugs in wait_for_acks and shutdown/ChangeLogTag: Fri Jul 1 19:47:14 UTC 2011 Adam Mitz Tue Jun 28 20:38:02 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jun 30 22:29:35 UTC 2011 Adam Mitz
,,0.213,OpenDDS,ChangeLogTag: Thu Aug 11 18:53:13 UTC 2011 Adam Mitz Aug 10 18:01:03 UTC 2011 Paul Calabrese Tue Aug 9 15:28:30 UTC 2011 Adam Mitz Tue Aug 2 19:13:57 UTC 2011 Adam Mitz Thu Jul 14 20:20:05 UTC 2011 Adam Mitz on transport refactoring: adapted the rest of the DDS_no_tests workspace; fixed bugs in wait_for_acks and shutdown/ChangeLogTag: Tue Jul 5 22:44:28 UTC 2011 Adam Mitz Fri Jul 1 19:47:14 UTC 2011 Adam Mitz Tue Jun 28 20:38:02 UTC 2011 Adam Mitz
,,0.0849,OpenDDS,Fri Aug 12 16:55:15 UTC 2011 Paul Calabrese Aug 10 18:01:03 UTC 2011 Paul Calabrese Wed Jul 6 15:40:25 UTC 2011 Adam Mitz
,,0.1246,OpenDDS,Wed Aug 10 18:01:03 UTC 2011 Paul Calabrese Tue Aug 2 19:13:57 UTC 2011 Adam Mitz Thu Jul 14 20:20:05 UTC 2011 Adam Mitz on transport refactoring: adapted the rest of the DDS_no_tests workspace; fixed bugs in wait_for_acks and shutdown/
,,0.0686,OpenDDS,ChangeLogTag: Thu Jun 30 22:29:35 UTC 2011 Adam Mitz
,,0.1019,OpenDDS,ChangeLogTag: Tue Aug 30 14:05:01 UTC 2011 Adam Mitz Wed Jul 20 19:33:39 UTC 2011 Adam Mitz Wed Jul 13 20:38:34 UTC 2011 Adam Mitz Mon Jul 11 22:28:50 UTC 2011 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Thu Jul 14 20:20:05 UTC 2011 Adam Mitz Tue Jul 5 22:44:28 UTC 2011 Adam Mitz Fri Jul 1 19:47:14 UTC 2011 Adam Mitz
,,0.0809,OpenDDS,Wed Aug 10 18:01:03 UTC 2011 Paul Calabrese Thu Jul 14 20:20:05 UTC 2011 Adam Mitz on transport refactoring: replacing TransportInterface with TransportClient/ChangeLogTag: Tue Jul 5 22:01:10 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jul 14 20:20:05 UTC 2011 Adam Mitz
,,0.1819,OpenDDS,ChangeLogTag: Tue Aug 30 14:05:01 UTC 2011 Adam Mitz Wed Aug 3 18:49:01 UTC 2011 Adam Mitz Thu Jul 28 22:11:26 UTC 2011 Adam Mitz Thu Jul 28 20:12:39 UTC 2011 Adam Mitz Fri Jul 15 14:25:32 UTC 2011 Adam Mitz Thu Jul 14 20:20:05 UTC 2011 Adam Mitz Mon Jul 11 22:28:50 UTC 2011 Adam Mitz on transport refactoring: replacing TransportInterface with TransportClient/ChangeLogTag: Thu Jun 30 22:29:35 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Jul 1 19:47:14 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Tue Aug 30 22:27:54 UTC 2011 Adam Mitz Tue Aug 30 14:05:01 UTC 2011 Adam Mitz Tue Jul 19 20:00:11 UTC 2011 Adam Mitz
,,0.1309,OpenDDS,ChangeLogTag: Tue Aug 30 14:05:01 UTC 2011 Adam Mitz Wed Aug 3 18:49:01 UTC 2011 Adam Mitz Thu Jul 28 22:11:26 UTC 2011 Adam Mitz Fri Jul 15 14:25:32 UTC 2011 Adam Mitz Thu Jul 14 20:20:05 UTC 2011 Adam Mitz on transport refactoring: replacing TransportInterface with TransportClient/ChangeLogTag: Thu Jun 30 22:29:35 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jul 5 16:23:02 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Aug 3 18:49:01 UTC 2011 Adam Mitz
,,0.0823,OpenDDS,ChangeLogTag: Wed Aug 3 18:49:01 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Sep 8 23:02:08 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Sep 8 23:02:08 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Nov 2 16:19:39 UTC 2011 Adam Mitz Mon Oct 3 18:18:35 UTC 2011 Adam Mitz
,,0.1339,OpenDDS,ChangeLogTag: Wed Oct 26 18:49:57 UTC 2011 Adam Mitz Tue Oct 25 18:30:43 UTC 2011 Adam Mitz Mon Oct 24 18:30:53 UTC 2011 Adam Mitz Wed Oct 19 17:52:38 UTC 2011 Adam Mitz
,,0.078,OpenDDS,ChangeLogTag: Mon Oct 31 21:25:08 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Sep 23 23:27:28 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Oct 13 20:57:30 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Sep 29 22:22:00 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Oct 4 19:00:04 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Sep 29 22:22:00 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Oct 12 20:20:32 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Oct 12 20:20:32 UTC 2011 Adam Mitz
,,0.1204,OpenDDS,ChangeLogTag: Mon Oct 31 21:25:08 UTC 2011 Adam Mitz Thu Oct 13 20:57:30 UTC 2011 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Thu Oct 13 20:57:30 UTC 2011 Adam Mitz Fri Sep 30 20:05:49 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Sep 29 22:22:00 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Sep 29 22:22:00 UTC 2011 Adam Mitz
,,0.1019,OpenDDS,ChangeLogTag: Thu Oct 13 20:57:30 UTC 2011 Adam Mitz Oct 10 16:04:48 UTC 2011 Paul Calabrese Fri Oct 7 18:06:11 UTC 2011 Adam Mitz Thu Oct 6 18:26:22 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Oct 12 20:20:32 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Oct 12 20:20:32 UTC 2011 Adam Mitz
,,0.1364,OpenDDS,ChangeLogTag: Wed Nov 2 16:13:29 UTC 2011 Adam Mitz Mon Oct 31 21:25:08 UTC 2011 Adam Mitz Thu Oct 13 20:57:30 UTC 2011 Adam Mitz Oct 10 16:04:48 UTC 2011 Paul Calabrese Thu Sep 29 14:19:32 UTC 2011 Adam Mitz Thu Sep 15 23:01:58 UTC 2011 Adam Mitz Thu Sep 8 19:15:42 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Sep 29 14:19:32 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Oct 13 20:57:30 UTC 2011 Adam Mitz
,,0.0849,OpenDDS,ChangeLogTag: Thu Oct 13 20:57:30 UTC 2011 Adam Mitz Tue Sep 13 17:56:55 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Oct 13 20:57:30 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Wed Nov 30 19:05:27 UTC 2011 Adam Mitz
,,0.1591,OpenDDS,ChangeLogTag: Tue Nov 22 22:41:31 UTC 2011 Adam Mitz Thu Nov 17 00:02:13 UTC 2011 Adam Mitz Wed Oct 19 17:52:38 UTC 2011 Adam Mitz Oct 17 21:19:19 UTC 2011 Paul Calabrese Oct 14 14:23:44 UTC 2011 Paul Calabrese Oct 10 16:04:48 UTC 2011 Paul Calabrese Oct 6 14:13:33 UTC 2011 Paul Calabrese Fri Sep 16 22:12:09 UTC 2011 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Tue Oct 25 18:30:43 UTC 2011 Adam Mitz Thu Sep 29 14:19:32 UTC 2011 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Fri Nov 11 21:58:07 UTC 2011 Adam Mitz Oct 27 21:06:32 UTC 2011 Paul Calabrese
,,0.1019,OpenDDS,ChangeLogTag: Tue Oct 25 18:30:43 UTC 2011 Adam Mitz Mon Oct 24 18:30:53 UTC 2011 Adam Mitz Wed Oct 19 17:52:38 UTC 2011 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Wed Sep 14 21:55:18 UTC 2011 Adam Mitz
,,0.1898,OpenDDS,Mon Oct 17 21:19:19 UTC 2011 Paul Calabrese Oct 14 14:23:44 UTC 2011 Paul Calabrese Thu Oct 13 20:57:30 UTC 2011 Adam Mitz Oct 12 20:51:13 UTC 2011 Paul Calabrese Oct 11 16:11:13 UTC 2011 Paul Calabrese Oct 10 16:04:48 UTC 2011 Paul Calabrese Thu Sep 29 14:19:32 UTC 2011 Adam Mitz Fri Sep 16 22:12:09 UTC 2011 Adam Mitz Thu Sep 8 19:15:42 UTC 2011 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Thu Jan 19 15:48:59 UTC 2012 Adam Mitz Fri Jan 6 18:11:41 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jan 19 15:48:59 UTC 2012 Adam Mitz
,,0.1182,OpenDDS,ChangeLogTag: Fri Mar 9 20:48:25 UTC 2012 Brian Johnson Tue Feb 14 17:14:27 UTC 2012 Adam Mitz Wed Jan 25 21:12:38 UTC 2012 Adam Mitz Wed Jan 11 19:00:23 UTC 2012 Adam Mitz Tue Dec 27 23:08:35 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jan 24 16:55:02 UTC 2012 Adam Mitz
,,0.0849,OpenDDS,ChangeLogTag: Tue Mar 6 23:31:35 UTC 2012 Adam Mitz Tue Mar 6 19:51:39 UTC 2012 Adam Mitz Fri Mar 2 21:18:21 UTC 2012 Brian Johnson
,,0.0686,OpenDDS,ChangeLogTag: Thu Jan 19 16:11:04 UTC 2012 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Mon Jan 30 16:56:02 UTC 2012 Adam Mitz Sat Jan 21 02:32:58 UTC 2012 Adam Mitz Thu Jan 19 16:11:04 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jan 19 16:11:04 UTC 2012 Adam Mitz
,,0.1638,OpenDDS,ChangeLogTag: Tue Feb 7 22:54:27 UTC 2012 Adam Mitz Mon Feb 6 20:33:21 UTC 2012 Adam Mitz Sat Jan 21 02:32:58 UTC 2012 Adam Mitz Fri Jan 20 20:04:14 UTC 2012 Adam Mitz Fri Jan 6 17:16:45 UTC 2012 Adam Mitz Wed Jan 4 20:49:39 UTC 2012 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Sat Jan 21 02:32:58 UTC 2012 Adam Mitz Fri Jan 6 17:16:45 UTC 2012 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Feb 1 21:56:53 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Sat Jan 21 02:32:58 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jan 26 23:09:08 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Dec 29 22:15:44 UTC 2011 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Wed Jan 4 20:49:39 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Mar 6 19:51:39 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu May 10 15:55:51 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu May 10 15:55:51 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 30 22:14:30 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Mar 29 20:42:00 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Mar 13 20:09:45 UTC 2012 Adam Mitz
,,0.1023,OpenDDS,send GAPs for durable data/ChangeLogTag: Wed Mar 14 15:41:55 UTC 2012 Adam Mitz Wed Mar 14 15:05:18 UTC 2012 Adam Mitz Tue Mar 13 20:09:45 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Mar 13 23:15:11 UTC 2012 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Wed Sep 5 22:07:45 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jun 12 21:32:52 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jun 12 21:32:52 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jun 12 21:32:52 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,Fri Aug 17 20:59:52 UTC 2012 Byron Harris
,,0.1038,OpenDDS,Fri Aug 17 20:59:52 UTC 2012 Byron Harris Fri Aug 10 21:08:28 UTC 2012 Adam Mitz Jun 27 17:42:36 UTC 2012 Paul Calabrese
,,0.0686,OpenDDS,ChangeLogTag: Tue Jun 12 21:32:52 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jun 12 21:32:52 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jun 12 21:32:52 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jun 12 21:32:52 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Jun 12 21:32:52 UTC 2012 Adam Mitz
,,0.1154,OpenDDS,"ChangeLogTag: Wed Sep 5 22:07:45 UTC 2012 Adam Mitz Aug 23 19:36:29 UTC 2012 Byron Harris Fri Jun 22 22:55:29 UTC 2012 Trevor Fields Jun 22 07:49:53 UTC 2012 Johnny Willemsen * dds/DCPS/DCPS_Utils.h: Doxygen improvements * dds/DCPS/DataWriterImpl.cpp: Initialise pointer with 0, const improvements/ChangeLogTag: Wed Jun 20 16:47:49 UTC 2012 Trevor Fields Tue Jun 12 21:32:52 UTC 2012 Adam Mitz"
,,0.0686,OpenDDS,ChangeLogTag: Wed Nov 21 16:08:18 UTC 2012 Adam Mitz
,,0.1017,OpenDDS,ChangeLogTag: Wed Sep 26 17:21:42 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Wed Nov 14 21:07:27 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Wed Nov 14 21:07:27 UTC 2012 Adam Mitz
,,0.0844,OpenDDS,ChangeLogTag: Mon Oct 22 01:23:03 UTC 2012 Adam Mitz
,,0.0823,OpenDDS,ChangeLogTag: Mon Oct 22 01:23:03 UTC 2012 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Mon Dec 10 21:43:28 UTC 2012 Adam Mitz Mon Nov 12 19:13:39 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Mon Sep 17 18:56:53 UTC 2012 Adam Mitz
,,0.08,OpenDDS,make sure sessions that are acked before accept_datalink() have the acked_ flag enabled so that reliability works/
,,0.0686,OpenDDS,ChangeLogTag: Fri Oct 12 22:19:51 UTC 2012 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Nov 21 16:08:18 UTC 2012 Adam Mitz Fri Oct 12 22:19:51 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Oct 9 20:28:00 UTC 2012 Adam Mitz
,,0.1459,OpenDDS,ChangeLogTag: Wed Dec 5 21:57:56 UTC 2012 Adam Mitz Tue Oct 16 19:41:59 UTC 2012 Adam Mitz Tue Oct 9 21:19:32 UTC 2012 Adam Mitz Tue Oct 9 20:28:00 UTC 2012 Adam Mitz fragment-level reliability (resends)/continued implementing resend of DataFrag due to NackFrag/work-in-progress: writer responding to NACK_FRAG by resending the requested fragments/ChangeLogTag: Wed Sep 26 17:21:42 UTC 2012 Adam Mitz work-in-progress on RTPS Fragmentation/
,,0.0686,OpenDDS,ChangeLogTag: Thu Sep 13 20:46:21 UTC 2012 Adam Mitz
,,0.1204,OpenDDS,ChangeLogTag: Mon Dec 10 21:43:28 UTC 2012 Adam Mitz Mon Nov 12 19:13:39 UTC 2012 Adam Mitz Tue Sep 11 14:42:53 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Mon Dec 10 21:43:28 UTC 2012 Adam Mitz
,,0.1017,OpenDDS,ChangeLogTag: Mon Oct 22 01:23:03 UTC 2012 Adam Mitz Fri Oct 12 22:19:51 UTC 2012 Adam Mitz
,,0.0673,OpenDDS,ChangeLogTag: Thu Nov 8 22:16:22 UTC 2012 Adam Mitz Wed Nov 7 23:36:58 UTC 2012 Trevor Fields
,,0.0686,OpenDDS,ChangeLogTag: Fri Dec 21 17:06:53 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 1 15:03:24 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 1 15:03:24 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Mon Jan 21 22:44:03 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Feb 22 00:01:59 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Feb 22 00:01:59 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Feb 22 00:01:59 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jan 10 16:06:08 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Feb 22 00:01:59 UTC 2013 Adam Mitz Tue Jan 22 22:34:30 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jan 10 16:06:08 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Jan 10 16:06:08 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Feb 22 00:01:59 UTC 2013 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Fri Feb 22 00:01:59 UTC 2013 Adam Mitz Fri Jan 11 23:16:49 UTC 2013 Adam Mitz Thu Jan 10 16:06:08 UTC 2013 Adam Mitz Fri Dec 14 23:23:38 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Jan 4 22:12:33 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Feb 22 00:01:59 UTC 2013 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Fri Feb 22 00:01:59 UTC 2013 Adam Mitz Fri Jan 11 16:15:31 UTC 2013 Adam Mitz
,,0.1182,OpenDDS,ChangeLogTag: Tue Mar 12 21:31:08 UTC 2013 Adam Mitz Fri Feb 22 00:01:59 UTC 2013 Adam Mitz Fri Dec 21 19:52:01 UTC 2012 Adam Mitz Mon Dec 17 21:02:38 UTC 2012 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Dec 12 21:55:09 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Wed Jan 2 22:28:43 UTC 2013 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Fri Feb 22 00:01:59 UTC 2013 Adam Mitz Thu Jan 3 15:57:11 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Tue Mar 12 21:31:08 UTC 2013 Adam Mitz
,,0.1038,OpenDDS,ChangeLogTag: Fri Feb 22 00:01:59 UTC 2013 Adam Mitz Thu Dec 13 19:32:31 UTC 2012 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri May 10 16:18:06 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri May 10 16:18:06 UTC 2013 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri May 10 16:18:06 UTC 2013 Adam Mitz
,,0.1161,OpenDDS,work in progress on async associations/ChangeLogTag: Wed Sep 11 00:46:49 UTC 2013 Adam Mitz Wed Sep 11 00:46:49 UTC 2013 Adam Mitz
,,0.1,OpenDDS,work in progress on async associations/ChangeLogTag: Wed Sep 11 23:19:17 UTC 2013 Adam Mitz
,,0.0673,OpenDDS,BranchChangeLogTag: Wed Apr 30 22:18:01 UTC 2014 Peter Oschwald
,,0.0828,OpenDDS,ChangeLogTag: Wed Sep 17 19:06:39 UTC 2014 Jeff Schmitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Dec 4 15:39:28 UTC 2014 Adam Mitz
,,0.1161,OpenDDS,ChangeLogTag: Wed Dec 17 18:47:39 UTC 2014 Peter Oschwald Mon Dec 15 15:44:19 UTC 2014 Peter Oschwald Tue Dec 2 21:40:08 UTC 2014 Peter Oschwald Thu Nov 13 22:52:28 UTC 2014 Peter Oschwald
,,0.0686,OpenDDS,ChangeLogTag: Thu Nov 20 19:37:43 UTC 2014 Adam Mitz
,,0.0929,OpenDDS,ChangeLogTag: Tue Dec 2 21:40:08 UTC 2014 Peter Oschwald Thu Nov 20 17:11:19 UTC 2014 Adam Mitz Fri Nov 14 20:43:11 UTC 2014 Peter Oschwald Thu Nov 13 23:00:54 UTC 2014 Peter Oschwald Mon Nov 10 23:16:12 UTC 2014 Adam Mitz
,,0.0849,OpenDDS,ChangeLogTag: Wed Dec 17 18:47:39 UTC 2014 Peter Oschwald Tue Dec 2 21:40:08 UTC 2014 Peter Oschwald
,,0.0686,OpenDDS,ChangeLogTag: Mon Dec 15 23:02:05 UTC 2014 Adam Mitz
,,0.0849,OpenDDS,ChangeLogTag: Thu Nov 20 20:09:46 UTC 2014 Adam Mitz Wed Nov 5 19:35:03 UTC 2014 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Dec 11 22:33:07 UTC 2014 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Thu Dec 11 22:33:07 UTC 2014 Adam Mitz
,,0.0865,OpenDDS,ChangeLogTag: Wed Dec 10 17:16:40 UTC 2014 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Wed Nov 12 21:37:13 UTC 2014 Adam Mitz
,,0.0686,OpenDDS,Mon Jan 5 15:10:55 UTC 2015 Justin Wilson
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 6 15:21:50 UTC 2015 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Mar 6 15:21:50 UTC 2015 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Jan 16 17:35:20 UTC 2015 Adam Mitz
,,0.0686,OpenDDS,Fri Jan 23 16:03:34 UTC 2015 Justin Wilson
,,0.0686,OpenDDS,Fri Jan 23 16:03:34 UTC 2015 Justin Wilson
,,0.0686,OpenDDS,Fri Jan 23 16:03:34 UTC 2015 Justin Wilson
,,0.0686,OpenDDS,ChangeLogTag: Wed Jan 14 23:18:18 UTC 2015 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Jan 30 19:02:26 UTC 2015 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Wed Jan 28 01:58:41 UTC 2015 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Jan 30 19:02:26 UTC 2015 Adam Mitz
,,0.0686,OpenDDS,ChangeLogTag: Fri Jan 30 19:02:26 UTC 2015 Adam Mitz
,,0.0865,OpenDDS,"String conversion: transport config, inst, registry/ChangeLogTag: Wed Feb 25 23:20:10 UTC 2015 Adam Mitz"
,,0.0686,OpenDDS,Mon Jan 19 15:54:56 UTC 2015 Justin Wilson
,,0.0686,OpenDDS,ChangeLogTag: Fri Jan 30 19:02:26 UTC 2015 Adam Mitz
,,0.066,OpenDDS,Removed calls to new in surrogate CORBA library./
,,0.0639,OpenDDS,Removed calls to new in surrogate CORBA library./
,,0.0906,OpenDDS,"Revert ""Merge pull request from huangminghuang/master"" This reverts commit e6e580b4487263e096153ab49dff81f900568056, reversing changes made to 3cf8fa1bb6c00e9b6d964b30a9fff7741e2d559e./Fix IPv6 address text representation for TCP UDP and RtpsUdp/"
,,0.0848,OpenDDS,"Revert ""Merge pull request from huangminghuang/master"" This reverts commit e6e580b4487263e096153ab49dff81f900568056, reversing changes made to 3cf8fa1bb6c00e9b6d964b30a9fff7741e2d559e./Fix IPv6 address text representation for TCP UDP and RtpsUdp/"
,,0.0863,OpenDDS,"At the moment we have a dispose message and we have data dont free that, it could contain the key fields in case of keyed data/At the moment we have a dispose message and we have data dont free that, it could contain the key fields in case of keyed data/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3086,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3116,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3071,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3116,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3116,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3116,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3116,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3086,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3268,OpenDDS,"Break TransportSendStrategy and ThreadSynch reference cycle/Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3086,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3136,OpenDDS,"remove TransportStrategy from the data members of RtpsUdpSendStrategy/Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3193,OpenDDS,"Refactor transport shutdown/Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3146,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.2979,OpenDDS,"Fix a memory access bug during shutdown/Break reference cycle between TcpSynchResource and TcpConnection/Remove TransportStrategy from the data members of TcpConnection/Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3101,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.3116,OpenDDS,"Make TransportRegistry the owner of TransportInst Objects The purpose of this commit is to mkae TransportRegistry is the sole owner of TransportInst objects and TranportInst is the sole owner of TransportImpl objects. Furthermore, make sure TransportRegistry objects would out-live those objects accessing the TransportImpl and TransportInst objects so that we can just use plain pointer or reference for TransportImpl and TransportInst objects without introducing reference cycle and locking. Conflicts: dds/DCPS/Service_Participant.cpp dds/DCPS/transport/udp/UdpDataLink.cpp dds/DCPS/transport/udp/UdpTransport.cpp/"
,,0.0588,OpenDDS,Make other changes from PR/
,,0.0673,OpenDDS,Apply suggestions from code review Co-Authored-By: Adam Mitz
,,0.0677,OpenDDS,marshal_generator.cpp: Common KeyOnly Key Iterate/Refactor Topic Type Code/
,,0.0599,OpenDDS,"add MultiTask for an adjustable timer in the vein of Sporadic/PeriodicTask, update SPDPs send_listener_ and RTPSs heartbeat_ to use it/"
,,0.0838,OpenDDS,Fixes for Secure Participant Discovery/Fixes for Secure Participant Discovery/Bug fixes for Secure Participant Discovery/Fixes for Secure Participant Discovery/
,,0.066,realm-java,Add unit tests for compatibility with iOS NSDate/
,,0.0686,realm-java,Correctly report Client Reset (#4313)/
,,0.0664,realm-java,Correctly report Client Reset (#4313)/
,,0.1025,realm-java,Add support for changing a users password. (#4538)/Added support for SyncUser.isAdmin() (#4427)/Correctly report Client Reset (#4313)/
,,0.0723,realm-java,Added support for SyncUser.isAdmin() (#4427)/
,,0.0686,realm-java,Correctly report Client Reset (#4313)/
,,0.0723,realm-java,Added support for SyncUser.isAdmin() (#4427)/
,,0.0744,realm-java,Added support for SyncUser.isAdmin() (#4427)/
,,0.2185,realm-java,"Create ProxyUtils class to simplify proxies (#4942) Move a simple task to a utility class, to reduce Proxy code size/Let Object Store handle table creations (#4674) This tries to progressively move more things about schemas to Object Store. First the concept of Schema in Object Store is not the same as what we have in Java. It is very much just a schema information holder and wont take care of the schema modifications. That says it is more like the ColumnIndices cache in the Java binding. So this commit try to: Instead of inheriting from the RealmSchema, change the OsRealmSchema/OsRealmObjectSchema to OsSchemaInfo/OsObjectSchemaInfo. They behave as a simple Java wrapper to the relevant OS objects. Add functions to Proxy classes which will create its own OsObjectSchemaInfo and those info can be used to create a OsSchemaInfo through the mediator. Call `SharedRealm.updateSchema` with the OsSchemaInfo which is got from the proxy interface to do table initialization. This will also fix a minor bug we have before, All tables are created even if the class is not in the module. Migration is still handled in the old way, and it will be solved in the future, to let Object Store handle it. ColumnIndices are still kept for now, but it should be computed from the OsSchemaInfo/OsObjectSchemaInfo in the future./Clean up the Proxies a bit, in prep for RealmInteger mods (#4770) * Clean up the Proxies a bit, in prep for RealmInteger mods/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
,,0.1902,realm-java,"Verify schema for backlinks by OS Add computed properties in the proxys expected schema info, so the schema validation in Object Store will check for linking objects. Throw IllegalStateException if the backlink schema validation fails. ISE is more suitable here since backlink does not exist in the schema. Normally the way to handle this issue is not by adding a migration block, instead, just changing the declaration in the model class. Rewrite and enable the tests which are related to the linking objects schema validation by mocking the expected schema info./Counters: Merge MutableRealmIntegers Feature (#5017) * Counters: Partial implementations * Counters: Wired to core (#4953) * Counters: Required, Index and Documentation (#4969) * Add tests for and * Document ManagedRealmInteger * Counters: JSON for MutableRealmIntegers (#5007) * Clean up documentation/Create ProxyUtils class to simplify proxies (#4942) Move a simple task to a utility class, to reduce Proxy code size/Let Object Store handle table creations (#4674) This tries to progressively move more things about schemas to Object Store. First the concept of Schema in Object Store is not the same as what we have in Java. It is very much just a schema information holder and wont take care of the schema modifications. That says it is more like the ColumnIndices cache in the Java binding. So this commit try to: Instead of inheriting from the RealmSchema, change the OsRealmSchema/OsRealmObjectSchema to OsSchemaInfo/OsObjectSchemaInfo. They behave as a simple Java wrapper to the relevant OS objects. Add functions to Proxy classes which will create its own OsObjectSchemaInfo and those info can be used to create a OsSchemaInfo through the mediator. Call `SharedRealm.updateSchema` with the OsSchemaInfo which is got from the proxy interface to do table initialization. This will also fix a minor bug we have before, All tables are created even if the class is not in the module. Migration is still handled in the old way, and it will be solved in the future, to let Object Store handle it. ColumnIndices are still kept for now, but it should be computed from the OsSchemaInfo/OsObjectSchemaInfo in the future./Clean up the Proxies a bit, in prep for RealmInteger mods (#4770) * Clean up the Proxies a bit, in prep for RealmInteger mods/Make the proxy toString methods reveal themselves (#4623) * Make the proxy toString methods reveal themselves * Fix preprocessor UTs * Address PR comments/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
,,0.3056,realm-java,"Counters: Merge MutableRealmIntegers Feature (#5017) * Counters: Partial implementations * Counters: Wired to core (#4953) * Counters: Required, Index and Documentation (#4969) * Add tests for and * Document ManagedRealmInteger * Counters: JSON for MutableRealmIntegers (#5007) * Clean up documentation/Explicitly specify Locale for String.format() in our annotation processor (#4853) * add Locale on formatting strings in our annotation processor. * fix formatting/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
,,0.3803,realm-java,"Update failing tests/Merge remote-tracking branch origin/master-4.0 into merge-407e6a-to-master-4.0/Immutable RealmSchema and RealmObjectSchema (#5003) Before this change, Realm.getSchema() and DynamicRealm.getSchema() both return a mutable version RealmSchema object. That would cause issue when changing the typed Realms schema then continue to use typed inferface to access the Realm where the column indices might be changed already and had not been refreshed before the transaction commited. After this change: Realm.getSchema() returns an immutable RealmSchema object. And all RealmObjectSchema objects retrieved from that will be immutable as well./Support stable IDs for sync (#4693) * Support stable IDs for sync A special column to store stable IDs are added by sync. See for details. Calling sync::create_object instead of add_empty_row to create new object. Calling sync::create_table/sync::create_table_with_primary_key instead of add_table to create a new object schema. addPrimaryKey() is not allowed for synced Realm anymore. New API RealmSchema.createWithPrimaryKeyField is added./LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
,,0.3433,realm-java,Move benchmarks to separate library/Fix queries not working on proguarded Realms model classes (#4690)/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3706,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3662,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3633,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3633,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3648,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.2965,realm-java,"Verify schema for backlinks by OS Add computed properties in the proxys expected schema info, so the schema validation in Object Store will check for linking objects. Throw IllegalStateException if the backlink schema validation fails. ISE is more suitable here since backlink does not exist in the schema. Normally the way to handle this issue is not by adding a migration block, instead, just changing the declaration in the model class. Rewrite and enable the tests which are related to the linking objects schema validation by mocking the expected schema info./LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
,,0.3618,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.0677,realm-java,Listeners and LinkingObjects (#4906) * Clarify current behaviour/
,,0.3677,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.2976,realm-java,"Immutable RealmSchema and RealmObjectSchema (#5003) Before this change, Realm.getSchema() and DynamicRealm.getSchema() both return a mutable version RealmSchema object. That would cause issue when changing the typed Realms schema then continue to use typed inferface to access the Realm where the column indices might be changed already and had not been refreshed before the transaction commited. After this change: Realm.getSchema() returns an immutable RealmSchema object. And all RealmObjectSchema objects retrieved from that will be immutable as well./Add CompactOnLaunch. (#4857) * Implement CompactOnLaunch. This commit adds RealmConfiguration.compactOnLaunch and RealmConfiguration.Builder.compactOnLaunch(CompactOnLaunchCallback). It makes a Realm determines if it should be compacted. * Throw an exception if it is read-only Realm. * Add readOnly_compactOnLaunch_throws * Fix a wrong signature. * Add tests to check compactOnLaunch. * Fix tests. * Add Javadoc. * Updated CHANGELOG>md * Fix a typo * PR feedback. * PR feedback: Improve tests. * PR feedback. * Improve Javadocs sentences. * Support Proguard. * Rename more accurate. * PR feedback. * Fix Proguard. * Fix JNI code. * PR feedback: Remove 2 createConfiguration. * PR feedback * Add RealmConfiguration.Builder.compactOnLaunch(). * Fix a bug of JNI code. * Add more tests. * Improve Javadoc. * PR feedback: Add a test to check a bug. * add a test to check a bug where compactOnLaunch is called each time a Realm is opened on a new thread. * PR feedback * Imporve Javadoc. * Fix a test (Thread). * PR: fix a test./Refactor object creation into OsObject (#4632) Hide the addEmptyRow from java to support future stable ID. Add bulk insertion benchmark. This wont be the final design of internal object creation API, when integration of OS object accessor, the internal API might be changed a bit since it doesnt look nice at all 5 params for the JNI call to create an object with integer primary key/Support readOnly() on Configurations (#4575)/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
,,0.3584,realm-java,"4788 is not fixed on master 4.0/Upgrade Realm Core to 2.8.6 (#4934) Upgrade Realm Core to 2.8.6, sync to 1.10.5. Add test case to Expose bug when querying an indexed field with Case.INSENSITIVE. Adapt DescriptorOdering changes from Object Store. See Update Object Store to 7c12b340e0/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
,,0.3662,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3692,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3648,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3474,realm-java,"Fix a bug that RealmSchema.remove() and RealmSchema.rename() dont update internal cache (#5037) * fix a bug that RealmSchema.remove() and RealmSchema.rename() dont update internal cache * remove tests for removed or renamed class. Ill add those tests again in another PR * remove tests for removed or renamed class. Ill add those tests again in another PR * check RealmObjectSchema is valid before obtaining target class name * add target class name check * fix class name check * make dynamicClassToSchema private/Immutable RealmSchema and RealmObjectSchema (#5003) Before this change, Realm.getSchema() and DynamicRealm.getSchema() both return a mutable version RealmSchema object. That would cause issue when changing the typed Realms schema then continue to use typed inferface to access the Realm where the column indices might be changed already and had not been refreshed before the transaction commited. After this change: Realm.getSchema() returns an immutable RealmSchema object. And all RealmObjectSchema objects retrieved from that will be immutable as well./Merge standard schema classes to their parents Since we changed the architecture, no need for this extra abstraction anymore./Fix queries not working on proguarded Realms model classes (#4690)/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
,,0.3589,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3242,realm-java,add Nullable annotation to methods that can return null (#4999) * add Nullable annotation to methods that can return null * add changelog entry for introfucinf annotation. * fix Kotlin example * fix Kotlin test/Fix queries not working on proguarded Realms model classes (#4690)/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3648,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3677,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3435,realm-java,"Fix a bug that RealmSchema.remove() and RealmSchema.rename() dont update internal cache (#5037) * fix a bug that RealmSchema.remove() and RealmSchema.rename() dont update internal cache * remove tests for removed or renamed class. Ill add those tests again in another PR * remove tests for removed or renamed class. Ill add those tests again in another PR * check RealmObjectSchema is valid before obtaining target class name * add target class name check * fix class name check * make dynamicClassToSchema private/Immutable RealmSchema and RealmObjectSchema (#5003) Before this change, Realm.getSchema() and DynamicRealm.getSchema() both return a mutable version RealmSchema object. That would cause issue when changing the typed Realms schema then continue to use typed inferface to access the Realm where the column indices might be changed already and had not been refreshed before the transaction commited. After this change: Realm.getSchema() returns an immutable RealmSchema object. And all RealmObjectSchema objects retrieved from that will be immutable as well./add Nullable annotation to methods that can return null (#4999) * add Nullable annotation to methods that can return null * add changelog entry for introfucinf annotation. * fix Kotlin example * fix Kotlin test/LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
,,0.3373,realm-java,"Support stable IDs for sync (#4693) * Support stable IDs for sync A special column to store stable IDs are added by sync. See for details. Calling sync::create_object instead of add_empty_row to create new object. Calling sync::create_table/sync::create_table_with_primary_key instead of add_table to create a new object schema. addPrimaryKey() is not allowed for synced Realm anymore. New API RealmSchema.createWithPrimaryKeyField is added./Merge standard schema classes to their parents Since we changed the architecture, no need for this extra abstraction anymore./LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/"
,,0.3648,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.1944,realm-java,"Immutable RealmSchema and RealmObjectSchema (#5003) Before this change, Realm.getSchema() and DynamicRealm.getSchema() both return a mutable version RealmSchema object. That would cause issue when changing the typed Realms schema then continue to use typed inferface to access the Realm where the column indices might be changed already and had not been refreshed before the transaction commited. After this change: Realm.getSchema() returns an immutable RealmSchema object. And all RealmObjectSchema objects retrieved from that will be immutable as well./"
,,0.3706,realm-java,LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.3636,realm-java,Backlink queries (#4704) * Fixing unit tests for backlink queries. * Reintroducing of a native implementation of isNotEmpty(). * Moving inverse relationships out of beta stage./LinkingObject queries (#4519) Refactor ColumnInfo and ColumnIndices for better control Refactor ColumnIndices to support new schema Refactor ColumnInfor to support new schema Refactor ProxyGeneration to support new Schema Refactor RealmQuery to support backlinked queries Code complete Static analysis tests passing Fix copy bug in ColumnIndices All non-backlink tests passing; All non-test FIXMEs gone Refactor all field parsing into the FieldDescriptor class Fix threading bugs in RunInLooperThread rule Standardize test timeouts Native isEmpty and isNotEmpty need to be taught about backlinks I believe all remaining work is C/C++ Disable Backlink Queries Respond to comments Revert createDynamicBacklinkResults signature Remove single leading space on line 1206 ins RealmQueryTests.java/
,,0.0724,realm-java,Support Partial Sync (#5359) * Add preview support for partial sync/
,,0.0755,rocksdb,"Do not allow Transaction Log Iterator to fall ahead when writer is writing the same file Summary: Store the last flushed, seq no. in db_impl. Check against it in transaction Log iterator. Do not attempt to read ahead if we do not know if the data is flushed completely. Does not work if flush is disabled. Any ideas on fixing that? * Minor change, iter->Next is called the first time automatically for * the first time. Test Plan: existing test pass. More ideas on testing this? Planning to run some stress test. Reviewers: dhruba, heyongqiang CC: leveldb Differential Revision:"
,,0.083,rocksdb,"Do not allow Transaction Log Iterator to fall ahead when writer is writing the same file Summary: Store the last flushed, seq no. in db_impl. Check against it in transaction Log iterator. Do not attempt to read ahead if we do not know if the data is flushed completely. Does not work if flush is disabled. Any ideas on fixing that? * Minor change, iter->Next is called the first time automatically for * the first time. Test Plan: existing test pass. More ideas on testing this? Planning to run some stress test. Reviewers: dhruba, heyongqiang CC: leveldb Differential Revision:"
,,0.1012,rocksdb,"[RocksDB] Minor iterator cleanup Summary: Was going through the iterator related code, did some cleanup along the way. Basically replaced array with vector and adopted range based loop where applicable. Test Plan: make check; make valgrind_check Reviewers: dhruba, emayanke Reviewed By: emayanke CC: leveldb Differential Revision:"
,,0.1128,rocksdb,"Turn on Summary: Compiling for iOS has by default turned on which causes rocksdb to fail compiling. This diff turns on in our compile options and cleans up all functions with missing prototypes. Test Plan: compiles Reviewers: dhruba, haobo, ljin, sdong Reviewed By: ljin CC: leveldb Differential Revision: for perf_context Summary: This will allow us to disable them completely for iOS or for better performance Test Plan: will run make all check Reviewers: igor, haobo, dhruba Reviewed By: haobo CC: leveldb Differential Revision: issue with iterator operations in this order: Prev(), Seek(), Prev() Summary: Due to a bad merge of D14163 and D14001 before checking in D14001, ""direction_ kForward;"" in MergeIterator::Seek() was deleted my mistake (in commit b135d01e7bcdf4186ea852a5b4e6d14a3a815d77 ). It will generate wrong results or assert failure after the sequence of Prev() (or SeekToLast()), Seek() and Prev(). Fix it Test Plan: make all check Reviewers: igor, haobo, dhruba Reviewed By: igor CC: yhchiang, i.am.jin.lei, ljin, leveldb Differential Revision:"
,,0.3074,rocksdb,"Changes to support unity build: * Script for building the unity.cc file via Makefile * Unity executable Makefile target for testing builds * Source code changes to fix compilation of unity build/In DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision:"
,,0.3075,rocksdb,"In DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision:"
,,0.3168,rocksdb,"In DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision:"
,,0.3075,rocksdb,"In DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision:"
,,0.3075,rocksdb,"In DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision:"
,,0.3102,rocksdb,"In DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision:"
,,0.1863,rocksdb,"Add histogram for DB_SEEK Summary: as title Test Plan: make release Reviewers: sdong, yhchiang Reviewed By: yhchiang Subscribers: leveldb Differential Revision: db_test and DBIter Summary: Fix old issue with DBTest.Randomized with BlockBasedTableWithWholeKeyHashIndex + added printing in DBTest.Randomized. Test Plan: make all check Reviewers: zagfox, igor, ljin, yhchiang, sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: statistics forward-able Summary: Make StatisticsImpl being able to forward stats to provided statistics implementation. The main purpose is to allow us to collect internal stats in the future even when user supplies custom statistics implementation. It avoids intrumenting 2 sets of stats collection code. One immediate use case is tuning advisor, which needs to collect some internal stats, users may not be interested. Test Plan: ran db_bench and see stats show up at the end of run Will run make all check since some tests rely on statistics Reviewers: yhchiang, sdong, igor Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: Prev() for merge operator Summary: Implement Prev() with merge operator for DBIterator. Request from mongoDB. Task 4673663. Test Plan: make all check Reviewers: sdong Reviewed By: sdong Subscribers: leveldb Differential Revision: DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision:"
,,0.3115,rocksdb,"In DB::NewIterator(), try to allocate the whole iterator tree in an arena Summary: In this patch, try to allocate the whole iterator tree starting from DBIter from an arena 1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it. 2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem tables iterators, all table readers iterators and two level iterator. 3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it. Limitations: (1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc (2) Two level iterator itself is allocated in arena, but not iterators inside it. Test Plan: make all check Reviewers: ljin, haobo Reviewed By: haobo Subscribers: leveldb, dhruba, yhchiang, igor Differential Revision:"
,,0.0997,rocksdb,"Some updates for SpatialDB Summary: 1. add db statistics 2. write out batch every millionth write Test Plan: unit tests Reviewers: ljin, sdong, yinwang Reviewed By: yinwang Differential Revision: SpatialDB as we go, not at the end/Optimize SpatialDB Summary: Two things: 1. Use hash-based index for data column family 2. Use Get() instead of Iterator Seek() when DB is opened read-only Test Plan: added read-only test in unit test Reviewers: yinwang Reviewed By: yinwang Subscribers: leveldb Differential Revision:"
,,0.0972,rocksdb,"Replace exception by setting valid_ false in DBIter::MergeValuesNewToOld() Summary: Replace exception by setting valid_ false in DBIter::MergeValuesNewToOld(). Test Plan: Not sure if I am right at this, but it seems we currently dont have a good way to test that code path as it requires dynamically set merge_operator nullptr at the time while Merge() is calling. Reviewers: igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1581,rocksdb,"rocksdb: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.0916,rocksdb,"Managed iterator Summary: This is a diff for managed iterator. A managed iterator is a wrapper around an iterator which saves the options for that iterator as well as the current key/value so that the underlying iterator and its associated memory can be released when it is aged out automatically or on the request of the user. Will provide the automatic release as a follow-up diff. Test Plan: Managed* tests in db_test and XF tests for managed iterator Reviewers: igor, yhchiang, anthony, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1684,rocksdb,"rocksdb: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1701,rocksdb,"rocksdb: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1735,rocksdb,"rocksdb: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.136,rocksdb,"Change the way options.compression_per_level is used when options.level_compaction_dynamic_level_bytes=true Summary: Change the way options.compression_per_level is used when options.level_compaction_dynamic_level_bytes=true so that options.compression_per_level[1] determines compression for the level L0 is merged to, options.compression_per_level[2] to the level after that, etc. Test Plan: run all tests Reviewers: rven, yhchiang, kradhakrishnan, igor Reviewed By: igor Subscribers: yoshinorim, leveldb, dhruba Differential Revision: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1684,rocksdb,"rocksdb: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1632,rocksdb,"rocksdb: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1559,rocksdb,"Fix possible SIGSEGV in CompactRange (github issue Summary: For very detailed explanation of whats happening read this: Test Plan: make check + new unit test Reviewers: yhchiang, anthony, rven Reviewed By: rven Subscribers: adamretter, dhruba, leveldb Differential Revision: Small refactoring before migrating to gtest Summary: These changes are necessary to make tests look more generic, and avoid feature conflicts with gtest. Test Plan: Make sure no build errors, and all test are passing. ``` % make check ``` Reviewers: igor, meyering Reviewed By: meyering Subscribers: dhruba, leveldb Differential Revision: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1752,rocksdb,"rocksdb: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1845,rocksdb,"rocksdb: Small refactoring before migrating to gtest Summary: These changes are necessary to make tests look more generic, and avoid feature conflicts with gtest. Test Plan: Make sure no build errors, and all test are passing. ``` % make check ``` Reviewers: igor, meyering Reviewed By: meyering Subscribers: dhruba, leveldb Differential Revision: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.1667,rocksdb,"rocksdb: Add missing override Summary: When using latest clang (3.6 or 3.7/trunck) rocksdb is failing with many errors. Almost all of them are missing override errors. This diff adds missing override keyword. No manual changes. Prerequisites: bear and clang 3.5 build with extra tools ```lang=bash % USE_CLANG=1 bear make all generate a compilation database % clang-modernize . . % make format ``` Test Plan: Make sure all tests are passing. ```lang=bash % default fb code clang. % make check ``` Verify less error and no missing override errors. ```lang=bash % Have trunk clang present in path. % ROCKSDB_NO_FBCODE=1 CC=clang CXX=clang++ make ``` Reviewers: igor, kradhakrishnan, rven, meyering, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.23399999999999999,rocksdb,"Add test case to repro the mispositional iterator in a low-chance data race case Summary: Iterator has a bug: if a child iterator reaches its end, and user issues a Prev(), and just before SeekToLast() of the child iterator is called, some extra rows is added in the end, the position of iterator can be misplaced. Test Plan: Run the tests with or without valgrind Reviewers: rven, yhchiang, IslamAbdelRahman, anthony Reviewed By: anthony Subscribers: tnovak, leveldb, dhruba Differential Revision: two diffs related to DBIter::FindPrevUserKey() Summary: This diff reverts the following two previous diffs related to DBIter::FindPrevUserKey(), which makes db_stress unstable. We should bake a better fix for this. * ""Fix a comparison in DBIter::FindPrevUserKey()"" ec70fea4c4025351190eba7a02bd09bb5f083790. * ""Fixed endless loop in DBIter::FindPrevUserKey()"" acee2b08a2d37154b8f9e2dc74b1966202c15ec5. Test Plan: db_stress Reviewers: anthony, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: a comparison in DBIter::FindPrevUserKey() Summary: When seek target is a merge key (`kTypeMerge`), `DBIter::FindNextUserEntry()` advances the underlying iterator _past_ the current key (`saved_key_`); see `MergeValuesNewToOld()`. However, `FindPrevUserKey()` assumes that `iter_` points to an entry with the same user key as `saved_key_`. As a result, `it->Seek(key) && it->Prev()` can cause the iterator to be positioned at the _next_, instead of the previous, entry (new test, written by reproduces the bug). This diff changes `FindPrevUserKey()` to also skip keys that are _greater_ than `saved_key_`. Test Plan: db_test Reviewers: igor, sdong Reviewed By: sdong Subscribers: leveldb, dhruba, lovro Differential Revision: SeekToLast with upper bound Summary: RocksDBs Iterator.SeekToLast should seek to the last key before iterate_upper_bound if presents Test Plan: ./db_iter_test run successfully with the new testcase Reviewers: rven, yhchiang, igor, anthony, kradhakrishnan, sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision:"
,,0.2667,rocksdb,"Fixing endless loop if seeking to end of key with seq num 0 Summary: When seeking to the last occurrence of a key with sequence number 0, db_iter ends up in an endless loop because it seeks to type kValueTypeForSeek which is larger than kTypeDeletion/kTypeValue. Added test case that triggers the behavior. Test Plan: make clean all check Reviewers: igor, rven, anthony, yhchiang, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: misplaced position for reversing iterator direction while current key is a merge Summary: While doing forward iterating, if current key is merge, internal iterator position is placed to the next key. If Prev() is called now, needs to do extra Prev() to recover the location. This is second attempt of fixing after reverting ec70fea4c4025351190eba7a02bd09bb5f083790. This time shrink the fix to only merge key is the current key and avoid the reseeking logic for max_iterating skipping Test Plan: enable the two disabled tests and make sure they pass Reviewers: rven, IslamAbdelRahman, kradhakrishnan, tnovak, yhchiang Reviewed By: yhchiang Subscribers: leveldb, dhruba Differential Revision: two diffs related to DBIter::FindPrevUserKey() Summary: This diff reverts the following two previous diffs related to DBIter::FindPrevUserKey(), which makes db_stress unstable. We should bake a better fix for this. * ""Fix a comparison in DBIter::FindPrevUserKey()"" ec70fea4c4025351190eba7a02bd09bb5f083790. * ""Fixed endless loop in DBIter::FindPrevUserKey()"" acee2b08a2d37154b8f9e2dc74b1966202c15ec5. Test Plan: db_stress Reviewers: anthony, igor, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: a comparison in DBIter::FindPrevUserKey() Summary: When seek target is a merge key (`kTypeMerge`), `DBIter::FindNextUserEntry()` advances the underlying iterator _past_ the current key (`saved_key_`); see `MergeValuesNewToOld()`. However, `FindPrevUserKey()` assumes that `iter_` points to an entry with the same user key as `saved_key_`. As a result, `it->Seek(key) && it->Prev()` can cause the iterator to be positioned at the _next_, instead of the previous, entry (new test, written by reproduces the bug). This diff changes `FindPrevUserKey()` to also skip keys that are _greater_ than `saved_key_`. Test Plan: db_test Reviewers: igor, sdong Reviewed By: sdong Subscribers: leveldb, dhruba, lovro Differential Revision: SeekToLast with upper bound Summary: RocksDBs Iterator.SeekToLast should seek to the last key before iterate_upper_bound if presents Test Plan: ./db_iter_test run successfully with the new testcase Reviewers: rven, yhchiang, igor, anthony, kradhakrishnan, sdong Reviewed By: sdong Subscribers: leveldb, dhruba Differential Revision:"
,,0.1567,rocksdb,"Add RocksDb/GeoDb Iterator interface Summary: This diff is a first step towards an iterator based interface for the SearchRadial method which replaces a vector of GeoObjects with an iterator for GeoObjects. This diff works by just wrapping the iterator for the encapsulated vector of GeoObjects. A future diff could extend this approach by defining an interator in terms of the underlying iteration in SearchRadial which would then remove the need to have an in-memory representation for all the matching GeoObjects. Fixes T8421387 Test Plan: The existing tests have been modified to work with the new interface. Reviewers: IslamAbdelRahman, kradhakrishnan, dhruba, igor Reviewed By: igor Subscribers: igor, dhruba, leveldb Differential Revision:"
,,0.1971,rocksdb,"Reuse file iterators in tailing iterator when memtable is flushed Summary: Under a tailing workload, there were increased block cache misses when a memtable was flushed because we were rebuilding iterators in that case since the version set changed. This was exacerbated in the case of iterate_upper_bound, since file iterators which were over the iterate_upper_bound would have been deleted and are now brought back as part of the Rebuild, only to be deleted again. We now renew the iterators and only build iterators for files which are added and delete file iterators for files which are deleted. Refer to for previous version Test Plan: DBTestTailingIterator.TailingIteratorTrimSeekToNext Reviewers: anthony, IslamAbdelRahman, igor, tnovak, yhchiang, sdong Reviewed By: sdong Subscribers: yhchiang, march, dhruba, leveldb, lovro Differential Revision: assert in forward iterator Summary: It looks like in some cases an assert in SeekInternal failed when computing the hints for the next level because user_key was the same as the largest key and not strictly smaller. Relaxing the assert to expect smaller or equal keys. Test Plan: make clean all check Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: comparison in ForwardIterator when computing hint for GetNextLevelIndex() Summary: When computing the hint for GetNextLevelIndex(), ForwardIterator was doing a redundant comparison. This patch fixes the comparison (using as a reference) and moves it inside an assert because we expect `level_files[f_idx]` to contain the next key after Seek(), so user_key should always be smaller than the largest key. Test Plan: make clean all check Reviewers: rven, anthony, yhchiang, igor, sdong Reviewed By: sdong Subscribers: tnovak, sdong, dhruba, leveldb Differential Revision: a whitebox test for deleted file iterators. Summary: We have earlier added a feature to delete file iterators when the current key is over the iterate upper bound. We now add a whitebox test to check if the file iterators were actually deleted. Test Plan: Add check for a range which has deleted iterators. Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: file iterators for files which are above the iterate upper bound to Improve memory utilization Summary: This diff improves the memory utilization for tailing iterators RocksDB, by freeing file iterators which are over the upper bound. It is an updating on Siyings original diff for improving the memory usage for tailing iterators. The changes for the seek and next path are now complete and a test has been added to exercise these paths while deleting file iterators which are above the upper bound. Test Plan: db_tailing_iter_test.TailingIteratorTrimSeekToNext Reviewers: march, tnovak, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: tailing iterator show new entries in memtable. Summary: Reseek mutable_iter if it is invalid in Next and immutable_iter is invalid. Test Plan: DBTestTailingIterator.TailingIteratorSeekToNext Reviewers: tnovak, march, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision:"
,,0.2297,rocksdb,"Add a whitebox test for deleted file iterators. Summary: We have earlier added a feature to delete file iterators when the current key is over the iterate upper bound. We now add a whitebox test to check if the file iterators were actually deleted. Test Plan: Add check for a range which has deleted iterators. Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: file iterators for files which are above the iterate upper bound to Improve memory utilization Summary: This diff improves the memory utilization for tailing iterators RocksDB, by freeing file iterators which are over the upper bound. It is an updating on Siyings original diff for improving the memory usage for tailing iterators. The changes for the seek and next path are now complete and a test has been added to exercise these paths while deleting file iterators which are above the upper bound. Test Plan: db_tailing_iter_test.TailingIteratorTrimSeekToNext Reviewers: march, tnovak, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision:"
,,0.2498,rocksdb,"Reuse file iterators in tailing iterator when memtable is flushed Summary: Under a tailing workload, there were increased block cache misses when a memtable was flushed because we were rebuilding iterators in that case since the version set changed. This was exacerbated in the case of iterate_upper_bound, since file iterators which were over the iterate_upper_bound would have been deleted and are now brought back as part of the Rebuild, only to be deleted again. We now renew the iterators and only build iterators for files which are added and delete file iterators for files which are deleted. Refer to for previous version Test Plan: DBTestTailingIterator.TailingIteratorTrimSeekToNext Reviewers: anthony, IslamAbdelRahman, igor, tnovak, yhchiang, sdong Reviewed By: sdong Subscribers: yhchiang, march, dhruba, leveldb, lovro Differential Revision: case when forward iterator misses a new update Summary: This diff fixes a case when the forward iterator misses a new insert when the mutable iterator is not current. The test is also improved and the check for deleted iterators is made more informative. Test Plan: DBTailingIteratorTest.*Trim Reviewers: tnovak, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision: a perf regression in ForwardIterator Summary: I noticed that memtable iterator usually crosses the `iterate_upper_bound` threshold when tailing. Changes introduced in D43833 made `NeedToSeekImmutable` always return true in such case, even when `Seek()` only needs to rewind the memtable iterator. In a test I ran, this caused the ""tailing efficiency"" (ratio of calls to `Seek()` that only affect the memtable versus all seeks) to drop almost to zero. This diff attempts to fix the regression by using a different flag to indicate that `current_` is over the limit instead of resetting `valid_` in `UpdateCurrent()`. Test Plan: `DBTestTailingIterator.TailingIteratorUpperBound` Reviewers: sdong, rven Reviewed By: rven Subscribers: dhruba, march Differential Revision: not delete iterators for immutable memtables. Summary: The immutable memtable iterators are allocated from an arena and there is no benefit from deleting these. Also the immutable memtables themselves will continue to be in memory until the version set containing it is alive. We will not remove immutable memtable iterators over the upper bound. We now add immutable iterators to the test. Test Plan: db_tailing_iter_test.TailingIteratorTrimSeekToNext Reviewers: tnovak, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: a whitebox test for deleted file iterators. Summary: We have earlier added a feature to delete file iterators when the current key is over the iterate upper bound. We now add a whitebox test to check if the file iterators were actually deleted. Test Plan: Add check for a range which has deleted iterators. Reviewers: sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: use of deleted file iterators with incomplete iterators Summary: After deleting file iterators which are over the iterate upper bound, we also need to check for null pointers in ResetIncompletIterators. Test Plan: db_tailing_iter_test.TailingIteratorTrimSeekToNext Reviewers: tnovak, sdong Reviewed By: sdong Subscribers: dhruba, leveldb Differential Revision: file iterators for files which are above the iterate upper bound to Improve memory utilization Summary: This diff improves the memory utilization for tailing iterators RocksDB, by freeing file iterators which are over the upper bound. It is an updating on Siyings original diff for improving the memory usage for tailing iterators. The changes for the seek and next path are now complete and a test has been added to exercise these paths while deleting file iterators which are above the upper bound. Test Plan: db_tailing_iter_test.TailingIteratorTrimSeekToNext Reviewers: march, tnovak, sdong Reviewed By: sdong Subscribers: dhruba Differential Revision:"
,,0.0815,rocksdb,Make Status moveable Status is a class which is frequently returned by value from functions. Making it movable avoids 99% of the copies automatically on return by value./
,,0.071,rocksdb,Fix empty vector write in ForwardIterator/
,,0.0669,rocksdb,Histogram Concurrency Improvement and Time-Windowing Support/
,,0.0951,rocksdb,"Cache to have an option to fail Cache::Insert() when full Summary: Cache to have an option to fail Cache::Insert() when full. Update call sites to check status and handle error. I totally have no idea whats correct behavior of all the call sites when they encounter error. Please let me know if you see something wrong or more unit test is needed. Test Plan: make check see tests pass. Reviewers: anthony, yhchiang, andrewkr, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.109,rocksdb,"Cache to have an option to fail Cache::Insert() when full Summary: Cache to have an option to fail Cache::Insert() when full. Update call sites to check status and handle error. I totally have no idea whats correct behavior of all the call sites when they encounter error. Please let me know if you see something wrong or more unit test is needed. Test Plan: make check see tests pass. Reviewers: anthony, yhchiang, andrewkr, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.0924,rocksdb,"Cache to have an option to fail Cache::Insert() when full Summary: Cache to have an option to fail Cache::Insert() when full. Update call sites to check status and handle error. I totally have no idea whats correct behavior of all the call sites when they encounter error. Please let me know if you see something wrong or more unit test is needed. Test Plan: make check see tests pass. Reviewers: anthony, yhchiang, andrewkr, IslamAbdelRahman, kradhakrishnan, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.0769,rocksdb,"Introduce Iterator::GetProperty() and replace Iterator::IsKeyPinned() Summary: Add Iterator::GetProperty(), a way for users to communicate with iterator, and turn Iterator::IsKeyPinned() with it. As a follow-up, Ill ask a property as the version number attached to the iterator Test Plan: Rerun existing tests and add a negative test case. Reviewers: yhchiang, andrewkr, kradhakrishnan, anthony, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision:"
,,0.1,rocksdb,"Add Iterator Property rocksdb.iterator.version_number Summary: We want to provide a way to detect whether an iterator is stale and needs to be recreated. Add a iterator property to return version number. Test Plan: Add two unit tests for it. Reviewers: IslamAbdelRahman, yhchiang, anthony, kradhakrishnan, andrewkr Reviewed By: andrewkr Subscribers: leveldb, dhruba Differential Revision:"
,,0.1,rocksdb,"Add Iterator Property rocksdb.iterator.version_number Summary: We want to provide a way to detect whether an iterator is stale and needs to be recreated. Add a iterator property to return version number. Test Plan: Add two unit tests for it. Reviewers: IslamAbdelRahman, yhchiang, anthony, kradhakrishnan, andrewkr Reviewed By: andrewkr Subscribers: leveldb, dhruba Differential Revision:"
,,0.4476,rocksdb,"Fix Iterator::Prev memory pinning bug Summary: We should not use IterKey::SetKey with copy false except if we are pinning the iterator thru its life time, otherwise we may release the temporarily pinned blocks and in this case the IterKey will be pointing to freed memory Test Plan: added a new test Reviewers: sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: memcpy in Iterator::Prev() by pinning blocks for keys spanning multiple blocks Summary: This diff is stacked on top of this diff The current Iterator::Prev() implementation need to copy every value since the underlying Iterator may move after reading the value. This can be optimized by making sure that the block containing the value is pinned until the Iterator move. which will improve the throughput by up to 1.5X master ``` 1000000_Keys_100Byte.txt readreverse : 0.449 micros/op 2225887 ops/sec; 246.2 MB/s readreverse : 0.433 micros/op 2311508 ops/sec; 255.7 MB/s readreverse : 0.436 micros/op 2294335 ops/sec; 253.8 MB/s readreverse : 0.471 micros/op 2121295 ops/sec; 234.7 MB/s readreverse : 0.465 micros/op 2152227 ops/sec; 238.1 MB/s readreverse : 0.454 micros/op 2203011 ops/sec; 243.7 MB/s readreverse : 0.451 micros/op 2216095 ops/sec; 245.2 MB/s readreverse : 0.462 micros/op 2162447 ops/sec; 239.2 MB/s readreverse : 0.476 micros/op 2099151 ops/sec; 232.2 MB/s readreverse : 0.472 micros/op 2120710 ops/sec; 234.6 MB/s avg : 242.34 MB/s 1000000_Keys_1KB.txt readreverse : 1.013 micros/op 986793 ops/sec; 978.7 MB/s readreverse : 0.942 micros/op 1061136 ops/sec; 1052.5 MB/s readreverse : 0.951 micros/op 1051901 ops/sec; 1043.3 MB/s readreverse : 0.932 micros/op 1072894 ops/sec; 1064.1 MB/s readreverse : 1.024 micros/op 976720 ops/sec; 968.7 MB/s readreverse : 0.935 micros/op 1069169 ops/sec; 1060.4 MB/s readreverse : 1.012 micros/op 988132 ops/sec; 980.1 MB/s readreverse : 0.962 micros/op 1039579 ops/sec; 1031.1 MB/s readreverse : 0.991 micros/op 1008924 ops/sec; 1000.7 MB/s readreverse : 1.004 micros/op 996144 ops/sec; 988.0 MB/s avg : 1016.76 MB/s 1000000_Keys_10KB.txt readreverse : 4.167 micros/op 239952 ops/sec; 2346.9 MB/s readreverse : 4.070 micros/op 245713 ops/sec; 2403.3 MB/s readreverse : 4.572 micros/op 218733 ops/sec; 2139.4 MB/s readreverse : 4.497 micros/op 222388 ops/sec; 2175.2 MB/s readreverse : 4.203 micros/op 237920 ops/sec; 2327.1 MB/s readreverse : 4.206 micros/op 237756 ops/sec; 2325.5 MB/s readreverse : 4.181 micros/op 239149 ops/sec; 2339.1 MB/s readreverse : 4.157 micros/op 240552 ops/sec; 2352.8 MB/s readreverse : 4.187 micros/op 238848 ops/sec; 2336.1 MB/s readreverse : 4.106 micros/op 243575 ops/sec; 2382.4 MB/s avg : 2312.78 MB/s 100000_Keys_100KB.txt readreverse : 41.281 micros/op 24224 ops/sec; 2366.0 MB/s readreverse : 39.722 micros/op 25175 ops/sec; 2458.9 MB/s readreverse : 40.319 micros/op 24802 ops/sec; 2422.5 MB/s readreverse : 39.762 micros/op 25149 ops/sec; 2456.4 MB/s readreverse : 40.916 micros/op 24440 ops/sec; 2387.1 MB/s readreverse : 41.188 micros/op 24278 ops/sec; 2371.4 MB/s readreverse : 40.061 micros/op 24962 ops/sec; 2438.1 MB/s readreverse : 40.221 micros/op 24862 ops/sec; 2428.4 MB/s readreverse : 40.084 micros/op 24947 ops/sec; 2436.7 MB/s readreverse : 40.655 micros/op 24597 ops/sec; 2402.4 MB/s avg : 2416.79 MB/s 10000_Keys_1MB.txt readreverse : 298.038 micros/op 3355 ops/sec; 3355.3 MB/s readreverse : 335.001 micros/op 2985 ops/sec; 2985.1 MB/s readreverse : 286.956 micros/op 3484 ops/sec; 3484.9 MB/s readreverse : 329.954 micros/op 3030 ops/sec; 3030.8 MB/s readreverse : 306.428 micros/op 3263 ops/sec; 3263.5 MB/s readreverse : 330.749 micros/op 3023 ops/sec; 3023.5 MB/s readreverse : 328.903 micros/op 3040 ops/sec; 3040.5 MB/s readreverse : 324.853 micros/op 3078 ops/sec; 3078.4 MB/s readreverse : 320.488 micros/op 3120 ops/sec; 3120.3 MB/s readreverse : 320.536 micros/op 3119 ops/sec; 3119.8 MB/s avg : 3150.21 MB/s ``` After memcpy elimination ``` 1000000_Keys_100Byte.txt readreverse : 0.395 micros/op 2529890 ops/sec; 279.9 MB/s readreverse : 0.368 micros/op 2715922 ops/sec; 300.5 MB/s readreverse : 0.384 micros/op 2603929 ops/sec; 288.1 MB/s readreverse : 0.375 micros/op 2663286 ops/sec; 294.6 MB/s readreverse : 0.357 micros/op 2802180 ops/sec; 310.0 MB/s readreverse : 0.363 micros/op 2757684 ops/sec; 305.1 MB/s readreverse : 0.372 micros/op 2689603 ops/sec; 297.5 MB/s readreverse : 0.379 micros/op 2638599 ops/sec; 291.9 MB/s readreverse : 0.375 micros/op 2663803 ops/sec; 294.7 MB/s readreverse : 0.375 micros/op 2665579 ops/sec; 294.9 MB/s avg: 295.72 MB/s (1.22 X) 1000000_Keys_1KB.txt readreverse : 0.879 micros/op 1138112 ops/sec; 1128.8 MB/s readreverse : 0.842 micros/op 1187998 ops/sec; 1178.3 MB/s readreverse : 0.837 micros/op 1194915 ops/sec; 1185.1 MB/s readreverse : 0.845 micros/op 1182983 ops/sec; 1173.3 MB/s readreverse : 0.877 micros/op 1140308 ops/sec; 1131.0 MB/s readreverse : 0.849 micros/op 1177581 ops/sec; 1168.0 MB/s readreverse : 0.915 micros/op 1093284 ops/sec; 1084.3 MB/s readreverse : 0.863 micros/op 1159418 ops/sec; 1149.9 MB/s readreverse : 0.895 micros/op 1117670 ops/sec; 1108.5 MB/s readreverse : 0.852 micros/op 1174116 ops/sec; 1164.5 MB/s avg: 1147.17 MB/s (1.12 X) 1000000_Keys_10KB.txt readreverse : 3.870 micros/op 258386 ops/sec; 2527.2 MB/s readreverse : 3.568 micros/op 280296 ops/sec; 2741.5 MB/s readreverse : 4.005 micros/op 249694 ops/sec; 2442.2 MB/s readreverse : 3.550 micros/op 281719 ops/sec; 2755.5 MB/s readreverse : 3.562 micros/op 280758 ops/sec; 2746.1 MB/s readreverse : 3.507 micros/op 285125 ops/sec; 2788.8 MB/s readreverse : 3.463 micros/op 288739 ops/sec; 2824.1 MB/s readreverse : 3.428 micros/op 291734 ops/sec; 2853.4 MB/s readreverse : 3.553 micros/op 281491 ops/sec; 2753.2 MB/s readreverse : 3.535 micros/op 282885 ops/sec; 2766.9 MB/s avg : 2719.89 MB/s (1.17 X) 100000_Keys_100KB.txt readreverse : 22.815 micros/op 43830 ops/sec; 4281.0 MB/s readreverse : 29.957 micros/op 33381 ops/sec; 3260.4 MB/s readreverse : 25.334 micros/op 39473 ops/sec; 3855.4 MB/s readreverse : 23.037 micros/op 43409 ops/sec; 4239.8 MB/s readreverse : 27.810 micros/op 35958 ops/sec; 3512.1 MB/s readreverse : 30.327 micros/op 32973 ops/sec; 3220.6 MB/s readreverse : 29.704 micros/op 33665 ops/sec; 3288.2 MB/s readreverse : 29.423 micros/op 33987 ops/sec; 3319.6 MB/s readreverse : 23.334 micros/op 42856 ops/sec; 4185.9 MB/s readreverse : 29.969 micros/op 33368 ops/sec; 3259.1 MB/s avg : 3642.21 MB/s (1.5 X) 10000_Keys_1MB.txt readreverse : 244.748 micros/op 4085 ops/sec; 4085.9 MB/s readreverse : 230.208 micros/op 4343 ops/sec; 4344.0 MB/s readreverse : 235.655 micros/op 4243 ops/sec; 4243.6 MB/s readreverse : 235.730 micros/op 4242 ops/sec; 4242.2 MB/s readreverse : 237.346 micros/op 4213 ops/sec; 4213.3 MB/s readreverse : 227.306 micros/op 4399 ops/sec; 4399.4 MB/s readreverse : 194.957 micros/op 5129 ops/sec; 5129.4 MB/s readreverse : 238.359 micros/op 4195 ops/sec; 4195.4 MB/s readreverse : 221.588 micros/op 4512 ops/sec; 4513.0 MB/s readreverse : 235.911 micros/op 4238 ops/sec; 4239.0 MB/s avg : 4360.52 MB/s (1.38 X) ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: prefix_same_as_start to avoid iteration in FindNextUserEntryInternal. (#1102) This avoids excessive iteration in tombstone fields./Introduce PinnedIteratorsManager (Reduce PinData() overhead / Refactor PinData) Summary: While trying to reuse PinData() / ReleasePinnedData() .. to optimize away some memcpys I realized that there is a significant overhead for using PinData() / ReleasePinnedData if they were called many times. This diff refactor the pinning logic by introducing PinnedIteratorsManager a centralized component that will be created once and will be notified whenever we need to Pin an Iterator. This implementation have much less overhead than the original implementation Test Plan: make check COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, sdong, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: std::deque initialization while iterating over merge operands Summary: This patch is similar to D52563, When we iterate over a DB with merge operands we keep creating std::queue to store the operands, optimize this by reusing merge_operands_ data member Before the patch ``` ./db_bench DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] mergerandom : 3.757 micros/op 266141 ops/sec; 29.4 MB/s ( updates:10000) DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.413 micros/op 2423538 ops/sec; 268.1 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.451 micros/op 2219071 ops/sec; 245.5 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.420 micros/op 2382039 ops/sec; 263.5 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.408 micros/op 2452017 ops/sec; 271.3 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] mergerandom : 3.947 micros/op 253376 ops/sec; 28.0 MB/s ( updates:10000) DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.441 micros/op 2266473 ops/sec; 250.7 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.471 micros/op 2122033 ops/sec; 234.8 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.440 micros/op 2271407 ops/sec; 251.3 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.429 micros/op 2331471 ops/sec; 257.9 MB/s ``` with the patch ``` ./db_bench DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] mergerandom : 4.080 micros/op 245092 ops/sec; 27.1 MB/s ( updates:10000) DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.308 micros/op 3241843 ops/sec; 358.6 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.312 micros/op 3200408 ops/sec; 354.0 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.332 micros/op 3013962 ops/sec; 333.4 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.300 micros/op 3328017 ops/sec; 368.2 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] mergerandom : 3.973 micros/op 251705 ops/sec; 27.8 MB/s ( updates:10000) DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.320 micros/op 3123752 ops/sec; 345.6 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.335 micros/op 2986641 ops/sec; 330.4 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.339 micros/op 2950047 ops/sec; 326.4 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.319 micros/op 3131565 ops/sec; 346.4 MB/s ``` Test Plan: make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: Iterator Property rocksdb.iterator.version_number Summary: We want to provide a way to detect whether an iterator is stale and needs to be recreated. Add a iterator property to return version number. Test Plan: Add two unit tests for it. Reviewers: IslamAbdelRahman, yhchiang, anthony, kradhakrishnan, andrewkr Reviewed By: andrewkr Subscribers: leveldb, dhruba Differential Revision: Iterator::GetProperty() and replace Iterator::IsKeyPinned() Summary: Add Iterator::GetProperty(), a way for users to communicate with iterator, and turn Iterator::IsKeyPinned() with it. As a follow-up, Ill ask a property as the version number attached to the iterator Test Plan: Rerun existing tests and add a negative test case. Reviewers: yhchiang, andrewkr, kradhakrishnan, anthony, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: leveldb, dhruba Differential Revision:"
,,0.3976,rocksdb,"Eliminate std::deque initialization while iterating over merge operands Summary: This patch is similar to D52563, When we iterate over a DB with merge operands we keep creating std::queue to store the operands, optimize this by reusing merge_operands_ data member Before the patch ``` ./db_bench DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] mergerandom : 3.757 micros/op 266141 ops/sec; 29.4 MB/s ( updates:10000) DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.413 micros/op 2423538 ops/sec; 268.1 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.451 micros/op 2219071 ops/sec; 245.5 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.420 micros/op 2382039 ops/sec; 263.5 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.408 micros/op 2452017 ops/sec; 271.3 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] mergerandom : 3.947 micros/op 253376 ops/sec; 28.0 MB/s ( updates:10000) DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.441 micros/op 2266473 ops/sec; 250.7 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.471 micros/op 2122033 ops/sec; 234.8 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.440 micros/op 2271407 ops/sec; 251.3 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.429 micros/op 2331471 ops/sec; 257.9 MB/s ``` with the patch ``` ./db_bench DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] mergerandom : 4.080 micros/op 245092 ops/sec; 27.1 MB/s ( updates:10000) DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.308 micros/op 3241843 ops/sec; 358.6 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.312 micros/op 3200408 ops/sec; 354.0 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.332 micros/op 3013962 ops/sec; 333.4 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.300 micros/op 3328017 ops/sec; 368.2 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] mergerandom : 3.973 micros/op 251705 ops/sec; 27.8 MB/s ( updates:10000) DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.320 micros/op 3123752 ops/sec; 345.6 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.335 micros/op 2986641 ops/sec; 330.4 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.339 micros/op 2950047 ops/sec; 326.4 MB/s DB path: [/dev/shm/bench_merge_memcpy_on_the_fly/] readseq : 0.319 micros/op 3131565 ops/sec; 346.4 MB/s ``` Test Plan: make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
,,0.44299999999999995,rocksdb,"Introduce FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision:"
,,0.4328,rocksdb,"Introduce FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision:"
,,0.4458,rocksdb,"Introduce FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision:"
,,0.2346,rocksdb,"Optimize BlockIter::Prev() by caching decoded entries Summary: Right now the way we do BlockIter::Prev() is like this Go to the beginning of the restart interval Keep moving forward (and decoding keys using ParseNextKey()) until we reach the desired key This can be optimized by caching the decoded entries in the first pass and reusing them in consecutive BlockIter::Prev() calls Before caching ``` DEBUG_LEVEL=0 make db_bench && ./db_bench DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.413 micros/op 2423972 ops/sec; 268.2 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.414 micros/op 2413867 ops/sec; 267.0 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.410 micros/op 2440881 ops/sec; 270.0 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.414 micros/op 2417298 ops/sec; 267.4 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.413 micros/op 2421682 ops/sec; 267.9 MB/s ``` After caching ``` DEBUG_LEVEL=0 make db_bench && ./db_bench DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.324 micros/op 3088955 ops/sec; 341.7 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.335 micros/op 2980999 ops/sec; 329.8 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.341 micros/op 2929681 ops/sec; 324.1 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.344 micros/op 2908490 ops/sec; 321.8 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.338 micros/op 2958404 ops/sec; 327.3 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: andrewkr, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, yoshinorim Differential Revision: BlockBasedTableOptions.hash_index_allow_collision=false. Summary: Deprecate this one option and delete code and tests that are now superfluous. Test Plan: all tests pass Reviewers: igor, yhchiang, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: msalib, leveldb, andrewkr, dhruba Differential Revision:"
,,0.4863,rocksdb,"Introduce FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision: BlockIter::Prev() by caching decoded entries Summary: Right now the way we do BlockIter::Prev() is like this Go to the beginning of the restart interval Keep moving forward (and decoding keys using ParseNextKey()) until we reach the desired key This can be optimized by caching the decoded entries in the first pass and reusing them in consecutive BlockIter::Prev() calls Before caching ``` DEBUG_LEVEL=0 make db_bench && ./db_bench DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.413 micros/op 2423972 ops/sec; 268.2 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.414 micros/op 2413867 ops/sec; 267.0 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.410 micros/op 2440881 ops/sec; 270.0 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.414 micros/op 2417298 ops/sec; 267.4 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.413 micros/op 2421682 ops/sec; 267.9 MB/s ``` After caching ``` DEBUG_LEVEL=0 make db_bench && ./db_bench DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.324 micros/op 3088955 ops/sec; 341.7 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.335 micros/op 2980999 ops/sec; 329.8 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.341 micros/op 2929681 ops/sec; 324.1 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.344 micros/op 2908490 ops/sec; 321.8 MB/s DB path: [/dev/shm/bench_prev_opt/] readreverse : 0.338 micros/op 2958404 ops/sec; 327.3 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: andrewkr, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, yoshinorim Differential Revision:"
,,0.1376,rocksdb,"Reuse TimedFullMerge instead of FullMerge + instrumentation Summary: We have alot of code duplication whenever we call FullMerge we keep duplicating the instrumentation and statistics code This is a simple diff to refactor the code to use TimedFullMerge instead of FullMerge Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
,,0.3952,rocksdb,"Minor PinnedIteratorsManager Refactoring Summary: This diff include these simple change Rename ReleasePinnedIterators to ReleasePinnedData Rename PinIteratorIfNeeded to PinIterator Use std::vector directly in PinnedIteratorsManager instead of std::unique_ptr<std::vector> Generalize PinnedIteratorsManager by adding PinPtr which can pin any pointer Test Plan: existing tests Reviewers: sdong, yiwu, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba Differential Revision: FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision:"
,,0.4449,rocksdb,"Introduce FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision: TimedFullMerge instead of FullMerge + instrumentation Summary: We have alot of code duplication whenever we call FullMerge we keep duplicating the instrumentation and statistics code This is a simple diff to refactor the code to use TimedFullMerge instead of FullMerge Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
,,0.4468,rocksdb,"Introduce FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision:"
,,0.1274,rocksdb,"Reuse TimedFullMerge instead of FullMerge + instrumentation Summary: We have alot of code duplication whenever we call FullMerge we keep duplicating the instrumentation and statistics code This is a simple diff to refactor the code to use TimedFullMerge instead of FullMerge Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
,,0.34,rocksdb,"Change options memtable_prefix_bloom_huge_page_tlb_size memtable_huge_page_size and cover huge page to memtable too Summary: Extend the option memtable_prefix_bloom_huge_page_tlb_size from just putting memtable bloom filter to huge page to memtable itself too. Test Plan: Run all existing tests. Reviewers: IslamAbdelRahman, yhchiang, andrewkr Reviewed By: andrewkr Subscribers: leveldb, andrewkr, dhruba Differential Revision: FullMergeV2 (eliminate memcpy from merge operators) Summary: This diff update the code to pin the merge operator operands while the merge operation is done, so that we can eliminate the memcpy cost, to do that we need a new public API for FullMerge that replace the std::deque<std::string> with std::vector<Slice> This diff is stacked on top of D56493 and D56511 In this diff we Update FullMergeV2 arguments to be encapsulated in MergeOperationInput and MergeOperationOutput which will make it easier to add new arguments in the future Replace std::deque<std::string> with std::vector<Slice> to pass operands Replace MergeContext std::deque with std::vector (based on a simple benchmark I ran Allow FullMergeV2 output to be an existing operand ``` [Everything in Memtable | 10K operands | 10 KB each | 1 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 0.607 micros/op 1648235 ops/sec; 16121.2 MB/s readseq : 0.478 micros/op 2091546 ops/sec; 20457.2 MB/s readseq : 0.252 micros/op 3972081 ops/sec; 38850.5 MB/s readseq : 0.237 micros/op 4218328 ops/sec; 41259.0 MB/s readseq : 0.247 micros/op 4043927 ops/sec; 39553.2 MB/s [master] readseq : 3.935 micros/op 254140 ops/sec; 2485.7 MB/s readseq : 3.722 micros/op 268657 ops/sec; 2627.7 MB/s readseq : 3.149 micros/op 317605 ops/sec; 3106.5 MB/s readseq : 3.125 micros/op 320024 ops/sec; 3130.1 MB/s readseq : 4.075 micros/op 245374 ops/sec; 2400.0 MB/s ``` ``` [Everything in Memtable | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 3.472 micros/op 288018 ops/sec; 2817.1 MB/s readseq : 2.304 micros/op 434027 ops/sec; 4245.2 MB/s readseq : 1.163 micros/op 859845 ops/sec; 8410.0 MB/s readseq : 1.192 micros/op 838926 ops/sec; 8205.4 MB/s readseq : 1.250 micros/op 800000 ops/sec; 7824.7 MB/s [master] readseq : 24.025 micros/op 41623 ops/sec; 407.1 MB/s readseq : 18.489 micros/op 54086 ops/sec; 529.0 MB/s readseq : 18.693 micros/op 53495 ops/sec; 523.2 MB/s readseq : 23.621 micros/op 42335 ops/sec; 414.1 MB/s readseq : 18.775 micros/op 53262 ops/sec; 521.0 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 1 operand per key] [FullMergeV2] $ DEBUG_LEVEL=0 make db_bench && ./db_bench readseq : 14.741 micros/op 67837 ops/sec; 663.5 MB/s readseq : 1.029 micros/op 971446 ops/sec; 9501.6 MB/s readseq : 0.974 micros/op 1026229 ops/sec; 10037.4 MB/s readseq : 0.965 micros/op 1036080 ops/sec; 10133.8 MB/s readseq : 0.943 micros/op 1060657 ops/sec; 10374.2 MB/s [master] readseq : 16.735 micros/op 59755 ops/sec; 584.5 MB/s readseq : 3.029 micros/op 330151 ops/sec; 3229.2 MB/s readseq : 3.136 micros/op 318883 ops/sec; 3119.0 MB/s readseq : 3.065 micros/op 326245 ops/sec; 3191.0 MB/s readseq : 3.014 micros/op 331813 ops/sec; 3245.4 MB/s ``` ``` [Everything in Block cache | 10K operands | 10 KB each | 10 operand per key] DEBUG_LEVEL=0 make db_bench && ./db_bench [FullMergeV2] readseq : 24.325 micros/op 41109 ops/sec; 402.1 MB/s readseq : 1.470 micros/op 680272 ops/sec; 6653.7 MB/s readseq : 1.231 micros/op 812347 ops/sec; 7945.5 MB/s readseq : 1.091 micros/op 916590 ops/sec; 8965.1 MB/s readseq : 1.109 micros/op 901713 ops/sec; 8819.6 MB/s [master] readseq : 27.257 micros/op 36687 ops/sec; 358.8 MB/s readseq : 4.443 micros/op 225073 ops/sec; 2201.4 MB/s readseq : 5.830 micros/op 171526 ops/sec; 1677.7 MB/s readseq : 4.173 micros/op 239635 ops/sec; 2343.8 MB/s readseq : 4.150 micros/op 240963 ops/sec; 2356.8 MB/s ``` Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: yhchiang, andrewkr, sdong Reviewed By: sdong Subscribers: lovro, andrewkr, dhruba Differential Revision: TimedFullMerge instead of FullMerge + instrumentation Summary: We have alot of code duplication whenever we call FullMerge we keep duplicating the instrumentation and statistics code This is a simple diff to refactor the code to use TimedFullMerge instead of FullMerge Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision: memtable_prefix_bloom_bits_ratio and deprecate memtable_prefix_bloom_probes Summary: memtable_prefix_bloom_probes is not a critical option. Remove it to reduce number of options. Its easier for users to make mistakes with memtable_prefix_bloom_bits, turn it to memtable_prefix_bloom_bits_ratio Test Plan: Run all existing tests Reviewers: yhchiang, igor, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: gunnarku, yoshinorim, MarkCallaghan, leveldb, andrewkr, dhruba Differential Revision:"
,,0.1342,rocksdb,"Reuse TimedFullMerge instead of FullMerge + instrumentation Summary: We have alot of code duplication whenever we call FullMerge we keep duplicating the instrumentation and statistics code This is a simple diff to refactor the code to use TimedFullMerge instead of FullMerge Test Plan: COMPILE_WITH_ASAN=1 make check Reviewers: andrewkr, yhchiang, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba Differential Revision:"
,,0.1591,rocksdb,"Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.1421,rocksdb,"add seeforprev in history Summary: update new feature in history and avoid breaking mongorocks Test Plan: make check Reviewers: sdong, yiwu, andrewkr Reviewed By: andrewkr Subscribers: andrewkr, dhruba, leveldb Differential Revision: SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.1685,rocksdb,"Avoid calling GetDBOptions() inside GetFromBatchAndDB() Summary: MyRocks hit a regression, generated perf reports showing that the reason is the cost of calling `GetDBOptions()` inside `GetFromBatchAndDB()` This diff avoid calling `GetDBOptions` and use the `ImmutableDBOptions` instead Test Plan: make check Reviewers: sdong, yiwu Reviewed By: yiwu Subscribers: andrewkr, dhruba, mung Differential Revision: SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.2052,rocksdb,"new Prev() prefix support using SeekForPrev() Summary: 1) The previous solution for Prev() prefix support is not clean. Since I add api SeekForPrev(), now the Prev() can be symmetric to Next(). and we do not need SeekToLast() to be called in Prev() any more. Also, Next() will Seek(prefix_seek_key_) to solve the problem of possible inconsistency between db_iter and merge_iter when there is merge_operator. And prefix_seek_key is only refreshed when change direction to forward. 2) This diff also solves the bug of Iterator::SeekToLast() with iterate_upper_bound_ with prefix extractor. add test cases for the above two cases. There are some tests for the SeekToLast() in Prev(), I will clean them later. Test Plan: make all check Reviewers: IslamAbdelRahman, andrewkr, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: bug in merge_iterator when data race happens Summary: core dump when run `./db_stress Actually the relevant flag is `--threads`, data race when > 1 cause problem. It is possible that multiple threads read/write memtable simultaneously. After one thread calls Prev(), another thread may insert a new key just between the current key and the key next, which may cause the assert(current_ CurrentForward()) failure when the first thread calls Next() again if in prefix seek mode Test Plan: rerun db_stress with >1 thread / make all check Reviewers: sdong, andrewkr, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: andrewkr, dhruba, leveldb Differential Revision: Prev() in prefix seek mode Summary: As title, make sure Prev() works as expected with Next() when the current iter->key() in the range of the same prefix in prefix seek mode Test Plan: make all check (add prefix_test with PrefixSeekModePrev test case) Reviewers: andrewkr, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, andrewkr, dhruba, leveldb Differential Revision:"
,,0.1574,rocksdb,"Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.2415,rocksdb,"new Prev() prefix support using SeekForPrev() Summary: 1) The previous solution for Prev() prefix support is not clean. Since I add api SeekForPrev(), now the Prev() can be symmetric to Next(). and we do not need SeekToLast() to be called in Prev() any more. Also, Next() will Seek(prefix_seek_key_) to solve the problem of possible inconsistency between db_iter and merge_iter when there is merge_operator. And prefix_seek_key is only refreshed when change direction to forward. 2) This diff also solves the bug of Iterator::SeekToLast() with iterate_upper_bound_ with prefix extractor. add test cases for the above two cases. There are some tests for the SeekToLast() in Prev(), I will clean them later. Test Plan: make all check Reviewers: IslamAbdelRahman, andrewkr, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: Prev() in prefix seek mode Summary: As title, make sure Prev() works as expected with Next() when the current iter->key() in the range of the same prefix in prefix seek mode Test Plan: make all check (add prefix_test with PrefixSeekModePrev test case) Reviewers: andrewkr, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, andrewkr, dhruba, leveldb Differential Revision:"
,,0.1335,rocksdb,"support Prev() in prefix seek mode Summary: As title, make sure Prev() works as expected with Next() when the current iter->key() in the range of the same prefix in prefix seek mode Test Plan: make all check (add prefix_test with PrefixSeekModePrev test case) Reviewers: andrewkr, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, andrewkr, dhruba, leveldb Differential Revision:"
,,0.1608,rocksdb,"Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.2006,rocksdb,"new Prev() prefix support using SeekForPrev() Summary: 1) The previous solution for Prev() prefix support is not clean. Since I add api SeekForPrev(), now the Prev() can be symmetric to Next(). and we do not need SeekToLast() to be called in Prev() any more. Also, Next() will Seek(prefix_seek_key_) to solve the problem of possible inconsistency between db_iter and merge_iter when there is merge_operator. And prefix_seek_key is only refreshed when change direction to forward. 2) This diff also solves the bug of Iterator::SeekToLast() with iterate_upper_bound_ with prefix extractor. add test cases for the above two cases. There are some tests for the SeekToLast() in Prev(), I will clean them later. Test Plan: make all check Reviewers: IslamAbdelRahman, andrewkr, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: prefix_seek_mode to db_iter_test Summary: add prefix_seek_mode to db_iter_test to enable data race test for iterator when prefix_extractor nullptr Test Plan: make all check Reviewers: andrewkr, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.1524,rocksdb,"Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.23800000000000002,rocksdb,"new Prev() prefix support using SeekForPrev() Summary: 1) The previous solution for Prev() prefix support is not clean. Since I add api SeekForPrev(), now the Prev() can be symmetric to Next(). and we do not need SeekToLast() to be called in Prev() any more. Also, Next() will Seek(prefix_seek_key_) to solve the problem of possible inconsistency between db_iter and merge_iter when there is merge_operator. And prefix_seek_key is only refreshed when change direction to forward. 2) This diff also solves the bug of Iterator::SeekToLast() with iterate_upper_bound_ with prefix extractor. add test cases for the above two cases. There are some tests for the SeekToLast() in Prev(), I will clean them later. Test Plan: make all check Reviewers: IslamAbdelRahman, andrewkr, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: Prev() in prefix seek mode Summary: As title, make sure Prev() works as expected with Next() when the current iter->key() in the range of the same prefix in prefix seek mode Test Plan: make all check (add prefix_test with PrefixSeekModePrev test case) Reviewers: andrewkr, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, andrewkr, dhruba, leveldb Differential Revision:"
,,0.2021,rocksdb,"new Prev() prefix support using SeekForPrev() Summary: 1) The previous solution for Prev() prefix support is not clean. Since I add api SeekForPrev(), now the Prev() can be symmetric to Next(). and we do not need SeekToLast() to be called in Prev() any more. Also, Next() will Seek(prefix_seek_key_) to solve the problem of possible inconsistency between db_iter and merge_iter when there is merge_operator. And prefix_seek_key is only refreshed when change direction to forward. 2) This diff also solves the bug of Iterator::SeekToLast() with iterate_upper_bound_ with prefix extractor. add test cases for the above two cases. There are some tests for the SeekToLast() in Prev(), I will clean them later. Test Plan: make all check Reviewers: IslamAbdelRahman, andrewkr, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.1439,rocksdb,"Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.1507,rocksdb,"Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.2778,rocksdb,"DeleteRange user iterator support Summary: Note: reviewed in DBIter maintains a range tombstone accumulator. We dont cleanup obsolete tombstones yet, so if the user seeks back and forth, the same tombstones would be added to the accumulator multiple times. DBImpl::NewInternalIterator() (used to make DBIters underlying iterator) adds memtable/L0 range tombstones, L1+ range tombstones are added on-demand during NewSecondaryIterator() (see D62205) DBIter uses ShouldDelete() when advancing to check whether keys are covered by range tombstones Closes Differential Revision: D4131753 Pulled By: ajkr fbshipit-source-id: be86559/fix assertion failure in Prev() Summary: fix assertion failure in db_stress. It happens because of prefix seek key is larger than merge iterator key when they have the same user key Test Plan: ./db_stress Reviewers: sdong, andrewkr, yiwu, yhchiang Reviewed By: yhchiang Subscribers: andrewkr, dhruba, leveldb Differential Revision: Prev() prefix support using SeekForPrev() Summary: 1) The previous solution for Prev() prefix support is not clean. Since I add api SeekForPrev(), now the Prev() can be symmetric to Next(). and we do not need SeekToLast() to be called in Prev() any more. Also, Next() will Seek(prefix_seek_key_) to solve the problem of possible inconsistency between db_iter and merge_iter when there is merge_operator. And prefix_seek_key is only refreshed when change direction to forward. 2) This diff also solves the bug of Iterator::SeekToLast() with iterate_upper_bound_ with prefix extractor. add test cases for the above two cases. There are some tests for the SeekToLast() in Prev(), I will clean them later. Test Plan: make all check Reviewers: IslamAbdelRahman, andrewkr, yiwu, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: unused variable for PrevInterval() Summary: delete unused variable Test Plan: make check Reviewers: sdong, andrewkr, IslamAbdelRahman, tianx Reviewed By: tianx Subscribers: andrewkr, dhruba, leveldb Differential Revision: SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision: revert Prev() prefix support Summary: Temporarily revert commits for supporting prefix Prev() to unblock MyRocks and RocksDB release These are the commits reverted 6a14d55bd913490dbd61d682567e6e0625756c0d b18f9c9eace89d63f37432ce1a3dba48bddbcef0 db74b1a21905336e2c178ff1f2ffd12c7852b7b8 2482d5fb45d8f01b0b065d649d01f43dacad799c Test Plan: make check Reviewers: sdong, lightmark Reviewed By: lightmark Subscribers: andrewkr, dhruba, yoshinorim Differential Revision: Prev() in prefix seek mode Summary: As title, make sure Prev() works as expected with Next() when the current iter->key() in the range of the same prefix in prefix seek mode Test Plan: make all check (add prefix_test with PrefixSeekModePrev test case) Reviewers: andrewkr, sdong, IslamAbdelRahman Reviewed By: IslamAbdelRahman Subscribers: yoshinorim, andrewkr, dhruba, leveldb Differential Revision:"
,,0.1692,rocksdb,"Add SeekForPrev() to Iterator Summary: Add new Iterator API, `SeekForPrev`: find the last key that target key support prefix_extractor support prefix_same_as_start support upper_bound not supported in iterators without Prev() Also add tests in db_iter_test and db_iterator_test Pass all tests Cheers Test Plan: make all check Reviewers: andrewkr, yiwu, IslamAbdelRahman, sdong Reviewed By: sdong Subscribers: andrewkr, dhruba, leveldb Differential Revision:"
,,0.141,rocksdb,"Support for range skips in compaction filter Summary: This adds the ability for compaction filter to say ""drop this key-value, and also drop everything up to key x"". This will cause the compaction to seek input iterator to x, without reading the data. This can make compaction much faster when large consecutive chunks of data are filtered out. See the changes in include/rocksdb/compaction_filter.h for the new API. Along the way this diff also adds ability for compaction filter changing merge operands, similar to how it can change values; were not going to use this feature, it just seemed easier and cleaner to implement it than to document that its not implemented :) The diff is not as big as it may seem, about half of the lines are a test. Closes Differential Revision: D4252092 Pulled By: al13n321 fbshipit-source-id: 41e1e48/"
,,0.3094,rocksdb,"Decouple data iterator and range deletion iterator in TableCache Summary: Previously we used TableCache::NewIterator() for multiple purposes (data block iterator and range deletion iterator), and returned non-ok status in the data block iterator. In one case where the caller only used the range deletion block iterator ( we didnt check/free the data block iterator containing non-ok status, which caused a valgrind error. So, this diff decouples creation of data block and range deletion block iterators, and updates the callers accordingly. Both functions can return non-ok status in an InternalIterator. Since the non-ok status is returned in an iterator that the callers will definitely use, it should be more usable/less error-prone. Closes Differential Revision: D4181423 Pulled By: ajkr fbshipit-source-id: 835b8f5/"
,,0.3094,rocksdb,"Decouple data iterator and range deletion iterator in TableCache Summary: Previously we used TableCache::NewIterator() for multiple purposes (data block iterator and range deletion iterator), and returned non-ok status in the data block iterator. In one case where the caller only used the range deletion block iterator ( we didnt check/free the data block iterator containing non-ok status, which caused a valgrind error. So, this diff decouples creation of data block and range deletion block iterators, and updates the callers accordingly. Both functions can return non-ok status in an InternalIterator. Since the non-ok status is returned in an iterator that the callers will definitely use, it should be more usable/less error-prone. Closes Differential Revision: D4181423 Pulled By: ajkr fbshipit-source-id: 835b8f5/"
,,0.1821,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/Support for range skips in compaction filter Summary: This adds the ability for compaction filter to say ""drop this key-value, and also drop everything up to key x"". This will cause the compaction to seek input iterator to x, without reading the data. This can make compaction much faster when large consecutive chunks of data are filtered out. See the changes in include/rocksdb/compaction_filter.h for the new API. Along the way this diff also adds ability for compaction filter changing merge operands, similar to how it can change values; were not going to use this feature, it just seemed easier and cleaner to implement it than to document that its not implemented :) The diff is not as big as it may seem, about half of the lines are a test. Closes Differential Revision: D4252092 Pulled By: al13n321 fbshipit-source-id: 41e1e48/"
,,0.1667,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/Support for range skips in compaction filter Summary: This adds the ability for compaction filter to say ""drop this key-value, and also drop everything up to key x"". This will cause the compaction to seek input iterator to x, without reading the data. This can make compaction much faster when large consecutive chunks of data are filtered out. See the changes in include/rocksdb/compaction_filter.h for the new API. Along the way this diff also adds ability for compaction filter changing merge operands, similar to how it can change values; were not going to use this feature, it just seemed easier and cleaner to implement it than to document that its not implemented :) The diff is not as big as it may seem, about half of the lines are a test. Closes Differential Revision: D4252092 Pulled By: al13n321 fbshipit-source-id: 41e1e48/"
,,0.2212,rocksdb,"Windows thread Summary: introduce new methods into a public threadpool interface, allow submission of std::functions as they allow greater flexibility. add Joining methods to the implementation to join scheduled and submitted jobs with an option to cancel jobs that did not start executing. Remove ugly `#ifdefs` between pthread and std implementation, make it uniform. introduce pimpl for a drop in replacement of the implementation Introduce rocksdb::port::Thread typedef which is a replacement for std::thread. On Posix Thread defaults as before std::thread. Implement WindowsThread that allocates memory in a more controllable manner than windows std::thread with a replaceable implementation. should be no functionality changes. Closes Differential Revision: D4492902 Pulled By: siying fbshipit-source-id: c74cb11/Eliminate redundant cache lookup with range deletion Summary: When we introduced range deletion block, TableCache::Get() and TableCache::NewIterator() each did two table cache lookups, one for range deletion block iterator and another for getting the table reader to which the Get()/NewIterator() is delegated. This extra cache lookup was very CPU-intensive (about 10% overhead in a read-heavy benchmark). We can avoid it by reusing the Cache::Handle created for range deletion block iterator to get the file reader. Closes Differential Revision: D4201167 Pulled By: ajkr fbshipit-source-id: d33ffd8/Decouple data iterator and range deletion iterator in TableCache Summary: Previously we used TableCache::NewIterator() for multiple purposes (data block iterator and range deletion iterator), and returned non-ok status in the data block iterator. In one case where the caller only used the range deletion block iterator ( we didnt check/free the data block iterator containing non-ok status, which caused a valgrind error. So, this diff decouples creation of data block and range deletion block iterators, and updates the callers accordingly. Both functions can return non-ok status in an InternalIterator. Since the non-ok status is returned in an iterator that the callers will definitely use, it should be more usable/less error-prone. Closes Differential Revision: D4181423 Pulled By: ajkr fbshipit-source-id: 835b8f5/"
,,0.3146,rocksdb,"Range deletions unsupported in tailing iterator Summary: change the iterator status to NotSupported as soon as a range tombstone is encountered by a ForwardIterator. Closes Differential Revision: D4246294 Pulled By: ajkr fbshipit-source-id: aef9f49/Decouple data iterator and range deletion iterator in TableCache Summary: Previously we used TableCache::NewIterator() for multiple purposes (data block iterator and range deletion iterator), and returned non-ok status in the data block iterator. In one case where the caller only used the range deletion block iterator ( we didnt check/free the data block iterator containing non-ok status, which caused a valgrind error. So, this diff decouples creation of data block and range deletion block iterators, and updates the callers accordingly. Both functions can return non-ok status in an InternalIterator. Since the non-ok status is returned in an iterator that the callers will definitely use, it should be more usable/less error-prone. Closes Differential Revision: D4181423 Pulled By: ajkr fbshipit-source-id: 835b8f5/"
,,0.314,rocksdb,"Decouple data iterator and range deletion iterator in TableCache Summary: Previously we used TableCache::NewIterator() for multiple purposes (data block iterator and range deletion iterator), and returned non-ok status in the data block iterator. In one case where the caller only used the range deletion block iterator ( we didnt check/free the data block iterator containing non-ok status, which caused a valgrind error. So, this diff decouples creation of data block and range deletion block iterators, and updates the callers accordingly. Both functions can return non-ok status in an InternalIterator. Since the non-ok status is returned in an iterator that the callers will definitely use, it should be more usable/less error-prone. Closes Differential Revision: D4181423 Pulled By: ajkr fbshipit-source-id: 835b8f5/"
,,0.2182,rocksdb,"Fix range deletion covering key in same SST file Summary: AddTombstones() needs to be before t->Get(), oops :( Closes Differential Revision: D4241041 Pulled By: ajkr fbshipit-source-id: 781ceea/Eliminate redundant cache lookup with range deletion Summary: When we introduced range deletion block, TableCache::Get() and TableCache::NewIterator() each did two table cache lookups, one for range deletion block iterator and another for getting the table reader to which the Get()/NewIterator() is delegated. This extra cache lookup was very CPU-intensive (about 10% overhead in a read-heavy benchmark). We can avoid it by reusing the Cache::Handle created for range deletion block iterator to get the file reader. Closes Differential Revision: D4201167 Pulled By: ajkr fbshipit-source-id: d33ffd8/Range deletion microoptimizations Summary: Made RangeDelAggregators InternalKeyComparator member a reference-to-const so we dont need to copy-construct it. Also added InternalKeyComparator to ImmutableCFOptions so we dont need to construct one for each DBIter. Made MemTable::NewRangeTombstoneIterator and the table readers NewRangeTombstoneIterator() functions return nullptr instead of NewEmptyInternalIterator to avoid the allocation. Updated callers accordingly. Closes Differential Revision: D4208169 Pulled By: ajkr fbshipit-source-id: 2fd65cf/Lazily initialize RangeDelAggregators map and pinning manager Summary: Since a RangeDelAggregator is created for each read request, these heap-allocating member variables were consuming significant CPU (~3% total) which slowed down request throughput. The map and pinning manager are only necessary when range deletions exist, so we can defer their initialization until the first range deletion is encountered. Currently lazy initialization is done for reads only since reads pass us a single snapshot, which is easier to store on the stack for later insertion into the map than the vector passed to us by flush or compaction. Note the Arena member variable is still expensive, I will figure out what to do with it in a subsequent diff. It cannot be lazily initialized because we currently use this arena even to allocate empty iterators, which is necessary even when no range deletions exist. Closes Differential Revision: D4203488 Pulled By: ajkr fbshipit-source-id: 3b36279/refactor TableCache Get/NewIterator for single exit points Summary: these functions were too complicated to change with exit points everywhere, so refactored them. btw, please review urgently, this is a prereq to fix the 5.0 perf regression Closes Differential Revision: D4198972 Pulled By: ajkr fbshipit-source-id: 04ebfb7/Decouple data iterator and range deletion iterator in TableCache Summary: Previously we used TableCache::NewIterator() for multiple purposes (data block iterator and range deletion iterator), and returned non-ok status in the data block iterator. In one case where the caller only used the range deletion block iterator ( we didnt check/free the data block iterator containing non-ok status, which caused a valgrind error. So, this diff decouples creation of data block and range deletion block iterators, and updates the callers accordingly. Both functions can return non-ok status in an InternalIterator. Since the non-ok status is returned in an iterator that the callers will definitely use, it should be more usable/less error-prone. Closes Differential Revision: D4181423 Pulled By: ajkr fbshipit-source-id: 835b8f5/"
,,0.2297,rocksdb,"Remove Arena in RangeDelAggregator Summary: The Arena construction/destruction introduced significant overhead to read-heavy workload just by creating empty vectors for its blocks, so avoid it in RangeDelAggregator. Closes Differential Revision: D4207781 Pulled By: ajkr fbshipit-source-id: 9d1c130/Decouple data iterator and range deletion iterator in TableCache Summary: Previously we used TableCache::NewIterator() for multiple purposes (data block iterator and range deletion iterator), and returned non-ok status in the data block iterator. In one case where the caller only used the range deletion block iterator ( we didnt check/free the data block iterator containing non-ok status, which caused a valgrind error. So, this diff decouples creation of data block and range deletion block iterators, and updates the callers accordingly. Both functions can return non-ok status in an InternalIterator. Since the non-ok status is returned in an iterator that the callers will definitely use, it should be more usable/less error-prone. Closes Differential Revision: D4181423 Pulled By: ajkr fbshipit-source-id: 835b8f5/Consider subcompaction boundaries when updating file boundaries for range deletion Summary: Adjusted AddToBuilder() to take lower_bound and upper_bound, which serve two purposes: (1) only range deletions overlapping with the interval [lower_bound, upper_bound) will be added to the output file, and (2) the output files boundaries will not be extended before lower_bound or after upper_bound. Our computation of lower_bound/upper_bound consider both subcompaction boundaries and previous/next files within the subcompaction. Test cases are here (level subcompactions: and universal subcompactions: but cant be included in this diff as they depend on committing the API first. They fail before this change and pass after. Closes Reviewed By: yhchiang Differential Revision: D4171685 Pulled By: ajkr fbshipit-source-id: ee99db8/"
,,0.1163,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/"
,,0.2488,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/Maintain position in range deletions map Summary: When deletion-collapsing mode is enabled (i.e., for DBIter/CompactionIterator), we maintain position in the tombstone maps across calls to ShouldDelete(). Since iterators often access keys sequentially (or reverse-sequentially), scanning forward/backward from the last position can be faster than binary-searching the map for every key. When Next() is invoked on an iterator, we use kForwardTraversal to scan forwards, if needed, until arriving at the range deletion containing the next key. Similarly for Prev(), we use kBackwardTraversal to scan backwards in the range deletion map. When the iterator seeks, we use kBinarySearch for repositioning After tombstones are added or before the first ShouldDelete() invocation, the current position is set to invalid, which forces kBinarySearch to be used. Non-iterator users (i.e., Get()) use kFullScan, which has the same behavior as before---scan the whole map for every key passed to ShouldDelete(). Closes Differential Revision: D4350318 Pulled By: ajkr fbshipit-source-id: 5129b76/Fixed CompactionFilter::Decision::kRemoveAndSkipUntil Summary: Embarassingly enough, the first time I tried to use my new feature in logdevice it crashed with this assertion failure: db/pinned_iterators_manager.h:30: void rocksdb::PinnedIteratorsManager::StartPinning(): Assertion `pinning_enabled false failed The issue was that `pinned_iters_mgr_.StartPinning()` was called but `pinned_iters_mgr_.ReleasePinnedData()` wasnt. Closes Differential Revision: D4265622 Pulled By: al13n321 fbshipit-source-id: 747b10f/Support for range skips in compaction filter Summary: This adds the ability for compaction filter to say ""drop this key-value, and also drop everything up to key x"". This will cause the compaction to seek input iterator to x, without reading the data. This can make compaction much faster when large consecutive chunks of data are filtered out. See the changes in include/rocksdb/compaction_filter.h for the new API. Along the way this diff also adds ability for compaction filter changing merge operands, similar to how it can change values; were not going to use this feature, it just seemed easier and cleaner to implement it than to document that its not implemented :) The diff is not as big as it may seem, about half of the lines are a test. Closes Differential Revision: D4252092 Pulled By: al13n321 fbshipit-source-id: 41e1e48/"
,,0.2593,rocksdb,"Maintain position in range deletions map Summary: When deletion-collapsing mode is enabled (i.e., for DBIter/CompactionIterator), we maintain position in the tombstone maps across calls to ShouldDelete(). Since iterators often access keys sequentially (or reverse-sequentially), scanning forward/backward from the last position can be faster than binary-searching the map for every key. When Next() is invoked on an iterator, we use kForwardTraversal to scan forwards, if needed, until arriving at the range deletion containing the next key. Similarly for Prev(), we use kBackwardTraversal to scan backwards in the range deletion map. When the iterator seeks, we use kBinarySearch for repositioning After tombstones are added or before the first ShouldDelete() invocation, the current position is set to invalid, which forces kBinarySearch to be used. Non-iterator users (i.e., Get()) use kFullScan, which has the same behavior as before---scan the whole map for every key passed to ShouldDelete(). Closes Differential Revision: D4350318 Pulled By: ajkr fbshipit-source-id: 5129b76/Collapse range deletions Summary: Added a tombstone-collapsing mode to RangeDelAggregator, which eliminates overlap in the TombstoneMap. In this mode, we can check whether a tombstone covers a user key using upper_bound() (i.e., binary search). However, the tradeoff is the overhead to add tombstones is now higher, so at first Ive only enabled it for range scans (compaction/flush/user iterators), where we expect a high number of calls to ShouldDelete() for the same tombstones. Point queries like Get() will still use the linear scan approach. Also in this diff I changed RangeDelAggregators TombstoneMap to use multimap with user keys instead of map with internal keys. Callers sometimes provided ParsedInternalKey directly, from which it wouldve required string copying to derive an internal key Slice with which we could search the map. Closes Differential Revision: D4270397 Pulled By: ajkr fbshipit-source-id: 93092c7/Iterator should be in corrupted status if merge operator return false Summary: Iterator should be in corrupted status if merge operator return false. Also add test to make sure if max_successive_merges is hit during write, data will not be lost. Closes Differential Revision: D4322695 Pulled By: yiwu-arbug fbshipit-source-id: b327b05/Less linear search in DBIter::Seek() when keys are overwritten a lot Summary: In one deployment we saw high latencies (presumably from slow iterator operations) and a lot of CPU time reported by perf with this stack: ``` rocksdb::MergingIterator::Next rocksdb::DBIter::FindNextUserEntryInternal rocksdb::DBIter::Seek ``` I think whats happening is: 1. we create a snapshot iterator, 2. we do lots of Put()s for the same key x; this creates lots of entries in memtable, 3. we seek the iterator to a key slightly smaller than x, 4. the seek walks over lots of entries in memtable for key x, skipping them because of high sequence numbers. CC IslamAbdelRahman Closes Differential Revision: D4083879 Pulled By: IslamAbdelRahman fbshipit-source-id: a83ddae/Lazily initialize RangeDelAggregators map and pinning manager Summary: Since a RangeDelAggregator is created for each read request, these heap-allocating member variables were consuming significant CPU (~3% total) which slowed down request throughput. The map and pinning manager are only necessary when range deletions exist, so we can defer their initialization until the first range deletion is encountered. Currently lazy initialization is done for reads only since reads pass us a single snapshot, which is easier to store on the stack for later insertion into the map than the vector passed to us by flush or compaction. Note the Arena member variable is still expensive, I will figure out what to do with it in a subsequent diff. It cannot be lazily initialized because we currently use this arena even to allocate empty iterators, which is necessary even when no range deletions exist. Closes Differential Revision: D4203488 Pulled By: ajkr fbshipit-source-id: 3b36279/"
,,0.314,rocksdb,"Decouple data iterator and range deletion iterator in TableCache Summary: Previously we used TableCache::NewIterator() for multiple purposes (data block iterator and range deletion iterator), and returned non-ok status in the data block iterator. In one case where the caller only used the range deletion block iterator ( we didnt check/free the data block iterator containing non-ok status, which caused a valgrind error. So, this diff decouples creation of data block and range deletion block iterators, and updates the callers accordingly. Both functions can return non-ok status in an InternalIterator. Since the non-ok status is returned in an iterator that the callers will definitely use, it should be more usable/less error-prone. Closes Differential Revision: D4181423 Pulled By: ajkr fbshipit-source-id: 835b8f5/"
,,0.2371,rocksdb,"Less linear search in DBIter::Seek() when keys are overwritten a lot Summary: In one deployment we saw high latencies (presumably from slow iterator operations) and a lot of CPU time reported by perf with this stack: ``` rocksdb::MergingIterator::Next rocksdb::DBIter::FindNextUserEntryInternal rocksdb::DBIter::Seek ``` I think whats happening is: 1. we create a snapshot iterator, 2. we do lots of Put()s for the same key x; this creates lots of entries in memtable, 3. we seek the iterator to a key slightly smaller than x, 4. the seek walks over lots of entries in memtable for key x, skipping them because of high sequence numbers. CC IslamAbdelRahman Closes Differential Revision: D4083879 Pulled By: IslamAbdelRahman fbshipit-source-id: a83ddae/"
,,0.1193,rocksdb,"Abort compactions more reliably when closing DB Summary: DB shutdown aborts running compactions by setting an atomic shutting_down=true that CompactionJob periodically checks. Without this PR it checks it before processing every _output_ value. If compaction filter filters everything out, the compaction is uninterruptible. This PR adds checks for shutting_down on every _input_ value (in CompactionIterator and MergeHelper). Theres also some minor code cleanup along the way. Closes Differential Revision: D4306571 Pulled By: yiwu-arbug fbshipit-source-id: f050890/"
,,0.1484,rocksdb,"Iterator should be in corrupted status if merge operator return false Summary: Iterator should be in corrupted status if merge operator return false. Also add test to make sure if max_successive_merges is hit during write, data will not be lost. Closes Differential Revision: D4322695 Pulled By: yiwu-arbug fbshipit-source-id: b327b05/"
,,0.2356,rocksdb,"Maintain position in range deletions map Summary: When deletion-collapsing mode is enabled (i.e., for DBIter/CompactionIterator), we maintain position in the tombstone maps across calls to ShouldDelete(). Since iterators often access keys sequentially (or reverse-sequentially), scanning forward/backward from the last position can be faster than binary-searching the map for every key. When Next() is invoked on an iterator, we use kForwardTraversal to scan forwards, if needed, until arriving at the range deletion containing the next key. Similarly for Prev(), we use kBackwardTraversal to scan backwards in the range deletion map. When the iterator seeks, we use kBinarySearch for repositioning After tombstones are added or before the first ShouldDelete() invocation, the current position is set to invalid, which forces kBinarySearch to be used. Non-iterator users (i.e., Get()) use kFullScan, which has the same behavior as before---scan the whole map for every key passed to ShouldDelete(). Closes Differential Revision: D4350318 Pulled By: ajkr fbshipit-source-id: 5129b76/"
,,0.1294,rocksdb,"Reduce the number of params needed to construct DBIter Summary: DBIter, and in-turn NewDBIterator and NewArenaWrappedDBIterator, take a bunch of params. They can be reduced by passing in ReadOptions directly instead of passing in every new param separately. It also seems much cleaner as a bunch of the params towards the end seem to be optional. (Recently I introduced max_skippable_internal_keys, which added one more to the already huge count). Idea courtesy IslamAbdelRahman Closes Differential Revision: D4857128 Pulled By: sagar0 fbshipit-source-id: 7d239df094b94bd9ea79d145cdf825478ac037a8/update IterKey that can get user key and internal key explicitly Summary: to void future bug that caused by the mix of userkey/internalkey Closes Differential Revision: D4825889 Pulled By: lightmark fbshipit-source-id: 28411db/Option to fail a request as incomplete when skipping too many internal keys Summary: Operations like Seek/Next/Prev sometimes take too long to complete when there are many internal keys to be skipped. Adding an option, max_skippable_internal_keys which could be used to set a threshold for the maximum number of keys that can be skipped, will help to address these cases where it is much better to fail a request (as incomplete) than to wait for a considerable time for the request to complete. This feature to fail an iterator seek request as incomplete, is disabled by default when max_skippable_internal_keys 0. It is enabled only when max_skippable_internal_keys > 0. This feature is based on the discussion mentioned in the PR Closes Differential Revision: D4753223 Pulled By: sagar0 fbshipit-source-id: 1c973f7/Reset DBIter::saved_key_ with proper user key anywhere before pass to DBIter::FindNextUserEntry Summary: fix db_iter bug introduced by [facebook#1413]( Closes Differential Revision: D4672369 Pulled By: lightmark fbshipit-source-id: 6a22953/"
,,0.1468,rocksdb,"Add Iterator::Refresh() Summary: Add and implement Iterator::Refresh(). When this function is called, if the super version doesnt change, update the sequence number of the iterator to the latest one and invalidate the iterator. If the super version changed, recreated the whole iterator. This can help users reuse the iterator more easily. Closes Differential Revision: D5464500 Pulled By: siying fbshipit-source-id: f548bd35e85c1efca2ea69273802f6704eba6ba9/"
,,0.15,rocksdb,"Add Iterator::Refresh() Summary: Add and implement Iterator::Refresh(). When this function is called, if the super version doesnt change, update the sequence number of the iterator to the latest one and invalidate the iterator. If the super version changed, recreated the whole iterator. This can help users reuse the iterator more easily. Closes Differential Revision: D5464500 Pulled By: siying fbshipit-source-id: f548bd35e85c1efca2ea69273802f6704eba6ba9/"
,,0.1408,rocksdb,"Propagate fill_cache config to partitioned index iterator Summary: Currently the partitioned index iterator creates a new ReadOptions which ignores the fill_cache config set to ReadOptions passed by the user. The patch propagates fill_cache from the users ReadOptions to that of partition index iterator. Also it clarifies the contract of fill_cache that i) it does not apply to filters, ii) it still charges block cache for the size of the data block, it still pin the block if it is already in the block cache. Closes Differential Revision: D7678308 Pulled By: maysamyabandeh fbshipit-source-id: 53ed96424ae922e499e2d4e3580ddc3f0db893da/Customized BlockBasedTableIterator and LevelIterator Summary: Use a customzied BlockBasedTableIterator and LevelIterator to replace current implementations leveraging two-level-iterator. Hope the customized logic will make code easier to understand. As a side effect, BlockBasedTableIterator reduces the allocation for the data block iterator object, and avoid the virtual function call to it, because we can directly reference BlockIter, a final class. Similarly, LevelIterator reduces virtual function call to the dummy iterator iterating the file metadata. It also enabled further optimization. The upper bound check is also moved from index block to data block. This implementation fits this iterator better. After the change, forwared iterator is slightly optimized to ensure we trim those iterators. The two-level-iterator now is only used by partitioned index, so it is simplified. Closes Differential Revision: D6809041 Pulled By: siying fbshipit-source-id: 7da3b9b1d3c8e9d9405302c15920af1fcaf50ffa/"
,,0.1548,rocksdb,"Customized BlockBasedTableIterator and LevelIterator Summary: Use a customzied BlockBasedTableIterator and LevelIterator to replace current implementations leveraging two-level-iterator. Hope the customized logic will make code easier to understand. As a side effect, BlockBasedTableIterator reduces the allocation for the data block iterator object, and avoid the virtual function call to it, because we can directly reference BlockIter, a final class. Similarly, LevelIterator reduces virtual function call to the dummy iterator iterating the file metadata. It also enabled further optimization. The upper bound check is also moved from index block to data block. This implementation fits this iterator better. After the change, forwared iterator is slightly optimized to ensure we trim those iterators. The two-level-iterator now is only used by partitioned index, so it is simplified. Closes Differential Revision: D6809041 Pulled By: siying fbshipit-source-id: 7da3b9b1d3c8e9d9405302c15920af1fcaf50ffa/"
,,0.1238,rocksdb,"move static msgs out of Status class (#4144) Summary: The member msgs of class Status contains all types of status messages. When users dump a Status object, msgs will confuse users. So move it out of class Status by making it as file-local static variable. Closes . Pull Request resolved: Differential Revision: D8941419 Pulled By: sagar0 fbshipit-source-id: 56b0510258465ff26db15aa6b04e01532e053e3d/Return new operator for Status allocations for Windows (#4128) Summary: Windows requires new/delete for memory allocations to be overriden. Refactor to be less intrusive. Differential Revision: D8878047 Pulled By: siying fbshipit-source-id: 35f2b5fec2f88ea48c9be926539c6469060aab36/Remove bogus gcc-8.1 warning (#3870) Summary: Various rearrangements of the cch maths failed or replacing \0 with memset failed to convince the compiler it was nul terminated. So took the perverse option of changing strncpy to strcpy. Return null if memory couldnt be allocated. util/status.cc: In static member function static const char* rocksdb::Status::CopyState(const char*): util/status.cc:28:15: error: char* strncpy(char*, const char*, size_t) output truncated before terminating nul copying as many bytes from a string as its length [-Werror=stringop-truncation] std::strncpy(result, state, cch 1); ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~ util/status.cc:19:18: note: length computed here std::strlen(state) + 1; // +1 for the null terminator ~~~~~~~~~~~^~~~~~~ cc1plus: all warnings being treated as errors make: *** [Makefile:645: shared-objects/util/status.o] Error 1 closes Closes Differential Revision: D8594114 Pulled By: anand1976 fbshipit-source-id: ab20f3a456a711e4d29144ebe630e4fe3c99ec25/"
,,0.1041,rocksdb,"Remove bogus gcc-8.1 warning (#3870) Summary: Various rearrangements of the cch maths failed or replacing \0 with memset failed to convince the compiler it was nul terminated. So took the perverse option of changing strncpy to strcpy. Return null if memory couldnt be allocated. util/status.cc: In static member function static const char* rocksdb::Status::CopyState(const char*): util/status.cc:28:15: error: char* strncpy(char*, const char*, size_t) output truncated before terminating nul copying as many bytes from a string as its length [-Werror=stringop-truncation] std::strncpy(result, state, cch 1); ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~ util/status.cc:19:18: note: length computed here std::strlen(state) + 1; // +1 for the null terminator ~~~~~~~~~~~^~~~~~~ cc1plus: all warnings being treated as errors make: *** [Makefile:645: shared-objects/util/status.o] Error 1 closes Closes Differential Revision: D8594114 Pulled By: anand1976 fbshipit-source-id: ab20f3a456a711e4d29144ebe630e4fe3c99ec25/"
,,0.37200000000000005,rocksdb,"WriteUnPrepared Txn: Disable seek to snapshot optimization (#3955) Summary: This is implemented by extending ReadCallback with another function `MaxUnpreparedSequenceNumber` which returns the largest visible sequence number for the current transaction, if there is uncommitted data written to DB. Otherwise, it returns zero, indicating no uncommitted data. There are the places where reads had to be modified. Get and Seek/Next was just updated to seek to max(snapshot_seq, MaxUnpreparedSequenceNumber()) instead, and iterate until a key was visible. Prev did not need need updates since it did not use the Seek to sequence number optimization. Assuming that locks were held when writing unprepared keys, and ValidateSnapshot runs, there should only be committed keys and unprepared keys of the current transaction, all of which are visible. Prev will simply iterate to get the last visible key. Reseeking to skip keys optimization was also disabled for write unprepared, since its possible to hit the max_skip condition even while reseeking. There needs to be some way to resolve infinite looping in this case. Closes Differential Revision: D8286688 Pulled By: lth fbshipit-source-id: 25e42f47fdeb5f7accea0f4fd350ef35198caafe/"
,,0.6925,rocksdb,"Change and clarify the relationship between Valid(), status() and Seek*() for all iterators. Also fix some bugs Summary: Before this PR, Iterator/InternalIterator may simultaneously have non-ok status() and Valid() true. That state means that the last operation failed, but the iterator is nevertheless positioned on some unspecified record. Likely intended uses of that are: * If some sst files are corrupted, a normal iterator can be used to read the data from files that are not corrupted. * When using read_tier kBlockCacheTier, read the data thats in block cache, skipping over the data that is not. However, this behavior wasnt documented well (and until recently the wiki on github had misleading incorrect information). In the code theres a lot of confusion about the relationship between status() and Valid(), and about whether Seek()/SeekToLast()/etc reset the status or not. There were a number of bugs caused by this confusion, both inside rocksdb and in the code that uses rocksdb (including ours). This PR changes the convention to: * If status() is not ok, Valid() always returns false. * Any seek operation resets status. (Before the PR, it depended on iterator type and on particular error.) This does sacrifice the two use cases listed above, but siying said its ok. Overview of the changes: * A commit that adds missing status checks in MergingIterator. This fixes a bug that actually affects us, and we need it fixed. `DBIteratorTest.NonBlockingIterationBugRepro` explains the scenario. * Changes to lots of iterator types to make all of them conform to the new convention. Some bug fixes along the way. By far the biggest changes are in DBIter, which is a big messy piece of code; I tried to make it less big and messy but mostly failed. * A stress-test for DBIter, to gain some confidence that I didnt break it. It does a few million random operations on the iterator, while occasionally modifying the underlying data (like ForwardIterator does) and occasionally returning non-ok status from internal iterator. To find the iterator types that needed changes I searched for ""public .*Iterator"" in the code. Heres an overview of all 27 iterator types: Iterators that didnt need changes: * status() is always ok(), or Valid() is always false: MemTableIterator, ModelIter, TestIterator, KVIter (2 classes with this name anonymous namespaces), LoggingForwardVectorIterator, VectorIterator, MockTableIterator, EmptyIterator, EmptyInternalIterator. * Thin wrappers that always pass through Valid() and status(): ArenaWrappedDBIter, TtlIterator, InternalIteratorFromIterator. Iterators with changes (see inline comments for details): * DBIter an overhaul: It used to silently skip corrupted keys (`FindParseableKey()`), which seems dangerous. This PR makes it just stop immediately after encountering a corrupted key, just like it would for other kinds of corruption. Let me know if there was actually some deeper meaning in this behavior and I should put it back. It had a few code paths silently discarding subiterators status. The stress test caught a few. The backwards iteration code path was expecting the internal iterators set of keys to be immutable. Its probably always true in practice at the moment, since ForwardIterator doesnt support backwards iteration, but this PR fixes it anyway. See added DBIteratorTest.ReverseToForwardBug for an example. Some parts of backwards iteration code path even did things like `assert(iter_->Valid())` after a seek, which is never a safe assumption. It used to not reset status on seek for some types of errors. Some simplifications and better comments. Some things got more complicated from the added error handling. Im open to ideas for how to make it nicer. * MergingIterator check status after every operation on every subiterator, and in some places assert that valid subiterators have ok status. * ForwardIterator changed to the new convention, also slightly simplified. * ForwardLevelIterator fixed some bugs and simplified. * LevelIterator simplified. * TwoLevelIterator changed to the new convention. Also fixed a bug that would make SeekForPrev() sometimes silently ignore errors from first_level_iter_. * BlockBasedTableIterator minor changes. * BlockIter replaced `SetStatus()` with `Invalidate()` to make sure non-ok BlockIter is always invalid. * PlainTableIterator some seeks used to not reset status. * CuckooTableIterator tiny code cleanup. * ManagedIterator fixed some bugs. * BaseDeltaIterator changed to the new convention and fixed a bug. * BlobDBIterator seeks used to not reset status. * KeyConvertingIterator some small change. Closes Differential Revision: D7888019 Pulled By: al13n321 fbshipit-source-id: 4aaf6d3421c545d16722a815b2fa2e7912bc851d/"
,,0.4659,rocksdb,"Index value delta encoding (#3983) Summary: Given that index value is a BlockHandle, which is basically an size> pair we can apply delta encoding on the values. The first value at each index restart interval encoded the full BlockHandle but the rest encode only the size. Refer to IndexBlockIter::DecodeCurrentValue for the detail of the encoding. This reduces the index size which helps using the block cache more efficiently. The feature is enabled with using format_version 4. The feature comes with a bit of cpu overhead which should be paid back by the higher cache hits due to smaller index block size. Results with sysbench read-only using 4k blocks and using 16 index restart interval: Format 2: 19585 rocksdb read-only range=100 Format 3: 19569 rocksdb read-only range=100 Format 4: 19352 rocksdb read-only range=100 Pull Request resolved: Differential Revision: D8361343 Pulled By: maysamyabandeh fbshipit-source-id: f882ee082322acac32b0072e2bdbb0b5f854e651/Refactor IndexBlockIter (#4141) Summary: Refactor IndexBlockIter to reduce conditional branches on key_includes_seq_. IndexBlockIter::Prev is also separated from DataBlockIter::Prev, not to cache the prev entries as they are of less importance when iterating over the index block. Pull Request resolved: Differential Revision: D8866437 Pulled By: maysamyabandeh fbshipit-source-id: fdac76880426fc2be7d3c6354c09ab98f6657d4b/Refactor BlockIter (#4121) Summary: BlockIter is getting crowded including details that specific only to either index or data blocks. The patch moves down such details to DataBlockIter and IndexBlockIter, both inheriting from BlockIter. Pull Request resolved: Differential Revision: D8816832 Pulled By: maysamyabandeh fbshipit-source-id: d492e74155c11d8a0c1c85cd7ee33d24c7456197/Exclude seq from index keys Summary: Index blocks have the same format as data blocks. The keys therefore similarly to the keys in the data blocks are internal keys, which means that in addition to the user key it also has 8 bytes that encodes sequence number and value type. This extra 8 bytes however is not necessary in index blocks since the index keys act as an separator between two data blocks. The only exception is when the last key of a block and the first key of the next block share the same user key, in which the sequence number is required to act as a separator. The patch excludes the sequence from index keys only if the above special case does not happen for any of the index keys. It then records that in the property block. The reader looks at the property block to see if it should expect sequence numbers in the keys of the index block.s Closes Differential Revision: D8118775 Pulled By: maysamyabandeh fbshipit-source-id: 915479f028b5799ca91671d67455ecdefbd873bd/class Block to store num_restarts_ Summary: Right now, every Block::NewIterator() reads num_restarts_ from the block, which is already read in Block::Block(). This sometimes cause a CPU cache miss. Although fetching this cacheline can usually benefit follow-up block restart offset reading, as they are close to each other, its almost free to get ride of this read by storing it in the Block class. Closes Differential Revision: D8052493 Pulled By: siying fbshipit-source-id: 9c72360f0c2d7329f3c198ce4eaedd2bc14b87c1/Change and clarify the relationship between Valid(), status() and Seek*() for all iterators. Also fix some bugs Summary: Before this PR, Iterator/InternalIterator may simultaneously have non-ok status() and Valid() true. That state means that the last operation failed, but the iterator is nevertheless positioned on some unspecified record. Likely intended uses of that are: * If some sst files are corrupted, a normal iterator can be used to read the data from files that are not corrupted. * When using read_tier kBlockCacheTier, read the data thats in block cache, skipping over the data that is not. However, this behavior wasnt documented well (and until recently the wiki on github had misleading incorrect information). In the code theres a lot of confusion about the relationship between status() and Valid(), and about whether Seek()/SeekToLast()/etc reset the status or not. There were a number of bugs caused by this confusion, both inside rocksdb and in the code that uses rocksdb (including ours). This PR changes the convention to: * If status() is not ok, Valid() always returns false. * Any seek operation resets status. (Before the PR, it depended on iterator type and on particular error.) This does sacrifice the two use cases listed above, but siying said its ok. Overview of the changes: * A commit that adds missing status checks in MergingIterator. This fixes a bug that actually affects us, and we need it fixed. `DBIteratorTest.NonBlockingIterationBugRepro` explains the scenario. * Changes to lots of iterator types to make all of them conform to the new convention. Some bug fixes along the way. By far the biggest changes are in DBIter, which is a big messy piece of code; I tried to make it less big and messy but mostly failed. * A stress-test for DBIter, to gain some confidence that I didnt break it. It does a few million random operations on the iterator, while occasionally modifying the underlying data (like ForwardIterator does) and occasionally returning non-ok status from internal iterator. To find the iterator types that needed changes I searched for ""public .*Iterator"" in the code. Heres an overview of all 27 iterator types: Iterators that didnt need changes: * status() is always ok(), or Valid() is always false: MemTableIterator, ModelIter, TestIterator, KVIter (2 classes with this name anonymous namespaces), LoggingForwardVectorIterator, VectorIterator, MockTableIterator, EmptyIterator, EmptyInternalIterator. * Thin wrappers that always pass through Valid() and status(): ArenaWrappedDBIter, TtlIterator, InternalIteratorFromIterator. Iterators with changes (see inline comments for details): * DBIter an overhaul: It used to silently skip corrupted keys (`FindParseableKey()`), which seems dangerous. This PR makes it just stop immediately after encountering a corrupted key, just like it would for other kinds of corruption. Let me know if there was actually some deeper meaning in this behavior and I should put it back. It had a few code paths silently discarding subiterators status. The stress test caught a few. The backwards iteration code path was expecting the internal iterators set of keys to be immutable. Its probably always true in practice at the moment, since ForwardIterator doesnt support backwards iteration, but this PR fixes it anyway. See added DBIteratorTest.ReverseToForwardBug for an example. Some parts of backwards iteration code path even did things like `assert(iter_->Valid())` after a seek, which is never a safe assumption. It used to not reset status on seek for some types of errors. Some simplifications and better comments. Some things got more complicated from the added error handling. Im open to ideas for how to make it nicer. * MergingIterator check status after every operation on every subiterator, and in some places assert that valid subiterators have ok status. * ForwardIterator changed to the new convention, also slightly simplified. * ForwardLevelIterator fixed some bugs and simplified. * LevelIterator simplified. * TwoLevelIterator changed to the new convention. Also fixed a bug that would make SeekForPrev() sometimes silently ignore errors from first_level_iter_. * BlockBasedTableIterator minor changes. * BlockIter replaced `SetStatus()` with `Invalidate()` to make sure non-ok BlockIter is always invalid. * PlainTableIterator some seeks used to not reset status. * CuckooTableIterator tiny code cleanup. * ManagedIterator fixed some bugs. * BaseDeltaIterator changed to the new convention and fixed a bug. * BlobDBIterator seeks used to not reset status. * KeyConvertingIterator some small change. Closes Differential Revision: D7888019 Pulled By: al13n321 fbshipit-source-id: 4aaf6d3421c545d16722a815b2fa2e7912bc851d/"
,,0.6762,rocksdb,"Fix regression bug of Prev() with upper bound (#3989) Summary: A recent change pushed down the upper bound checking to child iterators. However, this causes the logic of following sequence wrong: Seek(key); if (Valid()) SeekToLast(); Because Valid() may be caused by upper bounds, rather than the end of the iterator. In this case SeekToLast() points to totally wrong places. This can cause wrong results, infinite loops, or segfault in some cases. This sequence is called when changing direction from forward to backward. And this by itself also implicitly happen during reseeking optimization in Prev(). Fix this bug by using SeekForPrev() rather than this sequuence, as what is already done in prefix extrator case. Closes Differential Revision: D8385422 Pulled By: siying fbshipit-source-id: 429e869990cfd2dc389421e0836fc496bed67bb4/Change and clarify the relationship between Valid(), status() and Seek*() for all iterators. Also fix some bugs Summary: Before this PR, Iterator/InternalIterator may simultaneously have non-ok status() and Valid() true. That state means that the last operation failed, but the iterator is nevertheless positioned on some unspecified record. Likely intended uses of that are: * If some sst files are corrupted, a normal iterator can be used to read the data from files that are not corrupted. * When using read_tier kBlockCacheTier, read the data thats in block cache, skipping over the data that is not. However, this behavior wasnt documented well (and until recently the wiki on github had misleading incorrect information). In the code theres a lot of confusion about the relationship between status() and Valid(), and about whether Seek()/SeekToLast()/etc reset the status or not. There were a number of bugs caused by this confusion, both inside rocksdb and in the code that uses rocksdb (including ours). This PR changes the convention to: * If status() is not ok, Valid() always returns false. * Any seek operation resets status. (Before the PR, it depended on iterator type and on particular error.) This does sacrifice the two use cases listed above, but siying said its ok. Overview of the changes: * A commit that adds missing status checks in MergingIterator. This fixes a bug that actually affects us, and we need it fixed. `DBIteratorTest.NonBlockingIterationBugRepro` explains the scenario. * Changes to lots of iterator types to make all of them conform to the new convention. Some bug fixes along the way. By far the biggest changes are in DBIter, which is a big messy piece of code; I tried to make it less big and messy but mostly failed. * A stress-test for DBIter, to gain some confidence that I didnt break it. It does a few million random operations on the iterator, while occasionally modifying the underlying data (like ForwardIterator does) and occasionally returning non-ok status from internal iterator. To find the iterator types that needed changes I searched for ""public .*Iterator"" in the code. Heres an overview of all 27 iterator types: Iterators that didnt need changes: * status() is always ok(), or Valid() is always false: MemTableIterator, ModelIter, TestIterator, KVIter (2 classes with this name anonymous namespaces), LoggingForwardVectorIterator, VectorIterator, MockTableIterator, EmptyIterator, EmptyInternalIterator. * Thin wrappers that always pass through Valid() and status(): ArenaWrappedDBIter, TtlIterator, InternalIteratorFromIterator. Iterators with changes (see inline comments for details): * DBIter an overhaul: It used to silently skip corrupted keys (`FindParseableKey()`), which seems dangerous. This PR makes it just stop immediately after encountering a corrupted key, just like it would for other kinds of corruption. Let me know if there was actually some deeper meaning in this behavior and I should put it back. It had a few code paths silently discarding subiterators status. The stress test caught a few. The backwards iteration code path was expecting the internal iterators set of keys to be immutable. Its probably always true in practice at the moment, since ForwardIterator doesnt support backwards iteration, but this PR fixes it anyway. See added DBIteratorTest.ReverseToForwardBug for an example. Some parts of backwards iteration code path even did things like `assert(iter_->Valid())` after a seek, which is never a safe assumption. It used to not reset status on seek for some types of errors. Some simplifications and better comments. Some things got more complicated from the added error handling. Im open to ideas for how to make it nicer. * MergingIterator check status after every operation on every subiterator, and in some places assert that valid subiterators have ok status. * ForwardIterator changed to the new convention, also slightly simplified. * ForwardLevelIterator fixed some bugs and simplified. * LevelIterator simplified. * TwoLevelIterator changed to the new convention. Also fixed a bug that would make SeekForPrev() sometimes silently ignore errors from first_level_iter_. * BlockBasedTableIterator minor changes. * BlockIter replaced `SetStatus()` with `Invalidate()` to make sure non-ok BlockIter is always invalid. * PlainTableIterator some seeks used to not reset status. * CuckooTableIterator tiny code cleanup. * ManagedIterator fixed some bugs. * BaseDeltaIterator changed to the new convention and fixed a bug. * BlobDBIterator seeks used to not reset status. * KeyConvertingIterator some small change. Closes Differential Revision: D7888019 Pulled By: al13n321 fbshipit-source-id: 4aaf6d3421c545d16722a815b2fa2e7912bc851d/"
,,0.2544,rocksdb,"Index value delta encoding (#3983) Summary: Given that index value is a BlockHandle, which is basically an size> pair we can apply delta encoding on the values. The first value at each index restart interval encoded the full BlockHandle but the rest encode only the size. Refer to IndexBlockIter::DecodeCurrentValue for the detail of the encoding. This reduces the index size which helps using the block cache more efficiently. The feature is enabled with using format_version 4. The feature comes with a bit of cpu overhead which should be paid back by the higher cache hits due to smaller index block size. Results with sysbench read-only using 4k blocks and using 16 index restart interval: Format 2: 19585 rocksdb read-only range=100 Format 3: 19569 rocksdb read-only range=100 Format 4: 19352 rocksdb read-only range=100 Pull Request resolved: Differential Revision: D8361343 Pulled By: maysamyabandeh fbshipit-source-id: f882ee082322acac32b0072e2bdbb0b5f854e651/Remove random writes from SST file ingestion (#4172) Summary: RocksDB used to store global_seqno in external SST files written by SstFileWriter. During file ingestion, RocksDB uses `pwrite` to update the `global_seqno`. Since random write is not supported in some non-POSIX compliant file systems, external SST file ingestion is not supported on these file systems. To address this limitation, we no longer update `global_seqno` during file ingestion. Later RocksDB uses the MANIFEST and other information in table properties to deduce global seqno for externally-ingested SST files. Pull Request resolved: Differential Revision: D8961465 Pulled By: riversand963 fbshipit-source-id: 4382ec85270a96be5bc0cf33758ca2b167b05071/Fix bug when seeking backward against an out-of-bound iterator (#4187) Summary: 92ee3350e0ae02c0973af0fbd40fb67b0b958128 introduces an out-of-bound check in BlockBasedTableIterator::Valid(). However, this flag is not reset when re-seeking in backward direction. This caused the iterator to be invalide by mistake. Fix it by always resetting the out-of-bound flag in every seek. Pull Request resolved: Differential Revision: D8996600 Pulled By: siying fbshipit-source-id: b6235ea614f71381e50e7904c4fb036300604ac1/Avoid unnecessary big for-loop when reporting ticker stats stored in GetContext (#3490) Summary: Currently in `Version::Get` when reporting ticker stats stored in `GetContext`, there is a big for-loop through all `Ticker` which adds unnecessary cost to overall CPU usage. We can optimize by storing only ticker values that are used in `Get()` calls in a new struct `GetContextStats` since only a small fraction of all tickers are used in `Get()` calls. For comparison, with the new approach we only need to visit 17 values while old approach will require visiting 100+ `Ticker` Pull Request resolved: Differential Revision: D6969154 Pulled By: miasantreble fbshipit-source-id: fc27072965a3a94125a3e6883d20dafcf5b84029/BlockBasedTableReader: automatically adjust tail prefetch size (#4156) Summary: Right now we use one hard-coded prefetch size to prefetch data from the tail of the SST files. However, this may introduce a waste for some use cases, while not efficient for others. Introduce a way to adjust this prefetch size by tracking 32 recent times, and pick a value with which the wasted read is less than 10% Pull Request resolved: Differential Revision: D8916847 Pulled By: siying fbshipit-source-id: 8413f9eb3987e0033ed0bd910f83fc2eeaaf5758/Separate some IndexBlockIter logic from BlockIter (#4136) Summary: Some logic only related to IndexBlockIter is separated from BlockIter to IndexBlockIter. This is done by writing an exclusive Seek() and SeekForPrev() for DataBlockIter, and all metadata block iter and tombstone block iter now use data block iter. Dealing with the BinarySeek() sharing problem by passing in the comparator to use. Pull Request resolved: Reviewed By: maysamyabandeh Differential Revision: D8859673 Pulled By: siying fbshipit-source-id: 703e5e6824b82b7cbf4721f3594b94127797ca9e/Refactor BlockIter (#4121) Summary: BlockIter is getting crowded including details that specific only to either index or data blocks. The patch moves down such details to DataBlockIter and IndexBlockIter, both inheriting from BlockIter. Pull Request resolved: Differential Revision: D8816832 Pulled By: maysamyabandeh fbshipit-source-id: d492e74155c11d8a0c1c85cd7ee33d24c7456197/Charging block cache more accurately (#4073) Summary: Currently the block cache is charged only by the size of the raw data block and excludes the overhead of the c++ objects that contain the raw data block. The patch improves the accuracy of the charge by including the c++ object overhead into it. Closes Differential Revision: D8686552 Pulled By: maysamyabandeh fbshipit-source-id: 8472f7fc163c0644533bc6942e20cdd5725f520f/Pin mmap files in ReadOnlyDB (#4053) Summary: fixed a bug where PinnableSlice pin mmap files which could be deleted with background compaction. This is however a non-issue for ReadOnlyDB when there is no compaction running and max_open_files is This patch reenables the pinning feature for that case. Closes Differential Revision: D8662546 Pulled By: maysamyabandeh fbshipit-source-id: 402962602eb0f644e17822748332999c3af029fd/use user_key and iterate_upper_bound to determine compatibility of bloom filters (#3899) Summary: Previously in bloom filter will only be checked if `prefix_extractor` in the mutable_cf_options matches the one found in the SST file. This PR relaxes the requirement by checking if all keys in the range [user_key, iterate_upper_bound) all share the same prefix after transforming using the BF in the SST file. If so, the bloom filter is considered compatible and will continue to be looked at. Closes Differential Revision: D8157459 Pulled By: miasantreble fbshipit-source-id: 18d17cba56a1005162f8d5db7a27aba277089c41/Make BlockBasedTableIterator compaction-aware (#4048) Summary: Pass in `for_compaction` to `BlockBasedTableIterator` via `BlockBasedTableReader::NewIterator`. In 7103559f49b46b3287973045f741c0679e3e9e44, `for_compaction` was set in `BlockBasedTable::Rep` via `BlockBasedTable::SetupForCompaction`. In hindsight it was not the right decision; it also caused TSAN to complain. Closes Differential Revision: D8601056 Pulled By: sagar0 fbshipit-source-id: 30127e898c15c38c1080d57710b8c5a6d64a0ab3/Pin top-level index on partitioned index/filter blocks (#4037) Summary: Top-level index in partitioned index/filter blocks are small and could be pinned in memory. So far we use that by cache_index_and_filter_blocks to false. This however make it difficult to keep account of the total memory usage. This patch introduces pin_top_level_index_and_filter which in combination with cache_index_and_filter_blocks=true keeps the top-level index in cache and yet pinned them to avoid cache misses and also cache lookup overhead. Closes Differential Revision: D8596218 Pulled By: maysamyabandeh fbshipit-source-id: 3a5f7f9ca6b4b525b03ff6bd82354881ae974ad2/Improve direct IO range scan performance with readahead (#3884) Summary: This PR extends the improvements in to also work when using Direct IO. We see **4.5X performance improvement** in seekrandom benchmark doing long range scans, when using direct reads, on flash. **Description:** This change improves the performance of iterators doing long range scans (e.g. big/full index or table scans in MyRocks) by using readahead and prefetching additional data on each disk IO, and storing in a local buffer. This prefetching is automatically enabled on noticing more than 2 IOs for the same table file during iteration. The readahead size starts with 8KB and is exponentially increased on each additional sequential IO, up to a max of 256 KB. This helps in cutting down the number of IOs needed to complete the range scan. **Implementation Details:** Used `FilePrefetchBuffer` as the underlying buffer to store the readahead data. `FilePrefetchBuffer` can now take file_reader, readahead_size and max_readahead_size as input to the constructor, and automatically do readahead. `FilePrefetchBuffer::TryReadFromCache` can now call `FilePrefetchBuffer::Prefetch` if readahead is enabled. `AlignedBuffer` (which is the underlying store for `FilePrefetchBuffer`) now takes a few additional args in `AlignedBuffer::AllocateNewBuffer` to allow copying data from the old buffer. Made sure not to re-read partial chunks of data that were already available in the buffer, from device again. Fixed a couple of cases where `AlignedBuffer::cursize_` was not being properly kept up-to-date. **Constraints:** Similar to this gets currently enabled only when ReadOptions.readahead_size 0 (which is the default value). Since the prefetched data is stored in a temporary buffer allocated on heap, this could increase the memory usage if you have many iterators doing long range scans simultaneously. Enabled only for user reads, and disabled for compactions. Compaction reads are controlled by the options `use_direct_io_for_flush_and_compaction` and `compaction_readahead_size`, and the current feature takes precautions not to mess with them. **Benchmarks:** I used the same benchmark as used in Data fill: ``` TEST_TMPDIR=/data/users/$USER/benchmarks/iter ./db_bench ``` Do a long range scan: Seekrandom with large number of nexts ``` TEST_TMPDIR=/data/users/$USER/benchmarks/iter ./db_bench ``` ``` Before: seekrandom : 37939.906 micros/op 26 ops/sec; 29.2 MB/s (1636 of 1999 found) With this change: seekrandom : 8527.720 micros/op 117 ops/sec; 129.7 MB/s (6530 of 7999 found) ``` ~4.5X perf improvement. Taken on an average of 3 runs. Closes Differential Revision: D8082143 Pulled By: sagar0 fbshipit-source-id: 4d7a8561cbac03478663713df4d31ad2620253bb/Should only decode restart points for uncompressed blocks (#3996) Summary: The Block object assumes contents are uncompressed. Blocks constructor tries to read the number of restarts, but does not get an accurate number when its contents are compressed, which is causing issues like This PR address this issue by skipping reconstruction of restart points when blocks are known to be compressed. Somehow the restart points can be read directly when Snappy is used and some tests (for example expects blocks to be fully constructed even when Snappy compression is used, so here we keep the restart point logic for Snappy. Closes Differential Revision: D8416186 Pulled By: miasantreble fbshipit-source-id: 002c0b62b9e5d89fb7736563d354ce0023c8cb28/run make format for PR 3838 (#3954) Summary: PR made some changes that triggers lint warnings. Run `make format` to fix formatting as suggested by siying . Also piggyback two changes: 1) fix singleton destruction order for windows and posix env 2) fix two clang warnings Closes Differential Revision: D8272041 Pulled By: miasantreble fbshipit-source-id: 7c4fd12bd17aac13534520de0c733328aa3c6c9f/Extend some tests to format_version=3 (#3942) Summary: format_version=3 changes the format of SST index. This is however not being tested currently since tests only work with the default format_version which is currently 2. The patch extends the most related tests to also test for format_version=3. Closes Differential Revision: D8238413 Pulled By: maysamyabandeh fbshipit-source-id: 915725f55753dd8e9188e802bf471c23645ad035/Provide a way to override windows memory allocator with jemalloc for ZSTD Summary: Windows does not have LD_PRELOAD mechanism to override all memory allocation functions and ZSTD makes use of C-tuntime calloc. During flushes and compactions default system allocator fragments and the system slows down considerably. For builds with jemalloc we employ an advanced ZSTD context creation API that re-directs memory allocation to jemalloc. To reduce the cost of context creation on each block we cache ZSTD context within the block based table builder while a new SST file is being built, this will help all platform builds including those w/o jemalloc. This avoids system allocator fragmentation and improves the performance. The change does not address random reads and currently on Windows reads with ZSTD regress as compared with SNAPPY compression. Closes Differential Revision: D8229794 Pulled By: miasantreble fbshipit-source-id: 719b622ab7bf4109819bc44f45ec66f0dd3ee80d/Fix the bug of some test scenarios being put after kEnd Summary: DBTestBase::OptionConfig includes the scenarios that unit tests could iterate over them by calling ChangeOptions(). Some of the options have been mistakenly put after kEnd which makes them essentially invisible to ChangeOptions() caller. This patch fixes it except for kUniversalSubcompactions which is left as TODO since it would break some unit tests. Closes Differential Revision: D8230748 Pulled By: maysamyabandeh fbshipit-source-id: edddb8fffcd161af1809fef24798ce118f8593db/Check for rep_->table_properties being nullptr Summary: The very old sst formats do not have table_properties and rep_->table_properties is thus nullptr. The recent patch in does not check for nullptr and hence makes it backward incompatible. This patch adds the check. Closes Differential Revision: D8188638 Pulled By: maysamyabandeh fbshipit-source-id: b1d986665ecf0b4d1c442adfa8a193b97707d47b/Exclude seq from index keys Summary: Index blocks have the same format as data blocks. The keys therefore similarly to the keys in the data blocks are internal keys, which means that in addition to the user key it also has 8 bytes that encodes sequence number and value type. This extra 8 bytes however is not necessary in index blocks since the index keys act as an separator between two data blocks. The only exception is when the last key of a block and the first key of the next block share the same user key, in which the sequence number is required to act as a separator. The patch excludes the sequence from index keys only if the above special case does not happen for any of the index keys. It then records that in the property block. The reader looks at the property block to see if it should expect sequence numbers in the keys of the index block.s Closes Differential Revision: D8118775 Pulled By: maysamyabandeh fbshipit-source-id: 915479f028b5799ca91671d67455ecdefbd873bd/Fix a backward compatibility problem with table_properties being nullptr Summary: Currently when ldb built from master tries to open a DB from version 2.2, there will be a segfault because table_properties didnt exist back then. Closes Differential Revision: D8100914 Pulled By: miasantreble fbshipit-source-id: b255e8aedc54695432be2e704839c857dabdd65a/Move prefix_extractor to MutableCFOptions Summary: Currently it is not possible to change bloom filter config without restart the db, which is causing a lot of operational complexity for users. This PR aims to make it possible to dynamically change bloom filter config. Closes Differential Revision: D7253114 Pulled By: miasantreble fbshipit-source-id: f22595437d3e0b86c95918c484502de2ceca120c/Change and clarify the relationship between Valid(), status() and Seek*() for all iterators. Also fix some bugs Summary: Before this PR, Iterator/InternalIterator may simultaneously have non-ok status() and Valid() true. That state means that the last operation failed, but the iterator is nevertheless positioned on some unspecified record. Likely intended uses of that are: * If some sst files are corrupted, a normal iterator can be used to read the data from files that are not corrupted. * When using read_tier kBlockCacheTier, read the data thats in block cache, skipping over the data that is not. However, this behavior wasnt documented well (and until recently the wiki on github had misleading incorrect information). In the code theres a lot of confusion about the relationship between status() and Valid(), and about whether Seek()/SeekToLast()/etc reset the status or not. There were a number of bugs caused by this confusion, both inside rocksdb and in the code that uses rocksdb (including ours). This PR changes the convention to: * If status() is not ok, Valid() always returns false. * Any seek operation resets status. (Before the PR, it depended on iterator type and on particular error.) This does sacrifice the two use cases listed above, but siying said its ok. Overview of the changes: * A commit that adds missing status checks in MergingIterator. This fixes a bug that actually affects us, and we need it fixed. `DBIteratorTest.NonBlockingIterationBugRepro` explains the scenario. * Changes to lots of iterator types to make all of them conform to the new convention. Some bug fixes along the way. By far the biggest changes are in DBIter, which is a big messy piece of code; I tried to make it less big and messy but mostly failed. * A stress-test for DBIter, to gain some confidence that I didnt break it. It does a few million random operations on the iterator, while occasionally modifying the underlying data (like ForwardIterator does) and occasionally returning non-ok status from internal iterator. To find the iterator types that needed changes I searched for ""public .*Iterator"" in the code. Heres an overview of all 27 iterator types: Iterators that didnt need changes: * status() is always ok(), or Valid() is always false: MemTableIterator, ModelIter, TestIterator, KVIter (2 classes with this name anonymous namespaces), LoggingForwardVectorIterator, VectorIterator, MockTableIterator, EmptyIterator, EmptyInternalIterator. * Thin wrappers that always pass through Valid() and status(): ArenaWrappedDBIter, TtlIterator, InternalIteratorFromIterator. Iterators with changes (see inline comments for details): * DBIter an overhaul: It used to silently skip corrupted keys (`FindParseableKey()`), which seems dangerous. This PR makes it just stop immediately after encountering a corrupted key, just like it would for other kinds of corruption. Let me know if there was actually some deeper meaning in this behavior and I should put it back. It had a few code paths silently discarding subiterators status. The stress test caught a few. The backwards iteration code path was expecting the internal iterators set of keys to be immutable. Its probably always true in practice at the moment, since ForwardIterator doesnt support backwards iteration, but this PR fixes it anyway. See added DBIteratorTest.ReverseToForwardBug for an example. Some parts of backwards iteration code path even did things like `assert(iter_->Valid())` after a seek, which is never a safe assumption. It used to not reset status on seek for some types of errors. Some simplifications and better comments. Some things got more complicated from the added error handling. Im open to ideas for how to make it nicer. * MergingIterator check status after every operation on every subiterator, and in some places assert that valid subiterators have ok status. * ForwardIterator changed to the new convention, also slightly simplified. * ForwardLevelIterator fixed some bugs and simplified. * LevelIterator simplified. * TwoLevelIterator changed to the new convention. Also fixed a bug that would make SeekForPrev() sometimes silently ignore errors from first_level_iter_. * BlockBasedTableIterator minor changes. * BlockIter replaced `SetStatus()` with `Invalidate()` to make sure non-ok BlockIter is always invalid. * PlainTableIterator some seeks used to not reset status. * CuckooTableIterator tiny code cleanup. * ManagedIterator fixed some bugs. * BaseDeltaIterator changed to the new convention and fixed a bug. * BlobDBIterator seeks used to not reset status. * KeyConvertingIterator some small change. Closes Differential Revision: D7888019 Pulled By: al13n321 fbshipit-source-id: 4aaf6d3421c545d16722a815b2fa2e7912bc851d/"
,,0.3557,rocksdb,"WriteUnPrepared Txn: Disable seek to snapshot optimization (#3955) Summary: This is implemented by extending ReadCallback with another function `MaxUnpreparedSequenceNumber` which returns the largest visible sequence number for the current transaction, if there is uncommitted data written to DB. Otherwise, it returns zero, indicating no uncommitted data. There are the places where reads had to be modified. Get and Seek/Next was just updated to seek to max(snapshot_seq, MaxUnpreparedSequenceNumber()) instead, and iterate until a key was visible. Prev did not need need updates since it did not use the Seek to sequence number optimization. Assuming that locks were held when writing unprepared keys, and ValidateSnapshot runs, there should only be committed keys and unprepared keys of the current transaction, all of which are visible. Prev will simply iterate to get the last visible key. Reseeking to skip keys optimization was also disabled for write unprepared, since its possible to hit the max_skip condition even while reseeking. There needs to be some way to resolve infinite looping in this case. Closes Differential Revision: D8286688 Pulled By: lth fbshipit-source-id: 25e42f47fdeb5f7accea0f4fd350ef35198caafe/"
,,0.37200000000000005,rocksdb,"WriteUnPrepared Txn: Disable seek to snapshot optimization (#3955) Summary: This is implemented by extending ReadCallback with another function `MaxUnpreparedSequenceNumber` which returns the largest visible sequence number for the current transaction, if there is uncommitted data written to DB. Otherwise, it returns zero, indicating no uncommitted data. There are the places where reads had to be modified. Get and Seek/Next was just updated to seek to max(snapshot_seq, MaxUnpreparedSequenceNumber()) instead, and iterate until a key was visible. Prev did not need need updates since it did not use the Seek to sequence number optimization. Assuming that locks were held when writing unprepared keys, and ValidateSnapshot runs, there should only be committed keys and unprepared keys of the current transaction, all of which are visible. Prev will simply iterate to get the last visible key. Reseeking to skip keys optimization was also disabled for write unprepared, since its possible to hit the max_skip condition even while reseeking. There needs to be some way to resolve infinite looping in this case. Closes Differential Revision: D8286688 Pulled By: lth fbshipit-source-id: 25e42f47fdeb5f7accea0f4fd350ef35198caafe/"
,,0.5773,rocksdb,"Add tracing function of Seek() and SeekForPrev() to trace_replay (#4228) Summary: In the current trace_and replay, Get an WriteBatch are traced. This pull request track down the Seek() and SeekForPrev() to the trace file. timestamp, column_family_id> are write to the file. Replay of Iterator is not supported in the current implementation. Tested with trace_analyzer. Pull Request resolved: Differential Revision: D9201381 Pulled By: zhichao-cao fbshipit-source-id: 6f9cc9cb3c20260af741bee065ec35c5c96354ab/Range deletion performance improvements + cleanup (#4014) Summary: This fixes the same performance issue that fixes but with much more invasive cleanup. Im more excited about this PR because it paves the way for fixing another problem we uncovered at Cockroach where range deletion tombstones can cause massive compactions. For example, suppose L4 contains deletions from [a, c) and [x, z) and no other keys, and L5 is entirely empty. L6, however, is full of data. When compacting L4 L5, well end up with one file that spans, massively, from [a, z). When we go to compact L5 L6, well have to rewrite all of L6 If, instead of range deletions in L4, we had keys a, b, x, y, and z, RocksDB would have been smart enough to create two files in L5: one for a and b and another for x, y, and z. With the changes in this PR, it will be possible to adjust the compaction logic to split tombstones/start new output files when they would span too many files in the grandparent level. ajkr please take a look when you have a minute Pull Request resolved: Differential Revision: D8773253 Pulled By: ajkr fbshipit-source-id: ec62fa85f648fdebe1380b83ed997f9baec35677/WriteUnPrepared Txn: Disable seek to snapshot optimization (#3955) Summary: This is implemented by extending ReadCallback with another function `MaxUnpreparedSequenceNumber` which returns the largest visible sequence number for the current transaction, if there is uncommitted data written to DB. Otherwise, it returns zero, indicating no uncommitted data. There are the places where reads had to be modified. Get and Seek/Next was just updated to seek to max(snapshot_seq, MaxUnpreparedSequenceNumber()) instead, and iterate until a key was visible. Prev did not need need updates since it did not use the Seek to sequence number optimization. Assuming that locks were held when writing unprepared keys, and ValidateSnapshot runs, there should only be committed keys and unprepared keys of the current transaction, all of which are visible. Prev will simply iterate to get the last visible key. Reseeking to skip keys optimization was also disabled for write unprepared, since its possible to hit the max_skip condition even while reseeking. There needs to be some way to resolve infinite looping in this case. Closes Differential Revision: D8286688 Pulled By: lth fbshipit-source-id: 25e42f47fdeb5f7accea0f4fd350ef35198caafe/Move prefix_extractor to MutableCFOptions Summary: Currently it is not possible to change bloom filter config without restart the db, which is causing a lot of operational complexity for users. This PR aims to make it possible to dynamically change bloom filter config. Closes Differential Revision: D7253114 Pulled By: miasantreble fbshipit-source-id: f22595437d3e0b86c95918c484502de2ceca120c/Change and clarify the relationship between Valid(), status() and Seek*() for all iterators. Also fix some bugs Summary: Before this PR, Iterator/InternalIterator may simultaneously have non-ok status() and Valid() true. That state means that the last operation failed, but the iterator is nevertheless positioned on some unspecified record. Likely intended uses of that are: * If some sst files are corrupted, a normal iterator can be used to read the data from files that are not corrupted. * When using read_tier kBlockCacheTier, read the data thats in block cache, skipping over the data that is not. However, this behavior wasnt documented well (and until recently the wiki on github had misleading incorrect information). In the code theres a lot of confusion about the relationship between status() and Valid(), and about whether Seek()/SeekToLast()/etc reset the status or not. There were a number of bugs caused by this confusion, both inside rocksdb and in the code that uses rocksdb (including ours). This PR changes the convention to: * If status() is not ok, Valid() always returns false. * Any seek operation resets status. (Before the PR, it depended on iterator type and on particular error.) This does sacrifice the two use cases listed above, but siying said its ok. Overview of the changes: * A commit that adds missing status checks in MergingIterator. This fixes a bug that actually affects us, and we need it fixed. `DBIteratorTest.NonBlockingIterationBugRepro` explains the scenario. * Changes to lots of iterator types to make all of them conform to the new convention. Some bug fixes along the way. By far the biggest changes are in DBIter, which is a big messy piece of code; I tried to make it less big and messy but mostly failed. * A stress-test for DBIter, to gain some confidence that I didnt break it. It does a few million random operations on the iterator, while occasionally modifying the underlying data (like ForwardIterator does) and occasionally returning non-ok status from internal iterator. To find the iterator types that needed changes I searched for ""public .*Iterator"" in the code. Heres an overview of all 27 iterator types: Iterators that didnt need changes: * status() is always ok(), or Valid() is always false: MemTableIterator, ModelIter, TestIterator, KVIter (2 classes with this name anonymous namespaces), LoggingForwardVectorIterator, VectorIterator, MockTableIterator, EmptyIterator, EmptyInternalIterator. * Thin wrappers that always pass through Valid() and status(): ArenaWrappedDBIter, TtlIterator, InternalIteratorFromIterator. Iterators with changes (see inline comments for details): * DBIter an overhaul: It used to silently skip corrupted keys (`FindParseableKey()`), which seems dangerous. This PR makes it just stop immediately after encountering a corrupted key, just like it would for other kinds of corruption. Let me know if there was actually some deeper meaning in this behavior and I should put it back. It had a few code paths silently discarding subiterators status. The stress test caught a few. The backwards iteration code path was expecting the internal iterators set of keys to be immutable. Its probably always true in practice at the moment, since ForwardIterator doesnt support backwards iteration, but this PR fixes it anyway. See added DBIteratorTest.ReverseToForwardBug for an example. Some parts of backwards iteration code path even did things like `assert(iter_->Valid())` after a seek, which is never a safe assumption. It used to not reset status on seek for some types of errors. Some simplifications and better comments. Some things got more complicated from the added error handling. Im open to ideas for how to make it nicer. * MergingIterator check status after every operation on every subiterator, and in some places assert that valid subiterators have ok status. * ForwardIterator changed to the new convention, also slightly simplified. * ForwardLevelIterator fixed some bugs and simplified. * LevelIterator simplified. * TwoLevelIterator changed to the new convention. Also fixed a bug that would make SeekForPrev() sometimes silently ignore errors from first_level_iter_. * BlockBasedTableIterator minor changes. * BlockIter replaced `SetStatus()` with `Invalidate()` to make sure non-ok BlockIter is always invalid. * PlainTableIterator some seeks used to not reset status. * CuckooTableIterator tiny code cleanup. * ManagedIterator fixed some bugs. * BaseDeltaIterator changed to the new convention and fixed a bug. * BlobDBIterator seeks used to not reset status. * KeyConvertingIterator some small change. Closes Differential Revision: D7888019 Pulled By: al13n321 fbshipit-source-id: 4aaf6d3421c545d16722a815b2fa2e7912bc851d/"
,,0.3629,rocksdb,"WriteUnPrepared Txn: Disable seek to snapshot optimization (#3955) Summary: This is implemented by extending ReadCallback with another function `MaxUnpreparedSequenceNumber` which returns the largest visible sequence number for the current transaction, if there is uncommitted data written to DB. Otherwise, it returns zero, indicating no uncommitted data. There are the places where reads had to be modified. Get and Seek/Next was just updated to seek to max(snapshot_seq, MaxUnpreparedSequenceNumber()) instead, and iterate until a key was visible. Prev did not need need updates since it did not use the Seek to sequence number optimization. Assuming that locks were held when writing unprepared keys, and ValidateSnapshot runs, there should only be committed keys and unprepared keys of the current transaction, all of which are visible. Prev will simply iterate to get the last visible key. Reseeking to skip keys optimization was also disabled for write unprepared, since its possible to hit the max_skip condition even while reseeking. There needs to be some way to resolve infinite looping in this case. Closes Differential Revision: D8286688 Pulled By: lth fbshipit-source-id: 25e42f47fdeb5f7accea0f4fd350ef35198caafe/BlockBasedTableIterator to keep BlockIter after out of upper bound (#4004) Summary: b555ed30a4a93b80a3ac4781c6721ab988e03b5b makes the BlockBasedTableIterator to be invalidated if the current position if over the upper bound. However, this can bring performance regression to the case of multiple Seek()s hitting the same data block but all out of upper bound. For example, if an SST file has a data block containing following keys : {a, z} The user sets the upper bound to be ""x"", and it executed following queries: Seek(""b"") Seek(""c"") Seek(""d"") Before the upper bound optimization, these queries always come to this same current data block of the iterator, but now inside each Seek() the data block is read from the block cache but is returned again. To prevent this regression case, we keep the current data block iterator if it is upper bound. Closes Differential Revision: D8463192 Pulled By: siying fbshipit-source-id: 8710628b30acde7063a097c3184d6c4333a8ef81/"
,,0.1294,rocksdb,"Introduce CPU timers for iterator seek and next (#5076) Summary: Introduce CPU timers for iterator seek and next operations. Seek counter includes SeekToFirst, SeekToLast and SeekForPrev, w/ the caveat that SeekToLast timer doesnt include some post processing time if upper bound is defined. Pull Request resolved: Differential Revision: D14525218 Pulled By: fredfsh fbshipit-source-id: 03ba25df3b22b06c072621e4de0eacfa1445f0d9/"
,,0.1949,rocksdb,"Merging iterator to disble reseek optimization in prefix seek (#5815) Summary: We are seeing a bug of wrong results with merging iterators reseek avoidence feature and prefix extractor. Disable this optimization for now. Pull Request resolved: Test Plan: Validated the same MyRocks case was fixed; run all existing tests. Differential Revision: D17430776 fbshipit-source-id: aef664277ba0ab8a2e68331ff0db6ae682535371/merging_iterator.cc: Small refactoring (#5793) Summary: 1. Put the similar logic of adding valid iterator to heap and check invalid iterators status code to the same helper functions. 2. Because of 1, in the changing direction case, move around the places where we check status a little bit so that we can call the helper function there too. The logic would only divert in the case where the iterator is valid but status is not OK, which is not expected to happen. Add an assertion for that. 3. Put the logic of changing direction from forward to backward to a separate function so the unlikely code path is not in Prev(). Pull Request resolved: Test Plan: run all existing tests. Differential Revision: D17374397 fbshipit-source-id: d595ffcf156095c4bd0f5532bacba854482a2332/"
,,0.115,rocksdb,"Improve readability of DBIters two seek functions (#5794) Summary: Doing some code reordering in DBIter::Seek() and DBIter::SeekForPrev(). The logic largely remains the same, except slight difference when handling some stats when valid_ false, where they are not supposed to be used anyway. Also remove prefix_start_key_, which sometimes point a part of seek target, some times prefix_start_buf_, which is confusing. Pull Request resolved: Test Plan: Run all tests. Differential Revision: D17375257 fbshipit-source-id: 7339a23898cecd3a8475bf72340fcd6f82b933c5/Refactor ArenaWrappedDBIter into separate files (#5801) Summary: Move definition and implementation for ArenaWrappedDBIter into its own .h/.cc files. Also, change inlining of functions to better comply with the Google C++ style guide. Pull Request resolved: Test Plan: make check Differential Revision: D17371012 Pulled By: anand1976 fbshipit-source-id: c1361abc2851575111e357a63d88be3b3d6cb341/"
,,0.1139,rocksdb,"Improve readability of DBIters two seek functions (#5794) Summary: Doing some code reordering in DBIter::Seek() and DBIter::SeekForPrev(). The logic largely remains the same, except slight difference when handling some stats when valid_ false, where they are not supposed to be used anyway. Also remove prefix_start_key_, which sometimes point a part of seek target, some times prefix_start_buf_, which is confusing. Pull Request resolved: Test Plan: Run all tests. Differential Revision: D17375257 fbshipit-source-id: 7339a23898cecd3a8475bf72340fcd6f82b933c5/Refactor ArenaWrappedDBIter into separate files (#5801) Summary: Move definition and implementation for ArenaWrappedDBIter into its own .h/.cc files. Also, change inlining of functions to better comply with the Google C++ style guide. Pull Request resolved: Test Plan: make check Differential Revision: D17371012 Pulled By: anand1976 fbshipit-source-id: c1361abc2851575111e357a63d88be3b3d6cb341/"
,,0.1107,rocksdb,"db_stress sometimes generates keys close to SST file boundaries (#6037) Summary: Recently, a bug was found related to a seek key that is close to SST file boundary. However, it only occurs in a very small chance in db_stress, because the chance that a random key hits SST file boundaries is small. To boost the chance, with 1/16 chance, we pick keys that are close to SST file boundaries. Pull Request resolved: Test Plan: Did some manual printing out, and hack to cover the key generation logic to be correct. Differential Revision: D18598476 fbshipit-source-id: 13b76687d106c5be4e3e02a0c77fa5578105a071/db_stress to cover total order seek (#6039) Summary: Right now, in db_stress, as long as prefix extractor is defined, TestIterator always uses. There is value of cover total_order_seek true when prefix extractor is define. Add a small chance that this flag is turned on. Pull Request resolved: Test Plan: Run the test for a while. Differential Revision: D18539689 fbshipit-source-id: 568790dd7789c9986b83764b870df0423a122d99/db_stress to cover SeekForPrev() (#6022) Summary: Right now, db_stress doesnt cover SeekForPrev(). Add the coverage, which mirrors what we do for Seek(). Pull Request resolved: Test Plan: Run ""make crash_test"". Do some manual source code hack to simular iterator wrong results and see it caught. Differential Revision: D18442193 fbshipit-source-id: 879b79000d5e33c625c7e970636de191ccd7776c/"
,,0.0802,rocksdb,"Add operator[] to autovector::iterator_impl. (#6047) Summary: This is a required operator for random-access iterators, and an upcoming update for Visual Studio 2019 will change the C++ Standard Librarys heap algorithms to use this operator. Pull Request resolved: Differential Revision: D18618531 Pulled By: ltamasi fbshipit-source-id: 08d10bc85bf2dbc3f7ef0fa3c777e99f1e927ef5/"
,,0.0895,rocksdb,"Fix regression bug of hash index with iterator total order seek (#6328) Summary: introduces a bug for hash index in SST files. If a table reader is created when total order seek is used, prefix_extractor might be passed into table reader as null. While later when prefix seek is used, the same table reader used, hash index is checked but prefix extractor is null and the program would crash. Fix the issue by fixing in the way that prefix_extractor is preserved but ReadOptions.total_order_seek is checked Also, a null pointer check is added so that a bug like this wont cause segfault in the future. Pull Request resolved: Test Plan: Add a unit test that would fail without the fix. Stress test that reproduces the crash would pass. Differential Revision: D19586751 fbshipit-source-id: 8de77690167ddf5a77a01e167cf89430b1bfba42/Support options.max_open_files with periodic_compaction_seconds (#6090) Summary: options.periodic_compaction_seconds isnt supported when options.max_open_files Its because that the information of file creation time is stored in table properties and are not guaranteed to be loaded unless options.max_open_files Relax this constraint by storing the information in manifest. Pull Request resolved: Test Plan: Pass all existing tests; Modify an existing test to force the manifest value to take 0 to simulate backward compatibility case; manually open the DB generated with the change by release 4.2. Differential Revision: D18702268 fbshipit-source-id: 13e0bd94f546498a04f3dc5fc0d9dff5125ec9eb/"
